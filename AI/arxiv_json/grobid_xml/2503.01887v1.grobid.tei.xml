<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Continue Learning Meets Multimodal Large Language Model: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-27">27 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yukang</forename><surname>Huo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">When Continue Learning Meets Multimodal Large Language Model: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-27">27 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">27F64E7C9843232584BDA1A2B9C9764D</idno>
					<idno type="arXiv">arXiv:2503.01887v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal Large Language Model</term>
					<term>Continual Learning</term>
					<term>Benchmark Evaluations</term>
					<term>Model Innovation</term>
					<term>Catastrophic Forgetting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, significant progress has been made in the field of Artificial Intelligence with the development of Multimodal Large Language Models (MLLMs). However, adapting static, pre-trained MLLMs to dynamic data distributions and various tasks in an accurate and efficient manner remains a major challenge. When fine-tuning pre-trained MLLMs for specific tasks, a noticeable performance degradation often occurs in the model's prior knowledge domain -a phenomenon known as "Catastrophic Forgetting." While this issue has been extensively studied within the Continual Learning (CL) community, it presents new challenges in the context of MLLMs. As the first review paper in the field of continual learning for multimodal large models, this paper provides a comprehensive overview and detailed analysis of the 440 research papers on MLLM continual learning. Beyond introducing the fundamental concepts, the review is structured into four main sections. Firstly, it provides an overview of the latest research on MLLMs, including various model innovation strategies, benchmarks, and applications across diverse fields. Secondly, it presents a detailed categorization and overview of the latest research on continual learning, divided into three key areas: non-large language models(LLMs) unimoda continual learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimoda CL), and continual learning in large language models (CL in LLM). In-depth and extensive research in both the MLLM and CL domains has laid a solid foundation for research on MLLM continual learning. In the fourth section, we conduct an in-depth analysis of the current research status of MLLM continual learning, examining common benchmark evaluations, innovative improvements in model architectures and methods, and systematically summarizing and reviewing existing theoretical and empirical studies. This review aims to connect the basic setup, theoretical foundations, method innovations, and practical applications of continual learning in multimodal large models, shedding light on the research progress and challenges in the field. Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, aiming to inspire researchers in the field and promote the advancement of related technologies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Research on Multimodal Large Language Models (MLLMs) has rapidly advanced in recent years, becoming a significant direction in the field of artificial intelligence <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. By integrating multimodal information such as language, vision, and audio, these models demonstrate powerful cross-modal understanding and generation capabilities, providing innovative solutions to complex real-world problems <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. To enhance the performance of MLLMs, researchers have proposed various improvement strategies. Firstly, for cross-modal information fusion, more efficient architectural designs have been introduced <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, such as Transformerbased multimodal joint encoders and decoders, as well as lightweight cross-modal attention modules <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Secondly, pre-training techniques have been further developed, significantly improving the model's generalization ability and robustness through the introduction of multimodal contrastive learning, cross-modal consistency constraints, and self-supervised learning objectives <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>• Yukang Huo is with the School of College of Information and Electrical Engineering, China Agricultural University, Beijing 100193, China. Email: yukanghuo.ai@gmail.com • Hao Tang is with the School of Computer Science, Peking University, Beijing 100871, China. E-mail: haotang@pku.edu.cn</p><p>In addition, fine-tuning techniques have become increasingly refined <ref type="bibr" target="#b25">[26]</ref>, including efficient parameter adjustment methods (such as LoRA <ref type="bibr" target="#b26">[27]</ref>) and task-specific adaptation layer designs. These approaches enable MLLMs to adapt to diverse task scenarios with lower computational costs <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the performance evaluation of MLLMs is based on multimodal benchmarks that cover a wide range of task categories. For example, benchmarks in the vision and language domain include Visual Question Answering (VQA) <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>,</p><p>Image Captioning <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, and Visual Grounding <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>; in the audio and language domain, benchmarks include Audio-Text Alignment and Audio Generation <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>; there are also more complex cross-modal reasoning tasks, among others <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Moreover, MLLMs are also showing great potential in real-world applications. They are playing an increasingly important role in fields such as healthcare, education, robotics, and autonomous driving <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p><p>Continual learning aims to address the challenge of how models can effectively learn new tasks while retaining prior knowledge when faced with dynamically changing data streams, thus mitigating the problem of catastrophic forgetting <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>. In recent years, research in the field of continuous learning has been deepened, particularly with significant developments in its application across models of various scales and multimodal learning scenarios <ref type="bibr" target="#b57">[58]</ref>, [59], <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. In unimodal settings, the focus has mainly been on the design of algorithms to alleviate the problem of catastrophic forgetting, enabling models to maintain performance in previous tasks while incorporating new ones <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. Research in multimodal continual learning is more challenging than in unimodal settings, as models must simultaneously handle the characteristics of different modalities and their crossmodal interactions <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Researchers have primarily focused on techniques for cross-modal feature extraction, alignment, and processing, aiming to reduce cross-modal interference, enhance inter-modal consistency, and improve the model's generalization ability <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>. With the widespread application of large language models (LLMs) in natural language processing, research on their continual learning has become a new hotspot <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>. Due to the massive parameter scale of LLMs and their reliance on vast amounts of pretrained data, traditional continual learning strategies face challenges such as high computational costs and limited adaptability. To address these challenges, researchers have proposed several optimization directions: Parameter-Efficient Fine-Tuning (PEFT) methods (such as LoRA, Prefix Tuning, etc.) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, prompt learning methods, and so on. These approaches have shown tremendous potential in tasks such as open-domain question answering, continual dialogue systems, and cross-domain text generation <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>.</p><p>The rapid development of MLLMs and the in-depth integration of CL research have provided new perspectives for the exploration of the frontier in the field of artificial intelligence <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b85">[86]</ref>. A key research challenge in this domain is how to efficiently retain knowledge from previous tasks while learning new ones while maintaining cross-modal collaboration capabilities <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>. This has become a central research question in the field. Building on existing research, this paper provides a systematic review and summary of the research on continual learning in multimodal large models. It delves into the innovations in model structure and methods, including the design of various model frameworks, dynamic parameter adjustment mechanisms, and modules that support task adaptation <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>. These techniques not only significantly mitigate the problem of catastrophic forgetting, but also effectively enhance the task adaptability and generalization ability of MLLMs. In addition, this paper also introduces existing benchmarks for evaluating continual learning in multimodal large models, which provide important support for assessing model performance in continual learning tasks <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref>. Research on continual learning in multimodal large models not only provides new technological means for the dynamic adaptation of crossmodal tasks, but also offers innovative solutions for complex tasks in real-world domains such as intelligent education, healthcare, and robotic interaction <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>.</p><p>Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, covering aspects such as catastrophic forgetting, the improvement and standardization of evaluation benchmarks, and the enhancement of interpretability and transparency in multimodal large model continual learning. Through these discussions, the paper aims to provide valuable research insights for scholars in the field and promote the further development and application of continual learning technologies in multimodal large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MULTIMODAL LARGE LANGUAGE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary</head><p>In this section, we provide an overview of the latest research on MLLMs, including various model innovation strategies, a range of benchmarks, and the application of MLLMs in diverse domains. and functional modules to enhance model performance, generalization ability, and adaptability. This section reviews the main innovations, which focus on three core directions: framework design, method optimization, and functional module improvements. These innovations collectively drive the performance of MLLMs in complex multimodal tasks. This section will explore the latest research advancements in these areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Framework Innovation</head><p>Framework innovation is the foundation of MLLM development, aiming to achieve efficient fusion and processing of cross-modal information by improving the overall architectural design. In recent years, researchers have proposed many efficient framework designs. As shown in Table <ref type="table">1</ref>, researchers have proposed several efficient framework designs, such as MaVEn, MoVA, AutoM3L, DI-MML and et. These framework innovations provide more efficient tools and methods for MLLMs to handle multimodal tasks involving language, vision, and hearing. They enable MLLMs to achieve more precise reasoning and decision-making in the interaction of TABLE 2: Innovations in MLLM Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Starting point of the problem How to solve DenseFusion <ref type="bibr" target="#b122">[123]</ref> Enhancing the visual perception ability of MLLMs. DenseFusion proposes a multimodal perception fusion method that integrates visual experts. <ref type="bibr" target="#b123">[124]</ref> The complex training process hinders the broader application of MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E2E-MFD</head><p>E2E-MFD proposes a novel end-to-end algorithm for multimodal fusion detection.</p><p>NAM <ref type="bibr" target="#b124">[125]</ref> Neuron attribution in MLLMs has not been fully explored yet.</p><p>Method innovation is the core driving force behind the performance improvement of MLLMs. By designing more efficient training methods and optimization objectives, it helps models better adapt to dynamic task environments. As shown in Table <ref type="table">2</ref>, in recent years, researchers have proposed numerous novel and efficient methods to enhance the accuracy and robustness of MLLMs. These method research has explored cutting-edge techniques such as multimodal contrastive learning, self-supervised learning objectives, and multimodal alignment mechanisms. These methods not only enhance the model's generalization ability but also significantly improve the accuracy and robustness of crossmodal tasks. More details of the innovation of MLLMs methods are provided in Section 7.1 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Benchmarks</head><p>As MLLMs continue to achieve breakthroughs in multimodal tasks such as vision, language, and speech, comprehensive benchmarks have become crucial for systematically evaluating and comparing model performance. These benchmarks not only provide standardized datasets and tasks, but also  <ref type="bibr" target="#b145">[146]</ref> Addressing the catastrophic forgetting problem in graph neural networks.</p><p>NTE views a neural network as an ensemble of fixed experts.</p><p>IsCiL <ref type="bibr" target="#b146">[147]</ref> To address the issue of new data lacking labels due to annotation delays in continual learning.</p><p>tractor and a time-sensitive example selector. AutoActivator <ref type="bibr" target="#b161">[162]</ref> To address the issue of model forgetting old classes when continuously learning new classes.</p><p>AutoActivator dynamically adapts neural units to new tasks, enabling on-demand network expansion.</p><p>iNeMo <ref type="bibr" target="#b162">[163]</ref> To achieve efficient class-incremental learning. iNeMo proposes latent space initialization and position regularization.</p><p>TACO <ref type="bibr" target="#b163">[164]</ref> Offering a novel perspective for understanding and mitigating catastrophic forgetting.</p><p>TACO combines graph coarsening and continual learning to dynamically store information from previous tasks.</p><p>define metrics for assessing models' abilities in cross-modal reasoning, generation, classification, and other areas. They play a key role in guiding research directions, identifying model limitations, and advancing technological progress. More details of the overview of MLLM benchmarks are provided in Section 7.2 of the Appendix. Section 7.2 in the Appendix introduces some of the recent representative benchmarks, covering a wide range of scenarios from academic research to practical applications, reflecting the diverse needs and challenges in the multimodal field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Applications of MLLMs</head><p>Multimodal large models (MLLMs) have emerged as a significant direction in artificial intelligence research in recent years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b183">[184]</ref>, <ref type="bibr" target="#b184">[185]</ref>, <ref type="bibr" target="#b185">[186]</ref>. With the rapid development of technologies such as natural language processing, computer vision, and speech recognition, singlemodal intelligent systems can no longer meet the increasingly complex requirements of real-world applications <ref type="bibr" target="#b186">[187]</ref>, <ref type="bibr" target="#b187">[188]</ref>, <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b189">[190]</ref>. Multimodal learning, by integrating different types of data inputs, simulates the diversity and complexity of human information processing, offering more comprehensive and flexible intelligent services. At the same time, with the deepening of interdisciplinary research, MLLMs will not only play a role in traditional AI tasks but will also expand into more edge domains, driving artificial intelligence from closed systems to a more open and intelligent ecosystem. More details of the applications of MLLMs are provided in Section 7.3 of the Appendix. In summary, the application prospects of multimodal large models are vast. However, to fully unleash their potential, this requires the combined advancement of technological innovation and theoretical breakthroughs. In the future, with ongoing progress in algorithms, hardware, and cross-domain collaboration, it is expected that MLLMs will achieve more efficient and intelligent performance in a wider range of</p><p>TABLE 4: Innovations in Non-LLM Unimodal CL Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Starting point of the problem How to solve GACL <ref type="bibr" target="#b164">[165]</ref> Addressing the catastrophic forgetting problem of models in class-incremental learning.</p><p>GACL establishes the equivalence between incremental learning and joint training. <ref type="bibr" target="#b164">[165]</ref> Addressing the balance between new task training sensitivity and memory retention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-Flat</head><p>C-Flat optimizes the flatness of the loss landscape.</p><p>DSGD <ref type="bibr" target="#b165">[166]</ref> Addressing the practical deployment challenge. DSGD uses structural and semantic information for stable knowledge distillation. <ref type="bibr" target="#b166">[167]</ref> To improve continual learning performance. VQ-Prompt utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection. RanDumb <ref type="bibr" target="#b167">[168]</ref> Exploring whether the representations generated by continual learning algorithms are truly effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQ-Prompt</head><p>RanDumb uses random transformations and linear classifiers to address.</p><p>IWMS <ref type="bibr" target="#b168">[169]</ref> The label delay issue in online continual learning. IWMS prioritizes the memory of samples similar to new data.</p><p>PPE <ref type="bibr" target="#b169">[170]</ref> To address the catastrophic forgetting problem in non-sample online continual learning.</p><p>PPE learns class prototypes during the online learning phase.</p><p>GPCNS <ref type="bibr" target="#b170">[171]</ref> Improving the performance of continual learning. GPCNS enhances plasticity by utilizing gradient information from old tasks.</p><p>CILA <ref type="bibr" target="#b171">[172]</ref> Improving the performance of continual learning.</p><p>CILA proposes an adaptive distillation coefficient and theoretical performance guarantees.</p><p>POCL <ref type="bibr" target="#b172">[173]</ref> Existing methods fail to fully leverage the inter-task dependencies.</p><p>POCL models task relationships through Pareto optimization and dynamically adjusts weights.</p><p>Powder <ref type="bibr" target="#b173">[174]</ref> Addressing the cross-task and cross-client knowledge transfer in federated continual learning.</p><p>Powder enables prompt-based dual knowledge transfer. AdaPromptCL <ref type="bibr" target="#b174">[175]</ref> Addressing the challenge of task-specific semantic variations.</p><p>AdaPromptCL proposes dynamic semantic grouping and prompt adjustment.</p><p>LPR <ref type="bibr" target="#b174">[175]</ref> To reduce catastrophic forgetting and underfitting. LPR adjusts the optimization geometry to balance the learning of new and old data.</p><p>InfLoRA <ref type="bibr" target="#b175">[176]</ref> To address the issue of forgetting old tasks when adapting to new tasks.</p><p>InfLoRA injects parameter reparameterization into pre-trained weights. <ref type="bibr" target="#b176">[177]</ref> To alleviate the issue of catastrophic forgetting in online class-incremental learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F-OAL</head><p>F-OAL proposes a forward online analytical learning method.</p><p>PRL <ref type="bibr" target="#b177">[178]</ref> Improving performance in non-sample classincremental learning.</p><p>PRL aligns reserved space and latent space to adapt new class features to the reserved space.</p><p>CIL <ref type="bibr" target="#b178">[179]</ref> To address the issue of catastrophic forgetting. CIL proposes the CIL-balanced classification loss and distribution margin loss.</p><p>DSSP <ref type="bibr" target="#b179">[180]</ref> To eliminate the need for sample replay. DSSP leverages domain sharing and task-specific prompt learning.</p><p>MRFA <ref type="bibr" target="#b180">[181]</ref> To reduce catastrophic forgetting. MRFA optimizes the entire layer margin by enhancing the features of review samples.</p><p>DARE <ref type="bibr" target="#b181">[182]</ref> Improving the model's performance on old tasks. DARE reduces representation drift through a threestage training process.</p><p>EASE <ref type="bibr" target="#b182">[183]</ref> To reduce catastrophic forgetting. EASE constructs task-specific subspaces using lightweight adapters.</p><p>practical applications, further advancing the development of artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONTINUE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>Continual Learning (CL) has become a central focus in AI research due to the rapid growth of deep learning and LLMs <ref type="bibr" target="#b201">[202]</ref>, <ref type="bibr" target="#b202">[203]</ref>, <ref type="bibr" target="#b203">[204]</ref>, <ref type="bibr" target="#b204">[205]</ref>, <ref type="bibr" target="#b205">[206]</ref>, <ref type="bibr" target="#b206">[207]</ref>, <ref type="bibr" target="#b207">[208]</ref>, <ref type="bibr" target="#b208">[209]</ref>, <ref type="bibr" target="#b209">[210]</ref>.</p><p>The challenge is to enable models to retain and enhance learning capabilities when faced with continuously changing data and tasks. Traditional methods assume that models can learn all tasks at once and maintain a fixed knowledge base, but in reality, data and tasks evolve, often leading to "Catastrophic Forgetting" <ref type="bibr" target="#b210">[211]</ref>, <ref type="bibr" target="#b211">[212]</ref>, <ref type="bibr" target="#b212">[213]</ref>, <ref type="bibr" target="#b213">[214]</ref>, <ref type="bibr" target="#b214">[215]</ref>, <ref type="bibr" target="#b215">[216]</ref>, <ref type="bibr" target="#b216">[217]</ref>, <ref type="bibr" target="#b217">[218]</ref>. Therefore, CL, as a learning paradigm that better aligns with real-world application needs, aims to enable models to effectively accumulate and update knowledge across multiple stages, thereby better adapting to dynamic and evolving environments. This section will provide a detailed classification and overview of the latest innovative research in continual learning. The specific content is divided into three parts: 1) Exploring non-LLMs unimodal continual learning and focusing on traditional models' continual learning research in unimodal data; 2) Analyzing non-LLMs multimodal continual learning and discussing the challenges and research in continual learning across multi-modal data; 3) Analyzing and summarizing the latest advancements in continual learning for LLMs and examining the unique challenges and solutions they face when handling large-scale textual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-LLM Unimodal CL</head><p>In traditional unimodal learning, research on continual learning primarily focuses on how to prevent models from</p><p>TABLE 5: Innovations in Non-LLM Multimodal CL Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Starting point of the problem How to solve CPP <ref type="bibr" target="#b190">[191]</ref> Improving the performance of continual learning. CPP incorporates the CCE, TKD, and TPL mechanisms to achieve multimodal vision perception. <ref type="bibr" target="#b191">[192]</ref> To reduce catastrophic forgetting. CP-Prompt utilizes a dual-prompt strategy and parameter-efficient adjustments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CP-Prompt</head><p>MMAL <ref type="bibr" target="#b192">[193]</ref> Reducing forgetting and enhancing incremental learning performance.</p><p>MMAL proposes the modality fusion module and MSKC module.</p><p>MSPT <ref type="bibr" target="#b193">[194]</ref> To reduce catastrophic forgetting. MSPT optimizes multimodal learning through gradient modulation and attention distillation. MedCoSS <ref type="bibr" target="#b194">[195]</ref> To reduce catastrophic forgetting.</p><p>MSPT propose a staged multimodal self-supervised learning framework that avoids modality conflicts.</p><p>ZiRa <ref type="bibr" target="#b195">[196]</ref> Retaining zero-shot generalization ability. ZiRa proposes zero-interference loss and a reparameterized dual-branch structure.</p><p>STELLA <ref type="bibr" target="#b196">[197]</ref> To reduce forgetting of previously learned knowledge.</p><p>STELLA proposes a localized patch importance scoring method. <ref type="bibr" target="#b197">[198]</ref> To address the issue of overlap between old and new category spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCS-Prompt</head><p>RCS-Prompt proposes bidirectional prompt optimization and prompt magnitude normalization.</p><p>ZSCL <ref type="bibr" target="#b198">[199]</ref> To reduce catastrophic forgetting. ZSCL proposes feature space distillation and parameter space weight integration.</p><p>CoCoOp <ref type="bibr" target="#b199">[200]</ref> To address the issue of pretrained models lacking generalization ability to unseen classes when adapting to new tasks.</p><p>CoCoOp generates dynamic prompts using a lightweight neural network.</p><p>RAIL <ref type="bibr" target="#b200">[201]</ref> Improving cross-domain classification capabilities during continual learning.</p><p>RAIL uses recursive ridge regression and a notraining fusion module.</p><p>forgetting previously learned knowledge when learning new tasks. Many researchers have proposed solutions to this problem, including strategies based on knowledge retention, incremental learning methods, and improvements to neural network architectures <ref type="bibr" target="#b204">[205]</ref>, <ref type="bibr" target="#b218">[219]</ref>, <ref type="bibr" target="#b219">[220]</ref>, <ref type="bibr" target="#b220">[221]</ref>, <ref type="bibr" target="#b221">[222]</ref>, <ref type="bibr" target="#b222">[223]</ref>, <ref type="bibr" target="#b223">[224]</ref>, <ref type="bibr" target="#b224">[225]</ref>. For non-large models, the challenges of continual learning are particularly pronounced due to limitations in computational resources. Furthermore, the unimodal continual learning for non-large models primarily focuses on individual modalities such as vision, speech, and text. As show in Tables <ref type="table" target="#tab_0">3</ref> and <ref type="table">4</ref>, to address the specific characteristics of these tasks, researchers have proposed a variety of innovative frameworks and methods. Overall, unimodal continual learning with non-large models has made significant progress in scenarios with limited computational resources. Many innovative frameworks and methods have been developed to effectively mitigate catastrophic forgetting. However, how to scale these approaches to multimodal and large-scale data remains an important direction for future research. More details of the non-LLM unimodal continual learning are provided in Section 8.1 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-LLM Multimodal CL</head><p>Compared to unimodal continual learning, multimodal continual learning presents more complex challenges. Data from different modalities often exhibit heterogeneity, and the key difficulty in multimodal continual learning for non-large models lies in how to effectively fuse information across modalities while retaining previously acquired knowledge during the process of learning new modalities. In recent years, researchers have proposed various methods to address these challenges, including inter-modal collaborative learning, shared and independent representations for each modality, and others <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b225">[226]</ref>, <ref type="bibr" target="#b226">[227]</ref>, <ref type="bibr" target="#b227">[228]</ref>, <ref type="bibr" target="#b228">[229]</ref>, <ref type="bibr" target="#b229">[230]</ref>, <ref type="bibr" target="#b230">[231]</ref>, <ref type="bibr" target="#b231">[232]</ref>, <ref type="bibr" target="#b232">[233]</ref>, <ref type="bibr" target="#b233">[234]</ref>, <ref type="bibr" target="#b234">[235]</ref>, <ref type="bibr" target="#b235">[236]</ref>, <ref type="bibr" target="#b236">[237]</ref>. As shown in Table <ref type="table">5</ref>, these innovative methods enable non-large models to perform continual learning in multimodal environments, while minimizing knowledge conflicts between different modalities. More details of the non-LLM multimodal continual learning are provided in Section 8.2 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CL in LLM</head><p>LLMs such as GPT and BERT, with their powerful language understanding and generation capabilities, have achieved remarkable results on various natural language processing tasks <ref type="bibr" target="#b251">[252]</ref>, <ref type="bibr" target="#b252">[253]</ref>, <ref type="bibr" target="#b253">[254]</ref>, <ref type="bibr" target="#b254">[255]</ref>, <ref type="bibr" target="#b255">[256]</ref>, <ref type="bibr" target="#b256">[257]</ref>, <ref type="bibr" target="#b257">[258]</ref>, <ref type="bibr" target="#b258">[259]</ref>, <ref type="bibr" target="#b259">[260]</ref>, <ref type="bibr" target="#b260">[261]</ref>, <ref type="bibr" target="#b261">[262]</ref>, <ref type="bibr" target="#b262">[263]</ref>. However, LLMs still face unique challenges in continual learning. Particularly in the context of increasing data volume and task diversity, how to effectively update models, avoid catastrophic forgetting, and maintain efficient computational capabilities are key focuses in the research of LLMs for continual learning. As shown in Table <ref type="table">6</ref>, researchers have proposed a variety of instruction finetuning methods. Through model improvements and methods such as instruction fine-tuning, LLMs are able to expand their knowledge while effectively addressing the issue of catastrophic forgetting. However, as model sizes continue to grow, core challenges in the field of continual learning for LLMs remain, such as how to handle updates and learning with large-scale data, and how to maintain good adaptability in multi-task and cross-modal environments. These remain critical issues that need to be addressed. More details of the LLM continual learning are provided in Section 8.3 of the Appendix.</p><p>Continual learning is a multidimensional and complex research field, characterized by both challenges and opportunities. From unimodal to multimodal, and then to continual learning in LLMs, each category of methods and strategies presents its own unique challenges and innovations. Future research will not only need to deepen the understanding of existing methods, but also explore how to achieve more TABLE 6: Innovations in LLM Instruction Fine-tuning Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Starting point of the problem How to solve ConTinTin <ref type="bibr" target="#b237">[238]</ref> To reduce catastrophic forgetting. InstructionSpeak learns from negative outputs and revisites the instructions of previous tasks.</p><p>OLoRA <ref type="bibr" target="#b238">[239]</ref> Improving the performance of continual learning. OLoRA introduces orthogonal low-rank adaptation for CIT.</p><p>DAPT <ref type="bibr" target="#b239">[240]</ref> To reduce catastrophic forgetting. DAPT proposes a dual-attention learning and selection module.</p><p>ELM <ref type="bibr" target="#b240">[241]</ref> To reduce catastrophic forgetting. ELM trains a small expert adapter for each task on top of the LLM. LLaMA PRO <ref type="bibr" target="#b241">[242]</ref> Retaining the initial functionality through posttraining.</p><p>LLaMA PRO introduces an innovative block expansion technique.</p><p>AdaptLLM <ref type="bibr" target="#b242">[243]</ref> To help the model leverage domain-specific knowledge while enhancing prompt performance.</p><p>AdaptLLM adapts the LLM to different domains by enriching the original training corpus with a series of content-related reading comprehension tasks.</p><p>DynaInst <ref type="bibr" target="#b243">[244]</ref> To enhance the generalization of the LLM. DynaInst combines dynamic instruction replay with a local minima-inducing regularizer.</p><p>TAALM <ref type="bibr" target="#b244">[245]</ref> Enabling targeted knowledge updates and reducing forgetting.</p><p>TAALM uses meta-learning to dynamically predict token importance. D-CPT Law <ref type="bibr" target="#b245">[246]</ref> To reduce GPU resource consumption and improve domain adaptability.</p><p>D-CPT Law predicts the optimal training ratio.</p><p>COPAL <ref type="bibr" target="#b246">[247]</ref> High computational demands and model adaptability limitations.</p><p>COPAL enables continual pruning without the need for retraining.</p><p>MagMax <ref type="bibr" target="#b247">[248]</ref> To reduce catastrophic forgetting. MagMax proposes sequential fine-tuning and maximum magnitude weight selection.</p><p>SAPT <ref type="bibr" target="#b248">[249]</ref> Enabling effective knowledge retention and transfer. SAPT aligns the learning and selection of PET blocks through a shared attention mechanism.</p><p>SSR <ref type="bibr" target="#b249">[250]</ref> To reduce catastrophic forgetting. SSR utilizes LLM-generated synthetic instances for rehearsal. LoRAMoE <ref type="bibr" target="#b250">[251]</ref> Enhancing multi-task handling capabilities. LoRAMoE integrates LoRA and router networks, and introduces local balance constraints. F-Learning paradigm <ref type="bibr" target="#b250">[251]</ref> Improving the performance of continual learning.</p><p>F-Learning paradigm first forgets old knowledge before learning new knowledge.</p><p>efficient and robust continual learning in environments with large-scale, multimodal data and tasks. As computational power and data scale continue to expand, research in continual learning will provide a more solid theoretical and technological foundation for the adaptability, robustness, and sustainability of intelligent systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONTINUAL LEARNING IN MLLMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary</head><p>Recent advancements in MLLMs have shown remarkable capabilities across various domains. However, as their scale grows, maintaining long-term effectiveness in dynamic environments is a critical challenge <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b274">[275]</ref>, <ref type="bibr" target="#b275">[276]</ref>, <ref type="bibr" target="#b276">[277]</ref>, <ref type="bibr" target="#b277">[278]</ref>, <ref type="bibr" target="#b278">[279]</ref>, <ref type="bibr" target="#b279">[280]</ref>, <ref type="bibr" target="#b280">[281]</ref>, <ref type="bibr" target="#b281">[282]</ref>, <ref type="bibr" target="#b282">[283]</ref>, <ref type="bibr" target="#b283">[284]</ref>. CL addresses this by enabling models to learn new tasks without forgetting previously acquired knowledge in evolving data and task contexts. For MLLMs, continual learning is more complex due to the vast data and complex computations involved, requiring significant computational resources and storage.</p><p>Although existing research provides valuable theoretical and experimental insights <ref type="bibr" target="#b284">[285]</ref>, <ref type="bibr" target="#b285">[286]</ref>, <ref type="bibr" target="#b286">[287]</ref>, <ref type="bibr" target="#b287">[288]</ref>, <ref type="bibr" target="#b288">[289]</ref>, <ref type="bibr" target="#b289">[290]</ref>, <ref type="bibr" target="#b290">[291]</ref>, <ref type="bibr" target="#b291">[292]</ref>, <ref type="bibr" target="#b292">[293]</ref>, applying MLLMs to continual learning still faces many challenges. This section explores innovations in multimodal large model continual learning and the related evaluation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Innovation</head><p>As shown in Tables <ref type="table" target="#tab_3">7</ref> and <ref type="table">8</ref>, to achieve multi-task CL in multimodal large models and avoid catastrophic forgetting, researchers have proposed numerous innovative frameworks and methods <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b234">[235]</ref>, <ref type="bibr" target="#b269">[270]</ref>, <ref type="bibr" target="#b271">[272]</ref>, <ref type="bibr" target="#b272">[273]</ref>, <ref type="bibr" target="#b293">[294]</ref>, <ref type="bibr" target="#b294">[295]</ref>. These innovations not only facilitate knowledge sharing and transfer between multiple tasks but also effectively address challenges such as catastrophic forgetting, modality conflicts, and computational resource constraints. These efforts collectively advance the continual learning capabilities of multimodal large models in dynamic environments. More details of the model innovation in the continual learning of MLLMs are provided in Section 9 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarks</head><p>As the application of multimodal large models in continual learning increases, evaluating their CL capability has become a key issue. To comprehensively assess the continual learning performance of multimodal large models, benchmarks and evaluation frameworks have emerged. However, benchmarks specifically designed for continual learning in multimodal large models are still relatively scarce, and the relevant evaluation standards are still in the process of development. Section 9.1 in the Appendix analyzes and lists the few existing benchmarks to evaluate the continual learning capability of multimodal large models, exploring their design concepts, evaluation metrics, and applicability in different application scenarios. To reduce the dependency on large-scale joint pretraining.</p><p>PathWeave enhances modality alignment and collaboration.</p><p>CLAP <ref type="bibr" target="#b90">[91]</ref> To enhance the model's uncertainty estimation capabilities.</p><p>CLAP is compatible with various prompt methods.</p><p>DIKI <ref type="bibr" target="#b264">[265]</ref> To reduce catastrophic forgetting. DIKI proposes a residual mechanism and distribution-aware calibration.</p><p>GMM <ref type="bibr" target="#b265">[266]</ref> To reduce catastrophic forgetting. GMM implements incremental learning through generated label text and feature matching. PriViLege <ref type="bibr" target="#b266">[267]</ref> To address catastrophic forgetting and overfitting in MLLMs.</p><p>PriViLege proposes prompt functionality and knowledge distillation. ModalPrompt <ref type="bibr" target="#b267">[268]</ref> To address catastrophic forgetting and overfitting in MLLMs.</p><p>ModalPrompt proposes bi-modal guided prototype prompts and knowledge transfer.</p><p>CGIL <ref type="bibr" target="#b268">[269]</ref> To reduce catastrophic forgetting. CGIL uses VAEs to learn class-conditioned distributions and generate synthetic samples. CoLeCLIP <ref type="bibr" target="#b269">[270]</ref> To reduce interference between tasks.</p><p>CoLeCLIP proposes joint learning of task prompts and cross-domain vocabularies.</p><p>ICL <ref type="bibr" target="#b99">[100]</ref> To enhance the efficiency of continual learning in MLLMs.</p><p>ICL enables interaction between a fast intuition model and a slow deep thinking model.</p><p>EMT <ref type="bibr" target="#b270">[271]</ref> To evaluate catastrophic forgetting in MLLMs.</p><p>EMT offers a new perspective for improving finetuning strategies in MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Freeze-Omni</head><p>[99]</p><p>To reduce catastrophic forgetting.</p><p>Freeze-Omni implements a three-stage training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapt-∞ [272]</head><p>To reduce catastrophic forgetting. Adapt-∞ proposes dynamic data selection and a clustering-based permanent pruning strategy. <ref type="bibr" target="#b272">[273]</ref> To address the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mono-InternVL</head><p>Mono-InternVL integrates visual experts using a MOE structure and introduces endogenous visual pretraining. MoExtend <ref type="bibr" target="#b273">[274]</ref> To address the issues of catastrophic forgetting and high training costs.</p><p>MoExtend designes a three-stage training process, including alignment, extension, and fine-tuning.</p><p>Existing benchmarks for multimodal large model continual learning provide some reference value for assessing a model's learning ability. However, due to the scarcity of such benchmarks, with only a few available for use, many issues and limitations remain to be addressed. In the future, there is a need to design more comprehensive, flexible, and scalable evaluation benchmarks to meet the evolving demands of multimodal large model continual learning technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CHALLENGES AND FUTURE TRENDS IN MULTI-MODAL LARGE MODEL CONTINUAL LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Catastrophic Forgetting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Challenges Encountered</head><p>Catastrophic forgetting has long been a classic problem in continual learning tasks, and its presence significantly limits the adaptability and generalization ability of models in realworld dynamic environments. For multimodal large models, this issue becomes even more complex due to the need for training on large-scale data, as well as the immense computational resources and storage space required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Future Trends</head><p>Balancing forgetting management with learning efficiency, especially as tasks increase, is a complex optimization challenge. The goal is to prevent catastrophic forgetting while maintaining learning efficiency. Future research should focus on strategies to mitigate forgetting, such as frameworks or algorithms that preserve old knowledge while learning new information, or mechanisms for periodic knowledge consolidation. In addition, techniques such as self-supervised learning and transfer learning can be utilized. By sharing latent features or representations across different modalities, these methods can reduce interference between tasks, thereby alleviating the impact of catastrophic forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improvement and Standardization of Evaluation Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Challenges Encountered</head><p>Evaluation benchmarks should not only consider a model's performance in learning new tasks but also assess its ability to retain knowledge across different modalities, the effectiveness of cross-task transfer, and its stability over long-term learning. Currently, benchmarks for evaluating continual learning in multimodal large models are still relatively scarce. As multimodal large models become increasingly complex in real-world applications, developing comprehensive and systematic evaluation benchmarks for their continual learning capabilities is an urgent problem that needs to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Future Trends</head><p>Future research should focus on designing more comprehensive and flexible evaluation benchmarks that support the assessment of continual learning in multimodal large models within multi-task environments. Researchers need to develop evaluation metrics capable of measuring a model's performance in multi-task learning, knowledge transfer, catastrophic forgetting, and cross-modal consistency. Furthermore, the standardization of evaluation benchmarks will</p><p>TABLE 8: Innovations in MLLModel CL Methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Starting point of the problem How to solve NoRGa <ref type="bibr" target="#b263">[264]</ref> To enhance the continual learning performance of multimodal large language models.</p><p>NoRGa proposes the non-linear residual gate.</p><p>ZAF <ref type="bibr" target="#b295">[296]</ref> To reduce catastrophic forgetting. ZAF preserves knowledge through zero-shot stability regularization.</p><p>DualLoRA <ref type="bibr" target="#b91">[92]</ref> Improving the efficiency and effectiveness of continual learning in multimodal large language models.</p><p>DualLoRA utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity.</p><p>LPI <ref type="bibr" target="#b296">[297]</ref> To address the insufficient interaction between modalities and tasks.</p><p>LPI enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. Model Tailor <ref type="bibr" target="#b297">[298]</ref> To reduce catastrophic forgetting.</p><p>Retaining most of the pre-trained parameters and replacing a small number of fine-tuned parameters.</p><p>HVCLIP <ref type="bibr" target="#b92">[93]</ref> Enhancing the model's ability to retain critical information while adapting to new tasks or domains.</p><p>HVCLIP uses strategies such as forgetting reduction, discrepancy reduction, and feature enhancement. Continual LLaVA <ref type="bibr" target="#b95">[96]</ref> Enhancing the ability to preserve knowledge from previous tasks while accommodating new ones.. Continual LLaVA proposes a parameter-efficient tuning method that does not require rehearsal.</p><p>LLaCA <ref type="bibr" target="#b298">[299]</ref> To reduce forgetting and lower computational costs. LLaCA dynamically adjusts the EMA weights and introduces an approximation mechanism.</p><p>CVM <ref type="bibr" target="#b299">[300]</ref> To reduce forgetting and improve generalization. CVM maps the representations of small visual models to the knowledge space of a fixed LLM. <ref type="bibr" target="#b300">[301]</ref> Addressing challenges related to computational resources, data privacy, and catastrophic forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RE-tune</head><p>RE-tune freezes the backbone of the model and trains adapters, using text prompts to guide training.</p><p>CluMo <ref type="bibr" target="#b301">[302]</ref> Enhancing the performance of MLLMs in CL and improving their ability to retain old knowledge.</p><p>CluMo employs a two-stage training and modality fusion prompt strategy. Fwd-Prompt <ref type="bibr" target="#b302">[303]</ref> To achieve anti-forgetting and positive transfer. Fwd-Prompt utilizes gradient projection techniques and proposes a multimodal prompt pool. CPE-CLIP <ref type="bibr" target="#b303">[304]</ref> Enhancing the performance of few-shot class incremental learning in MLLMs.</p><p>CPE-CLIP using learnable prompts and regularization strategies.</p><p>TG <ref type="bibr" target="#b304">[305]</ref> To reduce catastrophic forgetting. TG proposes the model-agnostic self-uncompression method.</p><p>LiNeS <ref type="bibr" target="#b305">[306]</ref> Preserving the generalization ability of pretraining while improving fine-tuning task performance.</p><p>LiNeS proposes parameter updates with differentiated layer depth.</p><p>AttriCLIP <ref type="bibr" target="#b306">[307]</ref> Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks.</p><p>AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts.</p><p>AttriCLIP <ref type="bibr" target="#b306">[307]</ref> Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks.</p><p>AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts. <ref type="bibr" target="#b307">[308]</ref> To reduce catastrophic forgetting. C-LoRA performs continual adaptive low-rank adjustments in the cross-attention layers of MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-LoRA</head><p>be a key direction for future development. By establishing unified evaluation frameworks, it will be possible to more effectively compare the strengths and weaknesses of different models, thereby advancing research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improving the Interpretability and Transparency of Continual Learning in Multimodal Large Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Challenges Encountered</head><p>In multimodal learning tasks, models need to integrate information from different modalities (such as images, text, audio, etc.), which makes their decision-making process more complex and harder to trace. In particular in continual learning environments, the model must continuously learn new tasks while retaining knowledge from previous tasks.</p><p>The integration and transfer of information across different modalities during this learning process make the model's decision mechanism even more challenging to interpret. Enhancing the interpretability of multimodal large models in continual learning not only helps increase the model's trustworthiness but also provides effective debugging and error diagnosis mechanisms during the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Future Trends</head><p>In future research on continual learning for multimodal large models, to enhance model interpretability, researchers can design more transparent and traceable architectures that allow for clear tracking and analysis of the model's decision-making rationale when handling different tasks.</p><p>At the model design level, researchers can integrate the latest advances in explainable AI (XAI) to incorporate highly interpretable model structures, thus improving transparency in the decision-making process. Furthermore, by combining techniques such as cross-modal learning and transfer learning, researchers can effectively facilitate the transfer and retention of cross-task knowledge during continual learning, while also enhancing the understanding and explainability of the knowledge transfer mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this review, we systematically discuss the latest advancements and challenges in the continual learning of multimodal large models (MLLM). First, we review the innovative strategies of multimodal large models and their applications across different fields, highlighting their advantages in handling diverse data sources. We also introduce the most commonly used benchmark testing methods and provide application examples in various domains such as natural language processing and computer vision.</p><p>Next, we provide a detailed overview of the latest research in continual learning, offering a classification of unimodal and multimodal continual learning in non-large models, and delving into the current state of research on large language models (LLMs) in continual learning. By comparing research across these different areas, we further clarify their approaches and limitations in dealing with data distribution changes.</p><p>The extensive and in-depth research in both the multimodal large model and continual learning domains has laid a solid foundation for research in multimodal large model continual learning. We conduct a thorough analysis of the current state of research in this area, discussing aspects such as benchmark evaluation, model structures, and innovations in methods, revealing both the potential and the challenges faced by MLLM in continual learning.</p><p>Finally, we provide a forward-looking discussion on the challenges and future development trends in the continual learning of multimodal large models. Our goal is to inspire researchers in the field and provide valuable insights for future research directions, aiming to promote the advancement and innovation of technologies related to the continual learning of multimodal large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">MULTIMODAL LARGE LANGUAGE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Model Innovation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Framework Innovation</head><p>Chaoya Jiang et al. <ref type="bibr" target="#b100">[101]</ref> introduced the multi-granularity hybrid visual encoding framework MaVEn, which combines discrete visual symbol sequences representing abstract, coarse-grained semantic concepts with traditional continuous representation sequences that simulate fine-grained features. This combination enhances the model's ability to understand visual information in images.</p><p>Zhuofan Zong et al. <ref type="bibr" target="#b101">[102]</ref> proposed the MoVA framework, which incorporates coarse-grained context-aware expert routing and fine-grained expert fusion. This framework adaptively routes and fuses visual experts for specific tasks through a coarseto-fine mechanism, thereby mitigating the bias of the CLIP visual encoder and enhancing the model's ability to understand and process diverse image content.</p><p>Leyang Shen et al. <ref type="bibr" target="#b102">[103]</ref> proposed a multimodal expert mixing framework, MoME, which combines the visual expert mixture model (MoVE) and the language expert mixture model (MoLE) to reduce task interference.</p><p>Byung-Kwan Lee et al. <ref type="bibr" target="#b103">[104]</ref> proposed the Meteor model, based on the Mamba architecture, which enhances the comprehension and response capabilities of large language and vision models through multifaceted reasoning.</p><p>Hao Ma et al. <ref type="bibr" target="#b104">[105]</ref> proposed the sequential cooperative multi-agent reinforcement learning framework, CORY, which enhances the stability and performance of multimodal large models in reinforcement learning fine-tuning by leveraging the inherent collaborative evolution and emergent capabilities of multi-agent systems.</p><p>Yang Jiao et al. <ref type="bibr" target="#b105">[106]</ref> proposed a vision-centric multimodal large model framework, Lumen, which strengthens multimodal content understanding by decoupling task-agnostic and taskspecific learning. This framework enables flexible adaptation to various vision tasks, enhancing the LMM's capabilities in visual perception and instruction following.</p><p>Chuyang Zhao et al. <ref type="bibr" target="#b106">[107]</ref> proposed the "Parallel Recognition → Sequential Understanding" MLLM framework, Octopus. This framework achieves parallel recognition of object queries at the lower LLM layers and passes the results to the top LLM layers for sequential understanding, thereby improving the efficiency and accuracy of MLLMs.</p><p>Yikai Zhang et al. <ref type="bibr" target="#b107">[108]</ref> proposed the Wings framework, which introduces additional modules and mechanisms to compensate for attention shifts. This allows the model to effectively process visual information while maintaining focus on textual information.</p><p>Timin Gao et al. <ref type="bibr" target="#b108">[109]</ref> proposed the Cantor framework, which integrates visual inputs with logical reasoning and leverages the advanced cognitive functions of MLLMs. By acting as a multifaceted expert, it directly acquires higher-level information, thereby improving decision-making quality.</p><p>Daqin Luo et al. <ref type="bibr" target="#b109">[110]</ref> proposed the AutoM3L framework, based on the AutoML architecture, which automates the construction of multimodal training pipelines, feature engineering, and model selection using LLMs, thereby reducing manual intervention.</p><p>Yunfeng Fan et al. <ref type="bibr" target="#b110">[111]</ref> proposed the DI-MML framework, which addresses modality competition in multimodal learning by independently training modality encoders. They introduced a shared classifier and DUC loss to facilitate cross-modal interaction and knowledge transfer, thereby mitigating the modality competition issue in multimodal learning.</p><p>Xinwei Liu et al. <ref type="bibr" target="#b111">[112]</ref> proposed the multi-step error minimization framework, MEM, which optimizes by combining image noise and text triggers. This approach misleads the model into learning shortcuts, thereby protecting data privacy.</p><p>Jinxu Zhang et al. <ref type="bibr" target="#b112">[113]</ref> proposed the CREAM framework, which integrates high-performance retrieval enhancement, multiimage and multimodal processing, and efficient instruction tuning. This effectively addresses the challenges in documentbased VQA tasks.</p><p>Li Zheng et al. <ref type="bibr" target="#b113">[114]</ref> proposed the Adaptive Multimodal Data Augmentation framework, SLUDA, which generates finegrained data, optimizes the utilization of unlabeled data, and employs adaptive selection strategies and dynamic threshold adjustments. This approach addresses the issues of insufficient labeled data and the underutilization of unlabeled data.</p><p>Tao Wu et al. <ref type="bibr" target="#b114">[115]</ref> proposed the SAM model, which enhances semantic associations between images by introducing a bidirectional semantic guidance mechanism. This improves the semantic alignment ability of multimodal instructions.</p><p>Shichen Lu et al. <ref type="bibr" target="#b115">[116]</ref> proposed the Tiny-Large collaborative training framework, CTVLMs, which leverages knowledge distillation and multimodal alignment to enable large models to transfer knowledge to smaller models. This approach achieves a dual improvement in both performance and efficiency.</p><p>Minsu Kim et al. <ref type="bibr" target="#b116">[117]</ref> proposed the Bloom framework, which uses bidirectional modality transformation and adaptive cross-modal fusion. It pretrains a VSR (Visual Speech Recognition) model with visual and speech units and introduces a curriculum learning strategy to enhance training efficiency and multilingual recognition performance.</p><p>Yunshan Ma et al. <ref type="bibr" target="#b308">[309]</ref> proposed the CIRP framework, which uses a multimodal encoder and cross-item contrastive loss to learn individual item semantics and relationships. By introducing a relationship pruning module, this framework enhances the ability to align cross-modal information and capture cross-item relationships in cold-start items.</p><p>Puyi Wang et al. <ref type="bibr" target="#b117">[118]</ref> proposed the multimodal large model-assisted artificial intelligence-generated image quality assessment framework, MA-AGIQA. By combining multimodal models with traditional DNNs, and utilizing semantic information extraction and a mixture of experts (MoE) structure, the framework dynamically integrates quality perception features. This significantly improves the quality assessment performance of AGIs, particularly excelling in reducing the false-negative rate.</p><p>Zhiqi Ge et al. <ref type="bibr" target="#b118">[119]</ref> proposed a novel cognitive framework, WorldGPT, which includes memory offloading, knowledge retrieval, and a Context Reflector to enhance the model's performance in specific scenarios and long-term tasks.</p><p>Haoning Wu et al. <ref type="bibr" target="#b119">[120]</ref> proposed the ONEALIGN model, which unifies IQA, IAA, and VQA tasks, thereby enhancing the model's cross-task generalization ability.</p><p>Zixin Zhang et al. <ref type="bibr" target="#b309">[310]</ref> proposed the M2FEDSA framework, which combines segmentation learning and multimodal federated learning. By introducing dual-adaptive fine-tuning and dual knowledge transfer strategies, the framework improves both computational and storage efficiency, as well as performance, when deploying large-scale multimodal models in federated learning settings.</p><p>Ruisi Cai et al. <ref type="bibr" target="#b120">[121]</ref> proposed an elastic architecture called Flextron, which supports adaptive subnetwork selection. By using routers to choose different sub-models or subnetworks, Flextron addresses the deployment challenges of multimodal large models in resource-constrained environments.</p><p>Shengqiong Wu et al. <ref type="bibr" target="#b121">[122]</ref> proposed an end-to-end Anyto-Any multimodal large model framework, which achieves efficient cross-modal understanding and generation through lightweight alignment techniques and modality-switching instruction tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Method Innovation</head><p>Xiaotong Li et al. <ref type="bibr" target="#b122">[123]</ref> proposed a comprehensive multimodal perception fusion method that integrates visual experts, thereby enhancing the visual perception capability of MLLMs.</p><p>Jiaqing Zhang et al. <ref type="bibr" target="#b123">[124]</ref> proposed a novel end-to-end algorithm for multimodal fusion detection, achieving high performance through a single training phase and simplifying the overall process.</p><p>Junfeng Fang et al. <ref type="bibr" target="#b124">[125]</ref> proposed a neuron attribution method tailored for MLLMs, called NAM. NAM reveals the modality-specific semantic knowledge learned by neurons in MLLMs and highlights certain neuron characteristics that collectively elucidate the internal workings of MLLMs.</p><p>Jayneel Parekh et al. <ref type="bibr" target="#b310">[311]</ref> proposed a concept extraction method based on dictionary learning to interpret the internal representations of large multimodal models. They innovatively defined multimodal concepts and validated their effectiveness in interpreting models and understanding test sample representations.</p><p>Junho Kim et al. <ref type="bibr" target="#b125">[126]</ref> proposed CODE, which utilizes self-generated descriptions as contrastive references to dynamically adjust the information flow, enhancing the coherence and informativeness of responses. This approach addresses the hallucination problem in MLLMs when generating visual content.</p><p>Samyadeep Basu et al. <ref type="bibr" target="#b126">[127]</ref> proposed the model editing algorithm MULTEDIT, which can correct errors and insert new information. They also introduced a multimodal causal tracking method, extending research on information storage to other domains.</p><p>Jingjing Xie et al. <ref type="bibr" target="#b127">[128]</ref> proposed the Quantized Scale Learning Method (QSLAW), which effectively reduces quantization errors, prevents overfitting, and improves model adaptability and efficiency by learning the group scale factors of quantized weights and employing a multimodal pretraining strategy.</p><p>Yabing Wang et al. <ref type="bibr" target="#b128">[129]</ref> proposed the MLLM-enhanced cross-lingual, cross-modal retrieval method LECCR. This approach leverages MLLMs to generate visual descriptions, which are then aggregated into multi-view semantic slots to enhance the semantic richness of visual features. By incorporating English feature guidance, it improves the quality of cross-modal alignment.</p><p>Zihao Liu et al. <ref type="bibr" target="#b311">[312]</ref> proposed a visual perception adapter and fine-grained tri-modal contrastive learning method. By aligning tokens across modalities, they reduce semantic gaps, thereby improving the performance of multimodal video tasks.</p><p>Weixiang Han et al. <ref type="bibr" target="#b129">[130]</ref> proposed the ERL-MR strategy, which uses Euler transformations and multimodal constraint loss to transform inter-modal competition into cooperation, thereby achieving performance improvement.</p><p>Qiang Wang et al. <ref type="bibr" target="#b312">[313]</ref> proposed a bilateral adaptive cross-modal fusion prompt learning paradigm, Bloom, which achieves more flexible cross-modal interaction and alignment through bidirectional modal transformation and adaptive fusion functions. This significantly enhances the performance of CLIP on a variety of generalization tasks.</p><p>Zongqian Wu et al. <ref type="bibr" target="#b130">[131]</ref> proposed an adaptive multimodal prompt learning method, AMMPL, which effectively handles meaningless image patches and enhances the model's generalization ability through image prompts and cross-modal interaction learning.</p><p>Minghe Gao et al. <ref type="bibr" target="#b313">[314]</ref> proposed the Fact paradigm, which teaches MLLMs by generating Faithful, Concise, and Transferable multimodal rationales, enhancing the model's performance and reasoning ability across various visual tasks.</p><p>Lincan Cai et al. <ref type="bibr" target="#b131">[132]</ref> proposed the PaRe method, which enhances the stability and transferability of cross-modal finetuning by progressively generating intermediate modalities and replacing modality-agnostic fragments.</p><p>Wei Li et al. <ref type="bibr" target="#b132">[133]</ref> proposed the Multimodal Combination Learning (MCL) method, which strengthens the mapping between visual and language modalities. By leveraging LLMs to automatically generate multimodal learning samples, they introduced a stacked retrieval mechanism to extract diverse multimodal information.</p><p>Christian Schlarmann et al. <ref type="bibr" target="#b133">[134]</ref> proposed the FARE unsupervised adversarial fine-tuning scheme, which enhances the robustness of the CLIP model while preserving its original performance, without the need for retraining on downstream tasks.</p><p>Zhuo Huang et al. <ref type="bibr" target="#b134">[135]</ref> proposed the DICL strategy, which leverages MLLM knowledge to enhance the robustness of visual models and align MLLMs with visual tasks. This approach enables unsupervised fine-tuning, improving performance in out-of-distribution (OOD) scenarios.</p><p>Runpeng Yu et al. <ref type="bibr" target="#b135">[136]</ref> proposed the API technique, which enhances model perception through attention heatmaps guided by text queries. This approach enables model self-reflection and integration, improving performance on visual-linguistic tasks and addressing the limitations of traditional visual prompting techniques.</p><p>Kai Huang et al. <ref type="bibr" target="#b136">[137]</ref> proposed the Instruction-guided Visual Token Pruning method (IVTP), which includes an intragroup Token Pruning (GTP) module and cross-modal instructionguided pruning. This approach effectively reduces the number of visual tokens and lowers computational complexity, while maintaining model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Module Innovation</head><p>Wenfang Yao et al. <ref type="bibr" target="#b137">[138]</ref> proposed a novel reflection-based prompt optimization module, leveraging multimodal large language models to generate high-quality language descriptions to improve tracking performance. By iteratively refining the vague and inaccurate descriptions of targets through tracking feedback, this approach addresses the issue of frequent ambiguous language descriptions in annotations.</p><p>Zaijing Li et al. <ref type="bibr" target="#b138">[139]</ref> proposed a hybrid multimodal memory module that transforms knowledge into a hierarchical directed knowledge graph, enabling agents to explicitly represent and learn world knowledge. Additionally, historical information is summarized into an abstract multimodal experience pool, providing agents with rich contextual learning references. This approach addresses the challenge of general agents struggling to complete long-term tasks in open-world environments.</p><p>Jiachen Li et al. <ref type="bibr" target="#b139">[140]</ref> enhanced model capabilities by integrating sparse gated Top-K MoE (Mixture-of-Experts) blocks in the visual encoder and MLP connectors, and by introducing MoE blocks during the visual instruction fine-tuning phase. This approach improves the performance of MLLMs on multimodal tasks.</p><p>Haogeng Liu et al. <ref type="bibr" target="#b140">[141]</ref> innovatively identified visual anchors and proposed a novel vision-language connector, Ac-Former. By utilizing visual anchors to aggregate information, this approach significantly enhances the accuracy and computational efficiency of MLLMs.</p><p>Ziyuan Huang et al. <ref type="bibr" target="#b141">[142]</ref> proposed the Chain-of-Sight module, which captures visual details at different spatial scales through a multi-scale visual resampler. This module enables flexible expansion of the number of visual tokens after pretraining, accelerating the pretraining process while maintaining or improving model performance.</p><p>Huanjin Yao et al. <ref type="bibr" target="#b142">[143]</ref> proposed a new connector, the Dense Connector, which enhances the visual perception ability of MLLMs by integrating multi-layer visual features. It is characterized by high computational efficiency and ease of integration, addressing the issue of existing MLLMs underutilizing the visual encoder while overly emphasizing the language modality.</p><p>Haibo Wang et al. <ref type="bibr" target="#b143">[144]</ref> designed the Gaussian Contrastive Localization (GCG) module, which learns to represent the temporal structure of videos and selects key frames relevant to the question. This approach addresses the issue in video question answering where large multimodal models neglect question-related visual cues and lack key timestamp annotations.</p><p>Hanzi Wang et al. <ref type="bibr" target="#b144">[145]</ref> proposed a query-based hybrid expert connector, Q-MoE, which utilizes text-driven routing and an optimal expert path training strategy to achieve precise extraction and processing of task-specific visual information. This approach addresses the issue in MLLMs where the connection structure struggles to filter visual information according to task requirements in vision-language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">ROPE: Recognition-based Object Probing Evaluation Benchmark</head><p>Despite the impressive performance of MLLMs in various downstream applications, they often encounter the issue of object hallucination <ref type="bibr" target="#b314">[315]</ref>, <ref type="bibr" target="#b315">[316]</ref>, <ref type="bibr" target="#b316">[317]</ref>, <ref type="bibr" target="#b317">[318]</ref>, <ref type="bibr" target="#b318">[319]</ref>, <ref type="bibr" target="#b319">[320]</ref>, <ref type="bibr" target="#b320">[321]</ref>, <ref type="bibr" target="#b321">[322]</ref>, <ref type="bibr" target="#b322">[323]</ref>, where the model erroneously generates objects that do not exist in the image. Current benchmarks for evaluating object hallucination mainly focus on the presence of a single object category, rather than individual entities.</p><p>Xuweiyi Chen et al. <ref type="bibr" target="#b323">[324]</ref> conducted a systematic study of the multi-object hallucination problem, examining how models misidentify objects when attending to multiple objects simultaneously (e.g., inventing non-existent objects or being distracted). They introduced an automated evaluation protocol called Recognition-based Object Probing Evaluation (ROPE), which considers the distribution of object categories within a single image during testing. By using visual reference to disambiguate, the protocol systematically analyzes multi-object hallucination, revealing the hallucination behaviors and influencing factors when models process multiple objects. In addition, ROPE designs multiple task prompts, including Default Multi-Object, Student-Forcing, Teacher-Forcing, and Single-Object. The dataset is divided into four subsets, each considering different object category distributions: 1) Homogeneous: All test objects belong to the same category. 2) Heterogeneous: All test objects belong to different categories. 3) In-the-Wild: A mixed object category distribution, with test objects randomly selected and ordered. 4) Adversarial: After multiple repetitions of the same category, a different category object is introduced. The dataset is further divided into Seen and Unseen based on whether the model has encountered these images during instruction tuning.</p><p>More details of the overview of MLLM performance on the ROPE are provided in table 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark</head><p>Visual Question Answering (VQA) is a crucial task in MLLMs, designed to test their understanding and reasoning capabilities across visual and textual data <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. However, most existing VQA datasets primarily focus on English and a few major world languages, with images often being Western-centric. While recent efforts have expanded the linguistic coverage of VQA datasets, they still lack diversity in low-resource languages. Moreover, these datasets typically extend their language range through translation or other methods while keeping the images unchanged, leading to limited cultural representation. To address these limitations, David Romero et al. <ref type="bibr" target="#b322">[323]</ref> developed a new benchmark, CVQA, which aims to encompass rich linguistic and cultural diversity. This benchmark involves native speakers and cultural experts in the data collection process to ensure authenticity and inclusivity.</p><p>Figure <ref type="figure">2</ref> illustrates the scale and diversity of the CVQA benchmark, which includes 10,374 questions and languages from 30 different countries. This demonstrates how it covers a wide range of languages and cultures.</p><p>Figure <ref type="figure">3</ref> shows the performance of different models across various country-language pairs, including question-option pairs in both English and local languages. The blue line in the figure represents performance separated by continents. Despite differences in scale, it highlights the similar behavior of all models in most cases. This figure reveals the challenges models face when handling questions in local languages, as well as the performance variations across different regions and languages.</p><p>Table <ref type="table" target="#tab_6">10</ref> shows the average performance of different MLLMs on the CVQA dataset using English prompts (EN) and local language prompts (LOC). These results indicate that even the best-performing open models, such as LLaVA-1.5-7B, significantly lag behind closed models on CVQA. Furthermore, their performance is poorer with local language prompts, highlighting the challenges models face when processing non-English prompts.</p><p>Table <ref type="table" target="#tab_7">11</ref> compares the performance of LLaVA-1.5-7B and InstructBLIP on CVQA and other established English VQA benchmarks. The results show that while LLaVA-1.5-7B performs better on other English VQA benchmarks, it still faces challenges on CVQA, highlighting the difficulty of culturally specific questions in CVQA.</p><p>Table <ref type="table" target="#tab_8">12</ref> presents the performance of models across 10 categories in CVQA. It shows that models achieve the highest accuracy in the "People" category, while the accuracy in the "Food" and "Pop Culture" categories is lower with local language prompts. This indicates that the diversity of food and pop culture across different cultures presents a challenge for the generalization of MLLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">II-Bench: Image Implication Understanding Benchmark</head><p>Images often contain rich emotional and cultural narratives, and understanding their meaning and exploring the human emotions and cultural context they reflect requires attention to detail <ref type="bibr" target="#b275">[276]</ref>, <ref type="bibr" target="#b334">[335]</ref>, <ref type="bibr" target="#b335">[336]</ref>. While MLLMs have made significant progress in understanding and generating cross-modal content, achieving new breakthroughs in benchmarks like image captioning <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and visual question answering <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, there has been insufficient exploration of their higher-order perceptual abilities. Ziqiang Liu et al. <ref type="bibr" target="#b336">[337]</ref> introduced a new benchmark, II-Bench, designed to evaluate MLLMs' ability to understand and reason about the complex implicit meanings in images, addressing the gap in existing benchmarks for assessing higher-order perceptual abilities in MLLMs.</p><p>II-Bench includes 1,222 images across six different domains: life, art, society, psychology, environment, and others. The images consist of various types, including illustrations, memes, posters, comics, logos, and paintings. Each image is accompanied by one to three multiple-choice questions, totaling 1,434 questions. Of these, 1,399 questions are used to construct the test set, and 35 questions are used for the development and validation sets.</p><p>Table <ref type="table" target="#tab_9">13</ref> presents the overall results of different MLLMs and human participants on the II-Bench benchmark. It shows model performance across various domains, such as life, art, society, psychology, and environment, as well as across different emotional categories (positive, neutral, and negative). The table lists the average and best accuracies for multiple open-source and closed-source MLLMs, alongside the performance of human participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.4">ConBench: MLLMs Answer Consistency Evaluation Benchmark</head><p>MLLMs have made rapid progress in visual information perception and reasoning. Although MLLMs are capable of generating high-quality task prompt responses, simply modifying the prompt can lead to contradictory answers, even when the correct answer is provided. Specifically, under different prompt space sizes, these models lack consistency in answers to the same knowledge point, which significantly undermines trust in these models <ref type="bibr" target="#b344">[345]</ref>, <ref type="bibr" target="#b345">[346]</ref>. To ensure that MLLMs can predict correct and consistent answers when faced with various query formats, Yuan Zhang et al. <ref type="bibr" target="#b346">[347]</ref> proposed a multimodal benchmark tool, ConBench, designed to comprehensively assess the consistency of MLLMs-specifically, their ability to provide the same answer to the same knowledge point across different query formats.</p><p>ConBench evaluates MLLMs by offering a diverse set of question formats, including true/false questions, multiple-choice questions, and limited visual question answering (VQA) problems. It also introduces two multidimensional evaluation metrics: The specific structure of ConBench is shown in figure <ref type="figure" target="#fig_3">4</ref>, providing an overview of the 19 evaluation categories in ConBench. These categories are distributed across three core capabilities: Sensation, Cognition, and Knowledge. The benchmark comprehensively covers tasks of varying difficulty levels, thereby assessing the performance of MLLMs across different aspects.</p><p>Table <ref type="table" target="#tab_11">14</ref> presents the performance evaluation results of different MLLMs on ConBench. These results are based on ConScore[D], which evaluates the correctness of the model's answers to discriminative questions. The table includes three </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.5">COMPBENCH: Comparative Reasoning Benchmark</head><p>The ability to compare objects, scenes, or situations is crucial for decision-making and problem-solving in everyday life <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b350">[351]</ref>, <ref type="bibr" target="#b351">[352]</ref>. Although this ability is widespread in human cognition, it has not been fully explored in the field of Artificial General Intelligence (AGI). Jihyung Kil et al. <ref type="bibr" target="#b352">[353]</ref> proposed a benchmark, COMPBENCH, designed to evaluate the comparative reasoning ability of MLLMs.</p><p>As show in table <ref type="bibr" target="#b15">16</ref>. COMPBENCH questions are carefully crafted to distinguish relative features between two images, testing the models' performance across eight different comparative dimensions by providing image pairs and related questions. Table <ref type="table" target="#tab_14">17</ref> presents the performance of recent MLLMs on the COMPBENCH benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.6">Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs</head><p>Similarly, in the context of the hallucination problem faced by MLLMs in visual-language understanding and generation 98.2 97.9 98.8 98.3 97.4 100.0 100.0 98.0 98.0 98.8 tasks <ref type="bibr" target="#b314">[315]</ref>, <ref type="bibr" target="#b315">[316]</ref>, <ref type="bibr" target="#b316">[317]</ref>, <ref type="bibr" target="#b317">[318]</ref>, <ref type="bibr" target="#b318">[319]</ref>, <ref type="bibr" target="#b319">[320]</ref>, <ref type="bibr" target="#b320">[321]</ref>, <ref type="bibr" target="#b321">[322]</ref>, <ref type="bibr" target="#b322">[323]</ref>, Peng Ding et al. <ref type="bibr" target="#b354">[355]</ref> pointed out that previous studies have mainly focused on evaluating hallucinations on standard, undisturbed benchmarks, neglecting the prevalent interference inputs in the real world. This is crucial for a comprehensive evaluation of hallucinations in MLLMs. They proposed the first benchmark designed to evaluate hallucinations in MLLMs under disturbed inputs, called Hallu-PI, which includes seven types of disturbed scenarios: noise, blur, weather, digits, image stitching, image cropping, and prompt misdirection.</p><p>Table <ref type="table" target="#tab_15">18</ref> presents the performance of MLLMs under four basic disturbance types (noise, blur, weather, and digits). The "Before/After" columns compare the performance before and after the perturbation, using the ACC+ (Accuracy+) and CHAIR (Hallucinated Object Occurrence Rate) metrics to measure the level of hallucinations in the models.</p><p>Table <ref type="table" target="#tab_16">19</ref> focuses on the performance of MLLMs under three additional disturbance types in Hallu-PI: Concat, Cropping, and Prompt Mislead. The PI-Score (a comprehensive evaluation TABLE 15: Evaluation of Consistency between caption and three discriminative types of answer on ConBench. The Con[X] is the Consistency ratio between discriminative answer type X and caption. The "ordered" represents whether Con[T] &lt; Con[C] &lt; Con[V] is in its line. [347] Method ConScore[C] Con[T] Con[C] Con[V] Ordered Closed-source Vision Language Models GPT-4V [276] 55.6 51.20 56.50 59.10 Y GPT-4-Omni [329] 62.2 58.00 62.50 66.10 Y Gemini-Pro-Vision [348] 48.4 43.30 45.20 56.80 Y Gemini-Ultra-Vision [348] 54.6 47.80 55.20 60.70 Y Qwen-VL-Plus [278] 50.2 47.10 49.10 54.30 Y Qwen-VL-Max [278] 58.4 54.30 58.00 62.90 Y 7B Vision Language Models LLaVA-v1.5-7B [141] 38.4 39.20 36.60 39.50 N Qwen-VL-Chat [278] 48.0 42.00 50.80 51.30 Y ∼ 13B Vision Language Models LLaVA-v1.5-13B [141] 44.4 41.50 45.80 46.00 Y MiniGemini-13B [349] 41.7 38.80 42.90 43.30 Y InternVL-v1.5-26B [342] 50.9 44.50 53.90 54.20 Y ∼ 34B Vision Language Models LLaVA-NeXT-34B 48.3 46.00 52.20 46.80 N MiniGemini-34B [349] 49.6 56.80 48.00 44.10 N InternVL-v1.2P-40B [280] 53.7 49.80 55.50 55.80 Y</p><p>metric) is used to assess the overall performance of the models under these specific disturbance scenarios.</p><p>Table <ref type="table">20</ref> provides the performance details of MLLMs in generation tasks under the Concat, Cropping, and Prompt Mislead disturbances. The metrics CHAIR, Cover, Hal, and Cog are used to evaluate the models' performance in generation tasks. These metrics help us understand the models' accuracy and hallucination tendencies when generating descriptions that are consistent with the image content.</p><p>Table <ref type="table" target="#tab_18">21</ref> presents the performance of MLLMs in discriminative tasks under image stitching, cropping, and prompt misdirection disturbances. The metrics ACC, ACC+, and F1 are used to measure the models' accuracy in discriminative tasks. These data provide insights into the models' ability to handle disturbed inputs in discriminative tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.7">ReForm-Eval: Evaluating MLLMs via Unified Re-Formulation of Task-Oriented Benchmarks</head><p>MLLMs have made significant progress in understanding and reasoning about visual information <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b275">[276]</ref>, <ref type="bibr" target="#b355">[356]</ref>, <ref type="bibr" target="#b360">[361]</ref>, <ref type="bibr" target="#b361">[362]</ref>. However, this has posed challenges for the automatic evaluation of free-form text outputs from MLLMs. To leverage annotations from existing benchmarks and reduce the manual effort required to construct new benchmarks, Zejun Li et al. <ref type="bibr" target="#b362">[363]</ref> proposed a method for reformatting existing benchmarks into a unified format compatible with MLLMs. Through systematic data collection and reformatting, they introduced the ReForm-Eval benchmark, which is designed to comprehensively and quantitatively assess the capabilities of MLLMs. This approach overcomes the structural differences between existing taskoriented multimodal benchmarks and MLLMs. Figure <ref type="figure">5</ref> illustrates the capabilities and task dimensions of the ReForm-Eval benchmark. It categorizes the evaluation dimensions into two major categories with eight subcategories: 1)Visual Perception Tasks: Coarse-Grained Perception (CG), Fine-Grained Perception (FG), Scene Text Perception (STP). 2)Visual Cognition Tasks: Visually Grounded Reasoning (VGR), Spatial Understanding (Spatial), Cross-Modal Inference (CMI), Visual Description (Desc), Multi-Turn Dialogue (Dialog).</p><p>These categories and subcategories comprehensively cover different aspects of MLLMs' visual understanding and reasoning capabilities, providing a comprehensive benchmark for evaluating model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception Cognition</head><p>Image Classif.</p><p>Fig. <ref type="figure">5</ref>: Assessed capability dimensions and tasks in ReForm-Eval. "Desc" and "Classif" are respectively short for description and classification. <ref type="bibr" target="#b362">[363]</ref> Table <ref type="table" target="#tab_19">22</ref> shows a comprehensive performance evaluation of 16 open-source MLLMs across different capability dimensions, based on the ReForm-Eval benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.8">VisionGraph: Graph Theory Problems Benchmark in Visual Context</head><p>MLLMs have achieved significant success in visual understanding and reasoning <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b275">[276]</ref>, <ref type="bibr" target="#b361">[362]</ref>, <ref type="bibr" target="#b372">[373]</ref>, but multimodal graph reasoning remains a challenging task <ref type="bibr" target="#b373">[374]</ref>. It requires MLLMs to accurately understand graph structures and perform multi-step reasoning on visual graphs. To explore the ability of advanced MLLMs to address multimodal graph reasoning tasks, Yunxin Li et al. <ref type="bibr" target="#b374">[375]</ref> designed a benchmark called VisionGraph, which includes a series of graph reasoning problems aimed at testing MLLMs' understanding of graph structures and their multi-step reasoning capabilities.</p><p>Table <ref type="table" target="#tab_20">23</ref> presents the performance of different MLLMs on the VisionGraph benchmark, including evaluation metrics such as node recognition accuracy, edge recognition accuracy, and solution accuracy for specific graph theory problems. These results provide valuable insights for researchers into the models' abilities to understand and reason about graph structures.</p><p>Table <ref type="table">24</ref> shows the performance improvements of models on three representative graph theory problems (Connectivity, Cycle, and Shortest Path) after applying the Description-Program-Reasoning (DPR) method. The DPR approach enhances MLLMs' multi-step reasoning abilities by combining natural language processing and programming logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Applications of MLLMs</head><p>Zebang Cheng et al. <ref type="bibr" target="#b376">[377]</ref> proposed Emotion-LLaMA, which integrates audio, visual, and text inputs through an emotionspecific encoder, and significantly improves emotion recognition and reasoning accuracy through instruction tuning. This approach enhances the model's ability to understand and reason about emotional content across different modalities.</p><p>Xun Wu et al. <ref type="bibr" target="#b377">[378]</ref> created the VisionPrefer dataset, which includes fine-grained human preference annotations. They then trained the VP-Score reward model on this dataset to guide the training of image generation models, improving the alignment between images and text prompts. Finally, they fine-tuned the model using reinforcement learning to make the generated images more aligned with human aesthetics and preferences.</p><p>Zhenyu Wang et al. <ref type="bibr" target="#b378">[379]</ref> proposed the GenArtist system, which enables unified image generation and editing coordinated by a multimodal large language model. The system introduces location-aware tool execution and integrates tool libraries, enhancing the model's flexibility and applicability.</p><p>Yushi Hu et al. <ref type="bibr" target="#b379">[380]</ref> proposed the Visual SKETCHPAD framework, enabling multimodal language models to draw sketches and perform reasoning based on visual artifacts. This significantly enhances the model's performance in mathematical and visual tasks.</p><p>Haoyu Chen et al. <ref type="bibr" target="#b380">[381]</ref> proposed an MLLM-based intelligent image restoration system, RestoreAgent, which can automatically assess degradation, determine tasks, select models, and perform restoration.</p><p>Haodong Chen et al. <ref type="bibr" target="#b381">[382]</ref> proposed the FineCLIPER framework, which enhances facial expression recognition performance by incorporating text description augmentation, hierarchical information mining, and parameter-efficient fine-tuning to achieve multimodal feature fusion and cross-modal contrastive learning.</p><p>Shuo Ma et al. <ref type="bibr" target="#b382">[383]</ref> proposed SleepMG, which addresses the classification and domain-discrepancy performance issues in sleep staging by quantifying modal performance differences and adaptively adjusting gradients to achieve multimodal balance. This method specifically tackles the challenges posed by the classification of multimodal physiological signals, such as EEG, EOG, EMG, and ECG.</p><p>Yifeng Xie et al. <ref type="bibr" target="#b383">[384]</ref> proposed the MoBA model, which employs bidirectional adapters and a mixture of experts system to achieve efficient cross-modal interaction with a low parameter   30.0 61.4 48.2</p><p>count. This approach addresses the issues of large parameter sizes and low fine-tuning efficiency in multimodal sarcasm detection. Pinxue Guo et al. <ref type="bibr" target="#b384">[385]</ref> proposed the X-Prompt framework, which pretrains an RGB-based model and then adapts it to downstream tasks using multimodal prompts and specialized expert adapters. This approach addresses the limitations of traditional video object segmentation in complex scenarios such as extreme lighting and fast motion.</p><p>Daiqing Wu et al. <ref type="bibr" target="#b385">[386]</ref> proposed the DRF method, which addresses the issues of poor modality quality and missing data in sentiment analysis of image-text pairs on social media by approximating modality distributions using feature queues.</p><p>Lv Tang et al. <ref type="bibr" target="#b386">[387]</ref> proposed the MMCPF framework and CoVP strategy based on MLLMs, which effectively detect camouflaged objects without labeled data, addressing the issue of weak generalization in supervised learning models for zeroshot camouflaged object detection.</p><p>Deji Zhao et al. <ref type="bibr" target="#b387">[388]</ref> proposed AutoGraph, an automatic method for constructing visual context graphs. They designed a graph sampling syntax and employed a two-stage fine-tuning strategy to enhance the visual dialogue capabilities of LLMs.</p><p>Kangzheng Liu et al. <ref type="bibr" target="#b388">[389]</ref> proposed DySarl, which effectively enhances multimodal knowledge graph reasoning performance through dual-space multi-hop structural learning and interactive symmetric attention fusion.</p><p>Bowen Zhao et al. <ref type="bibr" target="#b389">[390]</ref> proposed the CT2C-QA dataset and the AED multi-agent system. The former includes three modalities, while the latter unifies multimodal data processing through collaborative agents and introduces new evaluation metrics to enhance question-answering performance.</p><p>Linhui Xiao et al. <ref type="bibr" target="#b390">[391]</ref> proposed the HiVG framework, which includes multi-level adaptive cross-modal bridges and hierarchical low-rank adaptation. This framework enables finegrained multimodal feature modulation, enhancing the accuracy and efficiency of visual localization.</p><p>Ruofan Wang et al. <ref type="bibr" target="#b391">[392]</ref> proposed a multimodal attack strategy with dual optimization objectives, which jointly attacks both the text and image modalities to increase the success rate of attacking MLLMs.</p><p>Feihong Lu et al. <ref type="bibr" target="#b392">[393]</ref> proposed the Miko framework, which combines LLMs and MLLMs to automatically capture user intentions by analyzing text and images, and constructs an intention knowledge base to enhance intention understanding in social media.</p><p>Pinhan Fu et al. <ref type="bibr" target="#b393">[394]</ref> proposed CoMO-NAS, which guides multi-objective search through core structure optimization to balance model complexity and performance, improving search efficiency and meeting the diverse needs of users.</p><p>Jianing Zhao et al. <ref type="bibr" target="#b394">[395]</ref> addressed the challenge of detecting implicit abnormal emotions in reconnaissance videos by proposing the scene-enhanced MLLM, Hawkeye, for the IasDig task. It integrates graph-structured scene modeling with a balanced heterogeneous MoE module to optimize scene information modeling and balance, effectively reducing false alarm rates and improving detection efficiency.</p><p>Xian Zhang et al. <ref type="bibr" target="#b395">[396]</ref> proposed the FINER-MLLM model, which enhances image feature extraction capabilities by finetuning the image encoder with LoRA and applying dual feature constraints. The model also introduces a retrieval-augmented mechanism to assist in generating accurate change descriptions.</p><p>Zhanyu Wang et al. <ref type="bibr" target="#b396">[397]</ref> proposed the GPT4Video framework, which aims to enhance the capabilities of large language  <ref type="bibr" target="#b397">[398]</ref> proposed the Reason-and-Execute prompting method, which enhances the model's ability to solve geometric problems by combining reasoning templates and execution templates.</p><p>Xuechen Guo et al. <ref type="bibr" target="#b51">[52]</ref> proposed the LLaVA-Ultra model, which introduces a fine-grained visual encoder and an adaptive sampling module through architecture improvements, addressing the performance limitations of current multimodal large language models in medical visual question answering (Med-VQA).</p><p>Yi Bin et al. <ref type="bibr" target="#b398">[399]</ref> constructed the large-scale painting analysis dataset, PaintingForm, and proposed the GalleryGPT model. By fine-tuning for tasks focused on visual feature analysis, the model significantly improved the performance and generalization ability of art analysis.</p><p>Dan Kondratyuk et al. <ref type="bibr" target="#b399">[400]</ref> proposed VideoPoet, a zeroshot video generation model based on LLMs. It uses a decoder architecture to process multimodal inputs and enables highquality video synthesis, demonstrating the ability to generate complex dynamic scenes.</p><p>Yongshuo Zong et al. <ref type="bibr" target="#b400">[401]</ref> proposed post hoc and hybrid fine-tuning strategies to effectively enhance the safety of MLLMs, addressing the issues of harmful content generation and susceptibility to attacks in MLLMs.</p><p>Yang Jin et al. <ref type="bibr" target="#b401">[402]</ref> proposed the Video-LaVIT framework, which achieves efficient video decomposition using keyframes and motion vectors. This approach enables unified pretraining for video, image, and text, improving the safety and efficiency of MLLMs.</p><p>Long Qian et al. <ref type="bibr" target="#b402">[403]</ref> proposed the Momentor model, which incorporates a time-aware module and event-based sequence modeling to achieve fine-grained temporal understanding and video segment-level reasoning.</p><p>Zhisheng Zheng et al. <ref type="bibr" target="#b403">[404]</ref> designed the SPATIAL-AST encoder, which jointly performs sound event detection, spatial localization, and distance estimation. By integrating SPATIAL-AST with LLaMA-2, they constructed the BAT model, capable of answering questions about sound source relationships in 3D environments. The model utilizes a multi-stage training strategy to progressively enhance its spatial audio perception and reasoning capabilities.</p><p>Guangzhi Sun et al. <ref type="bibr" target="#b404">[405]</ref> proposed Video-SALMONN, the first unified model to simultaneously process video, speech, and music. They designed the MRC Q-Former structure to achieve multi-resolution information extraction, enhancing the ability of AV-LLMs to integrate speech information for comprehensive video content understanding.</p><p>Ling Li et al. <ref type="bibr" target="#b405">[406]</ref> introduced the concept of "localizability" to quantify street view images and filter high-quality data. They proposed the GeoReasoner model, which combines human reasoning knowledge and employs a two-stage fine-tuning approach to achieve geographic localization and reasoning, addressing the challenges of geographic localization in street view images.</p><p>Yunheng Li et al. <ref type="bibr" target="#b406">[407]</ref> proposed the Cascade-CLIP frame- work, which aligns multi-level visual features with text embeddings in a cascading manner. By introducing independent decoders to handle features at different levels, the framework enhances the transferability to new categories. This approach addresses the issue where the pre-trained model CLIP fails to fully leverage intermediate visual feature information in zeroshot semantic segmentation tasks. Zhijian Huang et al. <ref type="bibr" target="#b53">[54]</ref> proposed the RDA-Driver model, which ensures the consistency between reasoning and decisionmaking in MLLMs through reasoning-decision alignment constraints and a redesigned Chain-of-Thought (CoT) framework. This approach enhances the interpretability and performance of autonomous driving systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONTINUE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Non-Large Language Model Unimodal Continual Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Framework Innovation</head><p>Xiaoxue Han et al. <ref type="bibr" target="#b163">[164]</ref> proposed the TACO framework, which combines graph coarsening and continual learning to dynamically store information from previous tasks. They designed an efficient graph coarsening algorithm, RePro, based on node similarity, and introduced a node fidelity preservation strategy. The effectiveness of this approach in preventing the disappearance of minority classes was theoretically validated.</p><p>Ari S. Benjamin et al. <ref type="bibr" target="#b145">[146]</ref> proposed the Neural Tangent Ensemble (NTE) framework, which views a neural network as an ensemble of fixed experts. They derived its posterior update rule, which is equivalent to a specific form of stochastic gradient descent (SGD), offering a novel perspective for understanding and mitigating catastrophic forgetting.</p><p>Daehee Lee et al. <ref type="bibr" target="#b146">[147]</ref> proposed the IsCiL framework, which improves sample efficiency and task adaptability by incrementally learning shared skills. They introduced prototypebased skill retrieval and adapter learning to enable effective knowledge sharing across different tasks.</p><p>Kunlun Xu et al. <ref type="bibr" target="#b147">[148]</ref> proposed the CKP framework, which purifies data through the CDP and ILR modules, and filters out erroneous knowledge using the EKF algorithm. This approach addresses the performance degradation issue caused by incorrect labels in the Lifelong Person Re-Identification task.</p><p>Lei Liu et al. <ref type="bibr" target="#b148">[149]</ref> proposed the PBR framework, which operates without prior knowledge. It reduces forgetting and enhances long-tail continual learning performance through an uncertainty-guided sampling strategy and two prior-free constraints.</p><p>Yusong Hu et al. <ref type="bibr" target="#b149">[150]</ref> proposed the Task-Aware Orthogonal Sparse Network (OSN), which explores shared knowledge between old and new tasks through parameter sharing. They introduced sharpness-aware orthogonal projections to optimize the update of shared parameters and reduce interference with old tasks.</p><p>Daeun Lee et al. <ref type="bibr" target="#b66">[67]</ref> proposed the Mixture-of-Domain Low-rank Experts (MoDE) framework, which includes domainadaptive routing and domain-expert collaborative loss. This framework enables input-dependent online expert fusion, improving adaptation to new domains while preserving old knowledge.</p><p>Meng Ding et al. <ref type="bibr" target="#b407">[408]</ref> proposed a theoretical analysis framework for linear regression applicable to different parameterization scenarios. They revealed the impact of task sequences and algorithm parameters on forgetting and experimentally validated the theoretical findings.</p><p>Soochan Lee et al. <ref type="bibr" target="#b150">[151]</ref> proposed the SB-MCL framework, which achieves continual learning through sequential Bayesian updates. The neural network is fixed to prevent forgetting, and the framework is domain-and model-agnostic.</p><p>Mikel et al. <ref type="bibr" target="#b152">[153]</ref> proposed CompoNet, a modular neural network with linearly growing parameters. By combining strategies, it prevents forgetting while achieving efficient knowledge transfer and scalability.</p><p>Raymond L. Wang et al. <ref type="bibr" target="#b153">[154]</ref> proposed a Vector-HaSH-based neural model that combines hetero-associative memory and spatially invariant CNNs to enable fast learning and continual memory. They introduced the vHSN method, which utilizes attention mechanisms and grid encoding to prevent catastrophic forgetting and enhance generalization across different environments.</p><p>Jinglin Liang et al. <ref type="bibr" target="#b154">[155]</ref> proposed the DDDR framework, which utilizes diffusion models to generate historical data. By employing contrastive learning, the framework enhances the model's generalization ability on both generated and real data, addressing the issue of catastrophic forgetting in federated continual learning.</p><p>Fernando Julio Cendra et al. <ref type="bibr" target="#b155">[156]</ref> proposed the PromptCCD framework, which uses GMM as a prompting method to address the CCD problem. They introduced the GMP module, which dynamically generates prompts to adapt to new classes, accurate prediction of task IDs and learning of task-specific knowledge without the need for replay. This approach effectively prevents forgetting and improves classification accuracy.</p><p>Huiping Zhuang et al. <ref type="bibr" target="#b176">[177]</ref> proposed a forward online analytical learning method, F-OAL, which does not rely on backpropagation. It updates the linear classifier using recursive least squares, helping to alleviate the issue of catastrophic forgetting in online class-incremental learning.</p><p>Wuxuan Shi et al. <ref type="bibr" target="#b177">[178]</ref> proposed Prospective Representation Learning (PRL), which aligns reserved space and latent space to adapt new class features to the reserved space. This method balances new and old classes, improving performance in nonsample class-incremental learning.</p><p>Zitong Huang et al. <ref type="bibr" target="#b417">[418]</ref> proposed the ACIL task and CBS strategy, which implement class balancing through clustering and greedy selection, enhancing performance in incremental learning.</p><p>Xuze Hao et al. <ref type="bibr" target="#b178">[179]</ref> proposed the CIL-balanced classification loss and distribution margin loss to reduce classifier bias and enhance class separability. This approach addresses the issue of catastrophic forgetting in class-incremental learning for medical image classification.</p><p>Zhiwen Yang et al. <ref type="bibr" target="#b179">[180]</ref> proposed the DSSP method, which leverages domain sharing and task-specific prompt learning, with the S²-Adapter to adapt to deep space variations. This approach eliminates the need for sample replay and effectively mitigates catastrophic forgetting.</p><p>Shiye Wang et al. <ref type="bibr" target="#b418">[419]</ref> proposed Shared Parameter Subspace Learning, which combines momentum updates and an importance-aware mechanism, along with cross-domain contrast and orthogonality constraints, to capture cross-domain shared information and reduce forgetting.</p><p>Bowen Zheng et al. <ref type="bibr" target="#b180">[181]</ref> proposed the MRFA method, which optimizes the entire layer margin by enhancing the features of review samples. By increasing the margin, this approach helps reduce catastrophic forgetting.</p><p>Kishaan Jeeveswaran et al. <ref type="bibr" target="#b181">[182]</ref> proposed the DARE method, which reduces representation drift through a three-stage training process. They introduced the IRS strategy to optimize buffer sampling, thereby improving the model's performance on old tasks.</p><p>Dawei Zhou et al. <ref type="bibr" target="#b182">[183]</ref> proposed the EASE method, which constructs task-specific subspaces using lightweight adapters and synthesizes new features for old classes by leveraging semantic information. This approach effectively alleviates catastrophic forgetting.</p><p>Table <ref type="table" target="#tab_22">26</ref> shows the results of Truth Alignment ability for different methods on the CoIN benchmark. These methods include multitask training, zero-shot learning, and fine-tuning. The table lists the performance of each method on individual tasks, as well as the average performance across all tasks, including metrics such as MAA, and BWT.</p><p>Table <ref type="table" target="#tab_3">27</ref> presents the results of Reasoning Capability for different methods on the CoIN benchmark. Similar to Table <ref type="table" target="#tab_22">26</ref>, these results provide a comprehensive evaluation of the model's understanding and reasoning capabilities across different tasks.</p><p>Table <ref type="table">28</ref> explores the impact of different data volumes on MLLMs' instruction following ability on the CoIN benchmark. By randomly selecting varying proportions of samples from each dataset, Table <ref type="table">28</ref> illustrates how the volume of data affects the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Non-large Language Model Multimodal Continual Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Framework Innovation</head><p>Bo Yuan et al. <ref type="bibr" target="#b190">[191]</ref> proposed the CPP model for multi-task joint learning, which incorporates the CCE, TKD, and TPL mechanisms to achieve end-to-end multimodal general vision perception, significantly enhancing the efficiency of continual learning.</p><p>Yu Feng et al. <ref type="bibr" target="#b191">[192]</ref> proposed the CP-Prompt framework, which utilizes a dual-prompt strategy and parameter-efficient adjustments to achieve domain-specific knowledge extraction and inter-domain knowledge sharing, significantly reducing the forgetting rate.</p><p>Xianghu Yue et al. <ref type="bibr" target="#b192">[193]</ref> proposed the MMAL framework, which includes the modality fusion module and MSKC module. It effectively integrates audio-visual information without requiring samples, reducing forgetting and enhancing incremental learning performance.</p><p>Yuchu Yu et al. <ref type="bibr" target="#b419">[420]</ref> proposed a selective dual-teacher knowledge transfer framework, which utilizes unlabeled data to identify teacher networks, thereby ensuring knowledge retention and maintaining zero-shot capability.</p><p>Xiang Chen et al. <ref type="bibr" target="#b193">[194]</ref> proposed the MSPT framework, which optimizes multimodal learning through gradient modulation and attention distillation. It balances knowledge retention and new data integration, effectively mitigating catastrophic forgetting.</p><p>Jiazuo Yu et al. <ref type="bibr" target="#b420">[421]</ref> proposed a dynamic expansion framework based on MoE adapters and DDAS, enabling parameterefficient and zero-shot continual learning.</p><p>Yiwen Ye et al. <ref type="bibr" target="#b194">[195]</ref> proposed MedCoSS, a staged multimodal self-supervised learning framework that avoids modality conflicts. It introduces rehearsal strategies and feature distillation, effectively preventing catastrophic forgetting and enhancing knowledge retention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Method Innovation</head><p>Jieren Deng et al. <ref type="bibr" target="#b195">[196]</ref> proposed the ZiRa method, which effectively alleviates the challenge of adapting visual-language object detection models to new domains while retaining zeroshot generalization capabilities in incremental learning. This is achieved through zero-interference loss and a reparameterized dual-branch structure, without increasing memory burden.</p><p>Tao Jin et al. <ref type="bibr" target="#b421">[422]</ref> proposed a historical prompt calibration strategy, which includes intra-modal correlation estimation and inter-modal consistency alignment to calibrate prompts in pre-trained models. This enhances the task and modality relationships, addressing the issues of task unfamiliarity and modality heterogeneity in multimodal continual learning.</p><p>Jaewoo Lee et al. <ref type="bibr" target="#b196">[197]</ref> proposed a localized patch importance scoring method, emphasizing the semantic interweaving of audio-visual patches. The replay-guided relevance assessment reduces forgetting of previously learned knowledge.</p><p>Longrong Yang et al. <ref type="bibr" target="#b197">[198]</ref> proposed the RCS-Prompt method, which reduces category space overlap and establishes clear boundaries between sessions through bidirectional prompt optimization and prompt magnitude normalization. This addresses the issue of overlap between old and new category spaces in continual learning.</p><p>Zangwei Zheng et al. <ref type="bibr" target="#b198">[199]</ref> proposed the ZSCL method, which mitigates forgetting through feature space distillation and parameter space weight integration. Kaiyang Zhou et al. <ref type="bibr" target="#b199">[200]</ref> proposed the CoCoOp method, which generates dynamic prompts using a lightweight neural network to enhance model generalization. This addresses the issue of insufficient zero-shot generalization to unseen categories when pre-trained vision-language models adapt to new tasks.</p><p>Martin Menabue et al. <ref type="bibr" target="#b422">[423]</ref> proposed a dual-level prompt mechanism and semantic residual prompts, combined with multimodal generative replay, to enhance the stability and adaptability of models in continual learning.</p><p>Yicheng Xu et al. <ref type="bibr" target="#b200">[201]</ref> proposed the RAIL method, which uses recursive ridge regression and a no-training fusion module, along with the introduction of the X-TAIL setup, aiming to address the challenge of improving cross-domain classification  capabilities in vision-language models during continual learning. Linlan Huang et al. <ref type="bibr" target="#b423">[424]</ref> proposed an adaptive representation adjustment and parameter fusion method, which adjusts the representations of old categories affected by new categories using text features. Additionally, they employ a decompositionbased parameter fusion strategy to reduce forgetting.</p><p>Through continuously innovative frameworks and methods, multimodal continual learning in non-large models has achieved a certain level of effective integration and learning across different modalities. However, with the diversification of data types and application scenarios, non-large model multimodal continual learning will face more complex tasks and dynamic environments, necessitating more flexible and efficient solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Continual Learning in Large Language Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Model Innovation</head><p>Yeongbin Seo et al. <ref type="bibr" target="#b244">[245]</ref> proposed the TAALM method, which uses meta-learning to dynamically predict token importance, enabling targeted knowledge updates and reducing forgetting.</p><p>Haoran Que et al. <ref type="bibr" target="#b245">[246]</ref> proposed the D-CPT Law and Cross-Domain D-CPT Law, which predict the optimal training ratio to address the issue of selecting the mixed corpus ratio during continual pre-training of large language models. These methods reduce GPU resource consumption and improve domain adaptability.</p><p>Srikanth Malla et al. <ref type="bibr" target="#b246">[247]</ref> proposed the COPAL algorithm, which enables continual pruning without the need for retraining, thereby avoiding model retraining. This solution addresses the high computational demands and model adaptability limitations faced by large language models when adapting to new domains.</p><p>Daniel Marczak et al. <ref type="bibr" target="#b247">[248]</ref> proposed the MagMax method, which achieves effective cross-task knowledge integration through sequential fine-tuning and maximum magnitude weight selection. This approach mitigates the problem of catastrophic forgetting of old knowledge in large pre-trained models during continual learning, enabling adaptation to the continuously evolving data stream.</p><p>Weixiang Zhao et al. <ref type="bibr" target="#b248">[249]</ref> proposed the SAPT framework, which aligns the learning and selection of PET blocks through a shared attention mechanism. They introduced the ARM module to recall old tasks using pseudo-samples, enabling effective knowledge retention and transfer.</p><p>Jianheng Huang et al. <ref type="bibr" target="#b249">[250]</ref> proposed the SSR framework, which utilizes LLM-generated synthetic instances for rehearsal. This approach effectively mitigates forgetting, improves data</p><p>9 CONTINUAL LEARNING IN MULTIMODAL LARGE LANGUAGE MODEL 9.1 Benchmark 9.1.1 CoIN: Continual Instruction Tuning Benchmark</p><p>MLLMs adapt to new tasks and users' evolving needs through tuning. However, these models face challenges in adapting to the constantly changing knowledge requirements of users. To address this, Cheng Chen et al. <ref type="bibr" target="#b93">[94]</ref> proposed the CoIN benchmark to evaluate MLLMs' performance under the sequential instruction tuning paradigm. They also introduced the MoELoRA method to help MLLMs retain previous instruction alignment, reducing catastrophic forgetting.</p><p>CoIN consists of 10 commonly used datasets, covering 8 different task categories, ensuring diversity in both instructions and tasks. Table <ref type="table" target="#tab_21">25</ref> provides a detailed list of the datasets included in the CoIN benchmark, along with their corresponding instruction types, training sample sizes, and test sample sizes. The datasets cover a variety of task types, including Referring Expression Comprehension (REC), Classification, Image Question Answering (IQA), and Knowledge Grounded IQA, among others. Each task has two versions of instructions, Type1 and Type2, to ensure the diversity and comprehensiveness of the evaluation.</p><p>Furthermore, CoIN evaluates MLLMs from two perspectives: 1) Truth Alignment. The ability to generate the correct result in the desired format to follow task instruc-tion is the basic requirement for instruction tuning. 2) Reasoning Capability. The performance of MLLMs depends not only on the instruction following but also on the knowledge maintained in MLLMs. Three metrics are used to measure the performance of MLLMs: 1) Backward Transfer (BWT): Measures the catastrophic forgetting that occurs after learning all tasks. 2) Mean Average Accuracy (MAA): Assesses the model's performance throughout the entire training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">CliMB: The Continual Learning in Multimodality Benchmark</head><p>Existing multimodal large language models are typically finetuned separately for each downstream task, requiring a new model to be fine-tuned and stored for each task. In contrast, multitask learning involves training on a fixed set of tasks, but it cannot dynamically learn new tasks. To address this, Tejas Srinivasan et al. <ref type="bibr" target="#b94">[95]</ref> proposed the CLiMB benchmark, designed to study the continual learning challenges faced by multimodal large models in multimodal tasks and to systematically evaluate how upstream continual learning can quickly generalize to new multimodal and unimodal tasks. The CLiMB benchmark includes vision-and-language input tasks, such as VQAv2, NLVR2, SNLI-VE, and VCR. Additionally, the evaluation phase of the CLiMB benchmark includes: 1) Upstream Continual Learning: The model is trained on a series of vision-language tasks, and its ability to forget old tasks and transfer knowledge to new tasks is evaluated after each task. 2) Downstream Low-Shot Transfer: After training on upstream tasks, the model's adaptability to new multimodal and unimodal tasks with limited samples is assessed.</p><p>Table <ref type="table" target="#tab_25">29</ref> presents the results of different continual learning algorithms for multimodal large models in upstream multimodal task learning. It compares the upstream knowledge transfer (T UK (i)) relative to direct fine-tuning, along with the task scores</p><formula xml:id="formula_0">[S i A ].</formula><p>Table <ref type="table" target="#tab_0">30</ref> presents the Forgetting Transfer results for six continual learning algorithms applied to multimodal large models. It shows the performance degradation on previous tasks after training on subsequent tasks, indicating the extent of catastrophic forgetting.</p><p>Table <ref type="table" target="#tab_26">31</ref> illustrates the impact of different upstream task sequences on the upstream knowledge forgetting of multimodal large models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.3">COAST: Continual Instruction Tuning Benchmark</head><p>An ideal MLLM should be able to continuously adjust to new tasks in the face of task flow distributions across different domains, new capabilities, and new datasets, while minimizing forgetting of prior knowledge. However, most existing MLLMs are limited to single-task adaptation and lack performance evaluation standards for continual learning of new tasks. To comprehensively assess MLLMs' continual learning performance across different domains, capabilities, and datasets, Meng Cao et al. <ref type="bibr" target="#b95">[96]</ref> proposed the COAST benchmark. COAST includes three incremental learning settings: 1) Domain-incremental: Simulates scenarios where MLLMs continuously adapt to different domains. Capability-incremental: Evaluates the ability of MLLMs to progressively acquire and integrate new capabilities.</p><p>2) Dataset-incremental: Assesses the ability of MLLMs to adapt to and generalize across varying dataset distributions. 3) By chaining and reusing existing benchmark tests, the COAST benchmark creates a streaming task distribution to evaluate the performance of MLLMs when continually learning new tasks.</p><p>Table <ref type="table" target="#tab_27">32</ref> presents the average accuracy (Avg.↑) and average forgetting rate (Fgt.↓) of different continual learning methods under the COAST-domain setting. These results reflect the performance of multimodal large models on new tasks and their ability to retain performance on previous tasks while learning new ones.</p><p>Table <ref type="table" target="#tab_28">33</ref> presents the performance of different methods on the continual instruction tuning tasks under the COASTcapability setting, focusing on the ability of MLLMs to acquire and integrate new capabilities. The table categorizes tasks into Conv. (Conversation), Desc. (Detail Description), Reason (Complex Reasoning), and Ref. (Referring qa).</p><p>Table <ref type="table" target="#tab_0">34</ref> presents the performance of various methods on the continual instruction tuning task under the COASTdataset setting, evaluating the ability of MLLMs to adapt to and generalize across dataset distributions. The terms "SciQA," "Text," "ImgNet," "GQA," "Viz," "REC," "VQA," and "OCR" in the table represent different visual question answering datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.4">ViLCo-Bench: Video Language Continual learning Benchmark</head><p>Multimodal large models in the domain of video-language continual learning involve the continuous adaptation to information from both video and text inputs, enhancing the model's ability to handle new tasks while retaining previous knowledge. This is a relatively under-explored field, and establishing appropriate benchmarks is crucial to promoting communication and research in this area. To address this, Tianqi Tang et al. <ref type="bibr" target="#b96">[97]</ref> proposed the first benchmark specifically designed for video-language continual learning in multimodal large models, called ViLCo-Bench. This benchmark aims to evaluate continual learning models across a range of video-text tasks.</p><p>ViLCo-Bench includes three unique video-language tasks: 1) Moment Queries (MQ). 2) Natural Language Queries (NLQ).</p><p>3) Visual Queries (VQ). These tasks require the model to understand video content and retrieve relevant segments of the video based on language queries. Table <ref type="table" target="#tab_0">35</ref> presents the results of different continual learning methods on the MQ task. The evaluation used Average Recall, including R@1 and R@5 (IoU=0.3 and IoU=0.5), to measure the model's performance at different Intersection over Union (IoU) thresholds.</p><p>Table <ref type="table" target="#tab_0">36</ref> presents the results of various continual learning methods on the NLQ task. The NLQ task is more complex than the MQ task, as language queries are not limited to human activities but involve open-vocabulary descriptions.</p><p>Table <ref type="table" target="#tab_3">37</ref> presents the results of various continual learning methods on the VQ task. The VQ task requires the system to understand the visual content of the queried image. tAP (temporal Average Precision) is used as the performance metric, which measures the distance between predicted and true locations in continuous tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Framework Innovation</head><p>Jiazuo Yu et al. <ref type="bibr" target="#b263">[264]</ref> introduced the Adapter-in-Adapter framework to enhance modality alignment and collaboration. They also proposed a flexible and scalable framework, PathWeave, which incorporates modality path switching and expansion capabilities. This allows MLLMs to continuously evolve on the modality used for X-modality reasoning, addressing the high computational burden when expanding to new modalities and reducing the dependency on large-scale joint pre-training. Saurav Jha et al. <ref type="bibr" target="#b90">[91]</ref> proposed the CLAP framework, which enhances the model's generalization ability and reduces forgetting through probabilistic fine-tuning. It is compatible with various prompt methods and strengthens the model's uncertainty estimation capabilities.</p><p>Longxiang Tang et al. <ref type="bibr" target="#b264">[265]</ref> proposed the DIKI framework, which efficiently preserves pre-trained knowledge through a residual mechanism and distribution-aware calibration. This approach addresses the problem of forgetting pre-trained knowledge in MLLMs during domain-category incremental learning, maintaining a balance between the model's adaptability to new tasks and the retention of old knowledge.</p><p>Xusheng Cao et al. <ref type="bibr" target="#b265">[266]</ref> proposed the GMM framework based on multimodal large models, which implements incremental learning through generated label text and feature matching. This approach reduces bias toward the current task and effectively minimizes forgetting.</p><p>Keon-Hee Park et al. <ref type="bibr" target="#b266">[267]</ref> proposed the PriViLege framework, which effectively addresses catastrophic forgetting and overfitting in MLLMs through prompt functionality and knowledge distillation.</p><p>Fanhu Zeng et al. <ref type="bibr" target="#b267">[268]</ref> proposed the ModalPrompt framework, which implements continuous learning without data replay through bi-modal guided prototype prompts and knowledge transfer. This approach addresses the issue of forgetting old tasks when large multimodal models sequentially learn new tasks.</p><p>Emanuele Frascaroli et al. <ref type="bibr" target="#b268">[269]</ref> proposed the CGIL framework, which combines prompt learning and latent generative replay. It uses VAEs to learn class-conditioned distributions and generate synthetic samples, effectively addressing the issue of catastrophic forgetting in multimodal large models during continual learning.</p><p>Yukun Li et al. <ref type="bibr" target="#b269">[270]</ref> proposed the CoLeCLIP framework, which enhances the performance of multimodal large models in open-domain continual learning through joint learning of task prompts and cross-domain vocabularies. It achieves crossdomain vocabulary learning, maintaining a unified semantic space for multimodal large models, and reduces interference between tasks. The framework introduces task prompt learning, addressing domain differences and category associations, thereby improving the model's adaptability and discriminative ability for new tasks.</p><p>Biqing Qi et al. <ref type="bibr" target="#b99">[100]</ref> proposed the ICL framework, which combines Vision Transformers (ViT) and MLLMs. By enabling interaction between a fast intuition model and a slow deep thinking model, the framework enhances the efficiency of continual learning in multimodal large language models.</p><p>Yuexiang Zhai et al. <ref type="bibr" target="#b270">[271]</ref> proposed the EMT framework to evaluate catastrophic forgetting in MLLMs. They found that moderate fine-tuning can improve continual learning performance, but excessive fine-tuning leads to a decline in performance and the emergence of hallucinations. This offers a new perspective for improving fine-tuning strategies in MLLMs.</p><p>Xiong Wang et al. <ref type="bibr" target="#b98">[99]</ref> proposed the Freeze-Omni model, which implements a three-stage training strategy to enable speech input-output capabilities without unfreezing the LLM parameters. This approach addresses the issue of catastrophic forgetting when integrating the speech modality into multimodal LLMs, preserving the LLM's intelligence level and enabling low-latency speech-to-speech conversations.</p><p>Adyasha Maharana et al. <ref type="bibr" target="#b271">[272]</ref> proposed the Adapt-inf ty framework, which optimizes model learning efficiency and reduces computational burden through dynamic data selection and a clustering-based permanent pruning strategy. This approach effectively mitigates catastrophic forgetting in multimodal large models.</p><p>Gen Luo et al. <ref type="bibr" target="#b272">[273]</ref> proposed Mono-InternVL, which integrates visual experts using a mixture-of-experts structure without altering the pre-trained language model. By introducing endogenous visual pretraining, it enables progressive learning of visual knowledge from noise to high-quality data through incremental learning, effectively preventing forgetting. This approach addresses the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of multimodal large language models. Shanshan Zhong et al. <ref type="bibr" target="#b273">[274]</ref> proposed the MoExtend framework, which expands modality capabilities without adjusting the pre-trained model by integrating new experts. They designed a three-stage training process, including alignment, extension, and fine-tuning, to enable rapid modality adaptation. Additionally, they introduced an image localization score as a new scoring function to optimize multimodal sample selection. This approach addresses the issues of catastrophic forgetting and high training costs that arise when large language models are extended to multimodal tasks, particularly in the visuallanguage understanding domain.</p><p>Artemis Panagopoulou et al. <ref type="bibr" target="#b88">[89]</ref> addressed the challenges faced by multimodal large language models in continual learning, particularly in self-supervised pretraining environments. They focused on how to effectively integrate and reason across knowledge from different modalities to overcome the performance limitations of traditional methods when handling multimodal data. They proposed the HiDe-Prompt framework, which is an scalable solution designed to align multiple modal-   while reducing its size. This approach addresses the issues of overfitting and catastrophic forgetting in MLLMs during downstream tasks, as well as the high storage and inference costs associated with large models.</p><p>Noranart Vesdapunt et al. <ref type="bibr" target="#b92">[93]</ref> proposed HVCLIP, which transforms CLIP features into a high-dimensional vector space. Through strategies such as forgetting reduction, discrepancy reduction, and feature enhancement, HVCLIP addresses the catastrophic forgetting issue encountered during fine-tuning of MLLM pre-trained models like CLIP in unsupervised domain adaptation. This approach helps mitigate the loss of pre-trained knowledge, enhancing the model's ability to retain critical information while adapting to new tasks or domains.</p><p>Meng Cao et al. <ref type="bibr" target="#b95">[96]</ref> proposed a parameter-efficient tuning method that does not require rehearsal. This approach constructs intrinsic and contextual incremental embeddings to encode task-specific features and inter-task dependencies. By doing so, the model can continuously adapt to new tasks while retaining prior knowledge. This significantly alleviates the catastrophic forgetting problem in MLLMs, enhancing their ability to preserve knowledge from previous tasks while accommodating new ones. Shikhar Srivastava et al. <ref type="bibr" target="#b437">[438]</ref> proposed and evaluated five MLLM continual learning methods aimed at mitigating linguistic forgetting. Their findings revealed that the bestperforming method significantly enhanced both language and vision task performance while maintaining multimodal accuracy.</p><p>Jingyang Qiao et al. <ref type="bibr" target="#b298">[299]</ref> proposed the LLaCA method, which dynamically adjusts the EMA weights to reduce forgetting and introduces an approximation mechanism to lower computational costs, thereby addressing the issue of catastrophic forgetting in MLLMs when learning new tasks. soft prompt method, which adapts to new tasks by adjusting input prompts rather than the entire model parameters. This approach enhances the performance and domain adaptability multimodal large language models in continual learning.</p><p>Runqi Wang et al. <ref type="bibr" target="#b306">[307]</ref> proposed an non-incremental learning method based on CLIP, called AttriCLIP. This method adapts to new tasks using an attribute lexicon and textual prompts, without the need for additional memory data, thereby enhancing the generalization and continual learning capabilities of multimodal large models in multimodal tasks.</p><p>Shipeng Yan et al. <ref type="bibr" target="#b234">[235]</ref> introduced pseudo-text replay and multimodal knowledge distillation to enhance negative sample diversity, align predictions between old and new models, and improve the performance of multimodal large models in multimodal continual learning tasks.</p><p>Andrea Cossu et al. <ref type="bibr" target="#b54">[55]</ref> explored how multimodal large language models can reduce catastrophic forgetting in continual learning environments through continuous pretraining, while maintaining adaptability to new knowledge. They demonstrated the advantages of self-supervised pretraining in preserving old knowledge and proposed effective pretraining strategies.</p><p>James Seale Smith et al. <ref type="bibr" target="#b307">[308]</ref> proposed the C-LoRA method, which effectively mitigates catastrophic forgetting by performing continual adaptive low-rank adjustments in the cross-attention layers of multimodal large models. This approach adapts to new concepts through a self-regulating mechanism while preserving knowledge of old concepts.</p><p>Tao He et al. <ref type="bibr" target="#b97">[98]</ref> introduced a lifelong scene graph generation task and a knowledge-aware contextual prompt learning strategy, enabling the model to effectively retain old knowledge in incremental learning. This approach addresses the issue of updating and forgetting old and new knowledge in multimodal large models during scene graph generation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Timeline of Multimodal Large Model Development.</figDesc><graphic coords="2,73.80,43.70,464.39,257.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Discriminative Domain Evaluation Metric (ConScore[D]): Assesses consistency based on the accuracy of the model's answers to discriminative questions. 2)Generative Domain Evaluation Metric (ConScore[C]): Evaluates consistency by comparing the coherence between the model-generated captions and the discriminative answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 : 2 Fig. 3 :</head><label>223</label><figDesc>Fig.2: Statistics of the CVQA Benchmark.<ref type="bibr" target="#b32">[33]</ref> </figDesc><graphic coords="25,58.32,43.69,495.35,247.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Overview of 19 evaluation detailed categories in ConBench.<ref type="bibr" target="#b346">[347]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 3 :</head><label>3</label><figDesc>Innovations in Non-LLM Unimodal CL Frameworks.</figDesc><table><row><cell>Framework</cell><cell>Starting point of the problem</cell><cell>How to solve</cell></row><row><cell>NTE</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 7 :</head><label>7</label><figDesc>Innovations in MLModel CL Frameworks.</figDesc><table><row><cell>Framework</cell><cell>Starting point of the problem</cell><cell>How to solve</cell></row><row><cell>PathWeave</cell><cell></cell><cell></cell></row><row><cell>[264]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 9 :</head><label>9</label><figDesc>Averaged accuracy of baselines on the In-the-Wild, Homogeneous, and Heterogeneous splits.</figDesc><table><row><cell></cell><cell cols="3">Default Multi-Object</cell><cell cols="3">Student-Forcing</cell><cell cols="3">Teacher-Forcing</cell><cell cols="2">Single-Object</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Wild Hom.</cell><cell cols="3">Het. Wild Hom.</cell><cell cols="3">Het. Wild Hom.</cell><cell cols="3">Het. Wild Hom.</cell><cell>Het.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Seen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yi-VL-6B [325]</cell><cell>2.95</cell><cell>5.65</cell><cell>1.99</cell><cell>3.44</cell><cell>6.80</cell><cell>3.78</cell><cell>5.45</cell><cell>26.25</cell><cell>4.36</cell><cell>0.19</cell><cell>0.30</cell><cell>0.13</cell></row><row><cell>Yi-VL-34B [325]</cell><cell>8.50</cell><cell>15.35</cell><cell>3.33</cell><cell>8.97</cell><cell>16.30</cell><cell cols="2">4.23 10.09</cell><cell>19.75</cell><cell>4.94</cell><cell>0.22</cell><cell>2.60</cell><cell>0.13</cell></row><row><cell>LLaVA-7B [141]</cell><cell>31.29</cell><cell>67.50</cell><cell cols="2">8.00 31.28</cell><cell cols="3">67.25 11.22 31.49</cell><cell cols="3">92.15 12.37 35.32</cell><cell cols="2">62.35 17.37</cell></row><row><cell>LLaVA-13B [141]</cell><cell>31.54</cell><cell cols="3">67.63 12.64 31.49</cell><cell cols="3">73.25 11.54 34.97</cell><cell cols="3">94.25 16.03 43.13</cell><cell cols="2">80.60 23.91</cell></row><row><cell>LLaVA-34B [141]</cell><cell>39.95</cell><cell cols="3">85.75 18.85 52.75</cell><cell cols="3">85.20 33.91 56.41</cell><cell cols="3">95.81 25.31 55.05</cell><cell cols="2">86.50 18.97</cell></row><row><cell>Qwen VL [278]</cell><cell>2.73</cell><cell>6.60</cell><cell>1.03</cell><cell>6.25</cell><cell>16.00</cell><cell cols="2">3.65 18.74</cell><cell>71.50</cell><cell>5.45</cell><cell>8.73</cell><cell>16.05</cell><cell>5.58</cell></row><row><cell>Qwen VL-C [278]</cell><cell>8.72</cell><cell>16.90</cell><cell>6.67</cell><cell>5.26</cell><cell>8.60</cell><cell cols="2">4.10 12.11</cell><cell>47.75</cell><cell cols="2">8.08 25.99</cell><cell cols="2">43.40 13.21</cell></row><row><cell>CogVLM [291]</cell><cell>0.04</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.10</cell><cell>0.95</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>CogVLM-G [291]</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>9.86</cell><cell>13.50</cell><cell cols="2">6.79 22.64</cell><cell>75.45</cell><cell cols="2">0.45 11.25</cell><cell>22.65</cell><cell>7.12</cell></row><row><cell>CogVLM-C [291]</cell><cell>12.89</cell><cell>22.75</cell><cell cols="2">7.18 25.37</cell><cell cols="3">43.63 12.03 28.25</cell><cell cols="3">72.80 17.50 30.16</cell><cell cols="2">56.00 16.35</cell></row><row><cell>LLaVA-7B [141]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>9.16</cell><cell>16.40</cell><cell>5.51</cell><cell>-</cell><cell>-</cell><cell cols="2">-11.68</cell><cell>23.55</cell><cell>9.36</cell></row><row><cell>GLaMM [326]</cell><cell>-</cell><cell>-</cell><cell cols="2">-27.11</cell><cell cols="2">53.35 13.01</cell><cell>-</cell><cell>-</cell><cell cols="2">-63.81</cell><cell cols="2">81.75 53.40</cell></row><row><cell>GroundHOG [318]</cell><cell>-</cell><cell>-</cell><cell cols="2">-23.57</cell><cell cols="2">30.80 24.23</cell><cell>-</cell><cell>-</cell><cell cols="2">-44.80</cell><cell cols="2">43.10 38.97</cell></row><row><cell>IDEFICS [327]</cell><cell>0.00</cell><cell>1.45</cell><cell>0.13</cell><cell>6.25</cell><cell>18.70</cell><cell cols="2">0.64 17.37</cell><cell cols="2">76.15 10.06</cell><cell>4.62</cell><cell>0.00</cell><cell>0.32</cell></row><row><cell>IDEFICS [327]</cell><cell>0.00</cell><cell>1.45</cell><cell>0.13</cell><cell>6.25</cell><cell>18.70</cell><cell cols="2">0.64 17.37</cell><cell cols="2">76.15 10.06</cell><cell>4.62</cell><cell>0.00</cell><cell>0.32</cell></row><row><cell>CogVLM-2 [291]</cell><cell>21.51</cell><cell cols="3">37.55 17.31 37.02</cell><cell cols="3">70.85 12.69 37.10</cell><cell cols="3">73.50 17.44 21.16</cell><cell cols="2">38.75 13.65</cell></row><row><cell>MiniCPM-V [328]</cell><cell>34.75</cell><cell cols="3">59.91 17.37 31.62</cell><cell cols="3">62.80 13.65 32.16</cell><cell cols="3">68.05 16.79 27.42</cell><cell cols="2">55.35 16.92</cell></row><row><cell>GPT-4V [276]</cell><cell>53.80</cell><cell cols="2">77.55 40.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-55.89</cell><cell cols="2">78.25 41.03</cell></row><row><cell>GPT-4O [329]</cell><cell>71.27</cell><cell cols="2">89.25 66.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-60.77</cell><cell cols="2">73.92 54.31</cell></row><row><cell>LLaVA-7B [141]</cell><cell>21.26</cell><cell>52.40</cell><cell>7.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-30.59</cell><cell cols="2">60.85 12.69</cell></row><row><cell>+OPERA [330]</cell><cell>24.07</cell><cell>58.65</cell><cell>7.35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-30.44</cell><cell cols="2">60.85 13.27</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unseen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yi-VL-6B [325]</cell><cell>2.74</cell><cell>3.88</cell><cell>1.14</cell><cell>3.18</cell><cell>4.24</cell><cell>5.20</cell><cell>4.04</cell><cell cols="2">10.90 10.57</cell><cell>0.14</cell><cell>0.45</cell><cell>0.08</cell></row><row><cell>Yi-VL-34B [325]</cell><cell>7.77</cell><cell>15.63</cell><cell cols="2">4.23 10.28</cell><cell>18.04</cell><cell cols="2">7.97 11.24</cell><cell cols="2">22.49 12.03</cell><cell>0.46</cell><cell>2.37</cell><cell>0.41</cell></row><row><cell>LLaVA-7B [141]</cell><cell>30.56</cell><cell cols="3">68.12 10.33 30.55</cell><cell cols="3">68.16 10.24 31.89</cell><cell cols="3">90.33 13.25 34.88</cell><cell cols="2">64.41 16.18</cell></row><row><cell>LLaVA-13B [141]</cell><cell>27.56</cell><cell>63.10</cell><cell cols="2">8.37 27.41</cell><cell>63.10</cell><cell cols="2">8.37 35.65</cell><cell cols="3">91.09 14.80 42.66</cell><cell cols="2">71.92 23.41</cell></row><row><cell>LLaVA-34B [141]</cell><cell>29.30</cell><cell cols="3">79.43 17.72 29.45</cell><cell cols="3">91.18 14.39 37.40</cell><cell cols="3">95.51 17.92 51.71</cell><cell cols="2">77.88 30.81</cell></row><row><cell>Qwen VL [278]</cell><cell>2.80</cell><cell>1.95</cell><cell>7.06</cell><cell>7.17</cell><cell>16.41</cell><cell cols="2">4.15 10.34</cell><cell>58.00</cell><cell cols="2">4.07 17.73</cell><cell>31.22</cell><cell>9.51</cell></row><row><cell>Qwen VL-C [278]</cell><cell>18.86</cell><cell>30.73</cell><cell cols="2">8.78 16.16</cell><cell>27.80</cell><cell cols="2">7.72 21.81</cell><cell cols="3">58.00 11.14 34.20</cell><cell cols="2">57.31 15.37</cell></row><row><cell>CogVLM [291]</cell><cell>0.03</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.15</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>CogVLM-G [291]</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>8.20</cell><cell>1.47</cell><cell cols="2">5.77 23.82</cell><cell>81.20</cell><cell cols="2">1.81 10.32</cell><cell>10.74</cell><cell>9.11</cell></row><row><cell>CogVLM-C [291]</cell><cell>15.56</cell><cell>26.57</cell><cell cols="2">5.53 17.18</cell><cell>41.27</cell><cell cols="2">6.02 22.81</cell><cell>56.04</cell><cell cols="2">6.67 30.56</cell><cell cols="2">52.00 13.50</cell></row><row><cell>LLaVA-7B [141]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.59</cell><cell>12.12</cell><cell>4.88</cell><cell>-</cell><cell>-</cell><cell cols="2">-12.71</cell><cell>22.49</cell><cell>8.46</cell></row><row><cell>GLaMM [326]</cell><cell>-</cell><cell>-</cell><cell cols="2">-29.11</cell><cell cols="2">54.53 14.23</cell><cell>-</cell><cell>-</cell><cell cols="2">-68.65</cell><cell cols="2">77.06 52.28</cell></row><row><cell>GroundHOG [318]</cell><cell>-</cell><cell>-</cell><cell cols="2">-23.11</cell><cell cols="2">24.69 26.26</cell><cell>-</cell><cell>-</cell><cell cols="2">-40.73</cell><cell cols="2">30.37 38.13</cell></row><row><cell>IDEFICS [327]</cell><cell>0.39</cell><cell>0.37</cell><cell>0.33</cell><cell>9.03</cell><cell>24.45</cell><cell cols="2">2.68 24.80</cell><cell>83.02</cell><cell>7.64</cell><cell>4.62</cell><cell>3.67</cell><cell>6.50</cell></row><row><cell>CogVLM-2 [291]</cell><cell>20.99</cell><cell cols="3">35.06 15.93 24.64</cell><cell cols="3">38.04 23.17 26.74</cell><cell cols="3">46.04 26.59 11.13</cell><cell>30.94</cell><cell>5.77</cell></row><row><cell>MiniCPM-V [328]</cell><cell>32.96</cell><cell cols="3">59.92 16.60 31.77</cell><cell cols="3">58.98 14.15 31.87</cell><cell cols="3">60.98 16.34 25.56</cell><cell cols="2">47.76 14.39</cell></row><row><cell>GPT-4V [276]</cell><cell>45.46</cell><cell cols="2">63.12 34.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-47.34</cell><cell cols="2">64.94 35.45</cell></row><row><cell>GPT-4O [329]</cell><cell>63.27</cell><cell cols="2">80.29 54.47</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-63.45</cell><cell cols="2">79.84 53.74</cell></row><row><cell>LLaVA-7B [141]</cell><cell>13.96</cell><cell>31.88</cell><cell>3.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-26.95</cell><cell cols="2">54.41 11.06</cell></row><row><cell>+OPERA [330]</cell><cell>13.20</cell><cell>37.14</cell><cell>3.82</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-27.90</cell><cell cols="2">56.69 11.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 10 :</head><label>10</label><figDesc>Average performance of MLLMs on our CVQA dataset with English prompts (EN) and local language prompts (LOC).<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell cols="11">LLaVA-1.5-7B [141]M-CLIP [331]CLIP [188]mBLIP-mT0 [332]mBLIP-BLOOMZ [332]InstructBLIP [333]Gemini-1.5-Flash [334]GPT-4o [329]</cell></row><row><cell>EN</cell><cell>LOC</cell><cell>EN LOC EN LOC EN</cell><cell>LOC</cell><cell>EN</cell><cell>LOC</cell><cell>EN</cell><cell>LOC</cell><cell>EN</cell><cell>LOC</cell><cell>EN LOC</cell></row><row><cell>49.6</cell><cell>35.5</cell><cell>38.0 33.7 42.7 30.6 31.3</cell><cell>30.9</cell><cell>39.3</cell><cell>32.7</cell><cell>49.0</cell><cell>31.9</cell><cell>66.9</cell><cell>68.5</cell><cell>75.4 74.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 11 :</head><label>11</label><figDesc>LLaVA-1.5-7B and InstructBLIP results on various VQA datasets.<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell>Model</cell><cell cols="7">VQAv2 GQA VizWiz SciQA-IMG TextVQA CVQA (EN) CVQA (LOC)</cell></row><row><cell>LLaVA-1.5-7B [141]</cell><cell>78.5</cell><cell>62.0</cell><cell>50.0</cell><cell>66.8</cell><cell>58.2</cell><cell>48.9</cell><cell>36.5</cell></row><row><cell>InstructBLIP [333]</cell><cell>-</cell><cell>49.2</cell><cell>34.5</cell><cell>60.5</cell><cell>50.1</cell><cell>47.8</cell><cell>32.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 12 :</head><label>12</label><figDesc>Accuracy of models across categories.<ref type="bibr" target="#b32">[33]</ref> </figDesc><table><row><cell>Categories</cell><cell cols="2">LLaVA-1.5-7B [141] EN LOC</cell><cell cols="2">M-CLIP [331] EN LOC</cell><cell cols="2">CLIP [188] EN LOC</cell><cell cols="2">mBLIP-mT0 [332] EN LOC</cell><cell cols="2">mBLIP-BLOOMZ [332] EN LOC</cell><cell cols="2">InstructBLIP [333] EN LOC</cell></row><row><cell>Brands</cell><cell>49.9</cell><cell>36.5</cell><cell>37.2</cell><cell>35.7</cell><cell>36.6</cell><cell>29.7</cell><cell>33.7</cell><cell>30.8</cell><cell>40.5</cell><cell>35.1</cell><cell>48.4</cell><cell>32.6</cell></row><row><cell>Food</cell><cell>45.4</cell><cell>31.9</cell><cell>34.5</cell><cell>29.1</cell><cell>39.2</cell><cell>30.4</cell><cell>28.1</cell><cell>27.6</cell><cell>37.7</cell><cell>29.8</cell><cell>44.4</cell><cell>30.6</cell></row><row><cell>Geography</cell><cell>47.1</cell><cell>38.2</cell><cell>37.1</cell><cell>34.2</cell><cell>41.8</cell><cell>31.9</cell><cell>30.6</cell><cell>31.6</cell><cell>35.0</cell><cell>32.3</cell><cell>45.3</cell><cell>33.2</cell></row><row><cell>Objects</cell><cell>51.8</cell><cell>33.0</cell><cell>39.4</cell><cell>34.5</cell><cell>39.7</cell><cell>25.4</cell><cell>34.3</cell><cell>33.0</cell><cell>43.1</cell><cell>34.0</cell><cell>52.3</cell><cell>29.1</cell></row><row><cell>People</cell><cell>58.9</cell><cell>38.1</cell><cell>45.0</cell><cell>37.8</cell><cell>46.8</cell><cell>30.9</cell><cell>35.3</cell><cell>34.7</cell><cell>46.3</cell><cell>36.7</cell><cell>59.8</cell><cell>34.0</cell></row><row><cell>Plants &amp; Animals</cell><cell>55.7</cell><cell>37.5</cell><cell>43.7</cell><cell>32.0</cell><cell>48.0</cell><cell>27.2</cell><cell>35.2</cell><cell>35.5</cell><cell>46.0</cell><cell>36.0</cell><cell>55.4</cell><cell>35.1</cell></row><row><cell>Pop Culture</cell><cell>44.5</cell><cell>36.3</cell><cell>33.7</cell><cell>31.5</cell><cell>46.1</cell><cell>36.3</cell><cell>28.8</cell><cell>29.9</cell><cell>35.7</cell><cell>30.7</cell><cell>45.1</cell><cell>34.6</cell></row><row><cell>Sports</cell><cell>50.7</cell><cell>39.1</cell><cell>39.3</cell><cell>33.3</cell><cell>43.5</cell><cell>32.4</cell><cell>32.6</cell><cell>31.4</cell><cell>40.1</cell><cell>34.9</cell><cell>50.5</cell><cell>34.7</cell></row><row><cell>Tradition</cell><cell>50.4</cell><cell>35.8</cell><cell>37.0</cell><cell>35.2</cell><cell>41.9</cell><cell>32.2</cell><cell>31.6</cell><cell>31.5</cell><cell>39.0</cell><cell>32.2</cell><cell>47.9</cell><cell>30.8</cell></row><row><cell>Vehicles</cell><cell>50.6</cell><cell>41.4</cell><cell>39.5</cell><cell>41.1</cell><cell>44.6</cell><cell>30.5</cell><cell>35.6</cell><cell>33.9</cell><cell>42.0</cell><cell>34.0</cell><cell>55.0</cell><cell>33.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 13 :</head><label>13</label><figDesc>Overall results of different MLLMs and humans on different domains and emotions.<ref type="bibr" target="#b336">[337]</ref> </figDesc><table><row><cell>Models</cell><cell>Overall</cell><cell>Life</cell><cell>Art</cell><cell>Society</cell><cell>Psy.</cell><cell>Env.</cell><cell>Others</cell><cell cols="3">Positive Neutral Negative</cell></row><row><cell></cell><cell>(1,399)</cell><cell>(585)</cell><cell>(85)</cell><cell>(461)</cell><cell>(152)</cell><cell>(51)</cell><cell>(65)</cell><cell>(196)</cell><cell>(789)</cell><cell>(414)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Open-source Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InstructBLIP-T5-XL [333]</cell><cell>47.3</cell><cell>45.6</cell><cell>48.2</cell><cell>48.8</cell><cell>44.7</cell><cell>52.9</cell><cell>50.8</cell><cell>46.9</cell><cell>48.3</cell><cell>45.4</cell></row><row><cell>BLIP-2 FLAN-T5-XL [338]</cell><cell>52.8</cell><cell>53.0</cell><cell>58.8</cell><cell>52.5</cell><cell>42.8</cell><cell>64.7</cell><cell>58.5</cell><cell>56.1</cell><cell>52.9</cell><cell>51.0</cell></row><row><cell>mPLUGw-OWL2 [339]</cell><cell>53.2</cell><cell>54.0</cell><cell>56.5</cell><cell>50.5</cell><cell>52.0</cell><cell>60.8</cell><cell>56.9</cell><cell>55.6</cell><cell>52.6</cell><cell>53.1</cell></row><row><cell>Qwen-VL-Chat [278]</cell><cell>53.4</cell><cell>53.2</cell><cell>49.4</cell><cell>52.1</cell><cell>50.0</cell><cell>60.8</cell><cell>72.3</cell><cell>56.1</cell><cell>52.6</cell><cell>53.6</cell></row><row><cell>InstructBLIP-T5-XXL [333]</cell><cell>56.7</cell><cell>56.2</cell><cell>58.8</cell><cell>58.6</cell><cell>45.4</cell><cell>64.7</cell><cell>64.6</cell><cell>63.3</cell><cell>56.1</cell><cell>54.6</cell></row><row><cell>Mantis-8B-siglip-Llama3</cell><cell>57.5</cell><cell>56.8</cell><cell>61.2</cell><cell>57.5</cell><cell>53.9</cell><cell>64.7</cell><cell>61.5</cell><cell>59.2</cell><cell>58.0</cell><cell>55.6</cell></row><row><cell>BLIP-2 FLAN-T5-XXL [338]</cell><cell>57.8</cell><cell>57.1</cell><cell>63.5</cell><cell>57.0</cell><cell>53.3</cell><cell>66.7</cell><cell>66.2</cell><cell>67.9</cell><cell>57.2</cell><cell>54.3</cell></row><row><cell>DeepSeek-VL-Chat-7B [340]</cell><cell>60.3</cell><cell>59.0</cell><cell>58.8</cell><cell>58.4</cell><cell>61.8</cell><cell>68.6</cell><cell>76.9</cell><cell>65.8</cell><cell>60.1</cell><cell>58.0</cell></row><row><cell>Yi-VL-6B-Chat [325]</cell><cell>61.3</cell><cell>60.9</cell><cell>63.5</cell><cell>60.7</cell><cell>56.6</cell><cell>66.7</cell><cell>72.3</cell><cell>61.7</cell><cell>61.7</cell><cell>60.1</cell></row><row><cell>InternLM-XComposer2-VL [341]</cell><cell>62.1</cell><cell>61.7</cell><cell>62.4</cell><cell>62.3</cell><cell>58.6</cell><cell>70.6</cell><cell>66.2</cell><cell>65.8</cell><cell>63.0</cell><cell>58.7</cell></row><row><cell>InternVL-Chat-1.5 [342]</cell><cell>66.3</cell><cell>63.6</cell><cell>65.9</cell><cell>68.5</cell><cell>65.8</cell><cell>64.7</cell><cell>76.9</cell><cell>73.5</cell><cell>65.4</cell><cell>64.5</cell></row><row><cell>Idefics2-8B [327]</cell><cell>67.7</cell><cell>67.2</cell><cell>74.1</cell><cell>67.7</cell><cell>62.5</cell><cell>74.5</cell><cell>70.8</cell><cell>68.9</cell><cell>67.0</cell><cell>68.4</cell></row><row><cell>Yi-VL-34B-Chat [325]</cell><cell>67.9</cell><cell>67.5</cell><cell>70.6</cell><cell>67.7</cell><cell>63.8</cell><cell>70.6</cell><cell>76.9</cell><cell>74.0</cell><cell>68.2</cell><cell>64.5</cell></row><row><cell>MiniCPM-Llama3-2.5 [328]</cell><cell>69.4</cell><cell>68.4</cell><cell>71.8</cell><cell>69.4</cell><cell>64.5</cell><cell>80.4</cell><cell>78.5</cell><cell>75.0</cell><cell>69.3</cell><cell>66.9</cell></row><row><cell>CogVLM2-Llama3-Chat [343]</cell><cell>70.3</cell><cell>68.9</cell><cell>68.2</cell><cell>70.9</cell><cell>67.8</cell><cell>72.5</cell><cell>86.2</cell><cell>69.9</cell><cell>71.1</cell><cell>69.1</cell></row><row><cell>LLaVA-1.6-34B [141]</cell><cell>73.8</cell><cell>73.8</cell><cell>71.8</cell><cell>73.3</cell><cell>71.1</cell><cell>78.4</cell><cell>81.5</cell><cell>79.1</cell><cell>72.9</cell><cell>72.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Closed-source Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-4V [276]</cell><cell>65.9</cell><cell>65.0</cell><cell>69.4</cell><cell>65.3</cell><cell>59.9</cell><cell>76.5</cell><cell>80.0</cell><cell>69.4</cell><cell>66.0</cell><cell>64.0</cell></row><row><cell>GPT-4o [329]</cell><cell>72.6</cell><cell>72.5</cell><cell>72.9</cell><cell>73.3</cell><cell>68.4</cell><cell>76.5</cell><cell>75.4</cell><cell>78.6</cell><cell>71.2</cell><cell>72.5</cell></row><row><cell>Gemini-1.5 Pro [344]</cell><cell>73.9</cell><cell>73.7</cell><cell>74.1</cell><cell>74.4</cell><cell>63.2</cell><cell>80.4</cell><cell>83.1</cell><cell>80.1</cell><cell>70.8</cell><cell>75.4</cell></row><row><cell>Qwen-VL-MAX [278]</cell><cell>74.8</cell><cell>74.7</cell><cell>71.8</cell><cell>74.6</cell><cell>73.0</cell><cell>76.5</cell><cell>84.6</cell><cell>80.1</cell><cell>74.5</cell><cell>72.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Humans</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human avg [337]</cell><cell>90.3</cell><cell>90.0</cell><cell>88.2</cell><cell>91.4</cell><cell>86.6</cell><cell>96.1</cell><cell>92.3</cell><cell>84.7</cell><cell>89.1</cell><cell>92.2</cell></row><row><cell>Human best [337]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 14 : Evaluation[D] of mainstreams series of MLLMs on ConBench.</head><label>14</label><figDesc>The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. †: Due to safety considerations, GPT-4V declined to answer the celebrity category.<ref type="bibr" target="#b346">[347]</ref> </figDesc><table><row><cell>Method</cell><cell>ConScore[D]</cell><cell>T</cell><cell cols="2">Sensation C V</cell><cell>Con</cell><cell>T</cell><cell cols="2">Cognition C V</cell><cell>Con</cell><cell>T</cell><cell cols="2">Knowledge C V</cell><cell>Con</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Closed-source Vision Language Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-4V  † [276]</cell><cell>29.20</cell><cell>80.4</cell><cell>79.0</cell><cell>61.7</cell><cell>48.3</cell><cell>68.8</cell><cell>53.2</cell><cell>39.9</cell><cell>20.4</cell><cell>63.1</cell><cell>57.2</cell><cell>30.0</cell><cell>14.2</cell></row><row><cell>GPT-4-Omni [329]</cell><cell>35.70</cell><cell>89.2</cell><cell>79.4</cell><cell>64.4</cell><cell>55.0</cell><cell>71.8</cell><cell>62.8</cell><cell>44.9</cell><cell>27.8</cell><cell>64.7</cell><cell>61.7</cell><cell>39.7</cell><cell>23.3</cell></row><row><cell>Gemini-Pro-Vision [348]</cell><cell>25.00</cell><cell>85.2</cell><cell>60.7</cell><cell>63.4</cell><cell>39.3</cell><cell>71.8</cell><cell>45.0</cell><cell>44.2</cell><cell>15.1</cell><cell>65.0</cell><cell>51.4</cell><cell>39.7</cell><cell>15.8</cell></row><row><cell>Gemini-Ultra-Vision [348]</cell><cell>33.10</cell><cell>78.9</cell><cell>78.6</cell><cell>66.3</cell><cell>50.3</cell><cell>68.1</cell><cell>58.5</cell><cell>47.9</cell><cell>28.5</cell><cell>62.9</cell><cell>62.2</cell><cell>44.7</cell><cell>19.7</cell></row><row><cell>Qwen-VL-Plus [278]</cell><cell>28.10</cell><cell>82.7</cell><cell>74.9</cell><cell>60.4</cell><cell>45.0</cell><cell>64.2</cell><cell>41.7</cell><cell>30.8</cell><cell>16.3</cell><cell>63.6</cell><cell>54.2</cell><cell>33.3</cell><cell>15.8</cell></row><row><cell>Qwen-VL-Max [278]</cell><cell>37.00</cell><cell>86.4</cell><cell>80.7</cell><cell>65.4</cell><cell>56.3</cell><cell>72.9</cell><cell>51.4</cell><cell>51.3</cell><cell>28.1</cell><cell>68.3</cell><cell>58.6</cell><cell>38.9</cell><cell>24.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">7B Vision Language Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaVA-v1.5-7B [141]</cell><cell>16.60</cell><cell>79.3</cell><cell>56.8</cell><cell>44.3</cell><cell>28.3</cell><cell>51.4</cell><cell>33.5</cell><cell>15.8</cell><cell>4.7</cell><cell>61.7</cell><cell>44.4</cell><cell>16.9</cell><cell>7.8</cell></row><row><cell>Qwen-VL-Chat [278]</cell><cell>26.40</cell><cell>81.0</cell><cell>79.6</cell><cell>54.2</cell><cell>39.0</cell><cell>55.0</cell><cell>46.3</cell><cell>33.2</cell><cell>13.5</cell><cell>60.3</cell><cell>54.2</cell><cell>28.9</cell><cell>14.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">∼ 13B Vision Language Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaVA-v1.5-13B [141]</cell><cell>24.00</cell><cell>82.9</cell><cell>77.1</cell><cell>49.6</cell><cell>39.5</cell><cell>53.6</cell><cell>37.8</cell><cell>20.1</cell><cell>10.4</cell><cell>65.6</cell><cell>50.3</cell><cell>17.2</cell><cell>9.7</cell></row><row><cell>MiniGemini-13B [349]</cell><cell>21.80</cell><cell>81.9</cell><cell>69.7</cell><cell>52.8</cell><cell>39.3</cell><cell>51.9</cell><cell>38.2</cell><cell>21.1</cell><cell>6.9</cell><cell>52.8</cell><cell>36.7</cell><cell>17.5</cell><cell>9.2</cell></row><row><cell>InternVL-v1.5-26B [342]</cell><cell>31.40</cell><cell>85.6</cell><cell>84.8</cell><cell>65.0</cell><cell>54.3</cell><cell>59.7</cell><cell>58.6</cell><cell>44.4</cell><cell>19.4</cell><cell>58.1</cell><cell>55.8</cell><cell>25.3</cell><cell>12.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">∼ 34B Vision Language Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LLaVA-NeXT-34B [350]</cell><cell>27.70</cell><cell>82.4</cell><cell>81.7</cell><cell>55.6</cell><cell>43.6</cell><cell>50.7</cell><cell>47.5</cell><cell>25.6</cell><cell>9.9</cell><cell>60.4</cell><cell>56.1</cell><cell>27.8</cell><cell>12.8</cell></row><row><cell>MiniGemini-34B [349]</cell><cell>23.00</cell><cell>80.8</cell><cell>76.8</cell><cell>48.2</cell><cell>39.7</cell><cell>36.9</cell><cell>30.7</cell><cell>18.9</cell><cell>6.0</cell><cell>58.1</cell><cell>42.3</cell><cell>20.8</cell><cell>8.2</cell></row><row><cell>InternVL-v1.2P-40B [280]</cell><cell>34.70</cell><cell>83.7</cell><cell>83.2</cell><cell>66.6</cell><cell>53.4</cell><cell>74.2</cell><cell>67.6</cell><cell>57.1</cell><cell>34.9</cell><cell>72.2</cell><cell>58.3</cell><cell>28.6</cell><cell>13.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 16 :</head><label>16</label><figDesc>Overall statistics of COMPBENCH.<ref type="bibr" target="#b352">[353]</ref> </figDesc><table><row><cell>Relativity</cell><cell>Dataset</cell><cell>Domain</cell><cell>samples</cell></row><row><cell></cell><cell>MIT-States</cell><cell>Open</cell><cell>0.2K</cell></row><row><cell></cell><cell>Fashionpedia</cell><cell>Fashion</cell><cell>2.4K</cell></row><row><cell>Attribute</cell><cell>VAW</cell><cell>Open</cell><cell>0.9K</cell></row><row><cell></cell><cell>CUB-200-2011</cell><cell>Bird</cell><cell>0.9K</cell></row><row><cell></cell><cell>Wildfish++</cell><cell>Fish</cell><cell>0.9K</cell></row><row><cell>Existence</cell><cell>MagicBrush Spot-the-diff</cell><cell>Open Outdoor Scene</cell><cell>0.9K 1.2K</cell></row><row><cell>State</cell><cell>MIT-States VAW</cell><cell>Open Open</cell><cell>0.6K 0.5K</cell></row><row><cell>Emotion</cell><cell>CelebA FER-2013</cell><cell>Face Face</cell><cell>1.5K 3.8K</cell></row><row><cell>Temporality</cell><cell>SoccerNet CompCars</cell><cell>Sport Car</cell><cell>8.3K 5K</cell></row><row><cell>Spatiality</cell><cell>NYU-Depth V2</cell><cell>Indoor Scene</cell><cell>1.9K</cell></row><row><cell>Quantity</cell><cell>VQAv2</cell><cell>Open</cell><cell>9.8K</cell></row><row><cell>Quality</cell><cell>Q-Bench2</cell><cell>Open</cell><cell>1K</cell></row><row><cell>Total</cell><cell>-</cell><cell>-</cell><cell>39.8K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 17 :</head><label>17</label><figDesc>results on COMPBENCH test split. Evaluating four leading MLLMs across eight relative comparisons spanning sixteen tasks.<ref type="bibr" target="#b352">[353]</ref> </figDesc><table><row><cell>Model</cell><cell>ST</cell><cell>FA</cell><cell>Attribute VA</cell><cell>CU</cell><cell>WF</cell><cell cols="2">Exist. MB SD</cell><cell>State ST VA</cell><cell>Emot. CE FE</cell><cell>Temp. SN CC</cell><cell>Spat. ND</cell><cell>Quan. VQ</cell><cell>Qual. QB</cell><cell>Avg</cell></row><row><cell>GPT-4V [276]</cell><cell cols="5">91.8 89.0 76.9 71.4 72.1</cell><cell cols="2">58.3 41.9</cell><cell>92.2 87.8</cell><cell>91.8 83.4</cell><cell>71.4 73.7</cell><cell>56.1</cell><cell>63.8</cell><cell>73.0</cell><cell>74.7</cell></row><row><cell>Gemini1.0-Pro [348]</cell><cell cols="5">71.9 76.3 69.3 59.9 54.9</cell><cell cols="2">53.7 53.0</cell><cell>81.8 70.7</cell><cell>60.6 71.2</cell><cell>55.1 58.2</cell><cell>56.6</cell><cell>54.6</cell><cell>59.5</cell><cell>63.0</cell></row><row><cell>LLaVA-1.6 [141]</cell><cell cols="5">84.9 72.1 77.7 72.6 68.7</cell><cell cols="2">26.5 20.7</cell><cell>89.7 79.3</cell><cell>96.2 83.5</cell><cell>51.0 50.2</cell><cell>67.2</cell><cell>50.1</cell><cell>64.8</cell><cell>66.0</cell></row><row><cell>VILA-1.5 [354]</cell><cell cols="5">69.9 66.2 70.9 55.9 52.0</cell><cell cols="2">49.5 36.8</cell><cell>71.9 74.5</cell><cell>57.1 55.6</cell><cell>51.1 52.9</cell><cell>51.8</cell><cell>47.7</cell><cell>64.8</cell><cell>58.0</cell></row><row><cell>Chance level [353]</cell><cell cols="5">50.0 50.0 50.0 50.0 50.0</cell><cell>8.6</cell><cell>9.7</cell><cell>50.0 50.0</cell><cell>50.0 50.0</cell><cell>50.0 50.0</cell><cell>50.0</cell><cell>33.3</cell><cell>37.4</cell><cell>43.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 18 :</head><label>18</label><figDesc>The results under noise, blur, weather, and digital perturbations. Before/After means before/after perturbation.<ref type="bibr" target="#b354">[355]</ref> </figDesc><table><row><cell>Model</cell><cell cols="2">Before ACC+ CHAIR</cell><cell cols="2">Noise ACC+ CHAIR</cell><cell cols="4">After ACC+ CHAIR Blur ACC+ CHAIR Weather</cell><cell cols="2">Digital ACC+ CHAIR</cell></row><row><cell>CogVLM [291]</cell><cell>49.0</cell><cell>62.0</cell><cell>48.5</cell><cell>68.2</cell><cell>47.4</cell><cell>68.6</cell><cell>42.8</cell><cell>67.9</cell><cell>48.4</cell><cell>69.8</cell></row><row><cell>Multi-GPT [276]</cell><cell>13.3</cell><cell>73.5</cell><cell>9.6</cell><cell>73.6</cell><cell>12.8</cell><cell>76.1</cell><cell>11.2</cell><cell>73.4</cell><cell>9.2</cell><cell>77.8</cell></row><row><cell>LLaVA [141]</cell><cell>6.3</cell><cell>68.5</cell><cell>4.33</cell><cell>67.7</cell><cell>5.0</cell><cell>70.6</cell><cell>4.17</cell><cell>69.8</cell><cell>3.6</cell><cell>74.2</cell></row><row><cell>LLaVA1.5 [141]</cell><cell>43.0</cell><cell>68.9</cell><cell>42.6</cell><cell>70.1</cell><cell>42.4</cell><cell>68.7</cell><cell>43.3</cell><cell>68.0</cell><cell>36.8</cell><cell>74.5</cell></row><row><cell>MiniGPT-4 [356]</cell><cell>16.0</cell><cell>72.4</cell><cell>15.8</cell><cell>70.2</cell><cell>15.9</cell><cell>72.1</cell><cell>14.5</cell><cell>72.6</cell><cell>13.8</cell><cell>73.9</cell></row><row><cell>MiniGPT4-v2 [357]</cell><cell>28.3</cell><cell>72.1</cell><cell>26.7</cell><cell>74.7</cell><cell>28.8</cell><cell>74.0</cell><cell>28.2</cell><cell>72.8</cell><cell>27.1</cell><cell>74.9</cell></row><row><cell>mPLUG2 [358]</cell><cell>38.0</cell><cell>65.0</cell><cell>33.3</cell><cell>67.6</cell><cell>33.1</cell><cell>69.1</cell><cell>35.3</cell><cell>66.9</cell><cell>32.3</cell><cell>73.6</cell></row><row><cell>Gemini [288]</cell><cell>46.0</cell><cell>57.3</cell><cell>44.2</cell><cell>60.0</cell><cell>45.1</cell><cell>59.7</cell><cell>44.8</cell><cell>58.5</cell><cell>37.5</cell><cell>61.3</cell></row><row><cell>GPT-4V [276]</cell><cell>47.3</cell><cell>66.1</cell><cell>42.3</cell><cell>66.9</cell><cell>41.8</cell><cell>68.4</cell><cell>47.8</cell><cell>60.9</cell><cell>34.0</cell><cell>65.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 19 :</head><label>19</label><figDesc>The results under image concatenation, image cropping, and prompt misleading perturbations.<ref type="bibr" target="#b354">[355]</ref> </figDesc><table><row><cell></cell><cell></cell><cell cols="2">PI-Score</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLLMs</cell><cell cols="6">Concat Before After Before After Before After Cropping Prompt Mislead</cell></row><row><cell>CogVLM [291]</cell><cell cols="3">45.4 22.5 10.0</cell><cell>5.0</cell><cell>39.6</cell><cell>11.4</cell></row><row><cell>Multi-GPT [276]</cell><cell>8.3</cell><cell cols="2">15.0 11.7</cell><cell>0.0</cell><cell>18.9</cell><cell>7.2</cell></row><row><cell>LLaVA [141]</cell><cell>6.5</cell><cell>2.2</cell><cell>3.4</cell><cell>6.7</cell><cell>14.4</cell><cell>5.2</cell></row><row><cell>LLaVA1.5 [141]</cell><cell>32.4</cell><cell>5.9</cell><cell>10.0</cell><cell>8.4</cell><cell>26.4</cell><cell>8.1</cell></row><row><cell>MiniGPT-4 [356]</cell><cell>8.9</cell><cell>5.9</cell><cell>10.0</cell><cell>8.4</cell><cell>18.5</cell><cell>7.0</cell></row><row><cell cols="6">MiniGPT-v2 [357] 15.8 12.3 16.7 15.0 26.4</cell><cell>11.3</cell></row><row><cell>mPLUG2 [358]</cell><cell cols="3">25.7 18.9 10.0</cell><cell>8.3</cell><cell>29.7</cell><cell>15.7</cell></row><row><cell>InternLM [359]</cell><cell cols="2">38.3 37.3</cell><cell>8.3</cell><cell cols="2">10.0 34.4</cell><cell>28.0</cell></row><row><cell>Qwen-VL [278]</cell><cell cols="5">46.3 19.6 20.0 11.7 53.2</cell><cell>38.2</cell></row><row><cell>VisualGLM [360]</cell><cell>6.8</cell><cell>0.6</cell><cell>34.0</cell><cell>0.0</cell><cell>21.2</cell><cell>11.3</cell></row><row><cell>Gemini [288]</cell><cell cols="5">44.6 21.4 45.0 26.7 59.2</cell><cell>39.4</cell></row><row><cell>GPT-4V [276]</cell><cell cols="3">42.0 18.0 43.4</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 21 :</head><label>21</label><figDesc>The results of discriminative task on image concatenation, cropping, and prompt misleading.<ref type="bibr" target="#b354">[355]</ref> </figDesc><table><row><cell></cell><cell></cell><cell>Image Concatenation</cell><cell></cell><cell>Image Cropping</cell><cell cols="3">Prompt Misleading</cell></row><row><cell>MLLMs</cell><cell cols="7">Before ACC ACC+ F1 ACC ACC+ F1 ACC ACC+ F1 ACC ACC+ F1 ACC ACC+ F1 After Before After After</cell></row><row><cell>CogVLM [291]</cell><cell cols="3">69.9 49.0 74.4 67.2 42.0 73.1 50.0</cell><cell>0.0 66.7 50.0</cell><cell cols="3">0.0 66.7 56.7 33.3 51.9</cell></row><row><cell>Multi-GPT [276]</cell><cell cols="3">46.8 13.3 52.4 41.8 16.3 48.9 48.3</cell><cell>0.0 65.2 45.0</cell><cell>0.0 62.1 28.3</cell><cell>6.7</cell><cell>41.1</cell></row><row><cell>LLava [141]</cell><cell>51.5</cell><cell>6.3 57.2 50.3</cell><cell>1.0 54.0 50.0</cell><cell>0.0 66.7 50.0</cell><cell>0.0 66.7 1.7</cell><cell>0.0</cell><cell>3.2</cell></row><row><cell>LLava1.5 [141]</cell><cell cols="2">70.5 43.0 76.1 51.7</cell><cell>8.0 61.7 51.7</cell><cell>6.7 56.7 48.3</cell><cell>6.7 45.6 40.0</cell><cell>3.3</cell><cell>5.2</cell></row><row><cell cols="3">MiniGPT-4 [356] 43.0 16.0 47.6 30.2</cell><cell>7.7 25.4 38.3</cell><cell>0.0 55.4 30.0</cell><cell>0.0 46.2 20.0</cell><cell>0.0</cell><cell>33.4</cell></row><row><cell cols="8">MiniGPT-v2 [357] 55.8 28.3 56.4 48.2 21.3 41.3 55.0 26.7 62.0 48.3 23.3 47.5 88.3 80.0 88.8</cell></row><row><cell>mPLUG2 [358]</cell><cell cols="7">62.3 38.0 68.3 51.5 27.3 54.5 50.0 13.3 62.5 48.3 13.3 59.7 43.3 13.3 34.6</cell></row><row><cell>InternLM [359]</cell><cell cols="3">68.2 48.3 70.8 61.2 37.0 55.9 50.0</cell><cell>3.3 60.5 51.7</cell><cell cols="3">6.7 61.3 75.0 50.0 68.1</cell></row><row><cell>Qwen-VL [278]</cell><cell cols="7">62.5 39.3 62.0 55.7 18.3 52.4 58.3 23.3 65.7 48.3 16.7 53.7 93.3 86.7 92.9</cell></row><row><cell cols="2">VisualGLM [360] 46.3</cell><cell>5.3 50.9 43.3</cell><cell>0.3 45.0 50.0</cell><cell>0.0 66.7 50.0</cell><cell cols="3">0.0 66.7 30.0 13.3 36.3</cell></row><row><cell>Gemini [288]</cell><cell cols="7">65.7 46.0 64.1 60.0 33.7 63.2 56.7 16.7 67.5 53.3 10.0 66.7 53.3 13.3 33.3</cell></row><row><cell>GPT-4V [276]</cell><cell cols="7">66.7 47.3 66.1 59.8 34.3 55.8 61.7 33.3 66.7 53.3 20.0 62.5 95.0 90.0 94.7</cell></row><row><cell cols="4">models in video understanding and generation, enabling them</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to better handle multimodal inputs and efficiently generate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>video content.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Xiuliang Duan et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 22 :</head><label>22</label><figDesc>General evaluation results MLLMs across different capability dimensions. "CG", "FG", "CMI", and "Desc" are respectively short for coarse-grained perception, fine-grained perception, cross-modal inference, and description. " R" represents the average rank across dimensions.<ref type="bibr" target="#b362">[363]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generation Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Likelihood Evaluation</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Perception</cell><cell></cell><cell></cell><cell>Cognition</cell><cell></cell><cell></cell><cell>R</cell><cell cols="2">Perception</cell><cell></cell><cell cols="2">Cognition</cell><cell>R</cell></row><row><cell></cell><cell>CG</cell><cell>FG</cell><cell>STP</cell><cell cols="5">Spatial VGR Dialog CMI Desc</cell><cell></cell><cell>CG</cell><cell>FG</cell><cell cols="4">Spatial VGR Dialog CMI</cell></row><row><cell>BLIP-2 F [364]</cell><cell cols="3">69.4 76.6 38.1</cell><cell>43.2</cell><cell>73.3</cell><cell>61.8</cell><cell>66.9</cell><cell>74.3</cell><cell>2</cell><cell cols="2">60.7 74.4</cell><cell>51.1</cell><cell>69.8</cell><cell>62.6</cell><cell>58.9</cell><cell>4</cell></row><row><cell>InstructBLIP F [365]</cell><cell cols="3">71.2 78.1 41.2</cell><cell>46.1</cell><cell>73.9</cell><cell>60.6</cell><cell>71.4</cell><cell>43.8</cell><cell>2</cell><cell cols="2">60.4 75.6</cell><cell>51.2</cell><cell>71.0</cell><cell>67.2</cell><cell>55.5</cell><cell>4</cell></row><row><cell>InstructBLIP V [365]</cell><cell cols="3">69.1 70.8 40.7</cell><cell>44.4</cell><cell>63.0</cell><cell>48.6</cell><cell>53.8</cell><cell>27.3</cell><cell>4</cell><cell cols="2">58.5 77.8</cell><cell>52.3</cell><cell>73.5</cell><cell>68.7</cell><cell>55.4</cell><cell>3</cell></row><row><cell>LLaVA V [141]</cell><cell cols="3">28.7 34.4 18.4</cell><cell>28.7</cell><cell>44.0</cell><cell>35.6</cell><cell>47.3</cell><cell>36.8</cell><cell>11</cell><cell cols="2">61.0 70.3</cell><cell>42.4</cell><cell>58.9</cell><cell>52.3</cell><cell>48.0</cell><cell>8</cell></row><row><cell>LLaVA L 2 [141]</cell><cell cols="3">48.3 59.8 21.5</cell><cell>41.2</cell><cell>59.7</cell><cell>46.3</cell><cell>49.9</cell><cell>39.5</cell><cell>6</cell><cell cols="2">49.9 65.6</cell><cell>47.4</cell><cell>56.7</cell><cell>48.6</cell><cell>49.7</cell><cell>11</cell></row><row><cell>MiniGPT4 [356]</cell><cell cols="3">46.2 53.2 33.0</cell><cell>34.6</cell><cell>45.6</cell><cell>39.5</cell><cell>45.4</cell><cell>47.5</cell><cell>7</cell><cell cols="2">54.9 70.6</cell><cell>49.2</cell><cell>57.3</cell><cell>54.1</cell><cell>50.9</cell><cell>8</cell></row><row><cell>mPLUG-Owl [339]</cell><cell cols="3">42.0 37.2 39.8</cell><cell>26.8</cell><cell>37.5</cell><cell>35.2</cell><cell>40.4</cell><cell>44.7</cell><cell>11</cell><cell cols="2">57.9 66.1</cell><cell>48.6</cell><cell>54.3</cell><cell>45.5</cell><cell>49.8</cell><cell>10</cell></row><row><cell>PandaGPT [366]</cell><cell cols="2">28.2 34.6</cell><cell>4.5</cell><cell>33.3</cell><cell>41.9</cell><cell>34.1</cell><cell>36.6</cell><cell>1.6</cell><cell>14</cell><cell cols="2">42.3 47.4</cell><cell>39.4</cell><cell>43.3</cell><cell>41.5</cell><cell>37.0</cell><cell>16</cell></row><row><cell>IB-LLM [367]</cell><cell cols="2">29.2 32.7</cell><cell>8.2</cell><cell>35.6</cell><cell>36.7</cell><cell>35.3</cell><cell>36.6</cell><cell>27.6</cell><cell>13</cell><cell cols="2">49.6 54.4</cell><cell>46.1</cell><cell>50.3</cell><cell>39.5</cell><cell>45.6</cell><cell>15</cell></row><row><cell>LA-V2 [368]</cell><cell cols="3">33.2 30.8 24.2</cell><cell>23.8</cell><cell>36.3</cell><cell>35.4</cell><cell>41.1</cell><cell>36.0</cell><cell>13</cell><cell cols="2">42.7 61.4</cell><cell>48.6</cell><cell>54.1</cell><cell>43.4</cell><cell>49.9</cell><cell>12</cell></row><row><cell>mmGPT [369]</cell><cell cols="3">30.4 30.3 16.7</cell><cell>26.9</cell><cell>33.0</cell><cell>31.8</cell><cell>38.2</cell><cell>27.7</cell><cell>14</cell><cell cols="2">52.6 62.4</cell><cell>47.2</cell><cell>56.2</cell><cell>43.1</cell><cell>44.1</cell><cell>13</cell></row><row><cell>Shikra [362]</cell><cell cols="2">47.2 47.5</cell><cell>8.3</cell><cell>33.3</cell><cell>41.2</cell><cell>35.2</cell><cell>44.5</cell><cell>31.8</cell><cell>11</cell><cell cols="2">60.9 66.8</cell><cell>45.5</cell><cell>58.5</cell><cell>59.5</cell><cell>59.3</cell><cell>7</cell></row><row><cell>Lynx [370]</cell><cell cols="3">59.5 62.6 18.6</cell><cell>40.2</cell><cell>58.4</cell><cell>47.0</cell><cell>53.0</cell><cell>60.7</cell><cell>5</cell><cell cols="2">66.1 76.2</cell><cell>53.9</cell><cell>69.9</cell><cell>60.0</cell><cell>57.4</cell><cell>3</cell></row><row><cell>Cheetor V [371]</cell><cell cols="3">52.0 50.3 25.9</cell><cell>30.6</cell><cell>49.9</cell><cell>40.3</cell><cell>47.4</cell><cell>61.6</cell><cell>7</cell><cell cols="2">56.1 69.0</cell><cell>48.4</cell><cell>58.7</cell><cell>57.6</cell><cell>50.6</cell><cell>8</cell></row><row><cell>Cheetor L 2 [371]</cell><cell cols="3">46.5 51.4 18.8</cell><cell>34.5</cell><cell>54.4</cell><cell>40.6</cell><cell>44.0</cell><cell>43.9</cell><cell>8</cell><cell cols="2">61.6 56.1</cell><cell>48.7</cell><cell>57.5</cell><cell>46.8</cell><cell>47.2</cell><cell>11</cell></row><row><cell>BLIVA [372]</cell><cell cols="3">41.7 43.4 40.8</cell><cell>33.3</cell><cell>42.4</cell><cell>39.8</cell><cell>45.2</cell><cell>52.5</cell><cell>8</cell><cell cols="2">64.9 78.2</cell><cell>51.7</cell><cell>72.9</cell><cell>68.1</cell><cell>53.7</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 23 :</head><label>23</label><figDesc>MLLMs results in the VisionGraph benchmark.<ref type="bibr" target="#b374">[375]</ref> </figDesc><table><row><cell>Model↓ Task Types →</cell><cell></cell><cell>Cycle</cell><cell>Topo. Sort</cell><cell>Shortest Path</cell><cell>Max. Flow</cell><cell>Bipartite Graph</cell><cell>Hamilton Path</cell><cell>GNNs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Node Recognition</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MiniGPT-4 (Vicuna-7b) [356]</cell><cell>19.14</cell><cell>12.04</cell><cell>42.96</cell><cell>42.19</cell><cell>32.76</cell><cell>8.33</cell><cell>60.34</cell><cell>53.85</cell></row><row><cell>BLIP-2 (FlanT5-xxl) [364]</cell><cell>37.74</cell><cell>52.88</cell><cell>47.41</cell><cell>81.25</cell><cell>67.24</cell><cell>22.62</cell><cell>62.07</cell><cell>61.54</cell></row><row><cell>InstructBLIP (FlanT5-xl) [365]</cell><cell>36.12</cell><cell>47.64</cell><cell>46.67</cell><cell>75.00</cell><cell>56.90</cell><cell>36.90</cell><cell>53.45</cell><cell>74.36</cell></row><row><cell>InstructBLIP (FlanT5-xxl) [365]</cell><cell>35.31</cell><cell>52.88</cell><cell>61.48</cell><cell>85.94</cell><cell>77.59</cell><cell>17.86</cell><cell>65.52</cell><cell>61.54</cell></row><row><cell>Sphinx [376]</cell><cell>61.99</cell><cell>98.95</cell><cell>94.07</cell><cell>100.00</cell><cell>91.38</cell><cell>55.95</cell><cell>100.00</cell><cell>97.44</cell></row><row><cell>Internlm [359]</cell><cell>67.92</cell><cell>100.00</cell><cell>97.78</cell><cell>100.00</cell><cell>98.25</cell><cell>77.38</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell>Llava-v1.5-7b [141]</cell><cell>64.15</cell><cell>96.86</cell><cell>92.59</cell><cell>100.00</cell><cell>93.10</cell><cell>13.10</cell><cell>100.00</cell><cell>94.87</cell></row><row><cell>Llava-v1.5-13b [141]</cell><cell>62.26</cell><cell>97.91</cell><cell>91.11</cell><cell>100.00</cell><cell>96.55</cell><cell>11.9</cell><cell>100.00</cell><cell>97.44</cell></row><row><cell>Qwen-Plus (0-shot) [278]</cell><cell>2.96</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>5.17</cell><cell>0.00</cell><cell>0.00</cell><cell>56.41</cell></row><row><cell>Qwen-max (0-shot) [278]</cell><cell>29.11</cell><cell>31.94</cell><cell>30.37</cell><cell>12.50</cell><cell>3.45</cell><cell>14.29</cell><cell>29.31</cell><cell>46.15</cell></row><row><cell>Gemini (0-shot) [348]</cell><cell>40.97</cell><cell>42.93</cell><cell>47.41</cell><cell>67.19</cell><cell>72.41</cell><cell>10.71</cell><cell>65.52</cell><cell>35.90</cell></row><row><cell>GPT-4V (0-shot) [276]</cell><cell>46.49</cell><cell>81.15</cell><cell>81.48</cell><cell>89.06</cell><cell>58.62</cell><cell>20.24</cell><cell>100.00</cell><cell>97.44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Edge Recognition (Correct / Error)</cell><cell></cell><cell></cell></row><row><cell>MiniGPT-4 (Vicuna-7b) [356]</cell><cell>11.78/31.78</cell><cell>0.68/1.59</cell><cell>12.54/58.89</cell><cell>4.78/87.20</cell><cell>0.61/61.15</cell><cell>14.45/47.53</cell><cell>28.48/34.69</cell><cell>37.48/55.05</cell></row><row><cell>BLIP-2 (FlanT5-xxl) [364]</cell><cell>12.49/84.03</cell><cell>15.11/84.69</cell><cell>0.08/2.14</cell><cell>1.75/96.84</cell><cell>0.00/0.00</cell><cell>9.92/75.89</cell><cell>11.73/45.55</cell><cell>17.26/88.84</cell></row><row><cell>Sphinx [376]</cell><cell>44.76/66.69</cell><cell>22.13/79.69</cell><cell>37.84/73.07</cell><cell>39.88/70.62</cell><cell>20.68/86.57</cell><cell>83.93/53.51</cell><cell>66.26/71.15</cell><cell>60.66/61.43</cell></row><row><cell>Internlm [359]</cell><cell>53.08/35.01</cell><cell>40.78/60.05</cell><cell>55.70/50.85</cell><cell>57.82/45.02</cell><cell>23.45/80.27</cell><cell>71.21/42.34</cell><cell>73.98/36.00</cell><cell>83.00/19.69</cell></row><row><cell>InstructBLIP (FlanT5-xl) [365]</cell><cell>17.24/87.62</cell><cell>26.02/88.06</cell><cell>0.00/0.00</cell><cell>5.70/93.93</cell><cell>0.00/0.00</cell><cell>12.72/83.13</cell><cell>37.07/82.85</cell><cell>49.18/81.28</cell></row><row><cell>InstructBLIP (FlanT5-xxl) [365]</cell><cell>16.34/81.50</cell><cell>16.04/85.54</cell><cell>0.00/0.00</cell><cell>3.58/98.31</cell><cell>0.00/0.00</cell><cell>13.26/76.86</cell><cell>32.05/65.84</cell><cell>37.70/67.57</cell></row><row><cell>Llava-v1.5-7b [141]</cell><cell>46.81/58.13</cell><cell>23.23/77.63</cell><cell>36.56/72.97</cell><cell>38.76/66.47</cell><cell>9.80/91.56</cell><cell>63.10/54.70</cell><cell>80.14/48.06</cell><cell>69.85/32.92</cell></row><row><cell>Llava-v1.5-13b [141]</cell><cell>51.18/53.41</cell><cell>22.60/76.91</cell><cell>38.80/70.26</cell><cell>41.93/63.50</cell><cell>9.89/91.72</cell><cell>67.88/54.21</cell><cell>76.26/45.21</cell><cell>67.40/33.59</cell></row><row><cell>Qwen-Plus [278]</cell><cell>30.46/64.78</cell><cell>27.42/82.37</cell><cell>10.59/68.46</cell><cell>6.16/81.60</cell><cell>1.32/64.62</cell><cell>75.93/58.65</cell><cell>48.63/50.41</cell><cell>33.71/60.56</cell></row><row><cell>Qwen-max [278]</cell><cell>25.71/63.21</cell><cell>20.92/83.50</cell><cell>16.70/76.00</cell><cell>1.63/95.70</cell><cell>1.12/96.58</cell><cell>42.59/55.55</cell><cell>40.47/51.61</cell><cell>35.17/55.81</cell></row><row><cell>Gemini (0-shot) [348]</cell><cell>23.26/52.35</cell><cell>21.65/80.09</cell><cell>19.11/66.94</cell><cell>16.18/83.09</cell><cell>4.79/94.78</cell><cell>66.01/53.90</cell><cell>39.40/37.80</cell><cell>40.83/52.60</cell></row><row><cell>GPT-4V (0-shot) [276]</cell><cell>14.10/23.09</cell><cell>17.50/72.97</cell><cell>9.64/30.58</cell><cell>23.01/66.85</cell><cell>5.31/43.62</cell><cell>24.13/32.33</cell><cell>29.22/38.03</cell><cell>46.14/42.74</cell></row><row><cell>GPT-4V (4-shot) [276]</cell><cell>20.63/34.52</cell><cell>26.25/69.95</cell><cell>13.19/51.75</cell><cell>23.40/61.90</cell><cell>6.12/84.94</cell><cell>46.33/51.69</cell><cell>58.49/49.79</cell><cell>48.06/35.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 25 :</head><label>25</label><figDesc>The statistic of collected datasets and instructions in CoIN benchmark.<ref type="bibr" target="#b93">[94]</ref> </figDesc><table><row><cell>Task</cell><cell></cell><cell>Instruction</cell><cell>Train Number</cell><cell>Test Number</cell></row><row><cell></cell><cell>RefCOCO</cell><cell>Please provide the bounding</cell><cell></cell><cell></cell></row><row><cell>Grounding</cell><cell>RefCOCO+</cell><cell>box coordinate of the region</cell><cell>55k</cell><cell>31k</cell></row><row><cell></cell><cell>RefCOCOg</cell><cell>this sentence describes</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>What is the object in the image?</cell><cell></cell><cell></cell></row><row><cell>Classification</cell><cell>ImageNet</cell><cell>Answer the question using a</cell><cell>129k</cell><cell>5k</cell></row><row><cell></cell><cell></cell><cell>single word or phrase</cell><cell></cell><cell></cell></row><row><cell>Image Question Answering (IQA)</cell><cell>VQAv2</cell><cell>Answer the question using a single word or phrase</cell><cell>82k</cell><cell>107k</cell></row><row><cell>Knowledge Grounded IQA</cell><cell>ScienceQA</cell><cell>Answer with the option's letter from the given choices directly</cell><cell>12k</cell><cell>4k</cell></row><row><cell>Reading Comprehension IQA</cell><cell>TextVQA</cell><cell>Answer the question using a single word or phrase</cell><cell>34k</cell><cell>5k</cell></row><row><cell>Visual Reasoning IQA</cell><cell>GQA</cell><cell>Answer the question using a single word or phrase</cell><cell>72k</cell><cell>1k</cell></row><row><cell>Blind People IQA</cell><cell>VizWiz</cell><cell>Answer the question using a single word or phrase</cell><cell>20k</cell><cell>8k</cell></row><row><cell>OCR IQA</cell><cell>OCR-VQA</cell><cell>Answer the question using a single word or phrase</cell><cell>165k</cell><cell>100k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 26 :</head><label>26</label><figDesc>The results evaluating the Truth Alignment ability are presented below. The first line of Sequential Finetune are the results for each task evaluated when just tuned on the corresponding task, and the second line displays the final results of each task after fine-tuning on the last task.<ref type="bibr" target="#b93">[94]</ref> </figDesc><table><row><cell>MLLM</cell><cell>Method</cell><cell cols="8">Accuracy on Each Task ScienceQA TextVQA ImageNet GQA VizWiz Grounding VQAV2 OCR-VQA</cell><cell cols="2">Overall Results MAA BWT</cell></row><row><cell></cell><cell>Multi-task</cell><cell>56.77</cell><cell>49.35</cell><cell>95.55</cell><cell>56.65</cell><cell>53.90</cell><cell>30.09</cell><cell>59.50</cell><cell>55.65</cell><cell>57.18</cell><cell>-</cell></row><row><cell>LLaVA [141]</cell><cell>Zero-shot Sequential Finetune</cell><cell>49.91 82.45 21.26</cell><cell>2.88 49.99 28.74</cell><cell>0.33 96.05 10.25</cell><cell>2.08 56.40 36.78</cell><cell>0.90 55.45 32.45</cell><cell>0.00 31.27 0.83</cell><cell>0.68 62.20 42.50</cell><cell>0.17 57.08 57.08</cell><cell>7.12 32.97</cell><cell>--32.62</cell></row><row><cell></cell><cell>Multi-task</cell><cell>25.70</cell><cell>60.88</cell><cell>17.05</cell><cell>56.77</cell><cell>35.58</cell><cell>6.78</cell><cell>68.67</cell><cell>63.50</cell><cell>41.87</cell><cell>-</cell></row><row><cell>Qwen-VL [278]</cell><cell>Zero-shot Sequential Finetune</cell><cell>64.56 67.69 31.05</cell><cell>48.15 66.36 42.45</cell><cell>11.82 53.70 29.57</cell><cell>44.50 59.30 55.57</cell><cell>9.57 36.38 15.30</cell><cell>0.00 63.10 40.33</cell><cell>64.10 71.00 67.75</cell><cell>27.50 47.80 47.80</cell><cell>33.78 43.35</cell><cell>--16.94</cell></row><row><cell></cell><cell>Multi-task</cell><cell>43.55</cell><cell>19.24</cell><cell>10.57</cell><cell>28.43</cell><cell>41.62</cell><cell>0.00</cell><cell>27.12</cell><cell>1.45</cell><cell>21.50</cell><cell>-</cell></row><row><cell>MiniGPT-v2 [357]</cell><cell>Zero-shot Sequential Finetune</cell><cell>32.16 28.81 44.35</cell><cell>6.83 10.40 29.89</cell><cell>0.07 7.25 11.90</cell><cell>11.58 31.55 36.95</cell><cell>35.20 41.35 42.58</cell><cell>0.00 0.00 0.00</cell><cell>12.20 36.10 38.10</cell><cell>0.03 6.15 6.15</cell><cell>12.26 25.45</cell><cell>-6.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE 29 :</head><label>29</label><figDesc>Upstream Knowledge Transfer T UK (i) relative to direct fine-tuning on each task, along with task score [S i A ] (%), for different algorithms A applied to ViLT. No CL algorithms achieve notable positive Knowledge Transfer, while the majority in fact hurt learning of new tasks.<ref type="bibr" target="#b94">[95]</ref> </figDesc><table><row><cell>Alg A</cell><cell>Params Trained</cell><cell>Task 1 VQAv2</cell><cell>Task 2 NLVR2</cell><cell>Task 3 SNLI-VE</cell><cell>Task 4 VCR</cell></row><row><cell>Direct FT</cell><cell>100%</cell><cell>[67.70]</cell><cell>[73.07]</cell><cell>[76.31]</cell><cell>[61.31]</cell></row><row><cell>SeqFT [430]</cell><cell>100%</cell><cell>0.13% [67.79]</cell><cell>-1.80% [72.66]</cell><cell>-3.33% [74.89]</cell><cell>-5.09% [59.47]</cell></row><row><cell>Frozen Enc [95]</cell><cell cols="5">7.88% -14.10% [58.15] -40.78% [63.66] -15.98% [69.45] -53.47% [41.90]</cell></row><row><cell>Frozen B9 [95]</cell><cell>25.92%</cell><cell>-0.58% [67.30]</cell><cell>-0.58% [72.94]</cell><cell cols="2">-3.31% [74.90] -15.49% [55.69]</cell></row><row><cell>ER [431]</cell><cell>100%</cell><cell>0.26% [67.87]</cell><cell>0.56% [73.20]</cell><cell>-2.89% [75.08]</cell><cell>-4.45% [59.70]</cell></row><row><cell>EWC [226]</cell><cell>100%</cell><cell>0.20% [67.84]</cell><cell>-2.79% [72.39]</cell><cell>-4.52% [74.38]</cell><cell>-4.86% [59.55]</cell></row><row><cell>Adapters [28]</cell><cell>13.02%</cell><cell>0.59% [68.10]</cell><cell>2.55% [73.66]</cell><cell>-0.56% [76.08]</cell><cell>-0.36% [61.18]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE 31 :</head><label>31</label><figDesc>Full forgetting results with different task orders.<ref type="bibr" target="#b94">[95]</ref> Task Order:VQAv2 → NLVR2 → SNLI-VE → VCR Task Order: SNLI-VE → VCR → VQAv2 → NLVR2</figDesc><table><row><cell>Evaluated on</cell><cell>Task</cell><cell>Task 2</cell><cell>Task 3</cell></row><row><cell>Checkpoint</cell><cell>VQAv2</cell><cell>NLVR2</cell><cell>SNLI-VE</cell></row><row><cell>After training on that task</cell><cell>[67.79]</cell><cell>[72.66]</cell><cell>[74.89]</cell></row><row><cell>Task 2: NLVR2</cell><cell>40.97% [40.02]</cell><cell>-</cell><cell>-</cell></row><row><cell>Task 3: SNLI-VE</cell><cell>39.25% [41.18]</cell><cell>43.81% [62.73]</cell><cell>-</cell></row><row><cell>Task 4: VCR</cell><cell>63.90% [24.47]</cell><cell>93.74% [51.24]</cell><cell>89.93% [37.52]</cell></row><row><cell>Evaluated on</cell><cell>Task 1</cell><cell>Task 2</cell><cell>Task 3</cell></row><row><cell>Checkpoint</cell><cell>SNLI-VE</cell><cell>VCR</cell><cell>VQAv2</cell></row><row><cell>After training on that task</cell><cell>[76.29]</cell><cell>[60.75]</cell><cell>[63.27]</cell></row><row><cell>Task 2: VCR</cell><cell>84.50% [39.99]</cell><cell>-</cell><cell>-</cell></row><row><cell>Task 3: VQAv2</cell><cell>85.86% [39.40]</cell><cell>91.47% [28.05]</cell><cell>-</cell></row><row><cell>Task 4: NLVR2</cell><cell>77.56% [42.97]</cell><cell>86.11% [29.97]</cell><cell>41.94% [36.73]</cell></row><row><cell cols="3">Task Order: NLVR2 → VQAv2 → VCR → SNLI-VE</cell><cell></cell></row><row><cell>Evaluated on</cell><cell>Task 1</cell><cell>Task 2</cell><cell>Task 3</cell></row><row><cell>Checkpoint</cell><cell>NLVR2</cell><cell>VQAv2</cell><cell>VCR</cell></row><row><cell>After training on that task</cell><cell>[73.25]</cell><cell>[66.55]</cell><cell>[59.10]</cell></row><row><cell>Task 2: VQAv2</cell><cell>58.06% [59.68]</cell><cell>-</cell><cell>-</cell></row><row><cell>Task 3: VCR</cell><cell>90.63% [52.16]</cell><cell>68.69% [20.87]</cell><cell>-</cell></row><row><cell>Task 4: SNLI-VE</cell><cell>91.75% [51.90]</cell><cell>62.59% [24.94]</cell><cell>34.04% [47.51]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>TABLE 32 :</head><label>32</label><figDesc>Evaluation results (%) of continual instruction tuning on COAST-domain. "Avg." and "Fgt." represent average accuracy and average forgetting, respectively. "Reh.", "Seq." and "Joint" denote rehearsal, sequential and joint training.<ref type="bibr" target="#b95">[96]</ref> </figDesc><table><row><cell>Methods</cell><cell>Params</cell><cell>Avg.</cell><cell>Fgt.</cell><cell cols="4">ChartQA DocVQA IconQA MedicalQA</cell></row><row><cell>Joint [96]</cell><cell>6.76B</cell><cell>42.79</cell><cell>-</cell><cell>21.99</cell><cell>20.08</cell><cell>64.37</cell><cell>64.73</cell></row><row><cell>CODA [432]</cell><cell>0.75M</cell><cell>36.06</cell><cell>2.72</cell><cell>15.03</cell><cell>16.93</cell><cell>58.96</cell><cell>53.33</cell></row><row><cell>Dual [433]</cell><cell>0.75M</cell><cell>35.80</cell><cell>2.79</cell><cell>14.92</cell><cell>16.77</cell><cell>58.60</cell><cell>52.92</cell></row><row><cell>L2P [58]</cell><cell>0.75M</cell><cell>35.06</cell><cell>2.91</cell><cell>14.77</cell><cell>16.73</cell><cell>57.55</cell><cell>51.20</cell></row><row><cell>LWF [202]</cell><cell>6.76B</cell><cell cols="2">27.06 15.05</cell><cell>14.07</cell><cell>13.19</cell><cell>37.93</cell><cell>43.05</cell></row><row><cell>EWC [226]</cell><cell>6.76B</cell><cell cols="2">25.82 15.23</cell><cell>13.73</cell><cell>11.89</cell><cell>35.12</cell><cell>42.53</cell></row><row><cell>Reh. [434]</cell><cell>6.76B</cell><cell cols="2">24.92 15.61</cell><cell>13.10</cell><cell>11.20</cell><cell>34.83</cell><cell>40.53</cell></row><row><cell>Seq. [96]</cell><cell>6.76B</cell><cell cols="2">24.02 15.83</cell><cell>11.77</cell><cell>11.29</cell><cell>33.73</cell><cell>39.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>TABLE 33 : Evaluation results (%) of continual instruction tuning on COAST-capability</head><label>33</label><figDesc>. "Conv.", "Desc.", "Reason" and "Ref." represent conversation, detail description, complex reasoning, and referring qa, respectively. "Reh.", "Seq." and "Joint" denote rehearsal, sequential, and joint training.<ref type="bibr" target="#b95">[96]</ref> </figDesc><table><row><cell>Methods</cell><cell>Params</cell><cell>Avg.</cell><cell>Fgt.</cell><cell cols="3">Conv. Desc. Reason</cell><cell>Ref.</cell></row><row><cell>Joint [96]</cell><cell>6.76B</cell><cell>57.95</cell><cell>-</cell><cell>62.48</cell><cell>43.45</cell><cell>74.02</cell><cell>51.84</cell></row><row><cell>CODA [432]</cell><cell>0.75M</cell><cell>54.21</cell><cell>4.99</cell><cell>58.91</cell><cell>40.12</cell><cell>70.71</cell><cell>47.08</cell></row><row><cell>Dual [433]</cell><cell>0.75M</cell><cell>53.62</cell><cell>5.01</cell><cell>58.09</cell><cell>39.85</cell><cell>70.03</cell><cell>46.52</cell></row><row><cell>L2P [58]</cell><cell>0.75M</cell><cell>53.31</cell><cell>5.04</cell><cell>57.90</cell><cell>39.33</cell><cell>69.70</cell><cell>46.32</cell></row><row><cell>LWF [202]</cell><cell>6.76B</cell><cell>44.15</cell><cell>9.77</cell><cell>46.11</cell><cell>24.16</cell><cell>61.43</cell><cell>44.90</cell></row><row><cell>EWC [226]</cell><cell>6.76B</cell><cell>43.69</cell><cell>9.72</cell><cell>46.23</cell><cell>24.20</cell><cell>60.11</cell><cell>44.20</cell></row><row><cell>Reh. [434]</cell><cell>6.76B</cell><cell>43.34</cell><cell>9.79</cell><cell>45.11</cell><cell>23.93</cell><cell>60.54</cell><cell>43.76</cell></row><row><cell>Seq. [96]</cell><cell>6.76B</cell><cell cols="2">41.51 10.56</cell><cell>44.29</cell><cell>23.25</cell><cell>58.39</cell><cell>40.13</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy on Specific Graph Theory Problems</head><p>MiniGPT-4 (Vicuna-7b) <ref type="bibr" target="#b355">[356]</ref> 50.67 48.69 0.00 0.00 0.00 5.95 0.00 0.00 BLIP-2 (FlanT5-xxl) <ref type="bibr" target="#b363">[364]</ref> 46.63 61.26 0.00 0.00 13.79 0.00 0.00 0.00 InstructBLIP (FlanT5-xl) <ref type="bibr" target="#b364">[365]</ref> 48.79 47.12 0.00 0.00 6.90 0.00 0.00 0.00 InstructBLIP (FlanT5-xxl) <ref type="bibr" target="#b364">[365]</ref> 48.25 52.88 0.00 0.00 12.07 0.00 0.00 0.00 Llava-v1.5-7b <ref type="bibr" target="#b140">[141]</ref> 53.37 47.12 0.00 3.12 1.72 0.00 0.00 0.00 Llava-v1.5-13b <ref type="bibr" target="#b140">[141]</ref> 52.83 47.12 0.00 4.69 3.45 0.00 0.00 0.00 Gemini (0-shot) <ref type="bibr" target="#b347">[348]</ref> 55. <ref type="bibr" target="#b51">52</ref> 48.69 0.00 0.00 3.45 1.72 0.00 0.00 GPT-4V (0-shot) <ref type="bibr" target="#b275">[276]</ref> 38.81 49.21 -3.12 --0.00 -GPT-4V (2-shot) <ref type="bibr" target="#b275">[276]</ref> 54.98 52.35 -6.25 --0.00 -GPT-4V (0-COT) <ref type="bibr" target="#b275">[276]</ref> 30.45 50.26 -7.69 --0.00 -GPT-4V (2-COT) <ref type="bibr" target="#b275">[276]</ref> 54.71 52.87 -6.25 --0.00 thus solving the problem of automatically discovering new classes in continuous data streams while mitigating catastrophic forgetting. Dong Li et al. <ref type="bibr" target="#b156">[157]</ref> proposed the Mecoin framework, which employs Structured Memory Units (SMU) and a Memory Construction Module (MeCo) for efficient storage and updating of class prototypes. They introduced the Memory Representation Adaptation Module (MRaM) and the Graph Knowledge Interchange Module (GKIM) to reduce parameter fine-tuning, lower the forgetting rate, and enhance the model's generalization ability.</p><p>Linglan Zhao et al. <ref type="bibr" target="#b408">[409]</ref> proposed the SAFE framework, which, in the first session, inherits the knowledge of the pretrained model through knowledge transfer loss. In subsequent sessions, the framework balances model stability and adaptability by fixing slow parameters and updating fast parameters. It introduces an entropy-based aggregation strategy to dynamically fuse the advantages of two types of learners. This approach enables the efficient use of the rich knowledge from pre-trained models in continual learning while maintaining the model's adaptability and stability when facing new data.</p><p>Wenju Sun et al. <ref type="bibr" target="#b157">[158]</ref> proposed the RP2F framework, which directly combines the posterior parameters of new and old tasks. They introduced a parameter robustness prior and used perturbation methods to approximate the Hessian matrix, enabling effective knowledge sharing and backward knowledge transfer.</p><p>Xiaoqian Liu et al. <ref type="bibr" target="#b158">[159]</ref> proposed the HAMMER framework, which identifies shared knowledge and guides multilingual learning through online knowledge analysis and a hierarchical language evaluation mechanism, effectively alleviating the forgetting problem.</p><p>Hao Yu et al. <ref type="bibr" target="#b159">[160]</ref> proposed the FedCBC framework, which overcomes forgetting through category-specific binary classifiers and selective knowledge fusion.</p><p>Xiaochen Li et al. <ref type="bibr" target="#b160">[161]</ref> proposed the TS-ILM framework, which includes a task-level temporal pattern extractor and a time-sensitive example selector. This framework effectively captures cross-task temporal patterns, selects representative frames for replay, reduces information redundancy, and enhances memory retention.</p><p>Depeng Li et al. <ref type="bibr" target="#b161">[162]</ref> proposed the AutoActivator model, which dynamically adapts neural units to new tasks, enabling ondemand network expansion. This approach addresses the issue of forgetting old classes when learning new classes incrementally in class-incremental learning.</p><p>Tom Fischer et al. <ref type="bibr" target="#b162">[163]</ref> proposed iNeMo, an incremental neural grid model, which achieves efficient class-incremental learning through latent space initialization and position regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Method Innovation</head><p>Huiping Zhuang et al. <ref type="bibr" target="#b164">[165]</ref> proposed the sample-free Generalized Analytical Continual Learning (GACL) technique, which avoids catastrophic forgetting through analytical learning. It establishes the equivalence between incremental learning and joint training, effectively addressing the challenges of handling mixed data categories.</p><p>Ang Bian et al. <ref type="bibr" target="#b409">[410]</ref> proposed the C-Flat method, which enhances continual learning (CL) performance by optimizing the flatness of the loss landscape. The method is easy to integrate and outperforms traditional approaches comprehensively. Yan Fan et al. <ref type="bibr" target="#b165">[166]</ref> proposed the Dynamic Subgraph Distillation (DSGD) method, which uses structural and semantic information for stable knowledge distillation. This approach enhances the model's robustness to distribution shifts and adapts to different supervision settings, addressing the practical deployment challenges in continual learning that arise from relying on a large number of labeled samples.</p><p>Li Jiao et al. <ref type="bibr" target="#b166">[167]</ref> proposed the VQ-Prompt method, which utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection. They introduced gradient estimation, regularization terms, and representation statistics to stabilize task knowledge learning and improve continual learning performance. Ameya Prabhu et al. <ref type="bibr" target="#b167">[168]</ref> proposed the RanDumb method, which uses random transformations and linear classifiers to investigate whether the representations produced by continual learning algorithms are truly effective in online continual learning.</p><p>Yue Lu et al. <ref type="bibr" target="#b410">[411]</ref> proposed two consistency conditions and an invariant prompt distribution constraint to reduce interference from new tasks on old tasks, overcoming catastrophic forgetting.</p><p>Botos Csaba et al. <ref type="bibr" target="#b168">[169]</ref> proposed the IWMS method, which addresses label delay by prioritizing the memory of samples similar to new data. This approach helps mitigate the label delay issue in online continual learning.</p><p>Qiwei Li et al. <ref type="bibr" target="#b169">[170]</ref> proposed the Progressive Prototype Evolution (PPE) method, which learns class prototypes during the online learning phase to alleviate forgetting. They introduced prototype similarity preservation and prototype-guided gradient constraint modules, effectively combating dual forgetting.</p><p>Chengyi Yang et al. <ref type="bibr" target="#b170">[171]</ref> proposed the Gradient Projection Common Null Space (GPCNS), which enhances plasticity by utilizing gradient information from old tasks. They integrated feature and gradient information through a collaborative framework, improving the performance of continual learning.</p><p>Zeyang Zhang et al. <ref type="bibr" target="#b411">[412]</ref> introduced a factor-based taskmodule router to optimize task routing and reduce forgetting. They designed an invariance-based architecture search mechanism to capture shared knowledge between tasks, enhancing knowledge sharing. This approach addresses the static assumptions and catastrophic forgetting issues in Graph Neural Architecture Search (GNAS) when handling continuous graph tasks.</p><p>Jeevan Thapa et al. <ref type="bibr" target="#b412">[413]</ref> proposed a non-parametric Bayesian method that infers network depth using a Beta process and adapts the width through a conjugate Bernoulli process. This approach enables joint inference of both network structure and weights, enhancing continual learning performance.</p><p>Nicolas Michel et al. <ref type="bibr" target="#b413">[414]</ref> proposed a new method based on momentum knowledge distillation, which dynamically updates the teacher model using exponential moving averages. This approach effectively overcomes the challenges of data stream processing and catastrophic forgetting in online continual learning.</p><p>Yichen Wen et al. <ref type="bibr" target="#b171">[172]</ref> proposed the CILA algorithm, which improves model performance in continual tasks through an adaptive distillation coefficient and theoretical performance guarantees.</p><p>Yichen Wu et al. <ref type="bibr" target="#b172">[173]</ref> proposed the POCL algorithm, which models task relationships through Pareto optimization and dynamically adjusts weights to reduce forgetting.</p><p>Hongming Piao et al. <ref type="bibr" target="#b173">[174]</ref> proposed the Powder algorithm, which enables prompt-based dual knowledge transfer. By selectively transferring knowledge based on task relevance, it reduces communication costs, addressing the challenge of cross-task and cross-client knowledge transfer in federated continual learning.</p><p>Weichen Lin et al. <ref type="bibr" target="#b414">[415]</ref> proposed the Dynamic Gradient Calibration (DGC) method, which effectively utilizes historical data to calibrate gradients. By combining it with existing continual learning methods, DGC helps alleviate the issue of catastrophic forgetting caused by data stream updates in continual learning. Doyoung Kim et al. <ref type="bibr" target="#b174">[175]</ref> proposed an adaptive prompting method, AdaPromptCL, which effectively adapts to varying degrees of semantic change through dynamic semantic grouping and prompt adjustment. This approach addresses the challenge of task-specific semantic variations in continual learning that fixed prompting strategies face.</p><p>Jason Yoo et al. <ref type="bibr" target="#b174">[175]</ref> proposed the Layerwise Proximal Replay (LPR) method, which adjusts the optimization geometry to balance the learning of new and old data, enabling progressive changes. This approach reduces catastrophic forgetting and underfitting, improving the model's adaptability to both new and old data.</p><p>Zhen Zhu et al. <ref type="bibr" target="#b415">[416]</ref> proposed a dynamic weight prediction method and attention-weighted PCA feature compression, enabling efficient updates and storage compression in continual learning. This approach enhances model accuracy and flexibility.</p><p>Yanshuo Liang et al. <ref type="bibr" target="#b175">[176]</ref> proposed the InfLoRA method, which injects parameter reparameterization into pre-trained weights, effectively fine-tuning within a subspace. The method designs subspace elimination to prevent new tasks from interfering with old tasks, addressing the issue of forgetting old tasks when adapting to new tasks in continual learning.</p><p>Chaoxi Niu et al. <ref type="bibr" target="#b416">[417]</ref> proposed a Laplace smoothing-based graph task analysis and prompting method, which enables  Shiwen Ni et al. <ref type="bibr" target="#b424">[425]</ref> proposed the F-Learning paradigm, which first forgets old knowledge before learning new knowledge. Experiments show that it outperforms traditional finetuning, and the LoRA parameter reduction method achieves results comparable to full-parameter fine-tuning.</p><p>Junhao Zheng et al. <ref type="bibr" target="#b425">[426]</ref> proposed the SEQ method, which enhances the performance of LLMs in incremental learning through simple strategies, reducing both parameters and training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Instruction Fine-tuning</head><p>To mitigate catastrophic forgetting, Continual-T0 <ref type="bibr" target="#b426">[427]</ref> uses a memory buffer for rehearsal <ref type="bibr" target="#b218">[219]</ref>, storing data from previous tasks and replaying them during training.</p><p>ConTinTin <ref type="bibr" target="#b237">[238]</ref> proposed InstructionSpeak, which includes two strategies that fully leverage task instructions to improve both forward and backward transfer. The first strategy involves learning from negative outputs, while the second focuses on revisiting the instructions of previous tasks.</p><p>ELM <ref type="bibr" target="#b240">[241]</ref> trains a small expert adapter for each task on top of the LLM. It then adopts a retrieval-based approach to select the most relevant expert LLM for each new task.</p><p>Based on the parameter-efficient tuning (PET) framework, OLoRA <ref type="bibr" target="#b238">[239]</ref> introduces orthogonal low-rank adaptation for CIT. O-LoRA gradually learns new tasks in orthogonal subspaces while preserving the LoRA parameters learned from past tasks, thereby minimizing catastrophic forgetting.</p><p>DAPT <ref type="bibr" target="#b239">[240]</ref> introduces an innovative dual-attention framework, which coordinates the learning and selection of LoRA parameters through a dual-attention learning and selection module.</p><p>LLaMA PRO <ref type="bibr" target="#b241">[242]</ref> introduces an innovative block expansion technique that allows new knowledge to be injected into the LLM while efficiently retaining the initial functionality through post-training.</p><p>AdaptLLM <ref type="bibr" target="#b242">[243]</ref> adapts the LLM to different domains by enriching the original training corpus with a series of contentrelated reading comprehension tasks. These tasks are designed to help the model leverage domain-specific knowledge while enhancing prompt performance.</p><p>[428] designed an adapt-retrieve-revise process to enable the LLM to adapt to new domains. <ref type="bibr" target="#b428">[429]</ref> analyzed LLMs that continuously adapt to different domains and found that the order of training data has a significant impact on the performance of LLMs.</p><p>DynaInst <ref type="bibr" target="#b243">[244]</ref> proposes a hybrid approach that combines dynamic instruction replay with a local minima-inducing regularizer. These two components enhance the generalization of the LLM while reducing memory and computational usage in the replay module. ities (such as images, 3D, audio, and video) with frozen large language models and enable cross-modal reasoning without joint optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Method Innovation</head><p>Minh Le et al. <ref type="bibr" target="#b89">[90]</ref> revealed the connection between self-attention and mixture-of-experts, proposing the Non-linear Residual Gate (NoRGa) to enhance the continual learning performance of multimodal large language models. Zangwei Zheng et al. <ref type="bibr" target="#b295">[296]</ref> proposed the ZAF method, which preserves knowledge through zero-shot stability regularization. They introduced the EMA-based parameter-efficient EMA-LoRA architecture, achieving the decoupling of learning and forgetting.</p><p>Huancheng Chen et al. <ref type="bibr" target="#b91">[92]</ref> proposed DualLoRA, which utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity, thereby improving the efficiency and effectiveness of continual learning in multimodal large language models. Weicai Yan et al. <ref type="bibr" target="#b296">[297]</ref> proposed the Low-Rank Prompt Interaction (LPI) method, which enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. They introduced task semantic distance to guide prompt learning, addressing the insufficient interaction between modalities and tasks in continual learning of multimodal large language models (MLLMs), thereby reducing catastrophic forgetting.</p><p>Didi Zhu et al. <ref type="bibr" target="#b297">[298]</ref> proposed the Model Tailor method, which alleviates catastrophic forgetting during fine-tuning by retaining most of the pre-trained parameters and only replacing a small number of fine-tuned parameters. This approach helps to mitigate the forgetting problem while improving performance on new tasks. Tianxiang Hao et al. <ref type="bibr" target="#b436">[437]</ref> proposed a quantized prompt technique, which uses quantization errors as a form of regularization. They designed an efficient quantization-aware training algorithm that enhances the model's generalization ability    Clea Rebillard et al. <ref type="bibr" target="#b299">[300]</ref> proposed the Continual Visual Mapping (CVM) method, which reduces forgetting and improves generalization by mapping the representations of small visual models to the knowledge space of a fixed large language model. Marco Mistretta et al. <ref type="bibr" target="#b300">[301]</ref> proposed the RE-tune method, which freezes the backbone of the model and trains adapters, using text prompts to guide training. This approach enables privacy-preserving, computationally efficient, and antiforgetting incremental learning. It optimizes pre-trained multimodal biomedical models for incremental learning scenarios in chest X-ray multi-label classification, addressing challenges related to computational resources, data privacy, and catastrophic forgetting.</p><p>Yuliang Cai et al. <ref type="bibr" target="#b301">[302]</ref> proposed the CluMo method, which employs a two-stage training and modality fusion prompt strategy to combine visual and textual modalities, thereby enhancing the performance of multimodal large models in continual learning and improving their ability to retain old knowledge.</p><p>Yiduo Guo et al. <ref type="bibr" target="#b438">[439]</ref> proposed three strategies to overcome the stability gap, including multi-round pretraining on smallscale high-quality datasets, selecting high-quality sub-corpora for pretraining, and employing a data-mixing strategy using data similar to pretraining data. These strategies effectively enhanced the performance and adaptability of multimodal large language models in new domains.</p><p>Jinghan He et al. <ref type="bibr" target="#b439">[440]</ref> proposed a task similarity-guided regularization and model expansion method, which effectively enhances the continual learning capability of multimodal large models.</p><p>Junhao Zheng et al. <ref type="bibr" target="#b302">[303]</ref> proposed the Fwd-Prompt method, which utilizes gradient projection techniques and a multimodal prompt pool to achieve anti-forgetting and positive transfer, without requiring old samples and with minimal parameter updates. This approach improves the performance of multimodal large models in multimodal continual learning tasks.</p><p>Yuliang Cai et al. <ref type="bibr" target="#b440">[441]</ref> proposed dynamic model expansion and task attention layers to adapt to different tasks, while employing knowledge distillation and experience replay to mitigate catastrophic forgetting in multimodal large models.</p><p>[304] proposed an incremental learning strategy for multimodal large language models, the CPE-CLIP method. By using learnable prompts and regularization strategies, it achieves parameter-efficient transfer learning for multimodal large language models, reducing the parameter size and training costs, while enhancing the performance of few-shot class incremental learning in multimodal large models.</p><p>Zilun Zhang et al. <ref type="bibr" target="#b304">[305]</ref> proposed the model-agnostic selfuncompression method, TG, which decompresses knowledge into the training corpus to reduce forgetting. They also designed the TG-SFT strategy for supervised fine-tuning of MLLMs, addressing the common issue of catastrophic forgetting encountered during post-training or supervised fine-tuning (SFT) on domain-specific data for multimodal large models.</p><p>Ke Wang et al. <ref type="bibr" target="#b305">[306]</ref> proposed the LiNeS technique, which performs parameter updates with layer-specific depth differentiation, preserving the generalization ability of pretraining while improving fine-tuning task performance. This approach addresses the issue of forgetting prior knowledge during the fine-tuning of multimodal pre-trained models.</p><p>Brian Lester et al. <ref type="bibr" target="#b293">[294]</ref> proposed an end-to-end learning</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Anygpt: Unified multimodal llm with discrete sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.12226</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">April 2023. 2023</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Motiongpt: Human motion as a foreign language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motiongpt: Finetuned llms are generalpurpose motion generators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="7368" to="7376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sphinx-x: Scaling data and parameters for a family of multi-modal large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05935</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ical: Continual learning of multimodal agents by transforming trajectories into actionable insights</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">2406</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08394</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single image unlearning: Efficient machine unlearning in multimodal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.12523</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06512</idno>
		<title level="m">Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Embodiedgpt: Vision-language pre-training via embodied chain of thought</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multimodal task vectors enable many-shot multimodal in-context learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arbelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.15334</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiood: Scaling outof-distribution detection for multiple modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chatzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17419</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hypergraph multi-modal large language model: Exploiting eeg and eye-tracking modalities to evaluate heterogeneous responses for video understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="7316" to="7325" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-modality co-learning for efficient skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="4909" to="4918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cloud-device collaborative learning for multimodal large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">655</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Implicit multimodal alignment: On the generalization of frozen llms to multimodal inputs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shukor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16700</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caption-aware multimodal relation extraction with mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="1148" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10140</idno>
		<title level="m">Libra: Building decoupled vision system on large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards multimodal-augmented pre-trained language models via selfbalanced expectation-maximization iteration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4670" to="4679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Decoding style: Efficient fine-tuning of llms for imageguided outfit recommendation with preference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Forouzandehmehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farrokhsiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Achan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12150</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clova: A closed-loop visual assistant with tool usage and update</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">268</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Grounding multimodal large language models in actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07904</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Controlmllm: Training-free visual prompt learning for multimodal large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21534</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Membership inference attacks against large vision-language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Rocamora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.02902</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Failures are fated, but can be faded: Characterizing and mitigating unwanted behaviors in large-scale vision and language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taparia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Senanayake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07145</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-tuning large vision-language models as decision-making agents via reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10292</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parameterefficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gpt understands, too</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Docvqa: A dataset for vqa on document images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF winter conference on applications of computer vision</title>
		<meeting>the IEEE/CVF winter conference on applications of computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2200" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Chartqa: A benchmark for question answering about charts with visual and logical reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10244</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dvqa: Understanding data visualizations via question answering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14517</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Textcaps: a dataset for image captioning with reading comprehension</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="742" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Captioning images taken by people who are blind</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="417" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Connecting vision and language with localized narratives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="647" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Nocaps: Novel object captioning at scale,&quot; in ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8948" to="8957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Who&apos;s waldo? linking people across text and images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1374" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generating easy-to-understand referring expressions for target identifications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Itamochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5794" to="5803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="776" to="780" />
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer: 20th International Conference</title>
		<meeting><address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 18-22, 2018. 2018</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
	<note>SPECOM 2018</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Contextual: Evaluating context-sensitive text-rich visual reasoning in large multimodal models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.13311</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">See say and segment: Teaching lmms to overcome false premises</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biamby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">469</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Llava-ultra: Large chinese language and vision assistant for ultrasound</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8845" to="8854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Llm-assisted multi-teacher continual learning for visual question answering in robotic surgery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16664</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Making large language models better planners with reasoning-decision alignment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="73" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Continual pre-training mitigates forgetting in language and vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page">106492</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Elle: Efficient lifelong pre-training for emerging data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ernie 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to prompt for continual learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Data engineering for scaling language models to 128k context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10171</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pilora: Prototype guided incremental lora for federated class-incremental learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="141" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ftf-er: Feature-topology fusion-based experience replay method for continual graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Tai</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8336" to="8344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dense network expansion for class incremental learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">867</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Conditional channel gated networks for taskaware continual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3931" to="3940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generating prompts in latent space for rehearsal-free continual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8913" to="8922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">On the diminishing returns of width for continual learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lakshman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06398</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Regularizing with pseudonegatives for continual self-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.05101" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Becotta: Input-dependent online blending of experts for continual test-time adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.08712</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A statistical theory of regularization-based continual learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06213</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Poet: Prompt offset tuning for continual human action adaptation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="436" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">One-stage prompt-based continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="163" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Semantic editing increment benefits zero-shot composed image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1245" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Task confusion and catastrophic forgetting in class-incremental learning: A mathematical framework for discriminative and generative modelings</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-M</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20768</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Open-world dynamic prompt and continual visual representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raychaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dabeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="357" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Revisiting supervision for continual representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cygert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trzci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learn to memorize and to forget: A continual learning perspective of dynamic slam</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="41" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Promptfusion: Decoupling stability and plasticity for continual learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07223</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Continuous training and fine-tuning for domain-specific language models in medical question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00204</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morgado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.03883</idno>
		<title level="m">Saullm-7b: A pioneering large language model for law</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">K2: A foundation language model for geoscience knowledge understanding and utilization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 17th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Econet: Effective continual pretraining of language models for event temporal reasoning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15283</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Ecomgpt-ct: Continual pre-training of e-commerce large language models with semi-structured data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.15696</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Rθ3: Reinforced readerranker for open-domain question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00023</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">End-to-end open-domain question answering with bertserini</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01718</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Reinforcement learning with token-level feedback for controllable text generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.11558</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Embracing change: Continual learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1028" to="1040" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">A practitioner&apos;s guide to continual multimodal pretraining</title>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Udandarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dziadzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.14471</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Citb: A benchmark for continual instruction tuning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Namazi-Rad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14510</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">X-instructblip: A framework for aligning x-modal instruction-aware representations to llms and emergent cross-modal reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Panagopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.18799</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Mixture of experts meets prompt-based continual learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14124</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Clap4clip: Continual learning with probabilistic finetuning for vision-language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19137</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Dual low-rank adaptation for continual learning with pre-trained models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gazagnadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00623</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Hvclip: High-dimensional vector in clip for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vesdapunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="36" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Coin: A benchmark of continual instruction tuning for multimodel large language model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08350</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Climb: A continual learning benchmark for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto Alva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chochlakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="440" to="469" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.02564</idno>
		<title level="m">Continual llava: Continual instruction tuning in large vision-language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Vilcobench: Video language continual learning benchmark</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deldari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.13123</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Towards lifelong scene graph generation with knowledge-ware in-context prompt learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.14626</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Freezeomni: A smart and low latency speech-to-speech dialogue model with frozen llm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00774</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Interactive continual learning: Fast and slow thinking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">892</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Maven: An effective multi-granularity hybrid visual encoding framework for multimodal large language model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hongrui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.12321</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Mova: Adapting mixture of vision experts to multimodal context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.13046</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Mome: Mixture of multimodal experts for generalist multimodal large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.12709</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Meteor: Mambabased traversal of rationale for large language and vision models</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.15574</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.06101</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Lumen: Unleashing versatile vision-centric capabilities of large multimodal models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07304</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Octopus: A multi-modal llm with parallel recognition and sequential understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03496</idno>
		<title level="m">Wings: Learning multimodal llms without text-only forgetting</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Cantor: Inspiring multimodal chain-of-thought of mllm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="9096" to="9105" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Autom3l: An automated multimodal machine learning framework with large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8586" to="8594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Detached and interactive multimodal learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="5470" to="5478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Multimodal unlearnable examples: Protecting data against multimodal contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Cream: Coarse-to-fine retrieval and multi-modal efficient tuning for document vqa</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="925" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Self-adaptive fine-grained multi-modal data augmentation for semi-supervised muti-modal coreference resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8576" to="8585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Semantic alignment for multimodal large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3489" to="3498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Collaborative training of tiny-large vision language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4928" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Efficient training for multilingual visual speech recognition: Pre-training with discretized visual speech representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Large multi-modality model assisted ai-generated image quality assessment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7803" to="7812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Worldgpt: Empowering llm as multimodal world model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Q-align: Teaching lmms for visual scoring via discrete text-defined levels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.17090</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Flextron: Many-in-one flexible large language model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10260</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Next-gpt: Any-to-any multimodal llm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05519</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Densefusion-1m: Merging vision experts for comprehensive multimodal perception</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08303</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">E2e-mfd: Towards end-to-end synchronous multimodal fusion detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09323</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Towards neuron attributions in multi-modal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Code: Contrasting selfgenerated description to combat hallucination in large multimodal models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01920</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Understanding information storage and transfer in multi-modal large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04236</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Advancing multimodal large language models with quantization-aware scale learning for efficient adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page">591</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Multimodal llm enhanced cross-lingual cross-modal retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8296" to="8305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Erl-mr: Harnessing the power of euler feature representations for balanced multi-modal learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Adaptive multimodality prompt learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8672" to="8680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Enhancing cross-modal fine-tuning with gradually intermediate modality generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09003</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Improving context understanding in multimodal large language models via multimodal composition learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schlarmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.12336</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Attention prompting on image for large vision-language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="251" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Ivtp: Instruction-guided visual token pruning for large vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="214" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Chattracker: Enhancing visual tracking performance via chatting with multimodal large language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.01756</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Optimus-1: Hybrid multimodal memory empowered agents excel in longhorizon tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03615</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Cumo: Scaling multimodal llm with co-upcycled mixture-of-experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.05949</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Visual anchors are strong information aggregators for multimodal large language model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17815</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Accelerating pre-training of multimodal llms via chain-of-sight</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.15819</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Dense connector for mllms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.13800</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Weakly supervised gaussian contrastive grounding with large multimodal models for video question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="5289" to="5298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Q-moe: Connector for mllms with text-driven routing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Continual learning with the neural tangent ensemble</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daruwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.17394</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Incremental learning of retrievable skills for efficient continual task adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Woo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.22658</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Mitigate catastrophic remembering via continual knowledge purification for noisy lifelong person re-identification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="5790" to="5799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Prior-free balanced replay: Uncertaintyguided reservoir sampling for long-tailed continual learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="2888" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Taskaware orthogonal sparse network for exploring shared knowledge in continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Learning to continually learn with the bayesian principle</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18758</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Regularizing with pseudo-negatives for continual self-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Self-composing policies for scalable continual reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ceberio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Rapid learning without catastrophic forgetting in the morris water maze</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boopathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Fiete</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Diffusion-driven data replay: A novel approach to combat forgetting in federated class continual learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="303" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Promptccd: Learning gaussian mixture prompt pool for continual category discovery</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Cendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="188" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">An efficient memory module for graph few-shot class-incremental learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.06659</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Incremental learning via robust parameter posterior fusion</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="4292" to="4301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label learning for incremental multilingual text recognition</title>
		<author>
			<persName><forename type="first">X.-Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8750" to="8758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Overcoming spatial-temporal catastrophic forgetting for federated class-incremental learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5280" to="5288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Ts-ilm: Class incremental learning for online action detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiaochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Harnessing neural unit dynamics for effective and scalable class-incremental learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02428</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">inemo: Incremental neural mesh models for robust class-incremental learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jesslen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="357" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">A topology-aware graph coarsening framework for continual graph learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03077</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Gacl: Exemplar-free generalized analytic continual learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Dynamic sub-graph distillation for robust semi-supervised continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">935</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Vector quantization prompting for continual learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20444</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Random representations outperform online continually learned representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Label delay in online continual learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Csaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Üller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Progressive prototype evolving for dual-forgetting mitigation in non-exemplar online continual learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Introducing common null space of gradients for gradient projection methods in continual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5489" to="5497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">Provable contrastive continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18756</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-K</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Federated continual learning via prompt-based dual knowledge transfer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">One size fits all for semantic shifts: Adaptive prompt tuning for continual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12048</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Inflora: Interference-free low-rank adaptation for continual learning</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">647</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">F-oal: Forward-only online analytic learning with fast training and low memory footprint in class incremental learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Prospective representation learning for nonexemplar class-incremental learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Addressing imbalance for class incremental learning in medical image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="2467" to="2476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Domain shared and specific prompt learning for incremental monocular depth estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8306" to="8315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Multi-layer rehearsal feature augmentation for class-incremental learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b181">
	<monogr>
		<title level="m" type="main">Gradual divergence for seamless adaptation: A novel domain incremental learning method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jeeveswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zonooz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.16231</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Expandable subspace ensemble for pre-trained model-based class-incremental learning</title>
		<author>
			<persName><forename type="first">D.-W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">564</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main">Visual instruction tuning towards general-purpose multimodal model: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.16602</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b184">
	<monogr>
		<title level="m" type="main">Large multimodal models: Notes on cvpr 2023 tutorial</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14895</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Generative agents: Interactive simulacra of human behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual acm symposium on user interface software and technology</title>
		<meeting>the 36th annual acm symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<monogr>
		<title level="m" type="main">Vision-language models are zero-shot reward models for reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rocamonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12921</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Aligning large multimodal models with factually augmented rlhf</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14525</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Continual panoptic perception: Towards multi-modal incremental interpretation of remote sensing images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Cp-prompt: Composition-based cross-modal prompting for domain-incremental continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2729" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Mmal: Multi-modal analytic learning for exemplarfree audio-visual class incremental tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2428" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08698</idno>
		<title level="m">Continual multimodal knowledge graph construction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Continual self-supervised learning: Towards universal multi-modal medical data representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="11" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Zeroshot generalizable incremental learning for vision-language object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01680</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Stella: Continual audio-video pre-training with spatiotemporal localized alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Rcs-prompt: Learning prompt to rearrange class space for prompt-based continual learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Preventing zero-shot transfer degradation in continual learning of visionlanguage models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="125" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Conditional prompt learning for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">825</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main">Advancing cross-domain discriminability in continual learning of vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.18868</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main">Generalized variational continual learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12328</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Continual learning at the edge: Real-time training on smartphone devices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Graffieti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13127</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Sparse coding in a dual memory system for lifelong learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zonooz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9714" to="9722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Sparsity and heterogeneous dropout for continual learning in the null space of neural activations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nooralinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Lifelong Learning Agents</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="617" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">etag: Class-incremental learning via embedding distillation and taskoriented generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">599</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Kfc: Knowledge reconstruction and feedback consolidation enable efficient and effective continual generative learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>in The Second Tiny Papers Track at ICLR 2024</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Continual learning of a mixed sequence of similar and dissimilar tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="493" to="511" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Semantic drift compensation for classincremental learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V D</forename><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6982" to="6991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<monogr>
		<title level="m" type="main">Efficient lifelong learning with a-gem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Mitigating forgetting in online continual learning via instanceaware parameterization</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17" to="466" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">A continual learning survey: Defying forgetting in classification tasks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3366" to="3385" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Continual learning with filter atom swapping</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Dualnet: Continual learning, fast and slow</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="16" to="131" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Parameter-level soft-masking for continual learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">505</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Memory efficient data-free distillation for continual learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">109875</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Variational data-free knowledge distillation for continual learning</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12" to="618" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Topologypreserving class-incremental learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="254" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Training networks in null space of feature covariance for continual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Decoupling learning and remembering: A bilevel memory framework with knowledge projection for task-incremental learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Ilcoc: An incremental learning framework based on contrastive one-class classifiers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3580" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Divide and not forget: Ensemble of selec-tively trained experts in continual learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rypeść</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cygert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trzci Ński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Prototype reminiscence and augmented asymmetric knowledge aggregation for non-exemplar class-incremental learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1772" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the national academy of sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Uncertainty-based continual learning with adaptive regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<monogr>
		<title level="m" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01547</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b230">
	<monogr>
		<title level="m" type="main">A neural dirichlet process mixture model for task-free continual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00689</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main">Representational continuity for unsupervised continual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06976</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Self-supervised models are continual learners</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G T</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Continual learners are incremental model generalizers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="40" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Generative negative text replay for continual vision-language pretraining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Audio-visual classincremental learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7799" to="7811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Class-incremental grouping network for continual audio-visual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7788" to="7798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<monogr>
		<title level="m" type="main">Contintin: Continual learning from task instructions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.08512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Orthogonal subspace learning for language model continual learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14152</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b239">
	<monogr>
		<title level="m" type="main">Dapt: A dual attention framework for parameter-efficient continual learning of large language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.08295</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Exploring the benefits of training expert language models over instruction tuning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="702" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<monogr>
		<title level="m" type="main">Llama pro: Progressive llama with block expansion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02415</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Adapting large language models via reading comprehension</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Large-scale lifelong learning of in-context instructions and how to tackle it</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">589</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<monogr>
		<title level="m" type="main">Train-attention: Meta-learning where to focus in continual knowledge learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.16920</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b245">
	<monogr>
		<title level="m" type="main">D-cpt law: Domain-specific continual pre-training scaling law for large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Que</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01375</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b246">
	<monogr>
		<title level="m" type="main">Copal: Continual pruning in large language generative models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.02347</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Magmax: Leveraging model merging for seamless continual learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trzci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cygert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="379" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Sapt: A shared attention framework for parameter-efficient continual learning of large language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">661</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<monogr>
		<title level="m" type="main">Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01244</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Loramoe: Alleviating world knowledge forgetting in large language models via moe-style plugin</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1932" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title level="m" type="main">Chinese tiny llm: Pretraining a chinesecentric large language model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.04167</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b253">
	<monogr>
		<title level="m" type="main">Gpts are gpts: An early look at the labor market impact potential of large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.10130</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">A literature survey on open source large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kukreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 7th International Conference on Computers in Management and Business</title>
		<meeting>the 2024 7th International Conference on Computers in Management and Business</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Chatgpt for good? on opportunities and challenges of large language models for education</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seßler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>K Üchemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bannert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dementieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ünnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Üllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and individual differences</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">102274</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<monogr>
		<title level="m" type="main">A survey of large language models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main">A comprehensive overview of large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saqib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06435</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Interpretutor: Using large language models for interpreter assessment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ünl Ü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HiT-IT 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="78" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">A survey on large language models for recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10792</idno>
		<title level="m">Instruction tuning for large language models: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b263">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.20178</idno>
		<title level="m">Llms can evolve continually on modality for x-modal reasoning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Mind the interference: Retaining pre-trained knowledge in parameter efficient continual learning of vision-language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="346" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Generative multi-modal models are good class incremental learners</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">717</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Pre-trained vision and language transformers are few-shot incremental learners</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">890</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<monogr>
		<title level="m" type="main">Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.05849</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b268">
	<monogr>
		<title level="m" type="main">Clip with generative latent replay: a strong baseline for incremental learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frascaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panariello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bonicelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.15793</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b269">
	<monogr>
		<title level="m" type="main">Coleclip: Open-domain continual learning via joint task prompt and vocabulary learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.10245</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
		<title level="m" type="main">Investigating the catastrophic forgetting in multimodal large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10313</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b271">
	<monogr>
		<title level="m" type="main">Adapt-inf ty: Scalable lifelong multimodal instruction tuning via dynamic data selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maharana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10636</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<title level="m" type="main">Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.08202</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b273">
	<monogr>
		<title level="m" type="main">Moextend: Tuning new experts for modality and task extension</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.03511</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b277">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12966</idno>
		<title level="m">Qwen-vl: A frontier large vision-language model with versatile abilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b278">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="0198">2024. 198</date>
			<biblScope unit="page" from="24" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<monogr>
		<title level="m" type="main">Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.21075</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Llavanext: Improved reasoning, ocr, and world knowledge</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<monogr>
		<title level="m" type="main">Mmbench: Is your multi-modal model an all-around player?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2025">2025</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="216" to="233" />
		</imprint>
	</monogr>
	<note>in ECCV</note>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">Cheap and quick: Efficient vision-language instruction tuning for large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<monogr>
		<title level="m" type="main">The dawn of lmms: Preliminary explorations with gpt-4v (ision)</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.17421</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b287">
	<monogr>
		<title level="m" type="main">Gemini: a family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b288">
	<monogr>
		<title level="m" type="main">Internlm: A multilingual language model with progressively enhanced capabilities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b290">
	<monogr>
		<title level="m" type="main">Cogvlm: Visual expert for pretrained language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.03079</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">Parameter and computation efficient transfer learning for vision-language pre-trained models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9556" to="9567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Pivot: Prompting for video continual learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Alcázar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alfarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alhamoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">223</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Stabilizing zero-shot prediction: A novel antidote to forgetting in continual vision-language tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Low-rank prompt interaction for continual vision-language retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8257" to="8266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<monogr>
		<title level="m" type="main">Model tailor: Mitigating catastrophic forgetting in multimodal large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.12048</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10868</idno>
		<title level="m">Llaca: Multimodal large language continual assistant</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b299">
	<monogr>
		<title level="m" type="main">Continually learn to map visual concepts to large language models in resource-constrained environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rebillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krutsylo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lomonaco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08279</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b300">
	<monogr>
		<title level="m" type="main">Re-tune: Incremental fine tuning of biomedical vision-language models for multi-label chest x-ray classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mistretta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17827</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b301">
	<monogr>
		<title level="m" type="main">Clumo: Cluster-based modality fusion prompt for continual learning in visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.11742</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b302">
	<monogr>
		<title level="m" type="main">Beyond antiforgetting: Multimodal continual instruction tuning with positive forward transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.09181</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">Multimodal parameter-efficient few-shot class incremental learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>D'alessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Calabrés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3393" to="3403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b304">
	<monogr>
		<title level="m" type="main">Preserving knowledge in large language model with modelagnostic self-decompression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">2406</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b305">
	<monogr>
		<title level="m" type="main">Lines: Post-training layer scaling prevents forgetting and enhances model merging</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Favero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ortiz-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17146</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Attriclip: A non-incremental learner for incremental knowledge learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3654" to="3663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<monogr>
		<title level="m" type="main">Continual diffusion: Continual customization of text-to-image diffusion with c-lora</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06027</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Cirp: Cross-item relational pre-training for multimodal product bundling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9641" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">Enhancing storage and computational efficiency in federated multimodal learning for large-scale models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<monogr>
		<title level="m" type="main">A concept-based explainability framework for large multimodal models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khayatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shukor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08074</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Adaptively building a video-language model for video captioning and retrieval without massive video pretraining</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4871" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">Bilateral adaptive cross-modal fusion prompt learning for clip</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="9001" to="9009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<monogr>
		<title level="m" type="main">Fact: Teaching mllms with faithful, concise and transferable rationales</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="846" to="855" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b314">
	<monogr>
		<title level="m" type="main">Object hallucination in image captioning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02156</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b315">
	<monogr>
		<title level="m" type="main">Plausible may not be faithful: Probing object hallucination in vision-language pretraining</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07688</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b316">
	<monogr>
		<title level="m" type="main">Evaluating object hallucination in large vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10355</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Groundhog: Grounding large language models to holistic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">238</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<monogr>
		<title level="m" type="main">Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01779</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Mitigating hallucination in large multi-modal models via robust instruction tuning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<monogr>
		<title level="m" type="main">Ferret: Refer and ground anything anywhere at any granularity</title>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07704</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b321">
	<monogr>
		<title level="m" type="main">Analyzing and mitigating object hallucination in large vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00754</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b322">
	<monogr>
		<title level="m" type="main">An llm-free multi-dimensional benchmark for mllms hallucination evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07397</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b323">
	<monogr>
		<title level="m" type="main">Multi-object hallucination in vision-language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06192</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b324">
	<monogr>
		<title level="m" type="main">Yi: Open foundation models by 01</title>
		<author>
			<persName><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04652</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">Glamm: Pixel grounding large multimodal model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Obelics: An open web-scale filtered dataset of interleaved imagetext documents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Laurenc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tronchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<monogr>
		<title level="m" type="main">Minicpm: Unveiling the potential of small language models with scalable training strategies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06395</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b328">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Goucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Welihinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21276</idno>
		<title level="m">Gpt-4o system card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">427</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">mclip: Multilingual clip via cross-lingual transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<monogr>
		<title level="m" type="main">mblip: Efficient bootstrapping of multilingual vision-llms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Glavaš</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06930</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b332">
	<monogr>
		<title level="m" type="main">Instructblip: Towards general-purpose vision-language models with instruction tuning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M H</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.06500" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tanzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.05530</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b334">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Does gpt-3 grasp metaphors? identifying metaphor mappings with generative language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wachowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gromann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1018" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<monogr>
		<title level="m" type="main">Ii-bench: An image implication understanding benchmark for multimodal large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05862</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b337">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.12597" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<monogr>
		<title level="m" type="main">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2311.04257" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<monogr>
		<title level="m" type="main">Deepseek-vl: Towards real-world vision-language understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.05525" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<monogr>
		<title level="m" type="main">Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.16420" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b341">
	<monogr>
		<title level="m" type="main">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.16821</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b342">
	<monogr>
		<title level="m" type="main">Cogvlm2: Visual language models for image and video understanding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2408.16500" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.05530" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b344">
	<monogr>
		<title level="m" type="main">Benchmarking and improving generator-validator consistency of language models</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01846</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b345">
	<monogr>
		<title level="m" type="main">Generating with confidence: Uncertainty quantification for black-box large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19187</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b346">
	<monogr>
		<title level="m" type="main">Unveiling the tapestry of consistency in large vision-language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14156</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b347">
	<monogr>
		<title level="m" type="main">Gemini: A family of highly capable multimodal models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.11805" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<monogr>
		<title level="m" type="main">Mini-gemini: Mining the potential of multimodality vision language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.18814" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<monogr>
		<title level="m" type="main">Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2407.07895" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for realworld visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2507" to="2521" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<monogr>
		<title level="m" type="main">Compbench: A comparative reasoning benchmark for multimodal llms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.16837</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Vila: On pre-training for visual language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">699</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Hallu-pi: Evaluating hallucination in multi-modal large language models within perturbed inputs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b356">
	<monogr>
		<title level="m" type="main">Minigpt-v2: large language model as a unified interface for vision-language multitask learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09478</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">mplug-2: A modularized multi-modal foundation model across text, image and video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">748</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.17297</idno>
		<title level="m">Internlm2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14178</idno>
		<title level="m">mplug-owl: Modularization empowers large language models with multimodality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b361">
	<monogr>
		<title level="m" type="main">Shikra: Unleashing multimodal llm&apos;s referential dialogue magic</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15195</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b362">
	<monogr>
		<title level="m" type="main">Reform-eval: Evaluating large vision language models via unified re-formulation of task-oriented benchmarks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">742</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<monogr>
		<title level="m" type="main">Instructblip: Towards general-purpose vision-language models with instruction tuning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M H</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.06500" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<monogr>
		<title level="m" type="main">Pandagpt: One model to instruction-follow them all</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16355</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b366">
	<monogr>
		<title level="m" type="main">Imagebind-llm: Multi-modality instruction tuning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.03905</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.15010</idno>
	</analytic>
	<monogr>
		<title level="m">Parameter-efficient visual instruction model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b368">
	<monogr>
		<title level="m" type="main">Multimodal-gpt: A vision and language model for dialogue with humans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04790</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b369">
	<analytic>
		<title level="a" type="main">What matters in training a gpt4-style language model with multimodal inputs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter</title>
		<meeting>the 2024 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7930" to="7957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b370">
	<monogr>
		<title level="m" type="main">Empowering vision-language models to follow interleaved vision-language instructions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.04152</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Bliva: A simple multimodal llm for better handling of text-rich visual questions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2256" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09583</idno>
		<title level="m">Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b373">
	<analytic>
		<title level="a" type="main">Can language models solve graph problems in natural language</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b374">
	<monogr>
		<title level="m" type="main">Visiongraph: Leveraging large multimodal models for graph theory problems in visual context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.04950</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b375">
	<monogr>
		<title level="m" type="main">Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.07575</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b376">
	<monogr>
		<title level="m" type="main">Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11161</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b377">
	<analytic>
		<title level="a" type="main">Multimodal large language models make text-to-image generative models align better</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b378">
	<monogr>
		<title level="m" type="main">Genartist: Multimodal llm as an agent for unified image generation and editing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.05600</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b379">
	<monogr>
		<title level="m" type="main">Visual sketchpad: Sketching as a visual chain of thought for multimodal language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.09403</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b380">
	<monogr>
		<title level="m" type="main">Restoreagent: Autonomous image restoration agent via multimodal large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18035</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2301" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<analytic>
		<title level="a" type="main">Sleepmg: Multimodal generalizable sleep staging with inter-modal balance of classification and domain discrimination</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4004" to="4013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b383">
	<analytic>
		<title level="a" type="main">Moba: Mixture of bi-directional adapter for multi-modal sarcasm detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="4264" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b384">
	<monogr>
		<title level="m" type="main">X-prompt: Multi-modal visual prompt for video object segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5151" to="5160" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b385">
	<analytic>
		<title level="a" type="main">Robust multimodal sentiment analysis of image-text pairs by distribution-based feature recovery and fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5780" to="5789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b386">
	<analytic>
		<title level="a" type="main">Chain of visual perception: Harnessing multimodal large language models for zero-shot camouflaged object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8805" to="8814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b387">
	<analytic>
		<title level="a" type="main">Autograph: Enabling visual context via graph alignment in open domain multi-modal dialogue generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2079" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b388">
	<analytic>
		<title level="a" type="main">Dysarl: Dynamic structureaware representation learning for multimodal knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="8247" to="8256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">Ct2c-qa: Multimodal question answering over chinese text, table and chart</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b390">
	<analytic>
		<title level="a" type="main">Hivg: Hierarchical multimodal fine-grained modulation for visual grounding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2024</title>
		<imprint>
			<biblScope unit="page" from="5460" to="5469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">White-box multimodal jailbreaks against large vision-language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6920" to="6928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<monogr>
		<title level="m" type="main">Miko: multimodal intention knowledge distillation from large language models for social-media commonsense discovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3303" to="3312" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b393">
	<analytic>
		<title level="a" type="main">Como-nas: Core-structures-guided multi-objective neural architecture search for multi-modal classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9126" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<analytic>
		<title level="a" type="main">Hawkeye: Discovering and grounding implicit anomalous sentiment in recon-videos via scene-enhanced video large language model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="592" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b395">
	<analytic>
		<title level="a" type="main">Differentialperceptive and retrieval-augmented mllm for change captioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b396">
	<monogr>
		<title level="m" type="main">Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b397">
	<monogr>
		<title level="m" type="main">Reason-and-execute prompting: Enhancing multi-modal large language models for solving geometry questions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6959" to="6968" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b398">
	<analytic>
		<title level="a" type="main">Gallerygpt: Analyzing paintings with large multimodal models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7734" to="7743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b399">
	<monogr>
		<title level="m" type="main">Videopoet: A large language model for zero-shot video generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14125</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b400">
	<monogr>
		<title level="m" type="main">Safety finetuning at (almost) no cost: A baseline for vision large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bohdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.02207</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b401">
	<monogr>
		<title level="m" type="main">Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03161</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b402">
	<monogr>
		<title level="m" type="main">Momentor: Advancing video large language model with fine-grained temporal reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.11435</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b403">
	<monogr>
		<title level="m" type="main">Bat: Learning to reason about spatial sounds with large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01591</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b404">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.15704</idno>
		<title level="m">video-salmonn: Speech-enhanced audiovisual large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b405">
	<monogr>
		<title level="m" type="main">Georeasoner: Geo-localization with reasoning in street views using a large vision-language model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b406">
	<monogr>
		<title level="m" type="main">Cascade-clip: Cascaded vision-language embeddings alignment for zero-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.00670</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b407">
	<monogr>
		<title level="m" type="main">Understanding forgetting in continual learning with linear regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17583</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b408">
	<monogr>
		<title level="m" type="main">Safe: Slow and fast parameter-efficient tuning for continual learning with pre-trained models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.02175</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b409">
	<monogr>
		<title level="m" type="main">Make continual learning stronger via c-flat</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.00986</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b410">
	<monogr>
		<title level="m" type="main">Visual prompt tuning in null space for continual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05658</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b411">
	<monogr>
		<title level="m" type="main">Disentangled continual graph neural architecture search with invariant modular supernet</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Forty-first ICML</note>
</biblStruct>

<biblStruct xml:id="b412">
	<analytic>
		<title level="a" type="main">Bayesian adaptation of network depth and width for continual learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b413">
	<monogr>
		<title level="m" type="main">Rethinking momentum knowledge distillation in online continual learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.02870</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b414">
	<monogr>
		<title level="m" type="main">An effective dynamic gradient calibration method for continual learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.20956</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">Anytime continual learning for open vocabulary classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="269" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<monogr>
		<title level="m" type="main">Replay-and-forget-free graph class-incremental learning: A task profiling and prompting approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10341</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b417">
	<monogr>
		<title level="m" type="main">Class balance matters to active class-incremental learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S M</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="9445" to="9454" />
		</imprint>
	</monogr>
	<note>in ACM MM, 2024</note>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Importance-aware shared parameter subspace learning for domain incremental learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8874" to="8883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Select and distill: Selective dual-teacher knowledge transfer for continual learning on vision-language models</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="219" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<analytic>
		<title level="a" type="main">Boosting continual learning of vision-language models via mixture-ofexperts adapters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">230</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<analytic>
		<title level="a" type="main">Calibrating prompt from history for continual vision-language retrieval and grounding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4302" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b422">
	<analytic>
		<title level="a" type="main">Semantic residual prompts for continual learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menabue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frascaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bonicelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<analytic>
		<title level="a" type="main">Class-incremental learning with clip: Adaptive representation adjustment and parameter fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="214" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b424">
	<monogr>
		<title level="m" type="main">Forgetting before learning: Utilizing parametric arithmetic for knowledge updating in large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08011</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b425">
	<monogr>
		<title level="m" type="main">Learn or recall? revisiting incremental learning with pre-trained language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07887</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b426">
	<monogr>
		<title level="m" type="main">Fine-tuned language models are continual learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12393</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b427">
	<monogr>
		<title level="m" type="main">Reformulating domain adaptation of large language models as adapt-retrieverevise</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03328</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b428">
	<monogr>
		<title level="m" type="main">How abilities in large language models are affected by supervised fine-tuning data composition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05492</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">Enhancing multiple-choice question answering through sequential fine-tuning and curriculum learning strategies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Amasyali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5025" to="5042" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<monogr>
		<title level="m" type="main">On tiny episodic memories in continual learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10486</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Coda-prompt: Continual decomposed attention-based prompting for rehearsalfree continual learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cascante-Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arbelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">919</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<analytic>
		<title level="a" type="main">Dualprompt: Complementary prompting for rehearsal-free continual learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b433">
	<analytic>
		<title level="a" type="main">On the effectiveness of lipschitz-driven rehearsal in continual learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bonicelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31" to="886" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b434">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b435">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b436">
	<monogr>
		<title level="m" type="main">Quantized prompt for efficient generalization of vision-language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b437">
	<monogr>
		<title level="m" type="main">Improving multimodal large language models using continual learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Harun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.19925</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b438">
	<monogr>
		<title level="m" type="main">Efficient continual pre-training by mitigating the stability gap</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.14833</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b439">
	<monogr>
		<title level="m" type="main">Continual instruction tuning for large multimodal models</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16206</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b440">
	<analytic>
		<title level="a" type="main">TABLE 20: The results of generative task on image concatenation, cropping, and prompt misleading</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15275</idno>
	</analytic>
	<monogr>
		<title level="m">MLLMs Image Concatenation Image Cropping Prompt Misleading CHAIR Cover Hal Cog Hal Hal Before After Before After Before After Before After Before After Before After CogVLM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">291</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dynamic transformer architecture for continual learning of multimodal tasks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
