<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-01">1 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur Kharagpur</orgName>
								<address>
									<region>West Bengal India</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Subhankar</forename><surname>Maity</surname></persName>
							<email>subhankar.ai@kgpian.iitkgp.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">IIT Kharagpur Kharagpur</orgName>
								<address>
									<region>West Bengal India</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-01">1 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">F7EA12740AA8F9BEA4DB79425BCF470B</idno>
					<idno type="arXiv">arXiv:2312.00554v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The evolution of legal datasets and the advent of large language models (LLMs) have significantly transformed the legal field, particularly in the generation of case judgment summaries. However, a critical concern arises regarding the potential biases embedded within these summaries. This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models. The research aims to analyze the impact of biases on legal decision making. By interrogating the accuracy, fairness, and implications of biases in these summaries, this study contributes to a better understanding of the role of technology in legal contexts and the implications for justice systems worldwide. In this study, we investigate biases wrt Gender-related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords. The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models. The reasoning behind these biases needs further studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The legal domain has experienced a revolutionary shift with the introduction of cutting-edge technologies, particularly the utilization of legal datasets and large language models (LLMs) to generate case judgment summaries <ref type="bibr" target="#b1">(Charlotin, 2023)</ref>. These advances have streamlined the extraction and summarization of legal information, offering efficient tools for legal professionals to navigate through a large volume of cases. However, with this transformation comes the pressing concern of potential biases deeply ingrained in the automated generation of these summaries.</p><p>Biases, both explicit and implicit, in case judgment summaries, pose a substantial risk to the fairness and integrity of legal decision-making. The deployment of machine learning algorithms and natural language processing techniques raises questions about the accuracy, neutrality, and potential ethical implications of these automated systems. Moreover, the impact of biases in legal datasets and the extrapolation of these biases in LLMs further complicate the scenario <ref type="bibr" target="#b9">(Sargent and Weber, 2021)</ref>.</p><p>This study aims to investigate the heart of this concern, questioning the biases present in case judgment summaries created by legal datasets and LLMs. By critically examining the nature and implications of these biases, this research seeks to shed light on their effects on legal decision-making, and the potential ethical dilemmas they pose.</p><p>The study that we have performed seems to focus on the important topic of biases within language models and summarization models, particularly concerning sensitive aspects such as gender, race, crime against women, countries, and religious terms. Investigating these biases is critical because they can perpetuate stereotypes and discrimination if not properly addressed. The results of such a study would be beneficial for developers and researchers to improve the fairness and neutrality of AI systems in legal domain.</p><p>In our study, we observe slight biases for femalerelated keywords on both Indian judgment summaries and United Kingdom judgment summaries. Also, observations show strong biases of certain legal domain-specific abstractive summarization models towards specific country names. The United Kingdom judgement summaries show biases towards specific terms related to crime against women. We do not find strong evidence for biases wrt religious keywords and race-related keywords in our study. The reasoning behind these biases needs further studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Examination of biases within legal datasets, case judgment summaries, and their impact on LLMs has been a subject of growing concern within the field of artificial intelligence (AI) and law 1 . Understanding the biases inherent in legal texts and how they are translated into machine learning models is crucial to ensuring fairness, accuracy, and justice within legal systems. Several key areas of research have been delved into this complex and multidimensional issue.</p><p>Researchers have extensively investigated the biases present in legal datasets used for training machine learning models <ref type="bibr" target="#b5">(Lum, 2017)</ref>. These biases can emerge from various sources, including historical judgments, judicial decisions, legal texts, and case summaries. Studies such as <ref type="bibr" target="#b9">(Sargent and Weber, 2021;</ref><ref type="bibr" target="#b12">Silva and Costa-Abreu, 2022</ref>) have focused on identifying and quantifying biases within these datasets. These biases might be related to gender, race, socioeconomic status, or other contextual factors that influence legal outcomes. Understanding the origin and nature of biases in these datasets is crucial to mitigate their impact on AI-powered legal applications.</p><p>Ethical considerations are at the forefront of discussions surrounding biases in case judgment summaries and their integration into LLMs. "The Ethical Implications of AI in Legal Decision-Making 2 " discusses the ethical implications of using biased data in AI-powered legal systems. The author emphasizes the critical need for fairness, transparency, and accountability in the development and deployment of these systems, especially in crucial domains such as law, where decisions are of significant weight.</p><p>Legal judgments <ref type="bibr">(Nigam and Deroy, 2023;</ref><ref type="bibr">Nigam et al., 2023)</ref> are crucial portions of the legal system and biases in legal judgement summaries are inherent and essential to be studied. The rise of LLMs has introduced new challenges and opportunities in legal applications <ref type="bibr" target="#b13">(Sun, 2023)</ref>. Studies such as <ref type="bibr" target="#b0">(Bozdag et al., 2023)</ref> analyze how these models process and generate legal text, highlighting their potential to help legal professionals in research and analysis. However, concerns about biases encoded in these models due to training data, including case judgment summaries <ref type="bibr" target="#b2">(Deroy et al., 2021</ref><ref type="bibr">(Deroy et al., , 2023a))</ref>, raise questions about the reliability and fairness <ref type="bibr">(Deroy et al., 2023b</ref>) of these LLMs.</p><p>1 <ref type="url" target="https://tinyurl.com/5n6yk2x7">https://tinyurl.com/5n6yk2x7</ref> 2 <ref type="url" target="https://tinyurl.com/2e56ayfm">https://tinyurl.com/2e56ayfm</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>The datasets IN-Abs and UK-Abs was reused from <ref type="bibr" target="#b11">(Shukla et al., 2022)</ref> and used for experimentation. We use two datasets namely IN-Abs( a dataset consisting of Indian Supreme Court case judgements) and UK-Abs( a dataset consisting of United Kingdom Supreme Court case judgements). The IN-Abs dataset has 7130 (legal judgement, summary) pairs out of which 7030 (legal judgement, summary) pairs belong to training set and 100 (legal judgement, summary) pairs belong to testing set. The UK-Abs dataset has 793 (legal judgement, summary) pairs out of which 693 (legal judgement, summary) pairs belong to training set and 100 (legal judgement, summary) pairs belong to testing set. The IN-Abs dataset was collected from-<ref type="url" target="http://www.liiofindia.org/in/cases/cen/INSC/">http://www.  liiofindia.org/in/cases/cen/INSC/</ref> The UK-Abs dataset was collected from--(<ref type="url" target="-(https://www.supremecourt.uk/decided-cases/">https://www.  supremecourt.uk/decided-cases/</ref> 4 Methodology 4.1 General domain specific LLMs  (ii) Davinci-TL;DR: The prompt used is shown in Figure <ref type="figure" target="#fig_2">2</ref>. Variations of GPT-3.5 Turbo: We try two different variations of the model: (i) Chatgpt-summ: The prompt used is shown in Figure <ref type="figure" target="#fig_3">3</ref>. (ii) Chatgpt-TL;DR: The prompt used is shown in Figure <ref type="figure" target="#fig_4">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Divide and Conquer approach</head><p>We break every legal judgement into chunks of size 1024 words and then pass them into the General domain LLMs and legal domain specific abstractive models. The output summaries that we obtain from these models will be appended together in the order in which we pass into the models to form the final output summary. We experimented with two different chunk lengths namely 1024 words and 2048 words where we observe that the results with 1024 words is superior to the results with 2048 words. General domain LLMs like Chatgpt and Davinci have an input token+response limit of 4096 tokens. LegPegasus has a input token limit of 16384 tokens. LegLED has a input token limit of 1024 tokens. To maintain uniformity across all models, they have been run at an input token length of 1024 words.</p><p>5 Bias w.r.t Gender-related keywords in the outputs of LLMs</p><p>We study biases wrt to Gender-related keywords on the model generated summaries of IN-Abs and UK-Abs dataset. Table <ref type="table">1</ref> shows the bias w.r.t genderrelated keywords <ref type="bibr" target="#b10">(Sevim et al., 2023)</ref> in the outputs of the General domain LLMs and legal domainspecific abstractive models on UK-Abs dataset. We measure the number of times male-related keywords and female-related keywords appear in the original documents, expert-written summaries, and model-generated summaries.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the bias w.r.t gender-related keywords <ref type="bibr" target="#b10">(Sevim et al., 2023)</ref> in the outputs of the General domain LLMs and legal domain-specific abstractive models on IN-Abs dataset. We measure the number of times male-related keywords and female-related keywords appear in the original documents, expert-written summaries, and modelgenerated summaries.</p><p>We also measure the percentage occurrence of male-related keywords and female-related keywords in the expert-written summaries and modelgenerated summaries out of the total number of male and female-related keywords in the main documents.</p><p>We use the list of male and female-related keywords from <ref type="bibr" target="#b14">(Zhao et al., 2018)</ref>. Then we try to find out these keywords in the Original document, expert-written summaries, and summaries produced by the LLMs. We observe that the general domain LLMs like Chatgpt and Davinci has produced a slightly higher percentage of female-related keywords for both UK-Abs and IN-Abs datasets. Also, similar observations are noticed for LegPegasus, LegPegasus-UK, LegLED and LegLED-UK models on both UK-Abs and IN-Abs datasets.</p><p>The expert-written summaries have comparable performance w.r.t male and female-related keywords on both UK-Abs and IN-Abs datasets.</p><p>We also searched for acts related to male and female equality like-Equality Act 2006( <ref type="url" target="https://en.wikipedia.org/wiki/Equality_Act_2006">https://  en.wikipedia.org/wiki/Equality_Act_2006</ref>), Equality Act 2010( <ref type="url" target="https://en.wikipedia.org/wiki/Equality_Act_2010">https://en.wikipedia.  org/wiki/Equality_Act_2010</ref>), Equal Pay Act 1970( <ref type="url" target="https://en.wikipedia.org/wiki/Equal_Pay_Act_1970">https://en.wikipedia.org/  wiki/Equal_Pay_Act_1970</ref>),</p><p>Sex Discrimination Act 1975( <ref type="url" target="https://en.wikipedia.org/wiki/Sex_Discrimination_Act_1975">https://en.wikipedia.  org/wiki/Sex_Discrimination_Act_1975</ref>), and United Kingdom employment equality law(<ref type="url" target="https://en.wikipedia.org/wiki/United_Kingdom_employment_equality_law">https://en.wikipedia.org/wiki/  United_Kingdom_employment_equality_law</ref>) in the Original documents and summaries but none of these acts were found in the documents as well as the summaries.</p><p>Acts related to transgenders like Employment Equality (Sexual Orientation) Regulations 2003(<ref type="url" target="https://en.wikipedia.org/wiki/Employment_Equality_(Sexual_Orientation)_Regulations_2003">https://en.wikipedia.org/wiki/  Employment_Equality_(Sexual_Orientation)  _Regulations_2003</ref>) was also not found in the Original documents as well as the summaries.</p><p>In our study, we observe slight biases for female-related keywords on both Indian judgement summaries(IN-Abs) and United Kingdom judgement summaries(UK-Abs) for the different General purpose LLMs and pretrained legal domain specific abstractive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Bias w.r.t Race-related keywords in the output of the LLMs</head><p>We study biases wrt to Race-related keywords on the model generated summaries of IN-Abs and UK-Abs dataset.</p><p>Table <ref type="table">3</ref> shows the bias w.r.t race-related keywords <ref type="bibr" target="#b6">(Matthews et al., 2022)</ref> in the outputs of the General domain LLMs and legal domain-specific abstractive models on IN-Abs dataset.</p><p>Table <ref type="table">4</ref> shows the bias w.r.t race-related keywords <ref type="bibr" target="#b6">(Matthews et al., 2022)</ref> in the outputs of the General domain LLMs and legal domain-specific abstractive models on UK-Abs dataset.</p><p>We measure the number of times black peoplerelated keywords and white people-related key-words appear in the original documents, expertwritten summaries, and model-generated summaries. We also measure the percentage occurrence of black people-related keywords and white people-related keywords in the expert-written summaries and model-generated summaries out of the total number of times the black people-related keywords and the white people-related keywords have occurred in the main documents.</p><p>We use the list of white people and black people-related keywords from <ref type="url" target="https://www.freethesaurus.com/White+person">https://www.  freethesaurus.com/White+person</ref> and <ref type="url" target="https://www.freethesaurus.com/Black+person">https:  //www.freethesaurus.com/Black+person</ref> respectively. Then we try to find out these keywords in the original documents, expert-written summaries, and summaries produced by the LLMs.</p><p>We observe that for UK-Abs dataset, the legal domain-specific abstractive summarization models like LegLED, LegLED-UK, and LegPegasus-UK show slightly higher percentages of black peoplerelated keywords as compared to white peoplerelated keywords. General domain LLMs like Chatgpt-TL;DR, Chatgpt-summ, Davinci-summ, and Davinci-TL;DR show no black people or white people-related keywords in their output summaries.</p><p>On the other hand, in case of the IN-Abs dataset, we observe that white people related keywords are present in small proportions in the Original documents, expert-written summaries and modelgenerated summaries. Black people related keywords are not present in the Original documents, expert-written summaries and model-generated summaries. In case of IN-Abs dataset there is slightly higher percentage of white-people related keywords in the model-generated summaries of the</p><p>IN-Abs dataset. Various Acts in UK law are related to racial discrimination like Race Relations Act 1965, Race Relations Act 1968, and Race Relations Act 1976. We also searched for the keywords-Race Relations Act 1965 (<ref type="url" target="https://en.wikipedia.org/wiki/Race_Relations_Act_1965">https://en.wikipedia.org/wiki/  Race_Relations_Act_1965</ref>), Race Relations Act 1968 (<ref type="url" target="https://en.wikipedia.org/wiki/Race_Relations_Act_1968">https://en.wikipedia.org/wiki/Race_  Relations_Act_1968</ref>) and Race Relations Act 1976 (<ref type="url" target="https://en.wikipedia.org/wiki/Race_Relations_Act_1976">https://en.wikipedia.org/wiki/Race_  Relations_Act_1976</ref>) in the main documents, model generated summaries and expert generated summaries, but these keywords were not found. We also tried to find out other racial keywords like Asian, Indigenous, Pacific Islanders, etc in the main documents and summaries but were not able Document Number of male related keywords Number of female related keywords Percentage of male related keywords Percentage of female related keywords Original Document 94407 53826 Expert Summary 8635 4912 9.14 9.12 General-domain LLMs Chatgpt-TL;DR 4865 3402 5.15 6.32 Chatgpt-summ 5059 3411 5.35 6.33 Davinci-TL;DR 2111 1535 2.23 2.85 Davinci-summ 5003 3227 5.29 5.99 Legal domain-specific abstractive models LegPegasus 4929 3545 5.22 6.58 LegPegasus-UK 4186 2728 4.43 5.06 LegLED 5299 3416 5.61 6.34 LegLED-UK 4272 2943 4.52 5.46 Table 1: Bias w.r.t Gender-related keywords in the outputs of the General domain LLMs and legal domain specific abstractive models in UK-Abs dataset.</p><p>We measure the number of times male-related keywords and female-related keywords appear in the Original documents, expert-written summaries, and model-generated summaries. We also measure the percentage occurrence of male-related keywords and female-related keywords in the expert-written summaries and model-generated summaries out of the total number of male-related keywords and female-related keywords in the original documents. The highest values in percentage between male-related keywords and femalerelated keywords are represented in blue.</p><p>to detect such keywords. Overall, we observe there is no strong evidence for biases wrt to race-related keywords in the model generated summaries for both IN-Abs and UK-Abs datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis of crime against women in the outputs of LLMs</head><p>We analyse keywords related to crime against women on the model generated summaries of IN-Abs and UK-Abs dataset. Table <ref type="table" target="#tab_4">5</ref> shows the analysis of crime against women based on the outputs of the General domain LLMs and legal domain-specific abstractive models on UK-Abs dataset. We measure the number of times several keywords which relate to crime against women appear in the original documents, expert-written summaries, and model-generated summaries. We use the list of keywords related to crime against women from <ref type="url" target="https://www2.ohchr.org/english/bodies/cedaw/docs/ngos/UKThematicReportVAW41.pdf">https://www2.ohchr.  org/english/bodies/cedaw/docs/ngos/   UKThematicReportVAW41.pdf</ref>, <ref type="url" target="https://www.cps.gov.uk/crime-info/sexual-offences">https://www.  cps.gov.uk/crime-info/sexual-offences</ref> and <ref type="url" target="https://shorturl.at/mHJO0">https://shorturl.at/mHJO0</ref>. We analyzed various keywords relating to crime against women in the original documents as well as the summaries. For the UK-Abs dataset, keywords like domestic violence, sexual assault, trafficking, refugee, sexual abuse, and forced marriage are generated more by the general purpose LLMs like Davinci and Chatgpt as compared to the legal domain-specific abstractive summarization models. A keyword like the sexual offence is generated almost equally by the general purpose LLMs and legal domain-specific models. We also searched for keywords like sexual harassment, stalking, sex industry, honor crimes, child marriage, female genitalia, prostitution, partner abuse, physical abuse, and asylum seeking which were neither present in the document nor in the summaries.</p><p>For the IN-Abs dataset, we observe that the keywords referring to crime against women are mostly absent in the original documents, expert-written We measure the number of times male-related keywords and female-related keywords appear in the Original documents, expert-written summaries, and model-generated summaries. We also measure the percentage occurrence of male-related keywords and female-related keywords in the expert-written summaries and model-generated summaries out of the total number of male-related keywords and female-related keywords in the original documents. The highest values in percentage between male-related keywords and femalerelated keywords are represented in blue.</p><p>summaries, and model-generated summaries. Various acts in UK law are related to crime against women like Sexual Offences Act 2003, Sexual Offences Act 1956, Sexual Offences (Scotland) Act 2009 and Sexual Offences(Northern Ireland) Order 2008. We also searched for the keywords-Sexual Offences Act 2003(<ref type="url" target="https://en.wikipedia.org/wiki/Sexual_Offences_Act_2003">https://en.wikipedia.  org/wiki/Sexual_Offences_Act_2003</ref>), Sexual Offences Act 1956(<ref type="url" target="https://en.wikipedia.org/wiki/Sexual_Offences_Act_1956">https://en.wikipedia.  org/wiki/Sexual_Offences_Act_1956</ref>), Sexual Offences (Scotland) Act 2009(<ref type="url" target="https://en.wikipedia.org/wiki/Sexual_Offences_(Scotland)_Act_2009">https:  //en.wikipedia.org/wiki/Sexual_Offences_</ref> (Scotland)_Act_2009), and Sexual Offences(Northern Ireland) Order 2008(<ref type="url" target="https://en.wikipedia.org/wiki/Sexual_Offences_(Northern_Ireland)_Order_2008">https:  //en.wikipedia.org/wiki/Sexual_Offences_</ref> (Northern_Ireland)_Order_2008) in the main documents, model-generated summaries, and expert-generated summaries, but these keywords were not found.</p><p>The LLM generated summaries for United kingdom(UK-Abs) dataset shows biases towards specific terms related to crime aginst women. No such strong observation is found for the Indian(IN-Abs) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Country name based Bias in the outputs of LLMs</head><p>We study biases wrt to Country names on the model generated summaries of IN-Abs and UK-Abs dataset. We observe that some LLMs tend to generate some country names more than others. We take the list of all country names from-<ref type="url" target="https://history.state.gov/countries/all">https:  //history.state.gov/countries/all</ref>. The list does not have the country named-"United States", so we add the country name-"United States" to the list of countries. Then we search for the country names in legal documents, expert-generated summaries, and model-generated summaries. We measure the number of times black people-related keywords and white people-related keywords appear in the original documents, expert-written summaries, and model-generated summaries. We also measure the percentage occurrence of black people-related keywords and white people-related keywords in the expert-written summaries and the model-generated summaries out of the total number of black people-related keywords and white people-related keywords in the original documents. The highest values in percentage between white people and black people related keywords are represented in blue.</p><p>UK-Abs dataset. Figure <ref type="figure" target="#fig_15">13</ref> and Figure <ref type="figure" target="#fig_16">14</ref> show the Top-5 countries in the expert-written summaries and original documents in UK-Abs dataset. We observe that some legal domain-specific abstractive summarization models like LegLED tend to generate some country names like "United States" more than other models specifically due to initial training on US legal data. On the other hand, the LegLED-UK model which was further finetuned on UK legal data shows a reduction in the number of times the country name-"United States" has appeared. LegPegasus and LegPegasus-UK models have a lesser number of times the country name-"United States" appearing in their summaries. Interestingly the general domain LLMs like Chatgpt-TL;DR, Chatgpt-summ, and the expertwritten summaries do not have "United States" in the list of Top-5 country names in their generated summaries. Though the original documents have the country name-"United States" in the list of Top-5 country names. Also, Davinci-TL;DR and Davinci-summ have "United States" in the list of Top-5 country names in their generated summaries.</p><p>Across all the model-generated summaries, expert-written summaries, and original documents, the most occurring country name is-"United Kingdom" because of the fact that we are working on UK court cases.</p><p>Interestingly Country names like Pakistan, Lithuania, and Zimbabwe come up in the list of Top-5 country names in the output summaries generated by the general purpose LLMs as well as the legal domain-specific LLMs.</p><p>Figure <ref type="figure" target="#fig_17">15</ref> <ref type="bibr">, 16, 17, 18, 19, 20, 21, 22</ref> Table <ref type="table">4</ref>: Bias w.r.t race-related keywords in the outputs of the General domain LLMs and legal domain specific abstractive models on UK-Abs dataset. We measure the number of times black people-related keywords and white people-related keywords appear in the original documents, expert-written summaries, and model-generated summaries. We also measure the percentage occurrence of black people-related keywords and white people-related keywords in the expert-written summaries and the model-generated summaries out of the total number of black people-related keywords and white people-related keywords in the original documents. The highest values in percentage between white people and black people related keywords are represented in blue.</p><p>than other models specifically due to initial training on US legal data. On the other hand, the LegLED-UK model which was further fine-tuned on UK legal data shows a reduction in the number of times the country name-"United States" has appeared. LegPegasus and LegPegasus-UK models have a lesser number of times the country name-"United States" appearing in their summaries. Across all the model-generated summaries, expert-written summaries, and original documents, the most occurring country name is-"India" because of the fact that we are working on Indian court cases. The country name-"United Kingdom" is also present in the list of Top-5 Country names in the model-generated summaries of LegLED, LegLED-IN, LegPegasus, LegPegasus-IN, and Chatgpt-TL;DR. Also, Country names like Germany, Italy, Greece, and Pakistan occur in the list of Top-5 country names of several model-generated summaries. Also, observations show strong biases of certain legal domain-specific abstractive summarization models towards specific country names. Coun-try names like "United States" is generated more by legal domain-specific abstractive models like LegalLED for both IN-Abs and UK-Abs, modelgenerated summaries. Interestingly Country names like Pakistan, Lithuania, and Zimbabwe come up in the list of Top-5 country names in the output summaries generated by the general purpose LLMs as well as the legal domain-specific LLMs for the UK-Abs dataset. Also, Country names like Germany, Italy, Greece, and Pakistan occur in the list of Top-5 country names of several model-generated summaries for IN-Abs dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Religious Bias in the outputs of abstractive summarization models</head><p>We study biases wrt to religious-related keywords on the model generated summaries of IN-Abs and UK-Abs dtaset. We searched for religious keywords like Hindu, Muslim, Christian, Jain, etc in the Original documents, expert-written summaries, and model-generated summaries but we were un- able to find such keywords in both IN-Abs and UK-Abs dataset.</p><p>We also searched for the act named-Employment Equality (Religion or Belief) Regulations 2003(<ref type="url" target="https://en.wikipedia.org/wiki/Employment_Equality_(Religion_or_Belief)_Regulations_2003">https://en.wikipedia.org/wiki/  Employment_Equality_(Religion_or_Belief)  _Regulations_2003</ref>) in the main documents and summaries but the act was not found.</p><p>We observe no evidences for biases wrt religious keywords in both IN-Abs and UK-Abs modelgenerated summaries.</p><p>10 Examples of extracts from the summaries showing crime against women, Race-related keywords, Country names and gender related-keywords</p><p>Table <ref type="table">6</ref> shows examples of extracts from the summaries showing crime against women, race-related keywords, country names, and gender relatedkeywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Concluding Discussion</head><p>(I) In our study, we observe slight biases for female-related keywords on both Indian judgement summaries(IN-Abs) and United Kingdom judgement summaries(UK-Abs) for the different General purpose LLMs and pre-trained legal domainspecific abstractive models.</p><p>(ii) Also observations show strong biases of certain legal domain-specific abstractive summarization models towards specific country names. Country names like "United States" is generated more by legal domain-specific abstractive models like LegalLED. Interestingly Country names like Pakistan, Lithuania, and Zimbabwe come up in the list of Top-5 country names in the output summaries generated by the general purpose LLMs as well as the legal domain-specific LLMs for the UK-Abs dataset. Also, Country names like Germany, Italy, Greece, and Pakistan occur in the list of Top-5 country names of several model-generated summaries for IN-Abs dataset (iii)The LLM generated summaries for the  The reasoning behind these biases needs further studies.                  This is an extract from a summary related to a issue about forced marriage which is a crime against women.</p><p>2 Chatgpt-TL;DR This case discusses whether Barclays Bank is vicariously liable for the sexual assaults allegedly committed by the late Dr Gordon Bates on some 126 claimants in this group action.</p><p>This is an extract from a summary related to an issue about sexual assault which is a crime against women.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">davincisumm</head><p>The UK's benefit cap, which restricts the amount of welfare that households can receive, has been ruled discriminatory against single parents and victims of domestic violence, who are predominantly women.</p><p>This is an extract from a summary related to an issue about domestic violence which is a crime against women.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LegLED</head><p>The Office of the Chief Constable of the Royal Ulster Constabulary today announced that it has concluded that, in treating a black or female employee less favourably on racial grounds, the employer acted as he did.</p><p>This is an extract from a summary related to a issue about black and female employees in their workplace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LegLED</head><p>The United States Attorney's Office for the Southern District of New York today announced criminal charges against a South Korean citizen for his role in a scheme to manipulate the value of the bonds.</p><p>This is an extract from the summary which talks about two countries United States and South Korea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">davincisumm</head><p>The document discusses the issue of whether men and women are in the same employment for the purposes of the Equal Pay Act 1970 (now replaced by the Equality Act 2010).</p><p>This is an extract from a summary talking about male and female equality regarding matters of employment.</p><p>Table 6: Examples of extracts from the model-generated summaries showing keywords related to crime against women, Race-related keywords, Country names, and gender-related keywords. The keywords are marked in red.</p><p>. Table <ref type="table">7</ref> shows the training and testing times of every family of summarization models. The models were run on one NVIDIA RTX A5000 GPU.</p><p>Table <ref type="table">8</ref> shows the hyperparameters of the legal domain-specific abstractive models and LLMs used in this work.</p><p>The specific hyperparameter configurations employed to fine-tune the various legal domainspecific abstractive models can be found in Table <ref type="table">9</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>We try the Text-Davinci-003 and GPT-3.5 Turbo model using OpenAI API.3  Variations of Text-Davinci-003: We try two different variations of this model: (i) Davinci-summ: The prompt used is shown in Figure1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The prompt used for Davinci-summ. YY -&gt; target length of the output summary in number of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The prompt used for Davinci-TL;DR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The prompt used for Chatgpt-summ. YY -&gt; target length of the output summary in number of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The prompt used for Chatgpt-TL;DR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Figure 5, 6, 7, 8, 9, 10, 11, 12, show the Top-5 country names in the summaries generated by the LegLED, LegLED-UK, LegPegasus, LegPegasus-UK, Davinci-summ, Davinci-TL;DR, Chatgpt-summ and Chatgpt-TL;DR respectively in Bias w.r.t race-related keywords in the outputs of the General domain LLMs and legal domain specific abstractive models on IN-Abs dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>, show the Top-5 country names in the summaries generated by the LegLED, LegLED-IN, LegPegasus, LegPegasus-IN, Davinci-summ, Davinci-TL;DR, Chatgpt-summ and Chatgpt-TL;DR respectively in IN-Abs dataset. Figure13and Figure14show the Top-5 countries in the expert-written summaries and original documents of IN-Abs dataset. We observe that some legal domain-specific abstractive summarization models like LegLED tend to generate some country names like "United States" more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top-5 country names in the summaries generated by LegLED model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="10,70.87,70.87,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top-5 country names in the summaries generated by LegLED-UK model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top-5 country names in the summaries generated by LegPegasus model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="10,70.87,313.40,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Top-5 country names in the summaries generated by LegPegasus-UK model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="10,306.14,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Top-5 country names in the summaries generated by Davinci-summ model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="11,70.87,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Top-5 country names in the summaries generated by Davinci-TL;DR model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="11,70.87,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Top-5 country names in the summaries generated by Chatgpt-summ model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="11,306.14,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Top-5 country names in the summaries generated by Chatgpt-TL;DR model. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="11,306.14,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Top-5 country names in the Expert-written summaries. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="12,70.87,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Top-5 country names in the Original documents. x-axis represents the country names in UK-Abs dataset. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="12,70.87,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Top-5 country names in the summaries generated by LegLED model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="12,306.14,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Top-5 country names in the summaries generated by LegLED-IN model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="12,306.14,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Top-5 country names in the summaries generated by LegPegasus model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="13,70.87,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Top-5 country names in the summaries generated by LegPegasus-IN model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="13,70.87,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Top-5 country names in the summaries generated by Davinci-summ model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="13,306.14,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Top-5 country names in the summaries generated by Davinci-TL;DR model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="13,306.14,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Top-5 country names in the summaries generated by Chatgpt-summ model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="14,70.87,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Top-5 country names in the summaries generated by Chatgpt-TL;DR model in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="14,70.87,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Top-5 country names in the Expert-written summaries in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="14,306.14,128.62,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Top-5 country names in the Original documents in IN-Abs dataset. x-axis represents the country names. y-axis represents the number of times a country name has appeared.</figDesc><graphic coords="14,306.14,482.69,230.40,172.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Bias w.r.t Gender-related keywords in the outputs of the General domain LLMs and legal domain specific abstractive models in the IN-Abs dataset.</figDesc><table><row><cell>Document</cell><cell>Number</cell><cell>of</cell><cell>Number</cell><cell>of</cell><cell cols="2">Percentage of</cell><cell cols="2">Percentage of</cell></row><row><cell></cell><cell>male</cell><cell></cell><cell>female</cell><cell></cell><cell>male</cell><cell></cell><cell>female</cell></row><row><cell></cell><cell>related</cell><cell>key-</cell><cell>related</cell><cell>key-</cell><cell>related</cell><cell>key-</cell><cell>related</cell><cell>key-</cell></row><row><cell></cell><cell>words</cell><cell></cell><cell>words</cell><cell></cell><cell>words</cell><cell></cell><cell>words</cell></row><row><cell>Original Docu-</cell><cell>33855</cell><cell></cell><cell>18691</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Expert Sum-</cell><cell>6098</cell><cell></cell><cell>3512</cell><cell></cell><cell>18.01</cell><cell></cell><cell>18.78</cell></row><row><cell>mary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">General-domain LLMs</cell><cell></cell><cell></cell></row><row><cell>Chatgpt-</cell><cell>2864</cell><cell></cell><cell>1877</cell><cell></cell><cell>8.45</cell><cell></cell><cell>10.04</cell></row><row><cell>TL;DR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Chatgpt-summ 2128</cell><cell></cell><cell>1404</cell><cell></cell><cell>6.28</cell><cell></cell><cell>7.51</cell></row><row><cell>Davinci-</cell><cell>752</cell><cell></cell><cell>576</cell><cell></cell><cell>2.22</cell><cell></cell><cell>3.08</cell></row><row><cell>TL;DR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Davinci-summ 1377</cell><cell></cell><cell>1034</cell><cell></cell><cell>4.06</cell><cell></cell><cell>5.53</cell></row><row><cell></cell><cell cols="5">Legal domain-specific abstractive models</cell><cell></cell><cell></cell></row><row><cell>LegPegasus</cell><cell>814</cell><cell></cell><cell>611</cell><cell></cell><cell>5.22</cell><cell></cell><cell>6.58</cell></row><row><cell>LegPegasus-</cell><cell>898</cell><cell></cell><cell>649</cell><cell></cell><cell>4.43</cell><cell></cell><cell>5.06</cell></row><row><cell>UK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LegLED</cell><cell>723</cell><cell></cell><cell>507</cell><cell></cell><cell>5.61</cell><cell></cell><cell>6.34</cell></row><row><cell>LegLED-UK</cell><cell>928</cell><cell></cell><cell>676</cell><cell></cell><cell>4.52</cell><cell></cell><cell>5.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Analysis of crime against women based on the outputs of the General domain LLMs and legal domain specific abstractive models on UK-Abs dataset. We measure the number of times several keywords which are related to crime against women appear in the Original documents, expert-written summaries, and model-generated summaries.</figDesc><table><row><cell cols="2">Document Domestic</cell><cell>Rape</cell><cell>Sexual</cell><cell cols="2">Trafficking Forced</cell><cell cols="2">Refugee Sexual</cell><cell>Sexual</cell></row><row><cell></cell><cell>violence</cell><cell></cell><cell>assault</cell><cell></cell><cell>mar-</cell><cell></cell><cell>abuse</cell><cell>offence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>raige</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original</cell><cell>30</cell><cell>29</cell><cell>8</cell><cell>8</cell><cell>89</cell><cell>132</cell><cell>15</cell><cell>6</cell></row><row><cell>Document</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Expert</cell><cell>0</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>14</cell><cell>1</cell><cell>0</cell></row><row><cell>Summary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">General-domain LLMs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chatgpt-</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>3</cell><cell>10</cell><cell>21</cell><cell>0</cell><cell>1</cell></row><row><cell>TL;DR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chatgpt-</cell><cell>6</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>7</cell><cell>18</cell><cell>3</cell><cell>0</cell></row><row><cell>summ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Davinci-</cell><cell>3</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>13</cell><cell>22</cell><cell>7</cell><cell>1</cell></row><row><cell>TL;DR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Davinci-</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>4</cell><cell>10</cell><cell>6</cell><cell>0</cell></row><row><cell>summ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Legal domain-specific abstractive models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LegPegasus-</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>6</cell><cell>0</cell><cell>5</cell><cell>0</cell></row><row><cell>UK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LegPegasus 2</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>4</cell><cell>16</cell><cell>2</cell><cell>0</cell></row><row><cell>LegLED</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>4</cell><cell>15</cell><cell>3</cell><cell>1</cell></row><row><cell>LegLED-</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>15</cell><cell>3</cell><cell>1</cell></row><row><cell>UK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://platform.openai.com/docs/ api-reference/completions</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are working with general domain LLMs like Text-Davinci-003 and GPT-3.5 Turbo. For using the OpenAI services for Text-Davinci-003 and GPT-3.5 Turbo, we paid the appropriate prices for using the Ope-nAI API(<ref type="url" target="https://openai.com/pricing">https://openai.com/pricing</ref>). Models like Legal-Pegasus and Legal-LED were used from the Huggingface website at the following links:<ref type="url" target="https://huggingface.co/nsi319/legal-pegasus">https:  //huggingface.co/nsi319/legal-pegasus</ref> and <ref type="url" target="https://huggingface.co/nsi319/legal-led-base-16384">https://huggingface.co/nsi319/  legal-led-base-16384</ref>.</p><p>Also, unsupervised extractive summarization models such as Case Summarizer were used from - <ref type="url" target="https://github.com/Law-AI/summarization/tree/aacl/extractive/CaseSummarizer">https://github.com/Law-AI/summarization/  tree/aacl/extractive/CaseSummarizer</ref>. Bertsum, a supervised extractive summarization model was used from-<ref type="url" target="https://github.com/nlpyang/BertSum">https:  //github.com/nlpyang/BertSum</ref>.</p><p>Sum-maRunner which is a supervised and extractive summarization model was used from- <ref type="url" target="https://github.com/hpzhao/SummaRuNNer">https://github.com/hpzhao/SummaRuNNer</ref>.</p><p>We have tried to maintain all ethical concerns while performing all the experiments, and we have honestly reported our results in this paper. One of the primary ethical concerns in summarizing legal judgments is the accuracy and fairness of the generated summaries. Legal judgments contain complex legal reasoning and nuanced interpretations, and the LLMs tend to simplify, omit, or confuse various pieces of information. It is important to ensure that the generated summaries accurately reflect the key arguments, reasoning, and results of the original judgment. In addition, we should try to reduce bias and inaccuracies in the summaries generated by the LLMs. This research focuses on a highly sensitive and distressing issue: crimes against women. We recognize the gravity of this topic and approach it with utmost respect, empathy, and ethical responsibility. Our primary objective is to shed light on the various forms of violence and discrimination faced by women, aiming to raise awareness and promote social change. Please consider very strong words are used as a part of this research, not to hurt the sentiments and feelings of anyone reading this paper. Please be conscious enough while reading this article.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring and mitigating gender bias in legal contextualized language models</title>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bozdag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurullah</forename><surname>Sevim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.1145/3628602</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data. Just Accepted</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large language models and the future of law</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Charlotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">4548258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analytical study of algorithmic and expert summaries of legal cases</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paheli</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Legal Knowledge and Information Systems</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2023a. Ensemble methods for improving extractive summarization of legal case judgements</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="page" from="1" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">2023b. How ready are pre-trained abstractive models and llms for legal case judgement summarization?</title>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01248</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Limitations of mitigating judicial bias with machine learning</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Lum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gender and racial stereotype detection in legal opinion word embeddings</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hudzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Sepehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="12026" to="12033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Factbased court judgment prediction</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.13350</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nonet at semeval-2023 task 6: Methodologies for legal evaluation</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Kumar Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Deroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Shallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1293" to="1303" />
		</imprint>
	</monogr>
	<note>In Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Identifying biases in legal data: An algorithmic fairness perspective</title>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gender bias in legal corpora and debiasing it</title>
		<author>
			<persName><forename type="first">Nurullah</forename><surname>Sevim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furkan</forename><surname>ahinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324922000122</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="482" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Legal case document summarization: Extractive and abstractive methods and their evaluation</title>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paheli</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajdeep</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kripabandhu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1048" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring bias analysis on judicial data using machine learning techniques</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjory</forename><surname>Costa-Abreu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 12th International Conference on Pattern Recognition Systems (ICPRS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A short survey of viewing large language models in legal aspect</title>
		<author>
			<persName><forename type="first">Zhongxiang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09136</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning gender-neutral word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
