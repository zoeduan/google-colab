<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-03">3 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hashmath</forename><surname>Shaik</surname></persName>
							<email>hashmath.shaik@stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-2350</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Doboli</surname></persName>
							<email>alex.doboli@stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of ECE Stony Brook University Stony Brook</orgName>
								<address>
									<postCode>11794-2350</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-03">3 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">FEE530ADB1AA1C00F3AF8AF94D11835D</idno>
					<idno type="arXiv">arXiv:2501.00562v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>implementation generation</term>
					<term>Large Language Models</term>
					<term>open-ended problem solving</term>
					<term>prompting</term>
					<term>Reinforcement Learning</term>
					<term>Retrieval-augmented Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Problem solving is the process of creating a solution for a problem description <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. The solution can be an explanation for a set of properties exhibited by a static or dynamic situation, e.g., a mathematical proof, or an implementation (realization), which is the construction of a new materialization (e.g., design) that exhibits the required properties as a result of their operation (functioning, execution). This report focuses on the implementation (realization) of problem solving.</p><p>Creating an implementation can pertain to the three generalpurpose problem-solving situations: well-defined problems, illdefined problems, and open-ended problems <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>:</p><p>1) Well-defined problem solving for implementation construction describes situations in which an existing solution can be reused with some incremental changes to solve a new problem. For example, textbook algorithms are utilized to solve a new problem by selecting proper data structures and customizing the algorithm parameters, like the conditions of conditional statements and the iterations of loops. Using parameterized templates for circuit design <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[11]</ref> belongs to this category too. 2) Ill-defined problem solving for implementation construction represents cases in which the existing implementa-tions cannot solve all requirements, i.e. they satisfy some but not others <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. Changing the parameters of the implementation does not address the issue. Problem solving includes options, like producing a description of the implementation trade-offs by parameter sampling and selecting the best compromise, exploring implementation alternatives for specific fragments of the implementation, so that better trade-offs result for the overall solution, and selecting a different approach (principle) for an implementation, including situations when a new implementation must be built, similar to open-ended solving for building a new implementation. 3) Open-ended problem solving for implementation generation requires devising new solutions with a significant departure and characteristics from previous implementations. The understanding of this process is still limited <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>. Also, there are insufficient metrics to describe the degree to which the process is systematically progressing towards success, e.g., building a new implementation. Typical activities include problem framing and problem understanding, identifying and selecting the solving approach, divide and conquer (e.g., problem partitioning into sub-problems), implementation elaboration through trial-end-error, feature combination, adjustment, abstraction and insight gaining, implementation analysis to find pros and cons and the impact of features on the implementation operation, implementation modification, error correction, and handling unexpected situations.</p><p>As summarized in the next section, traditional automated implementation generation focuses mainly on elaboration and parameter trade-off exploration, for which the domain knowledge of the implementation is captured by customized metrics <ref type="bibr" target="#b16">[16]</ref> or in a library of basic building blocks <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. The library is static and does not evolve to incorporate new knowledge either from external sources or as a byproduct of implementation generation. Moreover, traditional methods assume the existence of a problem specification expressing at least functional and performance requirements, but more often the algorithm or architecture (structure) of the implementation <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref>. Hence, it can be argued that existing methods focus mainly on well-defined and ill-defined problems but less on implementation generation for open-ended problem solving. Existing approaches cannot tackle problem framing and exploring solution approaches, even though trial-anderror and rapid prototyping are essential in understanding new opportunities and limitations. Moreover, there is little automated support for divide and conquer and architecture creation, combination of features from different solutions, and handling unexpected situations. In general, traditional methods struggle with any activity conducted at a level above an algorithmic description of an implementation.</p><p>However, recent advances in Large Language Models (LLMs) created opportunities to devise novel automated implementation generation methods that can tackle problems beyond algorithmic specifications and may use domain knowledge that is dynamically learned over time. Arguably, LLMs could contain knowledge that is continuously updated by learning new features either from external documents or based on their own previously generated implementations. Implementation assessment could be improved by comparing it to similar, externally available implementations and considering collective feedback and preferences expressed for other solutions. The opportunities and limitations of an implementation can be better understood by embedding it into the trend of related designs. Moreover, support can be offered for problem framing and exploring possible solution approaches, activities that are often collective, in a team. LLMs can process multimodal descriptions, including natural language and images with certain degrees of specification completeness, unknowns, and ambiguity. Hence, understanding the capabilities of LLMs for implementation generation, possibly in conjunction with traditional methods, is required. These capabilities mostly emerge from LLMs being able to learn a broad range of associations in multi-modal data and diverse contexts.</p><p>This report studied the degree to which LLMs, possibly using prompting, Reinforcement Learning (RL) and Retrieval-Augmented Generation (RAG), can model the activities of implementation generation for an open-ended problem solving. The goal was to identify how LLMs and their extensions can contribute to implementing problem-solving activities that are not addressed in traditional methods. The report offers an extensive presentation of prompting methods, RAG techniques, and RL approaches. Then, the using of LLMs to implement problem-solving activities not available in traditional automated implementation generation was discussed. New research requirements were also offered. The report argued that these requirements refer to topics, like constructing the implementation approach, effectively controlling elaboration, robust qualitative and quantitative assessment across abstraction levels, knowledge memorizing during learning, and managing the problem solving process.</p><p>The report has the following structure. Section II offers an overview of the work on traditional, automated implementation generation. Section III presents an overview of LLMs. Section IV discusses the similarities of LLMs and traditional automated implementation generation methods and summarizes the related research needs. Conclusions end the report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW OF TRADITIONAL AUTOMATED IMPLEMENTATION GENERATION</head><p>Traditional approaches to automatically generate implementations can be grouped into four broad categories: (i) approaches based on high-level specifications, (ii) methods using evolutionary algorithms, (iii) agent-based methods, and (iv) cognitive architectures. The four categories are summarized next.</p><p>(i) Approaches based on high-level specifications: These approaches include traditional compiling methods to generate executable code <ref type="bibr" target="#b16">[16]</ref> and high-level synthesis methods <ref type="bibr" target="#b18">[18]</ref>- <ref type="bibr" target="#b21">[21]</ref> and template-based synthesis <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref> to create electronic circuits and systems. They use high-level specifications described using a programming language. Conceptually, specifications serve as parameterized descriptions of the target implementation architecture. Specifically, internal representations are built using a set of predefined rules (e.g., language grammar) applied to the specifications and then used to create an optimized hardware design by exploring different optimization possibilities. Prediction models or simulation tools are integrated to evaluate the performance of possible implementation alternatives.</p><p>These methods address the problem-solving activities in the following ways: The specification gives an unambiguous, complete description of the parameterized architecture. Thus, there is no problem framing step and problem understanding is fully addressed during specification creation. Divide and conquer is defined by the structuring of the specification. Also, there is no step of exploring possible implementation alternatives, as the specification explicitly describes the data processing steps, including the connections between the sequences of processing steps, i.e. using the processing outputs as inputs for the next processing steps. Hence, feature combination during elaboration only connects predefined operators which do not change their function based on the connections. From the point of view of cognitive psychology, these combinations are relation-based combinations but do not reflect feature-based combinations, in which features of a concept are transferred to another concept <ref type="bibr" target="#b22">[22]</ref>. Hence, there are no unexpected situations, including emerging features. Implementation analysis uses performance models and simulation, even though the pros and cons of an implementation are rarely causally linked to the implementation fragments responsible for them. Hence, the insight gain is limited. Trial-and-error (possibly guided by priority functions), implementation modification, and adjustment are only at the level of optimizing the architecture parameters. There is no abstraction or summarization during the process. Error correction requires to modify the specification and then repeat the problem-solving process.</p><p>(ii) Methods using evolutionary algorithms: These methods create a dynamic process, in which large populations of solutions originate new populations through traditional operators, i.e. selection, crossover, and mutation <ref type="bibr" target="#b23">[23]</ref>. Selection means propagating high-fitness individuals from the current to the next population, crossover combines features of a set of solutions to produce new solutions, and mutation randomly changes solution features.</p><p>These methods do not include problem framing and understanding. Identifying and selecting the implementation approach has been studied less, even though it is possible to maintain separate sub-populations, each for a different approach, and then giving higher priority to the sub-populations that include more high-quality implementations. There is no divide-and-conquer to separate a problem into sub-problems and no explicit error correction. Trial-and-error is mimicked through the mutation operator, even though mutation does not implement a systematic exploration process guided by the learned knowledge. There is no insight gaining during the process, abstraction or summarization of the learned knowledge, and no explicit identification of unexpected situations. Crossover implements combination, including feature and relation combination. Similar to the previous category, implementation analysis uses performance models and simulation to produce a fitness value that controls the selection of the better implementations. However, there is no explicit identification of the causal features that produce the pros and cons of an implementation, thus there is no implementation adjustment, modification, or correction guided by causal information. There is no explicit memory mechanism, features being implicitly memorized through a population, and there is no possibility to backtrack to previous states to attempt exploring a different path.</p><p>(iii) Agent-based methods: These methods utilize multiple interacting agents, each agent having its own memory and running its own decision-making algorithm <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>. Even though traditional agents realize simple decision-making algorithms, e.g., through a set of simple rules in response to specific inputs, it is possible to consider more complex methods, such as each agent running its own synthesis algorithm or population-based evolution. Agents interact with each other by communicating high-quality implementations and features, or implementation steps, which then can be utilized by the other agents, too.</p><p>Depending on their decision-making procedure, agent-based methods have similar characteristics, like the methods of the previous two categories. Their main advantage is their capacity to simultaneously maintain multiple perspectives about the implementation creation process, e.g., through their local memory, preferences, priorities, etc., and then aggregate these perspectives to improve problem solving. It can be argued that they mimic the implementation creation process by a team (team problem solving) <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b26">[26]</ref>.</p><p>(iv) Cognitive architectures: Cognitive architectures (CAs) mimic the brain activities during problem solving <ref type="bibr" target="#b27">[27]</ref>- <ref type="bibr" target="#b31">[31]</ref>. Architectures include modules for knowledge representation, knowledge memory, knowledge classification, summarization, comparison, decision-making, prediction, learning, and goal setting. For example, SOAR CA models cognition-based problem solving <ref type="bibr" target="#b28">[28]</ref>, using operation selection and application (e.g., state elaboration, operator proposal and evaluation, and decision). Knowledge is procedural if-then rules selected through matching. Learning stores short-cuts to solutions, conditions for applying the rules, and utility updates. ACT-R CA uses multiple symbolic knowledge representations, declarative and procedural information learning, and utility-based decision making <ref type="bibr" target="#b27">[27]</ref>. EPIC CA matches in parallel production rules to the working memory, followed by the selection of firing rules for multiple goals <ref type="bibr" target="#b30">[30]</ref>. Sigma CA includes mixed symbolicprobabilistic, discrete-continuous representations, knowledge summarization and integration, and inference-based reasoning <ref type="bibr" target="#b29">[29]</ref>. Clarion CA maintains explicit and implicit cognition, each having different representations and processing methods, e.g., rule extraction, generalization, specialization, backpropagation, and reinforcement learning <ref type="bibr" target="#b31">[31]</ref>. InnovA is a CA for automated design of electronic circuits <ref type="bibr" target="#b32">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF LARGE LANGUAGE MODELS, PROMPT ENGINEERING, RETRIEVAL-AUGMENTED GENERATION, AND REINFORCEMENT LEARNING A. Large Language Models</head><p>Large Language Models (LLMs), primarily those built on transformer architectures, have made significant strides in producing coherent, contextually relevant text <ref type="bibr" target="#b33">[33]</ref>. They excel at pattern recognition and can generate fluent natural language by leveraging billions of parameters trained on massive corpora <ref type="bibr" target="#b34">[34]</ref>. However, their computational principle-selfattention over sequential data-imposes fundamental limitations that hinder their ability to perform the rich, open-ended problem-solving tasks described in the previous sections.</p><p>At the core of these limitations is the reliance on statistical correlations rather than genuine logical or conceptual understanding. While self-attention excels at identifying relevant tokens in a sequence, it does not inherently encode hierarchical structures, domain-specific causal rules, or strict logical constraints. This stands in contrast to open-ended problem solving, where the concept space can be segmented into three main categories-hierarchical concepts, alternative concepts, and fundamental concepts-and the action space encompasses complex operations, such as feature combination, dynamic adjustment, abstraction, insight generation, and summarization <ref type="bibr" target="#b35">[35]</ref>. LLMs struggle to engage these conceptual spaces in a principled way because they are not grounded in mechanisms that ensure hierarchical reasoning, strategic problem decomposition, or the flexible reuse of insights and intermediate representations <ref type="bibr" target="#b36">[36]</ref>.</p><p>Another critical shortcoming is that LLMs tend to produce generalized answers aligned with the statistical patterns seen in their training data <ref type="bibr" target="#b37">[37]</ref>. They are not inherently equipped to execute a true divide-and-conquer approach to complex tasks, nor can they systematically apply trial-and-error strategies. For example, while open-ended problem solving may demand iterative refinement-where a solver explores a space of possible solutions, backtracks as necessary, and learns from failed attempts-an LLM's output is typically a single forward pass <ref type="bibr" target="#b38">[38]</ref>. Without an internal model of logical inference, memory structures that accumulate knowledge over multiple steps, or explicit strategy formulations, LLMs cannot easily correct their reasoning or adapt their approach based on previous mistakes <ref type="bibr" target="#b39">[39]</ref>. This leads to issues such as hallucinations, where models confidently assert falsehoods; distractions, where irrelevant details are emphasized; and a general inability to build complex, causally grounded explanations.</p><p>Some researchers have explored techniques like constraintbased decoding to enforce logical or linguistic rules at inference time <ref type="bibr" target="#b40">[40]</ref>. This can improve consistency and coherence to some extent, but it remains an add-on rather than a fundamental solution. Constraint-based methods do not grant the model a deeper conceptual understanding; they merely prune outputs that violate predetermined constraints. Similarly, improvements like sparse attention mechanisms reduce computational complexity, adapter layers can inject domain-specific knowledge <ref type="bibr" target="#b41">[41]</ref>, and memory-augmented transformers attempt to store and reuse intermediate reasoning steps. While these approaches enhance performance on certain tasks, they do not fully overcome the inherent limitations of attention-based architectures or enable robust open-ended problem solving. The models are still limited by their training data, biased toward patterns present therein, and lack the ability to intentionally search concept space, systematically test hypotheses, or derive new conceptual abstractions beyond what is statistically suggested <ref type="bibr" target="#b42">[42]</ref>.</p><p>In response to these challenges, a body of methods has emerged to push LLMs closer toward more sophisticated reasoning and problem-solving behaviors. This work can be broadly divided into three interrelated categories: Prompt Engineering, knowledge retrieval through Retrieval-Augmented Generation, and model refinement through Reinforcement Learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prompt Engineering</head><p>Prompting techniques utilize carefully constructed input prompts to guide the model's response generation process. Techniques can be grouped into five categories dicussed next.</p><p>a) Single-stage prompting (SSP): SSP methods directly instruct the model without iterative refinement. Meanwhile, Basic + Annotation Guideline-Based Prompting + Error Analysis-Based Prompting <ref type="bibr" target="#b43">[43]</ref> uses formally defined entity annotation guidelines to specify how clinical terms should be identified and categorized, ensuring clarity in entity recognition. In addition, it incorporates instructions derived from analyzing common model errors, such as addressing ambiguous entity boundaries or redefining prompts for overlapping terms. This strategy significantly improves clinical Named Entity Recognition, with relaxed F1 scores reported as 0.794 for GPT-3.5 and 0.861 for GPT-4 on the MTSamples dataset <ref type="bibr" target="#b44">[44]</ref> and 0.676 for GPT-3.5 and 0.736 for GPT-4 on the VAERS dataset <ref type="bibr" target="#b45">[45]</ref>, demonstrating its effectiveness.</p><p>b) Reasoning strategies: These methods are of three types: linear, branching, and iterative reasoning.</p><p>Linear reasoning methods such as Chain-of-Thought (CoT), Complex CoT, Thread-of-Thought (ThoT), Chain-of-Knowledge (CoK), Chain-of-Code (CoC), Logical Thoughts (LoT), Chain-of-Event (CoE), and Chain-of-Table generate a single, step-by-step sequence (chain) of responses toward the final answer. Methods differ in the type of task they target, i.e., code generation, summarization, and logical inference, and in how they refine or represent intermediate steps. CoT shows that using intermediate prompting steps can enhance accuracy, e.g., up to 39% gains in mathematical problem solving <ref type="bibr" target="#b46">[46]</ref>. An example of in-context prompt for CoT might be: "If the problem is 'Calculate 123 × 456,' break it down as (100 + 20 + 3) × 456 and compute step-by-step." Complex CoT uses more involved in-context examples, improving performance by as much as 18% on harder tasks <ref type="bibr" target="#b47">[47]</ref>. ThoT tackles long or chaotic contexts by breaking them into manageable parts (e.g., dividing long passages into sections for sequential summarization) <ref type="bibr" target="#b48">[48]</ref>, while CoK strategically adapts and consolidates knowledge from multiple sources to ensure coherence and reduce hallucination <ref type="bibr" target="#b49">[49]</ref>. CoC specializes in code-oriented reasoning by simulating key code outputs (e.g., predicting intermediate variable states for debugging) <ref type="bibr" target="#b50">[50]</ref>, whereas LoT integrates logical equivalences and reductio ad absurdum checks to refine reasoning chains (e.g., validating statements by identifying contradictions in their negations) <ref type="bibr" target="#b51">[51]</ref>. CoE handles summarization by extracting, generalizing, filtering, and integrating key events (e.g., pinpointing main events from news articles) <ref type="bibr" target="#b52">[52]</ref>, and Chain-of-Table <ref type="table">extends</ref> CoT techniques to tabular data, dynamically applying transformations like filtering or aggregation to generate coherent answers <ref type="bibr" target="#b53">[53]</ref>.</p><p>Branching reasoning methods, like Self-Consistency, Contrastive CoT (or Contrastive Self-Consistency), Federated Same/Different Parameter Self-Consistency/CoT (Fed-SP/DP-SC/COT), Tree-of-Thoughts, and Maieutic Prompting, explore multiple possible reasoning paths in parallel. Branching techniques vary in how they sample or fuse paths, some relying on consensus votes and others on dynamic adaptation or tree-based elimination. Self-Consistency, for instance, samples diverse solution paths and selects the most consistent final answer, achieving gains of over 11% on math tasks <ref type="bibr" target="#b54">[54]</ref>. Contrastive CoT incorporates both correct and incorrect in-context examples to broaden the model's understanding, improving performance by over 10% compared to standard CoT <ref type="bibr" target="#b55">[55]</ref>. Fed-SP-SC leverages paraphrased queries to crowdsource additional hints <ref type="bibr" target="#b56">[56]</ref>, while ToT maintains a tree of partial solutions and systematically explores them with breadth-first or depth-first strategies, offering up to 65% higher success rates than CoT on challenging math tasks(ToT) <ref type="bibr" target="#b57">[57]</ref>. Maieutic Prompting likewise generates a tree of propositions to reconcile contradictory statements, surpassing linear methods by 20% on common-sense benchmarks <ref type="bibr" target="#b58">[58]</ref>.</p><p>Iterative reasoning approaches, such as Plan-and-Solve (PS), Program-of-Thoughts (PoT), Chain-of-Symbol (CoS), Structured Chain-of-Thought (SCoT), and Three-Hop Reasoning (THOR), refine solutions step by step, often by passing intermediate outputs back into the model to enhance accuracy.</p><p>PS explicitly decomposes tasks into planning and execution phases, where the planning phase structures the problem into smaller sub-tasks, and the execution phase solves them sequentially. This reduces semantic and calculation errors, outperforming Chain-of-Thought (CoT) prompting by up to 5% <ref type="bibr" target="#b59">[59]</ref>. PoT enhances performance by separating reasoning from computation: the model generates programmatic solutions executed by a Python interpreter, achieving up to 12% accuracy gains in numerical and QA tasks <ref type="bibr" target="#b60">[60]</ref>. CoS encodes spatial and symbolic relationships using concise symbolic representations, which improves reasoning in spatial tasks by up to 60.8% <ref type="bibr" target="#b61">[61]</ref>. SCoT introduces structured reasoning through program-like branching and looping, significantly improving code generation accuracy by up to 13.79% <ref type="bibr" target="#b62">[62]</ref>. Finally, THOR tackles emotion and sentiment analysis through a threestage approach: aspect identification, opinion analysis, and polarity inference. This structured method achieves superior performance compared to previous supervised and zero-shot models <ref type="bibr" target="#b63">[63]</ref>. These approaches exemplify the power of iterative methods in breaking complex problems into manageable components, thereby reducing errors and improving overall performance.</p><p>c) Multi-Stage Prompting (MSP): MSP techniques rely on iterative feedback loops or ensemble strategies. MSP methods systematically refine outputs and incorporate multiple response paths, e.g., through voting or iterative analysis, to yield more robust and accurate solutions, particularly in domains requiring deeper reasoning or tailored task adaptation. Ensemble Refinement (ER) <ref type="bibr" target="#b64">[64]</ref> builds on Chain-of-Thought (CoT) and Self-Consistency by generating multiple CoT-based responses at high temperature (introducing diversity) and then iteratively conditioning on generated responses to produce a more coherent and accurate output, leveraging insights from the strengths and weaknesses of initial explanations and majority voting. Auto-CoT <ref type="bibr" target="#b65">[65]</ref> constructs demonstrations automatically by clustering queries from a dataset and generating reasoning chains for representative queries using Zero-Shot-CoT. Clustering is achieved by partitioning questions into groups based on semantic similarity, ensuring that representative queries capture the diversity of the dataset. ReAct <ref type="bibr" target="#b66">[66]</ref> interleaves reasoning traces-thought processes that explain intermediate steps-with action steps that execute operations, enabling superior performance in complex tasks by seamlessly combining reasoning and action. Moreover, Active-Prompt <ref type="bibr" target="#b67">[67]</ref> adaptively selects the most uncertain training queries, identified via confidence metrics like entropy or variance, for human annotation, boosting few-shot learning performance by focusing on areas with the highest uncertainty.</p><p>d) Knowledge Enhancement: These approaches use highquality examples and strategic self-monitoring to improve LLM performance. They pertain to two types, example-based and meta-level guidance methods</p><p>Example-based methods leverage auxiliary examples or synthesized instances to guide the response creation process of LLMs. MathPrompter <ref type="bibr" target="#b68">[68]</ref> focuses on creating a symbolic template of the given mathematical query, solving it analyti-cally or via Python, and then validating the derived solution with random variable substitutions before finalizing the answer. The approach boosts accuracy from 78.7% to 92.5%. Analogical Reasoning <ref type="bibr" target="#b69">[69]</ref> prompts LLMs to generate and solve similar examples before addressing the main problem, resulting in a 4% average accuracy gain across various tasks. Synthetic Prompting <ref type="bibr" target="#b70">[70]</ref> involves a backward step, where a new query is generated from a self-constructed reasoning chain, and a forward step, where this query is re-solved; this strategy selects the most complex examples for few-shot prompts, leading to up to 15.6% absolute improvements in mathematical problem solving, common-sense reasoning, and logical reasoning.</p><p>Meta-Level Guidance (MLG) methods enhance LLMs by promoting self-reflection and focusing on pertinent information, thereby reducing errors. Self-Reflection involves the model evaluating its own outputs to identify and correct mistakes, leading to improved performance. For example, in translation tasks, self-reflection enables LLMs to retrieve bilingual knowledge, facilitating the generation of higherquality translations. Focusing is achieved through techniques like System 2 Attention (S2A) <ref type="bibr" target="#b71">[71]</ref>, which filters out irrelevant content by prompting the model to regenerate the context to include only essential information before producing a final response. This two-step approach enhances reasoning by concentrating on relevant details, thereby improving accuracy. S2A has been shown to outperform basic prompting methods, including Chain-of-Thought (CoT) and instructed prompting, particularly on truthfulness-oriented datasets. Metacognitive Prompting (MP) <ref type="bibr" target="#b72">[72]</ref> introduces a five-stage process to further enhance LLM performance: (1) Comprehension: The model attempts to understands the input, ensuring clarity before proceeding; (2) Preliminary Judgment: An initial assessment is made based on the understood information; (3) Critical Evaluation: The initial judgment is scrutinized, considering alternative perspectives and potential errors; (4) Final Decision with Explanation: A conclusive decision is reached, accompanied by a rationale to support it; and (5) Self-Assessment of Confidence: The model evaluates its confidence in the final decision, reflecting on the reasoning process. This structured approach enables LLMs to perform consistently better than methods like CoT and Program Synthesis (PS) across various natural language processing tasks, including paraphrasing, natural language inference, and named entity recognition.</p><p>e) Task Decomposition: These approaches break down complex tasks into smaller steps but vary in how they orchestrate and execute the sub-problems. They include problem breakdown and sequential solving methods.</p><p>Problem Breakdown approaches include the Least-to-Most method <ref type="bibr" target="#b73">[73]</ref>, which addresses the challenge of Chain-of-Thought (CoT) failing on problems more difficult than its exemplars by first prompting the LLM to decompose a query into sub-problems and then solving them sequentially, demonstrating notable improvements over CoT and basic prompting on tasks like commonsense reasoning and mathematical problem solving. The decompositions are characterized by their hierarchical structure, breaking down complex problems into simpler, manageable sub-tasks that build upon each other to facilitate step-by-step reasoning. Decomposed Prompting (De-comP) breaks complex tasks into simpler sub-tasks, each handled with tailored prompts or external tools, ensuring efficient and accurate execution. For instance, the task "Concatenate the first letters of words in 'Jack Ryan'" is decomposed into extracting words, finding their first letters, and concatenating them <ref type="bibr" target="#b74">[74]</ref>. DecomP leverages modular decomposers to partition problems hierarchically or recursively, assigning subtasks to specialized LLMs or APIs. This approach achieves a 25% improvement over CoT and Least-to-Most methods in Commonsense Reasoning. Program-Aided Language Models (PAL) <ref type="bibr" target="#b75">[75]</ref> further leverage interleaved natural language and programmatic steps to enable Python-based execution of the reasoning process, surpassing CoT and basic methods for mathematical and commonsense tasks.</p><p>Sequential Solving includes methods, like Binder and Dater algorithms. Binder <ref type="bibr" target="#b76">[76]</ref> integrates neural and symbolic parts by using an LLM both as a parser and executor for natural language queries, leveraging programming languages like Python or SQL for structured execution. Binding is achieved through a unified API that enables the LLM to generate, interpret, and execute code using a few in-context examples, leading to higher accuracy on table-based tasks compared to fine-tuned approaches. Dater <ref type="bibr" target="#b77">[77]</ref> focuses on few-shot table reasoning by splitting a large table into relevant subtables, translating complex queries into SQL sub-queries, and combining partial outcomes into a final solution. These three steps aim to systematically extract meaningful data, execute precise operations, and integrate results to address complex queries, outperforming fine-tuned methods by at least 2% on Table-Based Truthfulness and 1% on Table-Based QA, and surpassing Binder on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Retrieval-Augmented Generation</head><p>Retrieval-Augmented Generation (RAG) addresses one of the major issues of LLMs, which are their lack of a persistent, reliable memory and factual grounding <ref type="bibr" target="#b78">[78]</ref>. RAG methods integrate external knowledge sources into the generation process. Instead of relying solely on learned representations within the model's parameters, the system retrieves relevant documents, facts, or structured data at inference time and incorporates this information into its output. This grounding reduces hallucinations, ensures that the model's reasoning steps reference accurate and up-to-date information, and can improve the alignment of the solution with real-world constraints <ref type="bibr" target="#b79">[79]</ref>. The versatility of RAG has led to significant advancements in various domains, such as healthcare, finance, education, and scientific research facilitated by novel frameworks tailored to address challenges in reasoning, problem-solving, and knowledge integration. This review categorized these advancements into four areas: task-specific and schema-based techniques, self-aware and adaptive mechanisms, long-term memory integration, and multi-hop and multi-modal reasoning. The four areas are discussed next. a) Task-Specific and Schema-Based Retrieval (TSR): TSR approaches leverage structured methods to solve problems in domains such as mathematics and knowledge-intensive tasks. Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) <ref type="bibr" target="#b80">[80]</ref> employs schema-based instruction to solve math word problems by predicting relevant schemas, offering a structured problem-solving paradigm. For instance, given the problem, "If a worker earns $20 per hour, how much will they earn in 10 hours?", the model predicts the multiplicative schema, which involves calculating the product of hourly earnings and hours worked. Using this schema, the problemsolving process is guided step-by-step: the model multiplies the hourly rate ($20) by the number of hours <ref type="bibr" target="#b10">(10)</ref>, resulting in a total earning of $200. Schemas act as templates for organizing and applying domain-specific knowledge and are inherently tied to knowledge graphs that map relationships between concepts, further enhancing reasoning capabilities. By aligning the problem context with predefined patterns, SBI-RAG ensures systematic and accurate solutions while improving explainability. Similarly, Knowledge Graph-Enhanced RAG Framework (KRAGEN) <ref type="bibr" target="#b81">[81]</ref> employs advanced prompting techniques, notably the graph-of-thoughts (GoT) method, to dynamically decompose complex problems into smaller subproblems. Each subproblem is addressed using relevant knowledge retrieved through the RAG framework, minimizing hallucinations and enhancing solution accuracy. The individual solutions are then consolidated to form a comprehensive answer, with KRAGEN's graph visualization enabling users to interact with and assess the quality of the solution's GoT structure and logic <ref type="bibr" target="#b81">[81]</ref>. These techniques stand out for their ability to address domain-specific challenges while ensuring adaptability through schema-guided reasoning. The use of schemas not only structures the solution process but also facilitates explainability.</p><p>In data-driven tasks, Generative Retrieval-Augmented Matching (GRAM) addresses schema matching by employing a hierarchical classification model that dynamically generates prompts for matching attributes across schemas. Specifically, GRAM utilizes a two-step process: first, it performs a coarsegrained classification to identify potential attribute matches. For instance, given two schemas, GRAM might preliminarily match the attribute "Customer Name" in one schema with "Client Name" in another. Then, it refines these matches through fine-grained classification, analyzing the context and patterns in the data to confirm the match and enhance the precision of schema alignment. In this case, GRAM would validate that "Customer Name" and "Client Name" indeed refer to the same entity by assessing their usage and data properties. The prompt generation process, guided by LLMs, enables zero-shot and few-shot learning, allowing GRAM to perform efficiently and accurately in database integration tasks, even when minimal labeled data is available <ref type="bibr" target="#b82">[82]</ref>. Similarly, TableRAG <ref type="bibr" target="#b83">[83]</ref> focuses on reasoning over tabular data by retrieving and processing row-column relationships to interpret structured datasets accurately. It conducts reasoning by leveraging query expansion combined with schema and cell retrieval to pinpoint crucial information before providing it to the language models, enabling efficient data encoding and precise retrieval. This approach allows TableRAG to handle large-scale tables effectively, reducing prompt lengths and mitigating information loss during the reasoning process <ref type="bibr" target="#b83">[83]</ref>. b) Self-Aware and Adaptive Retrieval: Recent RAG frameworks emphasize self-awareness and adaptive mechanisms to address uncertainties in LLMs. Self-aware Knowledge Retrieval (SeaKR) <ref type="bibr" target="#b84">[84]</ref> activates retrieval during high uncertainty and re-ranks snippets to ensure reliability. Specifically, SeaKR <ref type="bibr" target="#b84">[84]</ref> addresses uncertainties arising from the LLM's internal state inconsistencies, triggering retrieval when the model's self-assessed confidence is low. The re-ranking process involves selecting knowledge snippets that most effectively reduce the model's uncertainty, thereby enhancing response accuracy <ref type="bibr" target="#b84">[84]</ref>. Self-RAG <ref type="bibr" target="#b85">[85]</ref> introduces iterative refinement, where retrieval queries generated during the response process enable reassessment and improvement of outputs. This reassessment involves evaluating the relevance of retrieved information during generation, allowing the model to iteratively refine its responses for enhanced accuracy. Critic-Guided Planning (CR-Planner) <ref type="bibr" target="#b86">[86]</ref> leverages critic models to iteratively guide retrieval and reasoning toward task-specific goals. The critic model operates by evaluating potential subgoals and their executions, assigning rewards to guide the selection of the most promising reasoning paths. This guidance ensures that the reasoning process aligns with task objectives, effectively navigating complex problem spaces <ref type="bibr" target="#b86">[86]</ref>. For domain-specific adaptation, SimRAG <ref type="bibr" target="#b87">[87]</ref> employs selftraining, generating and filtering synthetic data to fine-tune models for specialized fields. In biomedical applications, Self-Rewarding Tree Search (SeRTS) <ref type="bibr" target="#b88">[88]</ref> combines Monte Carlo Tree Search and Reinforcement Learning to optimize retrieval. Speculative RAG <ref type="bibr" target="#b89">[89]</ref> improves efficiency with a two-stage process: a smaller model drafts responses, while a larger model evaluates and finalizes them. This two-step process allows the system to balance efficiency and accuracy by leveraging the strengths of both models.</p><p>These approaches offer distinct benefits and limitations. SeaKR and Self-RAG provide dynamic adaptability and accuracy but demand significant computational resources. CR-Planner and SeRTS enhance task-specific precision but increase complexity. SimRAG excels in domain-specific tuning, however it is constrained by the need for high-quality synthetic data. Speculative RAG effectively reduces latency through parallel drafting and verification, but requires accurate evaluation by generalist models.</p><p>c) Long-Term Memory for Knowledge Retrieval: Long-term memory integration in RAG frameworks addresses the limitations of purely query-specific retrieval by enabling the retention and reuse of knowledge across tasks. HippoRAG <ref type="bibr" target="#b90">[90]</ref>, inspired by the hippocampal indexing theory, integrates longterm memory by linking a knowledge graph to an LLM and prioritizing relevant nodes using the Personalized PageRank algorithm. This approach allows the model to retrieve interconnected knowledge dynamically, consolidating past context for tasks like multi-hop reasoning, achieving up to 20% better performance. It excels in repetitive and longitudinal tasks by enabling adaptive and context-aware retrieval, mimicking how the brain organizes and recalls episodic memories.</p><p>Various architectures embed long-term memory into RAG. MemLong <ref type="bibr" target="#b91">[91]</ref> employs a dual-network design where a frozen LLM backbone serves as a memory encoder, while a residual side-network manages retrieval, enabling efficient caching and updating of extensive contexts (up to 65k tokens). Its key advantage is scalability without data staleness, though managing large contexts may introduce overhead. HAT <ref type="bibr" target="#b92">[92]</ref> introduces a Hierarchical Aggregate Tree structure that organizes dialogue history into a tree, where each node represents aggregated information from its child nodes. This design allows the system to manage extensive conversational contexts by traversing the tree to retrieve relevant information, enhancing coherence and summary quality. The recursive aggregation enables the model to handle long-term dependencies effectively, though challenges may arise in balancing tree depth with performance. MemoRAG <ref type="bibr" target="#b93">[93]</ref> combines a lightweight global memory model with a retrieval-generation module, using draft answers as "clues" to guide precise retrieval from extensive datasets. It efficiently handles up to one million tokens by separating memory updates from retrieval operations, ensuring scalability and contextual relevance. This architecture excels in complex tasks but requires fine-tuning to balance memory efficiency and retrieval accuracy. Pistis-RAG <ref type="bibr" target="#b94">[94]</ref> introduces a scalable, multi-stage framework for retrieval-augmented generation (RAG) systems, emphasizing alignment with human preferences through online learning and user feedback. Its architecture comprises distinct stages-matching, pre-ranking, ranking, reasoning, and aggregating-each refining the retrieval process to enhance response quality. A notable innovation is the ranking stage, which considers both semantic relevance and LLM preferences, addressing the sensitivity of LLMs to prompt ordering. By adopting a content-centric approach, Pistis-RAG integrates user feedback to continuously adapt and align with evolving user needs, resulting in a 9.3% performance improvement on the MMLU (English) benchmark. However, the reliance on continuous user feedback may introduce variability, necessitating careful system tuning to maintain consistent performance.</p><p>d) Multi-Hop and Multi-Modal Reasoning Retrieval: Multihop and multi-modal reasoning broaden Retrieval-Augmented Generation (RAG)'s capacity to tackle tasks requiring complex, step-by-step deliberation and integration of diverse data sources. Multi-hop reasoning connects information across multiple steps to derive coherent answers, while multi-modal reasoning combines data from various formats such as text, images, and audio. This systematic approach enhances RAG's ability to deliver comprehensive and well-founded responses to multifaceted queries.</p><p>Multi-layered Thoughts Enhanced RAG (METRAG) <ref type="bibr" target="#b95">[95]</ref> integrates similarity-and utility-based reasoning for deeper contextual understanding. It does so by combining similarityoriented retrieval with utility-oriented assessments, where a utility model, supervised by an LLM, evaluates the usefulness of retrieved documents beyond mere similarity using metrics like task relevance, informativeness, query-specific novelty, and completeness, enhancing the relevance and quality of the information utilized in generation.</p><p>RAG-Star <ref type="bibr" target="#b96">[96]</ref> integrates retrieval augmentation with Monte Carlo Tree Search (MCTS) to improve problem-solving accuracy by iteratively planning intermediate sub-queries. Retrieval augmentation enables the model to incorporate external information, enhancing its reasoning process. Using MCTS, RAG-Star systematically explores reasoning paths by generating and evaluating intermediate sub-queries and their potential answers. This approach balances exploration and exploitation to identify the most promising reasoning trajectories, guiding the model toward highly accurate and contextually relevant solutions.</p><p>Knowledge Graph-Enhanced RAG Framework (KRA-GEN) <ref type="bibr" target="#b81">[81]</ref> employs a "Graph-of-Thoughts" methodology to decompose multi-hop reasoning problems into explainable and systematic components. This approach structures the reasoning process by representing knowledge as interconnected concepts and relationships, often derived from knowledge graphs. During problem-solving, the model constructs this graph dynamically, allowing it to break down complex queries into smaller, manageable sub-tasks. By systematically addressing each component, KRAGEN enhances both the interpretability and accuracy of its reasoning process, providing a more transparent and effective framework for handling intricate queries.</p><p>However, a potential limitation of KRAGEN is its reliance on the quality and comprehensiveness of the underlying knowledge graph. If the knowledge graph lacks certain information or contains inaccuracies, the model's reasoning and outputs may be adversely affected. Ensuring the knowledge graph is up-to-date and accurately reflects the domain is crucial for maintaining the effectiveness of the KRAGEN framework.</p><p>Building upon the foundational concepts of multi-hop and multi-modal reasoning, recent research has proposed innovative frameworks to address the inherent complexities of such tasks. These advancements focus on refining the step-by-step reasoning process and integrating diverse modalities, enabling models to navigate intricate queries with enhanced accuracy and explainability. By tackling challenges in sequential inferencing and combining textual with visual or other modal data, these frameworks set a new benchmark for retrievalaugmented generation systems For instance, MultiHop-RAG provides a dedicated dataset and benchmarks to rigorously assess RAG systems on multistep queries <ref type="bibr" target="#b97">[97]</ref>, facilitating the evaluation of retrievalaugmented generation models in scenarios that necessitate reasoning across multiple documents. Retrieval-Augmented Multi-modal Chain-of-Thoughts Reasoning <ref type="bibr" target="#b98">[98]</ref> extends Chain-of-Thought (CoT) approaches to handle images and text in tandem, enabling models to process and reason over visual and textual data simultaneously. For purely textual multi-hop question answering, HOP, UNION, GENERATE (HUG) <ref type="bibr" target="#b99">[99]</ref> offers a three-step method that models rationales as sets of sentences, enhancing explainability without requiring explicit rationale supervision. In this framework, "Hop" involves selecting relevant sentences, "Union" aggregates these sentences into a coherent rationale set, and "Generate" produces the final answer based on the aggregated rationale. The rationales modeled are the sets of sentences that collectively support the answer, providing transparency in the reasoning process by explicitly outlining the evidence considered. Multimodal-CoT and Multi-Chain Reasoning (MCR) <ref type="bibr" target="#b100">[100]</ref> further advance reasoning by respectively separating rationale generation from answer inference for science question answering, and by prompting LLMs to examine multiple parallel chains of thought before synthesizing final solutions. These approaches address complex reasoning types that require integrating diverse information sources and evaluating multiple reasoning pathways. The rationale generated includes intermediate reasoning steps that elucidate the thought process leading to the answer. Prompting is generated by designing specific instructions that guide the model to consider various perspectives and reasoning chains, thereby enhancing the robustness and accuracy of the final output.</p><p>Although RAG improves factual correctness and can help the model explore a broader concept space by tapping into external repositories, it still does not imbue the model with a genuine, internal problem-solving strategy. e) Self-Reflection Methods: Recent advancements underscore the value of LLMs engaging in reflective reasoning before generating a final answer. Reflective reasoning involves the model's introspection and evaluation of its own thought processes to enhance decision-making and output quality.</p><p>Implicit Retrieval-Augmented Generation (RAG) <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b101">[101]</ref>, <ref type="bibr" target="#b102">[102]</ref> instructs LLMs to first retrieve key chunks of context, specifying the number of sections and words in each section, then use these snippets to answer queries. The selection of the number of snippets and their lengths is typically determined through empirical tuning, balancing the need for comprehensive context with the constraints of the model's input capacity. This method has achieved near state-of-the-art results in both general and biomedical contextual questionanswering tasks.</p><p>Metacognitive Prompting (MP) <ref type="bibr" target="#b72">[72]</ref> draws on the concept of metacognition, comprising five phases:</p><p>1. Interpreting the Input: The model analyzes the input text to grasp its context and meaning, ensuring a clear understanding of the task at hand. This is implemented by prompting the model to restate or summarize the input, confirming comprehension.</p><p>2. Forming an Initial Judgment: Based on the interpreted input, the model generates a preliminary response or hypothesis, reflecting its immediate understanding. This involves producing an initial answer without external validation.</p><p>3. Critically Assessing that Judgment: The model evaluates its preliminary response, identifying potential errors or uncertainties. This is achieved by prompting the model to question its initial answer, consider alternative interpretations, and assess the confidence level of its response.</p><p>4. Presenting a Final Decision with Reasoning: After critical assessment, the model formulates a refined answer, providing a rationale that outlines the reasoning process. This step ensures transparency and allows users to understand the basis of the model's conclusion.</p><p>5. Gauging Confidence in the Entire Process: The model reflects on the overall process, assigning a confidence score to its final answer, indicating the reliability of the response. This is implemented by having the model express its certainty level, guiding users in decision-making.</p><p>MP consistently outperforms Chain-of-Thought (CoT) and Plan-and-Solve methods across paraphrasing, natural language inference, and relation extraction tasks. f) Self-Critique Methods/Evaluation-and Verification-Focused Methods: To enhance reliability and reduce factual inaccuracies in automated reasoning, self-critique methods have emerged as critical tools. These methods address challenges in producing consistent, accurate outputs by systematically verifying and refining initial responses. Chain-of-Verification (CoVe) <ref type="bibr" target="#b103">[103]</ref> uses a four-step process: (1) generating an initial response, (2) formulating verification questions to identify potential errors or inconsistencies, (3) answering these questions to produce supporting evidence or rationale, and (4) revising the original response based on validated findings. CoVe has demonstrated over 10% performance improvements compared to basic prompting and Chain-of-Thought (CoT) methods in both context-free and contextual question-answering tasks.</p><p>Verify-and-Edit (VE) <ref type="bibr" target="#b104">[104]</ref> enhances uncertain CoT outputs by integrating external knowledge from reliable sources such as encyclopedias, knowledge graphs, or domain-specific repositories. Self-consistency identifies weak points in reasoning by generating multiple reasoning paths for the same problem and comparing their outputs for discrepancies or logical contradictions, revealing areas of low confidence or errors. The response is then revised by incorporating validated evidence, ensuring factual accuracy and logical coherence. Cross-referencing further verifies the revised response by rechecking it against retrieved knowledge to confirm it resolves inconsistencies while maintaining alignment across all reasoning steps, avoiding the introduction of new errors or contradictions. VE evaluates the reliability of the final output by analyzing agreement across revised reasoning paths and ensuring alignment with external knowledge. This approach has achieved up to 10% gains in multi-hop reasoning tasks and 2% improvements in truthfulness evaluations over CoT and self-consistency techniques.</p><p>In summary, self-critique methods, i.e., CoVe and VE, concentrate on verifying and refining initial outputs to reduce inaccuracies, while self-reflection techniques, e.g., Implicit RAG and MP, emphasize reflective reasoning for deepening understanding and clarity before producing an answer. CoVe and VE diverge in methodology: CoVe generates verification queries for self-checking, whereas VE specifically pinpoints uncertain outputs and edits them using external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Reinforcement Learning</head><p>Reinforcement Learning (RL) provides a systematic framework for refining LLM behavior by guiding models toward desired objectives through iterative feedback and carefully designed reward signals from human feedback, automated metrics, or a pre-trained reward model <ref type="bibr" target="#b105">[105]</ref>. There are six main components: agent, environment, state, action, reward, and policy <ref type="bibr" target="#b106">[106]</ref>. To apply RL for fine-tuning LLMs, the first step maps the six components to the LLM framework: the LLM represents the policy, while the current textual sequence is the state, and based on this state, the LLM generates an action, the next token. This action updates the state, creating a new state that incorporates the newly added token. After generating a complete textual sequence, a reward is determined by assessing the quality of the LLM output. This reward can be used to train a pre-trained reward model or can be directly integrated into the alignment process to guide the behavior of the model.</p><p>The RL methods adopted by these models can be divided into two main categories, model-based RL approaches and model-free approaches, which were discussed next. a) Model-based RL Approaches: The methods in this category can be grouped into three categories, RLHF, RLAIF and exploration, which are discussed next.</p><p>Reinforcement Learning from Human Feedback (RLHF): RL from Human Feedback (RLHF) re-train LLMs by incorporating a reward signal derived from human evaluations. RLHFs perform three fundamental stages: They initially perform supervised fine-tuning (SFT) using labeled datasets, followed by training a reward model (RM) based on human-evaluated outputs, and finally use this reward signal to inform the model's policy fine-tuning using the Proximal Policy Optimization (PPO) algorithm <ref type="bibr" target="#b107">[107]</ref>.</p><p>[108] created fine-tuned models like InstructGPT using human feedback to better adhere to user instructions. Similarly, <ref type="bibr" target="#b109">[109]</ref> and <ref type="bibr" target="#b110">[110]</ref> explored reward modeling and methods to address challenges such as length bias, ensuring outputs are concise and aligned with human expectations. Frameworks like trlX <ref type="bibr" target="#b111">[111]</ref> and high-quality datasets introduced by <ref type="bibr" target="#b112">[112]</ref> have scaled RLHF applications, improving the performance of LLMs in tasks such as summarization, translation, and dialogue generation. Summarization tasks, for example, leverage reinforcement learning (RL) through both extractive and abstractive methods; extractive summarization selects key sentences from the source, while abstractive summarization generates novel sentences to convey the essence of the content <ref type="bibr" target="#b113">[113]</ref>. RL optimizes summarization by using rewards based on metrics like ROUGE to iteratively enhance the quality of outputs. Policy optimization, on the other hand, employs pairwise feedback, comparing response pairs to align LLM outputs with human preferences. Techniques such as Pairwise Proximal Policy Optimization simplify the process by directly operating on comparative rewards, avoiding complexities like value function estimation and normalization <ref type="bibr" target="#b114">[114]</ref>.</p><p>PPO algorithms iteratively adjust the weights of a model to maximize the expected reward <ref type="bibr" target="#b107">[107]</ref>. Central to this process is the collection of human feedback, which is critical in training reward models. Studies, such as Skywork-Reward <ref type="bibr" target="#b115">[115]</ref> and T ÜLU-V2-mix <ref type="bibr" target="#b116">[116]</ref>, utilize human preferences by curating datasets of ranked examples, enabling models to align more effectively with human judgments. Additionally, <ref type="bibr" target="#b117">[117]</ref> introduces tool-augmented reward modeling, integrating external resources like calculators and search engines to refine alignment. Recent generative reward models use synthetic preferences, which are artificially created by sampling and ranking model outputs using a base preference model, to reduce reliance on extensive human feedback. <ref type="bibr" target="#b118">[118]</ref> examined efficient methods for collecting pairwise human preferences, optimizing reward model design within RLHF frameworks. Additionally, research on over-optimization risks underscores the importance of balanced training to prevent performance degradation <ref type="bibr" target="#b119">[119]</ref>. <ref type="bibr" target="#b114">[114]</ref> propose novel pairwise feedback pipelines that improve preference learning and policy optimization by comparing response pairs to better capture human preferences. RLHF's multi-step process remains resource-intensive and reliant on extensive human feedback <ref type="bibr" target="#b120">[120]</ref>. Over-optimization risks may cause models to exploit weaknesses in the reward function rather than achieving genuine alignment with human preferences <ref type="bibr" target="#b119">[119]</ref>.</p><p>RL from AI Feedback (RLAIF) is a training method designed to replace human evaluators with AI systems, offering better scalability and consistency by mitigating the variability of human judgment <ref type="bibr" target="#b121">[121]</ref>. In RLAIF, a Reward Model (RM) is trained using preference labels generated by an LLM. These labels are transformed into a probability distribution through a softmax function and optimized via cross-entropy loss, enabling the RM to guide the training of the target AI model <ref type="bibr" target="#b122">[122]</ref>. Various approaches have been proposed to address the specific challenges of RLAIF. For example, UltraFeedback compiles a large-scale dataset of over one million GPT-4 feedback annotations on 250,000 user-assistant conversations to train reward models <ref type="bibr" target="#b112">[112]</ref>. Similarly, Magpie employs a self-synthesis method, where an aligned LLM generates large-scale alignment data that fine-tunes reward models <ref type="bibr" target="#b123">[123]</ref>. HelpSteer2 introduces a permissively licensed preference dataset to train reward models, demonstrating improved alignment with human preferences <ref type="bibr" target="#b124">[124]</ref>. Another approach focuses on prompting LLMs to function as reward functions, directly guiding model training through reward scores, as seen in Exploring with LLMs (ELLM) Rewards <ref type="bibr" target="#b125">[125]</ref>. Additional work, such as Reward Design with Language Models, emphasizes constructing reward mechanisms that align model outputs with desired outcomes by leveraging LLM capabilities <ref type="bibr" target="#b126">[126]</ref>. Self-supervised feedback mechanisms have also been explored; for instance, the Eureka framework introduces a novel approach to reward optimization through self-generated feedback loops <ref type="bibr" target="#b127">[127]</ref>. Self-rewarding systems, including Self-Refined LLMs <ref type="bibr" target="#b128">[128]</ref> and Self-Rewarding Language Models (SRLM) <ref type="bibr" target="#b129">[129]</ref>, enable iterative refinement of model outputs based on their own evaluations.</p><p>Despite its potential, RLAIF remains less widely adopted compared to RLHF. This discrepancy stems from challenges, such as difficulties in achieving alignment and the risk of propagating biases inherent in AI-generated feedback <ref type="bibr" target="#b112">[112]</ref>, <ref type="bibr" target="#b127">[127]</ref>. These challenges can create feedback loops that amplify existing biases, constraining model diversity and limiting its ability to generalize effectively <ref type="bibr" target="#b129">[129]</ref>. Moreover, the absence of human evaluators in RLAIF can result in a lack of nuance, leading to a narrower latent space influenced by the biases of the training AI <ref type="bibr" target="#b128">[128]</ref>.</p><p>Exploration techniques in RL involves seeking new information to improve future decisions, whereas exploitation capitalizes on current knowledge to maximize immediate rewards <ref type="bibr" target="#b130">[130]</ref>. In these algorithms, each action decision can be made stochastic via epsilon-greedy <ref type="bibr" target="#b131">[131]</ref> or entropy regularization <ref type="bibr" target="#b132">[132]</ref> to ensure diverse coverage of the environment, but excessive exploration can be inefficient. Traditional approaches, such as epsilon-greedy <ref type="bibr" target="#b133">[133]</ref> and Boltzmann exploration <ref type="bibr" target="#b134">[134]</ref>, introduce randomness without leveraging prior knowledge, slowing convergence. Recent methods, like, ExploRLLM <ref type="bibr" target="#b135">[135]</ref> presents a hierarchical reinforcement learning framework that combines the strengths of LLMs and affordance-based policies. In this approach, LLMs generate high-level plans to outline strategic goals, while affordancebased policies, which identify actionable possibilities within the environment, execute specific actions to achieve those goals. This method improves exploration efficiency by prioritizing high-value states and reducing reliance on frequent LLM invocations. Despite its effectiveness in structured environments, the approach faces challenges in adapting to dynamic and open-ended domains <ref type="bibr" target="#b136">[136]</ref>.</p><p>Soft RLLF <ref type="bibr" target="#b137">[137]</ref> integrates natural language as logical feedback to balance exploration and exploitation, enabling improved performance in reasoning tasks such as negation understanding and logical consistency in high-stakes applications. This is achieved by encoding logical consistency checks and negation handling into the learning process, utilizing feedback loops to iteratively refine the agent's decisionmaking. However, its effectiveness diminishes when tackling problems requiring broader adaptability and creativity, as it is optimized for structured reasoning <ref type="bibr" target="#b137">[137]</ref>. Another recent approach, LLM+Exp <ref type="bibr" target="#b138">[138]</ref>, employs a dual-LLM framework: one LLM analyzes action-reward trajectories to derive exploration strategies, while the other dynamically adjusts action probabilities to refine future decisions. Action-reward trajectories represent sequences of actions taken by an agent and the corresponding rewards, offering insights into the learning process. Action probabilities define the likelihood of selecting specific actions based on learned patterns and anticipated outcomes. While this adaptive approach excels in structured environments, it faces scalability issues and struggles to generalize effectively to unpredictable or unstructured tasks. Guided Pretraining RL <ref type="bibr" target="#b125">[125]</ref> leverages LLMs to enhance exploration by providing contextual background knowledge. This knowledge helps prioritize relevant actions, improving sample efficiency-allowing the agent to learn more effectively from fewer interactions. The method involves generating structured trajectories, which represent meaningful sequences of actions related to the task, using LLMs. These trajectories are used to pretrain the agent, providing a solid foundation before finetuning its policies in the target environment.</p><p>While this approach excels in providing structure and reducing exploration costs, it struggles with tasks that require broader adaptability and creative problem-solving. The reliance on predefined trajectories limits its ability to generalize to highly variable or unpredictable environments, where more flexible reasoning is needed.</p><p>b) Model Free Approaches: These methods can be grouped into three categories, DPO, IPO, and actor critical. Their discussion follows next.</p><p>Direct Preference Optimization (DPO) addresses the limitations of RLHF/PPO, which necessitates meticulous oversight and significant computational resources due to the initial phase to train a reward model using a preference dataset, followed by training an RL policy with the pre-trained reward model serving as the environment. DPO offers a simpler alternative by directly optimizing LLM parameters using preference data, bypassing the need for a reward model <ref type="bibr" target="#b139">[139]</ref>. DPO relies on a preference loss function trained on datasets of paired human preferences (e.g., "Response A is better than Response B"). Several extensions to DPO improve upon this baseline. For instance, DPOP <ref type="bibr" target="#b140">[140]</ref> (also termed DPO-positive) introduces a margin-based term to prevent rewarding both preferred and disfavored outputs concurrently, thereby improving performance on tasks with small edit distances. Specifically, the margin-based term in DPOP introduces a penalty for assigning high probabilities to both preferred and disfavored outputs, ensuring that the model distinctly favors the preferred response to improve task performance. Iterative DPO <ref type="bibr" target="#b129">[129]</ref> (also known as online DPO) mitigates distribution shifts by continually updating the policy on newly generated responses, an advantage over vanilla DPO, which can overfit to a narrower distribution. Meanwhile, β-DPO <ref type="bibr" target="#b141">[141]</ref> adaptively tunes the regularization term based on the data quality, making it more robust to noisy preferences. Stepwise DPO (sDPO) <ref type="bibr" target="#b142">[142]</ref> partitions the preference dataset to perform incremental updates, leveraging a stronger intermediate reference model at each phase.</p><p>DPO methods are particularly advantageous for structured problem-solving, like in creative writing or complex reasoning because they can directly incorporate human preferences and avoid undesired behavior without heavily relying on largescale reward modeling or complex RL training loops <ref type="bibr" target="#b139">[139]</ref>. However, a recurring drawback is their sensitivity to distribution shifts, e.g., when the model starts generating outof-domain responses, alignment performance can drop unless the reference model or preference data is iteratively updated <ref type="bibr" target="#b143">[143]</ref>. Moreover, purely relying on pairwise or setwise human judgments can still introduce label noise or ambiguity, especially for creative or unstructured tasks <ref type="bibr" target="#b144">[144]</ref>. Despite these limitations, DPO-based techniques are promising for balancing helpfulness and correctness in open-ended LLM outputs <ref type="bibr" target="#b145">[145]</ref>.</p><p>Identity Preference Optimization (IPO) <ref type="bibr" target="#b146">[146]</ref> was introduced to address the overfitting inherent in RLHF and DPO. Unlike traditional methods that transform pairwise preferences into pointwise rewards using the Bradley-Terry (BT) model <ref type="bibr" target="#b147">[147]</ref>, IPO directly optimizes preferences without relying on nonlinear transformations, which are known to exacerbate overfitting. The objective function of Identity Preference Optimization (IPO), as defined in eq. ( <ref type="formula" target="#formula_1">1</ref>), aims to directly optimize preference probabilities while mitigating overfitting issues inherent in methods like RLHF and DPO. The function maximizes the expected preference utility, represented by</p><formula xml:id="formula_0">E x [E y,y ′ Ψ(P θ (y &gt; y ′ ))] ,</formula><p>where Ψ(P θ (y &gt; y ′ )) captures the model's ability to predict and optimize preference probabilities for pairs of outputs (y and y ′ ). To prevent excessive deviation from a reference policy, the KL divergence term D KL (π||π ref ) imposes a regularization constraint, controlled by the coefficient β. By balancing preference optimization and regularization, this approach avoids transforming pairwise preferences into pointwise rewards, which can exacerbate overfitting, and directly aligns the model's behavior with human preferences while maintaining stability.</p><formula xml:id="formula_1">π * θ = max π E x [E y,y ′ Ψ(P θ (y &gt; y ′ )) -βD KL (π||π ref )] .<label>(1)</label></formula><p>To address the overfitting caused by the nonlinear transformation Ψ(x), IPO simplifies Ψ(x) to a linear function, Ψ(x) = x, and formulates a robust loss function, as defined in eq. ( <ref type="formula" target="#formula_2">2</ref>). This loss function, L IPO , directly optimizes the policy π θ by aligning it with human preferences while mitigating overfitting. The expectation is taken over pairs of outputs (y w , y l ), where y w represents the preferred (winning) output and y l the less preferred (losing) output. The terms -log π θ (yw) πref(yw) and -log π θ (y l ) πref(y l ) measure how well the current policy π θ aligns with the reference policy π ref , accounting for both preferred and less preferred outputs. A regularization term, 1 2β , balances the trade-off between optimizing preferences and maintaining adherence to the reference policy, ensuring model stability and reducing the risk of overfitting. By incorporating a squared penalty term, L IPO captures and penalizes deviations from ideal preference alignment, whether positive or negative. The simplified approach avoids the complexity and instability of nonlinear transformations, providing a stable and effective framework for aligning policies with human preferences. This makes IPO a robust and efficient alternative to traditional preference-based learning methods that rely on pointwise rewards or complex transformations.</p><formula xml:id="formula_2">L IPO = -E (yw,y l ) log π θ (y w ) π ref (y w ) -log π θ (y l ) π ref (y l ) - 1 2β 2 .<label>(2)</label></formula><p>This approach proves particularly robust in scenarios with deterministic or near-deterministic feedback, where existing methods often struggle due to unstable gradients <ref type="bibr" target="#b148">[148]</ref>. By leveraging a simpler optimization framework and incorporating strong regularization, IPO effectively mitigates overfitting and outperforms DPO in experimental settings <ref type="bibr" target="#b149">[149]</ref>.</p><p>However, IPO faces challenges due to its reliance on static preference distributions, which limits adaptability to dynamic or diverse scenarios. Additionally, its sensitivity to noise and dependence on high-quality data reduce robustness in complex, evolving environments <ref type="bibr" target="#b150">[150]</ref>.</p><p>Actor-critic methods, such as Advantage Actor-Critic (A2C) and Deep Deterministic Policy Gradient (DDPG), have been effectively adapted to optimize prompts for LLMs. Frameworks like Prompt Actor-Critic Editing (PACE) <ref type="bibr" target="#b151">[151]</ref> employ an iterative process where the actor (the LLM) generates a response a based on a prompt p and input X. This process is formalized as</p><formula xml:id="formula_3">a = f actor ([p; X], M ),</formula><p>where f actor represents the decision-making mechanism of the actor, [p; X] is the concatenated context consisting of the prompt p and the specific input X, and M is the LLM being optimized. The actor function processes the concatenated context to produce the response a, guided by the prompt p and the input X.</p><p>The critic, another LLM or evaluation mechanism, evaluates the relevance, coherence, and task-specific accuracy of the response against the objective Y . The critique is calculated as follows <ref type="bibr" target="#b151">[151]</ref>:</p><formula xml:id="formula_4">c = f critic ([p; X; a; Y ], M ),</formula><p>where f critic represents the evaluation function of the critic. The input [p; X; a; Y ] consists of the prompt p, the input X, the actor-generated response a, and the objective Y , which defines the desired or target output. The critic processes this concatenated input using the language model M to generate a critique c. This critique assesses how well the response a aligns with the objective Y , considering both the input X and prompt p. <ref type="bibr" target="#b152">[152]</ref> leverages KL-regularization to balance fidelity to the original prompt while allowing modifications that improve task-specific performance. By iterating on this actor-critic loop, PACE enhances prompt effectiveness and guides LLMs toward better alignment with task objectives.</p><p>Additionally, actor-critic methods assume well-structured feedback loops, which might be unreasonable for problems with sparse or noisy signals. Recent work addresses these challenges. <ref type="bibr" target="#b153">[153]</ref> explores open-ended learning in the context of unsupervised skill discovery, highlighting the need for more flexible reward functions in high-dimensional environments. HDFlow <ref type="bibr" target="#b154">[154]</ref> combines fast and slow thinking modes to enhance complex reasoning. <ref type="bibr" target="#b155">[155]</ref> introduces Direct Q-function Optimization (DQO), which formulates response generation as a Markov Decision Process (MDP), allowing each token generation to be treated as a state transition. Leveraging the soft actor-critic (SAC) framework, DQO directly parameterizes the Q-function within the language model, enabling it to learn effectively from offline data, including unbalanced or negative samples that helps improve multi-step reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMILARITIES OF LLMS AND TRADITIONAL AUTOMATED IMPLEMENTATION GENERATION METHODS AND RELATED RESEARCH NEEDS</head><p>A broad analogy can be identified between using Genetic Algorithms (GAs) and LLMs for implementation creation.</p><p>1) Selection: GA selection chooses the fittest individuals to pass their genes to the next generation. In fine-tuning or training data selection, LLMs prioritize coherence and relevance when generating textS, similar to selecting relevant context for responses. Like choosing the best seeds from a harvest, LLMs select the most relevant words or sentences to continue a conversation [156]. 2) Crossover (Recombination): GA crossover combines the genomes of two parents to create a new individual. This is similar to blending knowledge from different domains during text generation. For example, merging insights from literature and science in a single response. Crossover is like an LLM writing poetry about quantum physics, e.g., combining Shakespearean elegance with scientific rigor [157]. 3) Mutation: GA mutation introduces random changes in a genome to explore new possibilities. Similar to the slight randomness added during sampling techniques like top-k or temperature settings, which allow LLMs to produce diverse responses. Mutation in GAs is like LLMs occasionally breaking patterns to say something unexpected or creative [158]. 4) Inversion: GA inversion reverses a segment of the genome to explore new configurations. This parallels rephrasing or reordering sentences during text generation while preserving the original meaning. Like flipping a playlist order for a new vibe, LLMs rephrase "The car is fast" into "A fast car it is" [159]. 5) Elitism: GA ensures the best solutions carry over unchanged to the next generation. Similar to checkpointing the best-performing weights during training or favoring high-confidence outputs in decoding strategies. Like archiving the best answers during an essay edit, LLMs retain their most confident responses for the final output [160]. 6) Replacement: GA decides how much of the old population to keep versus the new one. Similar to parameter updates during fine-tuning, where new knowledge replaces older information incrementally. Replacement is like LLMs balancing old facts while integrating new updates, ensuring a model doesn't "forget" but adapts to current knowledge [161]. 7) Fitness Evaluation: GA scores individuals based on quality to determine their survival. Similar to evaluating model outputs using metrics like BLEU, ROUGE, or user feedback in RLHF. Fitness evaluation is like an LLM receiving human feedback to improve its responses based on relevance, coherence, or creativity [162]. 8) Exploration vs. Exploitation: GA balances trying new possibilities (exploration) and refining known solutions (exploitation). Balancing randomness and coherence during response generation. Parameters like temperature encourage exploration, while context relevance drives exploitation. Just as genetic algorithms search for novel solutions, LLMs strike a balance between playful creativity and logical reasoning in ambiguous prompts <ref type="bibr" target="#b163">[163]</ref>. The next part discusses using Cognitive Architectures for implementation creation, possibly using the features of LLMs. Similar to <ref type="bibr" target="#b32">[32]</ref>, this report considers that devising an implementation for a problem specification utilizes the five strategies shown in Figure <ref type="figure" target="#fig_0">1</ref>. The problem solving process is a mixture of the five strategies.</p><p>Each strategy starts from a kernel, which is the invariant set of features used in the process. The problem solving process creates a solution cluster corresponding to the kernel features, e.g., each implementation in the cluster includes the features. Implementations are created through implementation elaboration by exploring a sequence of detailing alternatives. For example, the principle of the bubble sorting algorithm can be described as repeatedly comparing the adjacent values of an array and swapping them if they are in the wrong order until no more value swapping are needed. The kernel features include three features: (i) the values of an array, (ii) the swapping of adjacent values if they are in the wrong order, and (iii) the repetition of the process until no more swapping are needed. The corresponding cluster includes all implementations obtained by elaborating the three kernel features.</p><p>The five strategies are as follows <ref type="bibr" target="#b32">[32]</ref>:</p><p>• Strategy 1 describes the elaboration process in which each kernel is elaborated without changing the kernel.</p><p>A set of detailing alternatives can be used for each elaboration step to produce an implementation envelope. The envelopes are incrementally elaborated until the final implementation is created. • Strategy 2 represents the process, which in addition to the elaboration steps of Strategy 1 also uses elaboration re-sults corresponding to a different implementation cluster.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the using of features from Implementation cluster 1 (red arrow in the figure) to build the implementations of Implementation cluster 2. Hence, the subsequent solution include elaboration of all kernel features and the features adopted from another cluster. • Strategy 3 uses a kernel that combines kernel features from two different implementation clusters. The blue arrows in Figure <ref type="figure" target="#fig_0">1</ref> illustrates the combination. • Strategy 4 presents an elaboration process in which the selected detailing alternatives are excluded from the elaboration steps used for building other implementation clusters. It represents the excluded niche in Figure <ref type="figure" target="#fig_0">1</ref> (green arrow). • Strategy 5 creates a kernel bottom-up by identifying and generalizing the features of individual implementations. The individual implementations were produced through less-structured methods, like, for example, through experimental trial-and-error.</p><p>While the five strategies provide templates for the implementation elaboration process, automated implementation generation requires the following additional activities:</p><p>1) Divide and conquer: The activity partitions a problem into sub-problems and then provides ways to integrate the implementation for the sub-problems. Task decomposition methods in LLM prompting <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b74">[74]</ref> can produce certain decompositions, especially in situations for which the sb-problems are less coupled. However, design problems are often strongly coupled, so that even though there are specialized modules to implement a certain function, their operation and performance are tightly related. Decomposition requires not only a static problem partitioning based on the items in the prompts (i.e. words) but also the interpretation of a sub-problem within the context set-up by the interpretations of other sub-problems. LLM fine tuning through RL is likely infeasible due to the huge space of possible decompositions possible in real-life. A mechanism is also needed to track the analyzed decompositions, so that the information can be used to improve future decompositions. This capability is absent in current methods. 2) Kernel creation: The method creates kernels either by assembling the features likely to address the problem requirements and then elaborating them top-down or in a bottom-up process as detailed in Strategy 5. Separate kernels can be created for different sub-problems followed by integrating them into a single kernel and its elaboration or separately elaborating each kernel and integrating their implementations. Ideas on LLM selfreflection and focus on the main information <ref type="bibr" target="#b71">[71]</ref> can help identify the features to be included in a kernel. However, finding kernels, e.g., the invariant features present in all implementations pertaining to a cluster, remains mostly a manual process. Methods similar to RLHF <ref type="bibr" target="#b107">[107]</ref> can help retrieving similar features, but their scalability is likely low. Moreover, combining features from different kernels to generate a new kernel (Strategy 3) has not been studied by current LLM methods. The combination of features needs a way to predict the expected performance at a high level (possibly a qualitative evaluation), which can be offered to some degree by LLM, similar to the use of LLM to solve ambiguities <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b83">[83]</ref>. However, it is likely that the current methods are insufficient for this purpose. 3) Elaboration: Executing the five strategies requires devising additional methods for detailing alternatives, predicting the effectiveness of each alternative in the context of a partial implementation, assigning a priority (preference) to each alternative, and incorporating the alternative into the partial implementation. A possible approach is to use schema for elaboration, similar to RAG methods for LLMs <ref type="bibr" target="#b80">[80]</ref>, <ref type="bibr" target="#b81">[81]</ref>. Schema matching can benefit from LLMs to clarify certain ambiguities, such as in <ref type="bibr" target="#b82">[82]</ref>, <ref type="bibr" target="#b83">[83]</ref>. However, schema are static structures, useful in analogical reasoning, even though problem solving often requires performing new sequences of decisions beyond a static schema. 4) Implementation assessment: LLMs can be used for two kinds of performance assessments. Qualitative assessment, including comparing implementations, such as pairs of circuit designs, can be obtained by prompting traditional LLMs. CoT prompting can be used to obtain performance assessment at a finer granularity. RLHF can fine tune assessment by adding human feedback about the quality of the implementations <ref type="bibr" target="#b112">[112]</ref>. Moreover, self-critique methods could be used to improve the correctness and completeness of the LLM responses, like self-consistency and cross-referencing methods in VE <ref type="bibr" target="#b104">[104]</ref>. A second approach uses datasets of characterized implementations to train an LLM, similar to exploration techniques in RL <ref type="bibr" target="#b133">[133]</ref>, <ref type="bibr" target="#b134">[134]</ref>. Then, the generalization capacity of LLMs are used to quantitatively predict the performance parameters of a new implementation. Nevertheless, the two approaches do not scale beyond the samples used in training an LLM, including situations in which a new implementation uses a nonlinear combination of the features of different implementation. There is no mechanism similar to settingup precise physical models of an implementation, so that the models can be solved to produce quantitative performance assessment, like in traditional automated implementation creation methods. 5) Memory and learning: Similar to using long-term memory for knowledge retrieval in RAG, memory systems are needed to for learning to store associations, like kernel features, their most relevant implementations fragments, and their performance values or between high-level features and their detailed elaborations, the causal relationships of main features and performance attributes, and elaboration sequences that produced highquality implementations. Similar to schema-based re-trieval, memory cueing must solve semantic ambiguities. 6) Adaptive process: It includes the sequence of automated activities performed to create an implementation. It requires devising new means to predict the expected outcomes of the available activities, selecting and adapting an activity to the current context, understanding the degree to which the sequence advances towards creating an implementation, and learning new knowledge available during the process. Also, when addressing collaboration between humans and LLMs to tackle unexpected challenges, such as handling zero-day attacks, the process necessitates reasoning, understanding of prior instructions, and intuitive decision-making within the context of new parameters and constraints. To automate this process, exploring reasoning techniques, including deductive reasoning, inductive reasoning, analogical reasoning, common sense reasoning, tree-of-thoughts, multiple chains of thought, causal reasoning, heuristic reasoning, and symbolic reasoning, is required. Among these, the primary human thought process often involves mapping the current problem to a previously encountered one or identifying similarities with analogous problems, like in analogical reasoning. Consequently, an effective approach to problem modeling could involve neuro-symbolic representations that allow LLMs to dynamically learn and adapt in real-time. Techniques such as grokking <ref type="bibr" target="#b164">[164]</ref>, which enable models to discover relationships and patterns through iterative refinement, and masked LLMs are promising methods to achieve this goal. These approaches empower the model to derive connections on the fly, effectively merging learned representations with reasoning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>Recent advances in Large Language Models (LLMs) offer the opportunity to extend automated implementation generation techniques beyond the current methods that require algorithmic specifications as input and can use only statically domain knowledge. LLMs can process multi-modal descriptions, including ideas communicated in natural language and using images and with certain degrees of specification completeness, unknowns, and ambiguity. LLMs learn a broad range of associations and for diverse contexts. These new capabilities might offer intriguing paths beyond traditional implementation generation, such as support problem framing and exploration of possible solution approaches, improved implementation assessment across abstraction levels by comprehensive comparison to similar, externally available implementations, collective feedback and preferences, and enhanced elaboration by incorporating continuously updated domain knowledge. These features are critical in solving open-ended problem, currently hard to address with existing methods. Summarizing the state-of-the-art on LLMs and their related improvements is a first step towards devising nocel LLM-based methods for implementation generation. This report offers a comprehensive overview of existing LLM techniques and studied the degree to which they can model the activities needed for implementation generation for open-ended problem solving. The overview presents LLM enhancements, like prompting, Reinforcement Learning (RL) and Retrieval-Augmented Generation (RAG). Then the report discusses the possibility of using LLMs to realize problem solving activities that are not available in traditional automated implementation generation methods. New research requirements are also presented, e.g., support for problem framing, creating an implementation approach, effective elaboration control, robust qualitative and quantitative assessment across abstraction levels, knowledge memorizing during learning, and managing the problem solving process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Five strategies for automated implementation creation</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward an understanding of macrocognition in teams: Pre-dicting processes in complex collaborative contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith-Jentsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Warner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The process of solving complex problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Problem Solving</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards a generalized competency model of collaborative problem solving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yonehiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">103672</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Problem-solving phase transitions during team collaboration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiltshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Butner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="167" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cognitive processes in well-defined and ill-defined problem solving</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Dunkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bendixen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="523" to="538" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of precedents in increasing creativity during iterative design of electronic embedded systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Umbarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Design Studies</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="326" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling semantic knowledge structures for creative problem solving: Studies on expressing concepts, categories, associations, goals and context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Umbarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledgebased Systems</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="34" to="50" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reuse, parameterized reuse, and hierarchical reuse of substructures in evolving electrical circuits using genetic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<idno>ICES96</idno>
	</analytic>
	<monogr>
		<title level="m">Evolvable Systems: From Biology to Hardware: First International Conference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Japan</forename><surname>Tsukuba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">October 7-8, 1996 Proceedings 1. Springer, 1997</date>
			<biblScope unit="page" from="312" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Problem frame patterns: an exploration of patterns in the problem space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wirfs-Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Pattern Languages of Programs</title>
		<meeting>Conference on Pattern Languages of Programs</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-level synthesis of delta-sigma modulators optimized for complexity, sensitivity and power consumption</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CADICS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="607" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Systematic methodology for designing reconfigurable delta sigma modulator topologies for multimode communication systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CADICS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="480" to="496" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improvement of skills for solving-illdefined problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weitzenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychologist</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Assessment of student problem-solving on ill-defined tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Maguire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alberta Journal of Educational Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A novel agent-based, evolutionary model for expressing the dynamics of creative open-problem solving in small groups</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doboli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2094" to="2127" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stanley</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9940" to="9951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The SOAR Cognitive Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A librarybased approach to analog synthesis from vhdl-ams specifications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dhanwada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nunez-Aldana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Design Automation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="271" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">High-Level Synthesis Blue Book</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fingeroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Xlibris Us</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mcconaghy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gielen</surname></persName>
		</author>
		<title level="m">Variation-aware Analog Structural Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Behavioral modeling for high-level synthesis of analog and mixed-signal systems from vhdl-ams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CADICS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploration-based high-level synthesis of linear analog systems operating at low/medium frequencies</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CADICS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">When concepts combine</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wisniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="183" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Darwin: Cmos opamp synthesis by means of genetic algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kruiskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leenaerts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Design Automation Conference</title>
		<meeting>Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="433" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Research directions in agent communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bentahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colombetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dignum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fornara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yolum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Agent-based modeling: methods and techniques for simulating human systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bonabeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7280</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborating with style: Using an agent-based model to simulate cognitive style diversity in problem solving teams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jablokow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</title>
		<meeting>ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Act: A simple theory of complex cognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="355" to="365" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Laird</surname></persName>
		</author>
		<title level="m">Compilers: Principles, Techniques, and Tools</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The sigma cognitive architecture and system: towards functionally elegant grand unification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenbloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Volkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial General Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An overview of the epic architecture for cognition and performance with application to human-computer interaction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kieras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="438" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://www.cogsci.rpi.edu/rsun/sun.tutorial.pdf" />
		<title level="m">A tutorial on clarion 5.0. Cognitive Science Department, Rensselaer Polytechnic</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Innova: A cognitive architecture for computational innovation through robust divergence and its application for analog circuit design</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doboli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doboli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on CADICS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1943" to="1956" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">e253</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fast lexically constrained decoding with dynamic beam allocation for neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06609</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving large language models for clinical named entity recognition via prompt engineering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Keloth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="page">259</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Medical transcriptions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boyle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2024" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="https://vaers.hhs.gov/data/datasets.html" />
		<title level="m">Centers for Disease Control and Prevention (CDC) and U.S. Food and Drug Administration (FDA)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2036" />
		</imprint>
	</monogr>
	<note>Vaccine adverse event reporting system (vaers)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24" to="824" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Complexitybased prompting for multi-step reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Thread of thought unraveling chaotic contexts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08734</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13269</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Chain of code: Reasoning with a language model-augmented code emulator</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ichter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.04474</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Enhancing zero-shot chain-of-thought reasoning in large language models through logic</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.13339</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Chain-of-event prompting for multidocument summarization by large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Web Information Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>no. ahead-of-print</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Chain-of-table: Evolving tables in the reasoning chain for table understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04398</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Contrastive chain-of-thought prompting</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09277</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Federated prompting and chain-ofthought reasoning for improving llms answering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Science, Engineering and Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Maieutic prompting: Logically consistent reasoning with recursive explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11822</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04091</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12588</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Chainof-symbol prompting elicits planning in large langauge models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10276</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Structured chain-of-thought prompting for code generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Reasoning implicit sentiment with chain-of-thought prompting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11255</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Towards expertlevel medical question answering with large language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09617</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03493</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Active prompting with chain-of-thought for large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12246</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Mathprompter: Mathematical reasoning using large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05398</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01714</idno>
		<title level="m">Large language models as analogical reasoners</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Synthetic prompting: Generating chain-of-thought demonstrations for large language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">775</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">System 2 attention (is something you might need too)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.11829</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Metacognitive prompting improves understanding in large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.05342</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10625</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Decomposed prompting: A modular approach for solving complex tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02406</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pal: Program-aided language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">799</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Binding language models in symbolic languages</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nadkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02875</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13808</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Retrievalaugmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Reducing hallucination in structured outputs via retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Béchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Ayala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.08189</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Sbi-rag: Enhancing math word problem solving for students through schema-based instruction and retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13293</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Kragen: a knowledge graph-enhanced rag framework for biomedical problem solving using large language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Gram: Generative retrieval augmented matching of data schemas in the context of data security</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5476" to="5486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Tablerag: Million-token table understanding with language models</title>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miculicich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.04739</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19215</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Self-rag: Selfreflective retrieval augmented generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.01428</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Simrag: Self-improving retrieval-augmented generation for adapting large language models to specialized domains</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17952</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.findings-emnlp.71" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2024</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024-11">Nov. 2024</date>
			<biblScope unit="page" from="1321" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mattapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08223</idno>
		<title level="m">Speculative rag: Enhancing retrieval augmented generation through drafting</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Hipporag: Neurobiologically inspired long-term memory for large language models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14831</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Augmenting language models with long-term memory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Enhancing long-term memory using hierarchical aggregate tree for retrieval augmented generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aadhithya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">2406</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.05591</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.00072</idno>
		<title level="m">Pistis-rag: Enhancing retrieval-augmented generation with human feedback</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Similarity is not all you need: Endowing retrieval augmented generation with multi layered thoughts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19893</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.12881</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Multihop-rag: Benchmarking retrievalaugmented generation for multi-hop queries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15391</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Retrievalaugmented multi-modal chain-of-thoughts reasoning for large language models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.01714</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Hop, union, generate: Explainable multi-hop reasoning without rationale supervision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14237</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Multimodal chain-of-thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00923</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Can gpt improve the state of prior authorization via guideline based automated question answering?&quot; in AI for Health Equity and Fairness: Leveraging AI to Address Social Determinants of Health</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vatsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tafreshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="147" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Can gpt redefine medical understanding? evaluating gpt on biomedical machine reading comprehension</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vatsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18682</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Chain-of-verification reduces hallucination in large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11495</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Verify-and-edit: A knowledge-enhanced chain-of-thought framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03268</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Fine-tuning large vision-language models as decision-making agents via reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10292</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27" to="730" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05199</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06080</idno>
		<title level="m">Secrets of rlhf in large language models part ii: Reward modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">trlx: A framework for large scale reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Havrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhuravinskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castricato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8578" to="8595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Ultrafeedback: Boosting language models with high-quality feedback</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Nash learning from human feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calandriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Michi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00886</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.18451</idno>
		<title level="m">Skywork-reward: Bag of tricks for reward modeling in llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Camels in a changing climate: Enhancing lm adaptation with tulu 2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.10702</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01045</idno>
		<title level="m">Toolaugmented reward modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Scheid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ménard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17055</idno>
		<title level="m">Optimal design for reward modeling in rlhf</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Scaling laws for reward model overoptimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="835" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">A survey of reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bengs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14925</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Rlaif: Scaling reinforcement learning from human feedback with ai feedback</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phatale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Carbune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Hrlaif: Improvements in helpfulness and harmlessness in open-domain reinforcement learning from ai feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08309</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poovendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08464</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Helpsteer2-preference: Complementing ratings with preferences</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bukharin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Egert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.01257</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Guiding pretraining in reinforcement learning with large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="8657" to="8677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Reward design with language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bullard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Eureka: Humanlevel reward design via coding large language models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12931</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.06687</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Self-rewarding language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10020</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Exploration versus exploitation in reinforcement learning: A stochastic control approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zariphopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01552</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Guarantees for epsilon-greedy reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sekhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4666" to="4689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01783</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Adaptive ε-greedy exploration in reinforcement learning based on value differences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tokic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Boltzmann exploration done right</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Explorllm: Guiding exploration in reinforcement learning with large language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luijkx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ajanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09583</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Epo: Hierarchical llm agents with environment preference optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.16090</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Balancing exploration and exploitation in llm using soft rllf for enhanced negation understanding</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01185</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Empower large language model to perform better on industrial domain-specific question answering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajmohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11541</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.13228</idno>
		<title level="m">Smaug: Fixing failure modes of preference optimisation with dpopositive</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08639</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19270</idno>
		<title level="m">sdpo: Don&apos;t use your data all at once</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Weighted-reward preference optimization for implicit model fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03187</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Provably robust dpo: Aligning language models with noisy feedback</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.00409</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">A comprehensive survey of datasets, theories, variants, and applications in direct preference optimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15595</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">A general theoretical paradigm to understand learning from human preferences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calandriello</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Ton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.04991</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Self-play preference optimization for language model alignment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.00675</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Preference optimization with multi-sample comparisons</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12138</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Robust preference optimization through reward model distillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.19316</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Pace: Improving prompt with actor-critic editing for large language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10088</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b152">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Open-ended reinforcement learning with neural reward functions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mujika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2465" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Hdflow: Enhancing llm complex problemsolving with hybrid thinking and dynamic workflows</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.17433</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Enhancing multi-step reasoning abilities of language models through direct qfunction optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.09302</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Llms as debate partners: Utilizing genetic algorithms and adversarial search for adaptive arguments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.06229</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-H. Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10034</idno>
		<title level="m">Evolutionary computation in the era of large language model: Survey and roadmap</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b158">
	<monogr>
		<title level="m" type="main">Controlling the mutation in large language models for the efficient evolution of algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Kononova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bäck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Stein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03250</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Connecting large language models with evolutionary algorithms yields powerful prompt optimizers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08532</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">Large language model-based evolutionary optimizer: Reasoning with elitism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brahmachary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koneripalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sagotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jagtap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.02054</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">A match made in consistency heaven: when large language models meet evolutionary algorithms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10510</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Large language models as surrogate models in evolutionary algorithms: A preliminary study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10675</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Wese: Weak exploration to strong exploitation for llm agents</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07456</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Towards understanding grokking: An effective theory of representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">663</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
