<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ChatCoder: Chat-based Refine Requirement Improves LLMs&apos; Code Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-01">1 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zejun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Lab of HCST (PKU)</orgName>
								<orgName type="institution">MOE; SCS Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Lab of HCST (PKU)</orgName>
								<orgName type="institution">MOE; SCS Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Key Lab of HCST (PKU)</orgName>
								<orgName type="institution">MOE; SCS Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Lab of HCST (PKU)</orgName>
								<orgName type="institution">MOE; SCS Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ChatCoder: Chat-based Refine Requirement Improves LLMs&apos; Code Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-01">1 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">DBDC62925CC090F32F291D30D6B93808</idno>
					<idno type="arXiv">arXiv:2311.00272v1[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computer systems organization → Embedded systems</term>
					<term>Redundancy</term>
					<term>Robotics</term>
					<term>• Networks → Network reliability code generation, refine requirement, large language model, human interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>requirements. Not only do the well-known close-source LLMs for business show the ability to generate code with high quality (e.g., GPT-4 <ref type="bibr" target="#b12">[13]</ref> pass 67% of the tests in HumanEval <ref type="bibr" target="#b3">[4]</ref>), but also the recent open-source LLMs have reported their good results on code generation (e.g., <ref type="bibr">Gunasekar et al.</ref> have designed an open-source LLM called phi-1 <ref type="bibr" target="#b5">[6]</ref> which has passed 50.6% of the tests in Hu-manEval). Thus, applying LLMs to assist human programmers in their everyday coding tasks is promising.</p><p>However, human's poor requirement expressions in natural language restrict LLMs' ability to generate better programs. Human expressions can be vague, incomplete, and ambiguous. These lowquality requirement expressions mislead large language models to generate the wrong code. We raise an example from the sanitized-MBPP dataset <ref type="bibr" target="#b1">[2]</ref> in Figure <ref type="figure" target="#fig_0">1</ref> to illustrate the issue, which is thought unambiguous by the authors. Suppose that we want gpt-3.5-turbo to write a function to find the largest negative number from the given list. Based on the original requirement, the large language model generates a program which can extract the negative numbers with the largest actual value correctly. However, the authors of sanitized-MBPP think that the 'largest negative number' means the largest absolute value. Thus the large language model generates the wrong code due to the bad expression 'largest'.</p><p>The problem can be solved via requirements refinement. Requirements refinement is the process of revealing the underlying dependencies and hidden structures <ref type="bibr" target="#b10">[11]</ref>. With more details revealed, incomplete information will be filled up during requirement refinements, and the ambiguities will be clarified. In our example illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, we can simply reveal the hidden structure of 'the largest' as 'the largest absolute value' to the large language model. With the refined requirement, the large language model generated the code that fulfilled the MBPP's authors' expectations.</p><p>Requirements refinement asks for the collaboration of human users and large language models. In the context of requirement engineering, requirements refinement is performed by a series of interactions between the software supplier (the coder) and the software customer (the user). The software supplier analyzes the initial expression of the customer's requirements and raises the points of refinement. The software customers need to respond to the points based on which the supplier can finish a round of refinement. Neither the software customer nor the software supplier is qualified to perform requirements refinement by themselves. According to IEEE Std 830-1998 <ref type="bibr" target="#b0">[1]</ref>, customers usually do not understand the software design and development process well enough to write a usable one. Suppliers usually do not understand the customer's problem and field of endeavour well enough to specify requirements for a satisfactory system. In the scenario of asking LLMs to generate programs to fulfil human requirements, the human user of LLM is the customer, and the LLM itself is the supplier. To let the supplier LLM produce code that better fulfils the user's requirements via requirements refinement, we need to develop a method for humans and LLMs to collaborate.</p><p>We propose ChatCoder, a new method for code generation with large language models through requirements refinement via chat. It is a concise dialogue framework that assists LLMs and humans' collaboration on requirements refinement via chatting. The key problem is how to chat with the large language model. Our solution, ChatCoder, has a novel chatting schema designed inspired by IEEE Recommended Practice for Software Requirements Specifications(IEEE SRS) <ref type="bibr" target="#b0">[1]</ref>. This paper mainly discusses method-level code generation. Referring to the contents of software requirement specifications raised by IEEE SRS covering every corner of a software's life cycle, we raise six angles covering the development of a method and provide the angles to large language models to analyze the requirement specifications. Then the large language models lead the human user to refine the requirements based on its analysis by adding information, correcting mistakes, giving examples, and answering questions. The whole process is in the chat form. In this paper, we test ChatCoder on the HumanEval dataset and Sanitized-MBPP dataset, and the test results show that the refined requirements with ChatCoder improve the LLM's code generation performances by a large margin, at an average of the percentage of 10. The results show that ChatCoder's refinement is effective and efficient.</p><p>Our contribution is summarized as follows:</p><p>• We find and raise the problem that human's poor requirement expressions in natural language limit LLMs' ability to generate better programs. • We point out the necessity to ask for the collaboration of humans and large language models. • We raise ChatCoder, a dialogue framework effectively assisting human and LLM's collaboration on requirements refinement for better code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Large Language Model for Code Generation</head><p>Large language models are currently pre-trained Transformer-based language models with at least tens of billions of parameters. The first well-known large language model is GPT-3 <ref type="bibr" target="#b2">[3]</ref> proposed by OpenAI, and GPT-3 presented its extraordinary code generation ability. Following GPT-3, a series of business-oriented close-source large language models have been proposed, e.g. GPT-3.5 and GPT-4, whose code generation abilities improve day by day. Besides, several open-source large language models for code-related tasks have been published, e.g., StarCoder <ref type="bibr" target="#b8">[9]</ref>, CodeT5+ <ref type="bibr" target="#b13">[14]</ref>. WizardCoder <ref type="bibr" target="#b11">[12]</ref> etc. They have been proven to have comparable code generation capabilities with the close-source large language models. The current way of applying large language models in code generation is via prompting techniques. A prompt is a formatted text wrapping the user's original instruction for the large language model. Then the prompt is sent to the large language model as input to get the large language model's response. Given the user's description of a programming task, properly designed prompts will make it easier for large language models to generate the correct corresponding code. For example, Li et al. <ref type="bibr" target="#b7">[8]</ref>propose that providing examples closely related to the programming tasks can help large language models to generate better code. Jiang et al. <ref type="bibr" target="#b6">[7]</ref>propose that appending the text that encourages the LLMs to decompose the programming task helps large language models solve complicated problems. In this paper, our proposed method can be categorized as prompt engineering as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Requirements Refinement</head><p>Requirements refinement is both a process of deriving specifications and a necessary means towards preparing architecture designs. During requirements refinement, the design of requirement specifications should reveal its underlying dependencies and hidden structure. Requirements refinement is the start from requirements to implementation design. It is important because many users in practice do not understand what functions they want precisely at the beginning of a software project <ref type="bibr" target="#b9">[10]</ref>. With requirements refinement, the users and software suppliers can agree on what function the user truly needs.</p><p>Previous studies of requirements refinement focus on providing a formal method for the software supplier to analyse and refine the software customer's requirements. Liu <ref type="bibr" target="#b10">[11]</ref> raises a hierarchical framework from the business level to the component level to refactor the customer's requirements. Darimont and Lamsweerde <ref type="bibr" target="#b4">[5]</ref> propose formal refinement patterns for goal-driven requirements elaboration via KAOS. Liu <ref type="bibr" target="#b9">[10]</ref> proposes to use the SOFL language to describe the refinement process and raises the model of successive refinements in which the requirements refinement is a process from coarse to fine with a loop back. Jong et al. propose to use nondeterminism and parameterised specifications to support step-wise specifications and have the specifications written and analysed using the language and proof checker of PVS.</p><p>Requirements refinement requires collaboration with both the software provider and the software user. On the one hand, requirement analysing methods, e.g., the formal approaches mentioned above, can not achieve complete and accurate themselves. Domain knowledge and the customer's personal quality (e.g., the ability to express oneself clearly) are essential in requirements refinement. On the other hand, the users can not refine their requirements by themselves since they may not understand the software design enough, leading to the phenomenon that their proposed requirements may not fulfil what they actually want. So what is important is to find a friendly way of interaction: the software provider find the points in the user's requirement which need refinement, then the provider asks for the software user's comments on the refinement in the way that the software user can understand and give proper answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Overall Structure of ChatCoder</head><p>ChatCoder is code generation method through requirements refinement via a dialogue framework designed for the communication</p><p>#MBPP/443 Write a python function to find the largest negative number from the given list. def largest_neg(list1): if len(list1) == 0: return None max_neg = None for num in list1: if num &lt; 0: if max_neg is None or num &gt; max_neg: max_neg = num return max_neg # User's Real Intention &gt;&gt;&gt; largest_neg([1,2,3,-4,-6]) -&gt; -6 &gt;&gt;&gt; largest_neg([1,2,3,-8,-9]) -&gt; -9 #MBPP/443 Refined Requirements Write a python function to find the largest negative number from the given list. largest negative number: The largest negative number refers to the negative number with the largest absolute value in the given list. Refine the concept of "largest" def largest_neg(list1): largest = None for num in list1: if num &lt; 0: if largest is None or abs(num) &gt; abs(largest): largest = num return largest Orignal Requirement Refined Requirement largest: largest actual value? largest abstract value? Code generated before refinement Code generated after refinement between a large language model and its user to refine the requirements. Within the framework, a large language model can analyse the arguments to refine the user's original requirement expression, then return the arguments back to the users in a way that human users can easily understand and give responses.</p><p>The overall structure of ChatCoder is a two-round dialogue illustrated in Fig 2 . The first round is Paraphrase and Extend. Since the human user's expression of requirements can be vague, incomplete and ambitious, ChatCoder uses prompts to ask the large language model to paraphrase the user's original requirements from several angles that complete requirement specifications must be clear. For the missing or ambitious arguments which require refinement, ChatCoder asks the large language model to extend them with its assumptions gained from its training data. Human users need to review the refined specifications and correct the mistakes within. The second round is Going-deep and Loop-back. In this round, Chat-Coder requires large language models to ask the human users about their confusion about the refined specifications in Paraphrase and Extend for losing information and further refinement. Human users need to answer the questions and loop back to correct the refined specifications when the users find the large language model's questions are raised based on wrong requirement specifications. After the two rounds of refinement, the refined requirement is obtained and then sent to large language models to get the user's desired programs.</p><p>In the following paragraphs, we will explain the design of each round in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Paraphrase and Extend</head><p>The large language model is asked to paraphrase the user's initial requirement expression in this round. The paraphrase is performed by extending the initial requirement on the preset angles extracted from existing research of requirement engineering. Chat-Coder wraps the instruction to paraphrase the user's requirement and the angles used for extension in a prompt to order the large language model to perform the paraphrase. Then the prompt is sent to the large language model for its response. The format of the prompt is presented in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>The angles selected are based on the environment of applying ChatCoder. Since this paper mainly discusses generating methodlevel programs, the angles for ChatCoder are all about method-level requirements refinement. In particular, the ChatCoder in this paper has five angles for the Paraphrase and Extend round, inspired by IEEE Recommended Practice for Software Requirements Specifications:</p><p>• Key Concepts This angle asks the large language model to extract and explain the key concepts involved in the user's requirements, including objects and actions. By extending this angle, the user and the large language model can align their understanding of the key concepts, setting a firm basis for further discussion. • Method Purpose This angle asks the large language model to paraphrase the function provided by the method to be implemented. In this angle, the large language model will describe the transformation for the input and the changes of the running states in a more detailed way. The LLM's description reflects its ongoing implementation based on the LLM's understanding of the initial requirement expression</p><p>initial requirement specifications Key Concept Method Purpose Input Requirements Output Requirements Edge Cases Exceptions and Errors Generate by Paraphase &amp; Extend Key Concepts Input Output Eage Cases going deep going deep going deep going deep going deep going deep Generate Review and Edit Review and Edit Loop-back to Correct Mistakes Large language model Human user Human user Phase 1: Paraphrase and Extend Phase 2: Going-deep and Loop-back Refined Requierment Specifications • Input Requirements This angle asks the large language model to extend the requirements for the method's inputs, including the parameters' types, actual meaning, boundaries and properties. Explaining the meanings is another chance for the LLM and the user to align their understanding of the requirements. The type, boundary and property are easily missing but play important roles in the design of the algorithm.</p><p>• Output Requirements This angle asks the large language model to extend the requirements for the method outputs, including the types, the meaning and the format. Explaining the meanings is another chance for the LLM and the user to align their understanding of the requirements. While a method may serve other methods, its returning type and format matter but can be missing, e.g., the decimals to reserve for a floating-point output number. • Edge Cases This angle asks the large language model to extend possible edge cases and solutions. Since a method can run in complicated outer environments, the input and the global variable states may not fulfil the method's running preconditions. So properly handling edge cases is necessary for a robust method implementation but can be easily ignored by software customers. • Exceptions and Errors This angle asks the large language model to extend the solutions for possible exceptions and errors during the method's execution. Like edge cases, handling exceptions and errors are necessary but can be easily missed by the users because of their unprofessional software design. The large language model must analyse, raise solutions and wait for the users' review.</p><p>The human user is supposed to review the large language model's response to the instructions for refining requirements. For the key concepts and method purpose, the human user is requested to correct the mistakes made by the large language model. For the input and output requirements, the human user is requested to correct the mistakes for the meanings and review whether the large language model's inference on the input and output formats meets the real needs. For the edge cases, exceptions and errors, the users are requested to review whether they can occur and whether the large language model's proposed solution is satisfactory. If the human user encounters an expression that is difficult to understand and rewrite, the user can directly delete the expression.</p><p>Our design of Paraphrase and Extend is an effective and efficient way for the large language model and the human user to communicate for requirements refinement. First, our instructions for the large language model are in natural language. Compared with the formal language designed for human coders to analyse the requirements for refinement, large language models are more familiar with natural languages since most of their training data is in natural languages. Second, the angles mentioned in the instructions cover many reasons humans and AI programmers make mistakes. Refining the requirements from these angles helps reduce programming mistakes. Third, it is easy for human users to read, understand and modify the refined requirements, thanks to the LLM's string expression power. Most of the refined specifications are generated by the large language model. All the work left for human users is only to make modifications, which is a small workload compared to generating the whole refined specifications, not to say that human users may not know what to write for the refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Going-deep and Loop-back</head><p>In this round, the large language model is asked to going-deep: to further refine the requirements based on the specifications obtained in Paraphrase and Extend; the human user is requested to loop back: check for possible errors in the reviews and the errors corrected.</p><p>Going-deep The large language model is asked to raise questions in the angles based on the existing specifications obtained in Paraphrase and Extend. The instruction for the large language model is also wrapped in a prompt, presented in Figure <ref type="figure" target="#fig_2">3</ref>. We design Goingdeep to refine the requirements further because the large language model is a black box, and it is hard to say we have used up its potential to refine requirements through Paraphrase and Extend. In this round, we let the large language model ask questions in a free form for what confused the most about the requirements, then give possible answers based on its observations or assumptions. Suppose the large language model keeps raising questions which are answered in the specifications. In that case, we regard the specifications are complete enough for the large language model to generate corresponding programs.</p><p>Loop-back The user is asked to review the questions and answers generated by the large language model in Going-deep and correct the wrong answers for further refinement. The user may find that the large language model raises wrong questions, e.g., it asks whether the output list should be sorted. However, the desired output is an integer. In this scenario, the user must "Loop-back": review the specifications in Paraphrase and Extend to look for the wrong expressions leading to the wrong questions, then have them corrected. Loop-back is important because it is difficult to guarantee that the users never make mistakes.</p><p>After Going-deep and Loop-back, the user will have the updated specifications from Paraphrase and Extend and the further refined specifications from Going-deep. Then these refined specifications are appended to the original expression of requirements and sent to the large language model to get the large language model's generated programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experiment Settings</head><p>Datasets: We select three datasets for our experiments:</p><p>• Sanitized-MBPP A well-known and widely-used dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Its test set contains 257 programming questions which standalone Python methods can solve. We choose this dataset for the following reasons. First, its task descriptions are short, which means they are more likely to be incomplete and ambiguous than longer descriptions, so we can find out whether ChatCoder can let LLMs analyze the points within each description for refinement. Second, the authors of Sanitized-MBPP claimed that these task descriptions are manually checked for disambiguation. It provided a chance to validate whether ChatCoder can make LLM analyze the task description from a different angle from the dataset's authors. • HumanEval A well-known and widely-used dataset <ref type="bibr" target="#b3">[4]</ref>. It has 164 programming questions to be solved by Python programs.</p><p>We chose this dataset because its task descriptions are longer and more complicated than those of Sanitized-MBPP, from which we want to evaluate whether ChatCoder can still find the points for refinement and keep improving LLMs' code generation performances.</p><p>Baselines: We select four baselines for our experiments:</p><p>• gpt-3.5-turbo. The latest version of the gpt-3.5-turbo family, a family of closed-source large language models published by OpenAI. It is powerful enough and easy to access, leaving the time long enough for anyone to reproduce our experiments before it is deprecated. • gpt-4. The newest generation of the closed-source large language model, published by OpenAI, performs extraordinarily well on code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation Configurations</head><p>For HumanEval, we perform greedy generation, which means the generation is zero-shot, and the sampling is performed only once with a temperature of 0. For Sanitized-MBPP, we perform 3-shot generation. For each task, we sample 20 programs with top_p=0.2 when evaluating models for gpt-3.5-turbo. As for GPT-4, because there is a calling rate limit and the calling fee is high, it is difficult and expensive to sample 20 programs for a programming task. So we sample one program for a programming task with temperature 0 like HumanEval. The version of GPT-4 is gpt-4-0613. The version of gpt-3.5-turbo is gpt-3.5-turbo-0613. For a fair comparison, we rerun all the baselines with the same prompts and our generation configuration rather than copy the results from the original papers.</p><p>Metrics We report the test pass rate <ref type="bibr" target="#b3">[4]</ref>. For HumanEval and Sanitized-MBPP on GPT-4, we report pass@1. We report pass@1, pass@2, pass@5, and pass@10 for the other settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Research Questions</head><p>To evaluate our proposed ChatCoder, we raise and investigate the following research questions:</p><p>• 1) How does ChatCoder perform compared with existing code generation models? • 2) Is ChatCoder an efficient method for LLM and human users to communicate for requirement refinement? • 3) How much improvement is brought by human involvement in ChatCoder?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RQ1: Code Generation Performances</head><p>RQ1 is to evaluate ChatCoder's overall code generation performances compared with the baselines. Our results are reported in Table <ref type="table">1</ref>. When investigating RQ1, we try ChatCoder with GPT-4 and gpt-3.5-turbo-0613. We performed Paraphrase and Extend and Going-deep and Loop-back on gpt-3.5-turbo-0613 and obtained the</p><p>Prompt for the LLM: Paraphase and Extend Example Output from Large Language Model Here is a programming task: {task} Please help refine the requirement specification following the steps: 1. List and clarify the definitions of the key concepts 2. Explain the purpose of the method to be implemented with your understandings in details 3. Analyze each parameter's type, meanings, format, properties and restrictions. 4. Analyze the return values' types, meanings, format, properties and restrictions. 5. Raise possible edge cases and solutions 5. Raise possible exceptions and errors and the solutions. e.g. Here is a programming task: def filter_by_prefix(strings: List[str], prefix: str) -&gt; List[str]: """ Filter an input list of strings only for ones that start with a given prefix. &gt;&gt;&gt; filter_by_prefix([], 'a') [] &gt;&gt;&gt; filter_by_prefix(['abc', 'bcd', 'cde', 'array'], 'a') ['abc', 'array'] """ Please help refine the requirement specification following the steps: 1. List and clarify the definitions of the key concepts 2. Explain the purpose of the method to be implemented with your understandings in details 3. Analyze each parameter's type, meanings, format, properties and restrictions. 4. Analyze the return values' types, meanings, format, properties and restrictions. 5. Raise possible edge cases and solutions 5. Raise possible exceptions and errors and the solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Key concepts:</head><p>-`strings`: A list of strings that will be filtered.</p><p>-`prefix`: A string that will be used to filter the list of strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Purpose of the method:</head><p>The purpose of the `filter_by_prefix` method is to filter an input list of strings and return a new list that only contains strings starting with a given prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Parameter analysis:</head><p>-`strings`: This parameter is of type `List[str]`, which means it expects a list of strings as input.</p><p>-`prefix`: This parameter is of type `str`, which means it expects a string as input. The prefix is the string that will be used to filter the list of strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Return value analysis:</head><p>The return value of the `filter_by_prefix` method is a new list of strings that only contains strings starting with the given prefix. The return type is `List[str]`.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Possible edge cases and solutions:</head><p>-If the input list `strings` is empty, the method should return an empty list.</p><p>-If none of the strings in the input list `strings` start with the given prefix, the method should return an empty list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Possible exceptions and errors:</head><p>-There are no specific exceptions or errors mentioned in the provided code. However, some possible exceptions that could occur are:</p><p>-If the `strings` parameter is not a list, a `TypeError` should be raised.</p><p>-If the `prefix` parameter is not a string, a `TypeError` should be raised.</p><p>-If any of the strings in the `strings` list are not of type `str`, a `TypeError` should be raised.</p><p>To handle these exceptions, appropriate type checks and error handling should be implemented in the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt for LLM: Going-deep and Loop-back Example Output from Large Language Model</head><p>Here is a programming task:</p><p>{the task: with the refinement from previous step aggregated} Based on the given task descriptions, please: Raise up to 5 question about the requirements that you concern the most.</p><p>-Each questions should be followed by an answer based on your understanding of the task or your guess.</p><p>-You can raise questions from the following aspects: [Key Concept] [Input Requirement] [Output Requirement] [Edge Case], and any other categories you concern.</p><p>-Don't ask questions that have answers in the task description.</p><p>1. [Input Requirement] Can the input list `strings` contain duplicate strings? -the input list can contain duplicate strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">[Output Requirement</head><p>] Should the returned list of strings be in the same order as the original list? -Yes. the returned list of strings should be in the same order as the original list</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">[Edge Case]</head><p>What should be the behavior if the input list `strings` is None? -raise a `TypeError` since it expects a list of strings as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">[Edge Case]</head><p>What should be the behavior if the input prefix is an empty string? -the method should return an empty list since no strings in the input list will start with an empty string.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">[Edge Case]</head><p>What should be the behavior if the input prefix is longer than any string in the input list?</p><p>-the method should return an empty list since no strings in the input list will start with a prefix longer than themselves. refined requirement specifications. Then, we feed these refined requirement specifications to GPT-4 and gpt-3.5-turbo-0613 to get their generated code and test the pass rates. We obtained the refined requirement specifications from gpt-3.5-turbo-0613 because it is an LLM with the ability to perform requirement analysis and is easy to access. Compared with gpt-3.5-turbo-0613, GPT-4 is both expensive and strict with access.</p><p>According to Table <ref type="table">1</ref>, ChatCoder successfully helps large language models improve their generated program's execution accuracy through the refined requirements by a large margin. For example, for gpt-3.5-turbo, its pass@1 on Sanitized-MBPP is improved from 57.04% to 71.25%, and the margin is the percentage of 14. Compared horizontally, for both gpt-3.5-turbo and gpt-4, the performance improvements on Sanitized-MBPP is more prominent than those on HumanEval, which is because the task descriptions of Sanitized-MBPP are single sentences and method signatures, much more simple than the task descriptions of HumanEval. Thus the information for code generation of Sanitized-MBPP is far less sufficient than the information of HumanEval. As a result, when ChatCoder brings the refined requirement specifications full of additional information, the code generation performance on MBPP is more prominent than the improvement on HumanEval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RQ2: Communication Efficiency Evaluation</head><p>We evaluate whether ChatCoder is an efficient way for large language models and humans to communicate for requirements refinement. The key of ChatCoder is the constraints, i.e., the angles provided for the large language models to analyse the initial expression of requirements for refinement and the instructions we designed to convey LLMs the angles. So we compare ChatCoder with two other ways of communicating with the large language</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1: Code Generation Performances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HumanEval</head><p>Sanitized-MBPP pass@1 pass@1 pass@2 pass@5 pass@10 gpt-3. We let the large language model paraphrase the original programming task without giving any angles and ask the human user to have it edited and corrected for cognition alignment; 2) Free QA: We let the large language model to ask human users questions about their confusion about the original programming task and collect the human users' responses. All these experiments are conducted based on gpt-3.5-turbo-0613. The results are presented on Table <ref type="table" target="#tab_3">2</ref> According to Table <ref type="table" target="#tab_3">2</ref>, all three communication methods with LLMs for requirements refinement help the LLM improve its code generation results. This finding points out that any form of requirements refinement is useful and important in applying LLMs to generate code. Compared with ChatCoder, Free Paraphrase and Free QA do not instruct the LLM to perform certain kinds of refinement, leading to lower improvements. With careful inspection, we find that the additional contents generated by the LLM for requirements refinement surround our proposed analysis angles spontaneously. However, due to lacking explicit instructions, the refinement can not cover all the points covered by ChatCoder. So explicitly instructing the LLM with the angles for refining requirements is important for ChatCoder. Designing better instructions to order the LLM to refine requirements is part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">RQ3: Human Intervention Evaluation</head><p>We evaluate how important human intervention is to ChatCoder. This experiment is to prove the argument that requirements refinement should involve the participation of both software provider and software supplier, in this paper, the human user and the large language model.</p><p>We evaluate the human intervention by comparing it with asking the large language model to paraphrase and generate further questions without human's edit and correction, referred to as 'Auto-Refine' in the following description. We compare the LLM's code generation performances of Auto-Refine and our ChatCoder. All experiments are conducted on gpt-3.5-turbo-0613. The results are presented in Table <ref type="table" target="#tab_4">3</ref> It is not surprising that Self-Refine hurts the LLM's code generation performances. Since ChatCoder utilizes requirements refinement to improve the large language model's code generation performance, human intervention is necessary and can not be neglected. The process of ChatCoder is to reveal the inner structure of the requirements from the given angles, which are not expressed explicitly, even with ambiguity. The answer to solving the ambiguity is known only by the human user. But Auto-Refine just guesses an answer based on the large language model's training data, representing how most people understand the requirement. Suppose the large language model's guess or explanation of the requirement is wrong without human edits. In that case, the large language model will generate code following the wrong understanding of requirements and give up the other possible understandings. Thus, Auto-Refine hurts the LLM's code generation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study</head><p>This section raises several real test cases illustrating how ChatCoder helps LLMs generate code with refined requirements. Due to the page limit, we select three cases from MBPP covering refinement about the input, the output and the purpose since they influence the functional requirements directly. In contrast, edge cases and exceptions influence the robustness, requiring more space to illustrate. We put the cases in Figure <ref type="figure">4</ref>.</p><p>• MBPP/91 This task asks the coder to write a method checking if a string is presented in any string as a substring within a list. Due to the word 'if', we know the output of this method should be of judgement. However, the large language model misunderstands the task and returns a list of words. Because ChatCoder asks the large language model to analyze the The human user reviews the refined expression and corrects this mistake. With the corrected refined requirement, the large language generates the correct code. • MBPP/307 This task asks the coder to write a method to get a colon of a tuple. However, the expression is incomplete. The meaning of the parameters is missing, requiring refinement. Without refinement, the large language model thinks 'm' and 'n' are some indexes, leading to generating the wrong code. ChatCoder asks the LLM to analyze the meaning of each input parameter. The LLM responds that 'm' and 'n' are the index of the colon, which is wrong. The human user reviews the refined specification and corrects the meaning that 'm' is the index of the colon and 'n' is the value to be appended to the colon. The large language model generates the correct code with the corrected refined requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Savings of Human Labor Costs</head><p>Compared with performing requirements refinement with requirement engineers, ChatCoder asks the large language model to generate most of the text. At the same time, human users just need to review and edit, saving lots of human labour. This section will analyze how much human labour costs are saved. We evaluate the savings of the human labour costs by calculating how many tokens in the final refined requirement specifications are from humans. The statistics are shown in Figure <ref type="figure">5</ref>. From Figure <ref type="figure">5</ref>, we can see that tokens from human users take only a tiny proportion of the refined specifications. To boost the code generation performance, the users need to review the text, delete anything they do not like, and input, on average, ten tokens due to the help of ChatCoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Relevance and Completeness</head><p>We need to evaluate whether the improvement is due to Chat-Coder's refined requirements and whether the users think Chat-Coder's refined requirement specifications fulfil their needs well. Thus we invited three people outside the research group to give scores on ten randomly selected ChatCoder's refined requirements about the 'relevance' and 'completeness'. The results are depicted in Figure <ref type="figure">6</ref>. We ask the testers to compare the requirements before and after refinement and the code generated before and after the requirement refinement. Then we ask them to give a score (1-5) to judge whether the refinement relates to the improvement of the generated code (The real score, 1 for unrelated and 5 for directly related). Besides, we ask them to give a score (1-5) to judge whether the refinement makes them clearer about the user's requirements (The comp score, 1 for getting confused and 5 for getting clear). We calculate the average scores with error bars and have the results depicted in Figure <ref type="figure">6</ref>.</p><p>Through Figure <ref type="figure">6</ref>, we find that all testers agree that the refined requirements help the large language model generate better code and help themselves better understand the requirements. However, compared with the real score, the confidence that people get clearer about the problems is slightly weaker. That is because people judge the quality of the code partially based on the execution test results. However, execution tests are not perfect. The program passing certain test cases may not really fulfil the user's requirements. So ChatCoder still needs to be improved to refine the requirements better to fulfil the user's true needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Threats to Validations</head><p>There are a few threats to our methods • 1) The user quality. The reviews and edits are performed by the volunteer professional programmers in our research group. So they deeply understand large language models, programming languages and algorithms. However, it can not be guaranteed that every user of large language models is as good at these things as our researchers. So finding some way to test ChatCoder for ordinary users of large language models is on our future work list. • 2) The dataset. We use the datasets, HumanEval and Sanitized-MBPP in this paper to align the other research in this field. However, there is a flaw: these datasets do not really come from 'our requirements' and are too simple compared with real-world programs. One reasonable but difficult-to-realize solution is to recruit a group of full-time programming workers to evaluate the effect of ChatCoder in their real-world  The long text brings two problems. First, it is a heavy burden for the human user to review and edit, leading to a high opportunity to make mistakes. Second, we find through our observation that the current large language models have difficulty in coping with long text: they can ignore the logic dependency of two distanced terms. One of our future works is compressing the refined requirement specifications and preserving all the necessary information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of Refinement Improving Code Generation Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall Structure of the ChatCoder Dialogue Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompts for Large Language Models and Example Outputs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>Histogram of tokens from humans x-axis: numbers of tokens from human y-axis: number of tasks Comparison of tokens from llms and humans for each task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Statistics of Human Labor Savings</figDesc><graphic coords="10,125.32,278.91,92.70,60.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Communication Efficiency Comparison</figDesc><table><row><cell>5-turbo</cell><cell>70.12%</cell><cell cols="2">57.04%</cell><cell cols="2">58.17%</cell><cell>59.13%</cell><cell>59.75%</cell></row><row><cell>gpt-4</cell><cell>81.10%</cell><cell cols="2">66.15%</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>ChatCoder(gpt-3.5-turbo)</cell><cell>79.87%</cell><cell cols="2">71.25%</cell><cell cols="2">73.23%</cell><cell>75.18%</cell><cell>76.25%</cell></row><row><cell>ChatCoder(gpt-4)</cell><cell>90.24%</cell><cell cols="2">76.65%</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">HumanEval</cell><cell></cell><cell cols="4">Sanitized-MBPP</cell></row><row><cell></cell><cell>pass@1</cell><cell cols="5">pass@1 pass@2 pass@5 pass@10</cell></row><row><cell>gpt-3.5-turbo</cell><cell>70.12%</cell><cell>56.95%</cell><cell cols="2">58.16%</cell><cell cols="2">59.48%</cell><cell>60.48%</cell></row><row><cell>Free Paraphrase</cell><cell>78.05%</cell><cell>64.61%</cell><cell cols="2">65.47%</cell><cell cols="2">66.17%</cell><cell>66.68%</cell></row><row><cell>Free QA</cell><cell>71.95%</cell><cell>66.47%</cell><cell cols="2">68.82%</cell><cell cols="2">70.91%</cell><cell>72.00%</cell></row><row><cell>ChatCoder</cell><cell>79.87%</cell><cell cols="5">71.25% 73.23% 75.18%</cell><cell>76.25%</cell></row><row><cell>model: 1) Free Paraphrase:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Human Intervention EvaluationThis task asks the coder to write a method converting a string to a list. The large language model misunderstands the purpose of the method. The string should be split into words. However, the LLM thinks the method should be split into characters. The purpose of this method expressed by the original requirement is incomplete. ChatCoder asks the LLM to analyze the purpose of the method, and the LLM returns that the method should split the string by characters.</figDesc><table><row><cell></cell><cell>HumanEval</cell><cell></cell><cell cols="2">Sanitized-MBPP</cell><cell></cell></row><row><cell></cell><cell>pass@1</cell><cell cols="4">pass@1 pass@2 pass@5 pass@10</cell></row><row><cell>gpt-3.5-turbo</cell><cell>70.12%</cell><cell>56.95%</cell><cell>58.16%</cell><cell>59.48%</cell><cell>60.48%</cell></row><row><cell>Auto-Refine</cell><cell>68.90%</cell><cell>52.82%</cell><cell>54.77%</cell><cell>56.30%</cell><cell>57.12%</cell></row><row><cell>ChatCoder</cell><cell>79.87%</cell><cell cols="3">71.25% 73.23% 75.18%</cell><cell>76.25%</cell></row><row><cell cols="3">output, the output requirement is refined, indicating that</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the method should return a boolean value. The large lan-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">guage model generates the correct code based on the refined</cell><cell></cell><cell></cell><cell></cell></row><row><cell>requirement.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>• MBPP/118</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose ChatCoder, an effective method to improve large language models' code generation performances by requirement refinement via chat. We design a two-round dialogue framework to guide the large language model, refine the original requirements through five angles, and go deeper. Then we ask the human users to review and edit the generated refined requirement specifications. We apply ChatCoder to the famous large language models: gpt-4 and gpt-3.5-turbo and prove that ChatCoder improves their code generation ability by a large margin. Besides, we prove that ChatCoder is an efficient way of communicating with the LLMs for requirements refinement, and human intervention is needed in requirements refinement.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">IEEE Recommended Practice for Software Requirements Specifications</title>
		<idno type="DOI">10.1109/IEEESTD.1998.88286</idno>
		<idno>IEEE Std 830-1998</idno>
		<ptr target="https://doi.org/10.1109/IEEESTD.1998.88286" />
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Program Synthesis with Large Language Models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><forename type="middle">I</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<ptr target="https://arxiv.org/abs/2108.07732" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374[cs.LG]</idno>
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Refinement in requirements specification and analysis: a case study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Pol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hooman</surname></persName>
		</author>
		<idno type="DOI">10.1109/ECBS.2000.839888</idno>
		<ptr target="https://doi.org/10.1109/ECBS.2000.839888" />
	</analytic>
	<monogr>
		<title level="m">Proceedings Seventh IEEE International Conference and Workshop on the Engineering of Computer-Based Systems</title>
		<meeting>Seventh IEEE International Conference and Workshop on the Engineering of Computer-Based Systems</meeting>
		<imprint>
			<publisher>ECBS</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="290" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11644[cs.CL]</idno>
		<title level="m">Textbooks Are All You Need</title>
		<editor>
			<persName><forename type="first">Gustavo</forename><surname>De</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rosa</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adil</forename><surname>Salim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shital</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Harkirat</forename><surname>Singh Behl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adam</forename><surname>Tauman Kalai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Selfplanning Code Generation with Large Language Model</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.06689</idno>
		<idno type="arXiv">arXiv:2303.06689</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.06689" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards Enhancing In-Context Learning for Code Generation</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.17780</idno>
		<idno type="arXiv">arXiv:2303.17780</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.17780" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mishig</forename><surname>Davaadorj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Gontier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ho</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lipkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhtasham</forename><surname>Oblokulov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudra</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Stillerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sankalp</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Abulkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Zocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nour</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swayam</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Kunakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Zhdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Timor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161[cs.CL]</idno>
		<title level="m">StarCoder: may the source be with you</title>
		<editor>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Robinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</editor>
		<imprint>
			<publisher>Leandro von Werra, and Harm de Vries</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capturing complete and accurate requirements by refinement</title>
		<author>
			<persName><forename type="first">Shaoying</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICECCS.2002.1181498</idno>
		<ptr target="https://doi.org/10.1109/ICECCS.2002.1181498" />
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Engineering of Complex Computer Systems</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="57" to="67" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A requirements refinement framework</title>
		<author>
			<persName><forename type="first">Wenqian</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1363686.1363844</idno>
		<ptr target="https://doi.org/10.1145/1363686.1363844" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM Symposium on Applied Computing (SAC)</title>
		<editor>
			<persName><forename type="first">Roger</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hisham</forename><surname>Haddad</surname></persName>
		</editor>
		<meeting>the 2008 ACM Symposium on Applied Computing (SAC)<address><addrLine>Fortaleza, Ceara, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-03-16">2008. March 16-20, 2008</date>
			<biblScope unit="page" from="658" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.08568</idno>
		<idno type="arXiv">arXiv:2306.08568</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2306.08568" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report. CoRR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Hoi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.07922</idno>
		<idno type="arXiv">arXiv:2305.07922</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2305.07922" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
