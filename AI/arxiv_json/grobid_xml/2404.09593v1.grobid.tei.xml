<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction</title>
				<funder ref="#_HyXzGuc">
					<orgName type="full">Chinese NSF Youth Fund</orgName>
				</funder>
				<funder ref="#_rczftAu">
					<orgName type="full">Major Research Plan</orgName>
				</funder>
				<funder ref="#_X2SYJEC">
					<orgName type="full">Shanghai Science and Technology Innovation Action Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ding</forename><surname>Zepeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>♢♡</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Data Science ♡ School of Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">♢♠</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Data Science ♡ School of Data Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
							<email>liangjiaqing@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Data Science ♡ School of Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>♢♡</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Data Science ♡ School of Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Data Science ♡ School of Data Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Recall of Large Language Models: A Model Collaboration Approach for Relational Triple Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42FDDC41134DBF1483B3CA37F4FCA5CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Extraction</term>
					<term>Language Modelling</term>
					<term>Evaluation Methodologies</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation triple extraction, which outputs a set of triples from long sentences, plays a vital role in knowledge acquisition. Large language models can accurately extract triples from simple sentences through few-shot learning or fine-tuning when given appropriate instructions. However, they often miss out when extracting from complex sentences. In this paper, we design an evaluation-filtering framework that integrates large language models with small models for relational triple extraction tasks. The framework includes an evaluation model that can extract related entity pairs with high precision. We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model. We conduct extensive experiments to demonstrate that the proposed method can assist large language models in obtaining more accurate extraction results, especially from complex sentences containing multiple relational triples. Our evaluation model can also be embedded into traditional extraction models to enhance their extraction precision from complex sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Relational triple extraction plays an important role in knowledge acquisition. This task aims at extracting triples (subject, predicate, object) (or (s, p, o)) from a given natural language sentence. Current large language models (LLMs) have demonstrated the capacity to effectively extract triples from simple sentences via zero-shot or few-shot learning <ref type="bibr" target="#b35">(Wei et al., 2023;</ref><ref type="bibr" target="#b32">Wadhwa et al., 2023)</ref>. However, it is still unsatisfactory when the sentences contain multiple relational triples or mention many entities and relations. When LLMs executing multiple triple extraction tasks, they often miss out triples, which means the low recall of the results (see Table <ref type="table">1</ref>).</p><p>In the field of relational triple extraction, numerous short sentences contain a substantial number of triples <ref type="bibr" target="#b4">(Cheng et al., 2021)</ref>. This presents a significant challenge for LLM-based extraction. Although we can meticulously design instructions or use few-shot in-context learning to improve the triple extraction capabilities of LLMs, it is still difficult to rectify the issue of incomplete extraction from complex sentences by just modifying instructions or incorporating phrases such as 'extract as many results as possible' into the prompts, as shown in Figure <ref type="figure">1</ref>(a) and Table <ref type="table">1</ref>. This phenomenon could potentially be attributed to the fact that the majority of the training corpus of LLMs is composed of simple sentences, which means the distribution is significantly biased towards contain-ing few triples, leading models to overlook some correct triples when dealing with complex sentences. Previous research has demonstrated that fine-tuning a large model using task data can effectively enhance its relation extraction capabilities, yielding more accurate extraction results <ref type="bibr" target="#b32">(Wadhwa et al., 2023)</ref>. However, we observed that the finetuned LLMs still encounter the issue of a significantly lower recall compared to precision in the case of multiple triples, as Table <ref type="table">1</ref> and <ref type="table">Table 3</ref> show. On one hand, the volume of fine-tuning task data is relatively small compared to the original training data of LLMs, making it insufficient to alter the bias of the triplet distribution. On the other hand, the decoding method of the generative language model is not well-suited for the extraction of multiple different relational triples. Even if we compel the large model not to generate the &lt;EOS&gt; token unless it produces enough triples, the model still lacks the capability to find more valid triples. Instead, it repeats the generated contents, as shown in Figure <ref type="figure">1(b)</ref>.</p><p>As a result, relying solely on LLMs to achieve complete extraction results in multiple triple extraction tasks is proved to be a considerable challenge. Conversely, traditional small models are prone to extract an excessive number of triples, leading to high recall but low precision results. It is because these models lack the ability to identify what triples are not mentioned <ref type="bibr" target="#b5">(Chu et al., 2020;</ref><ref type="bibr" target="#b16">Jiang et al., 2020)</ref>. Dataset Fine-tune Precision Recall NYT10 w/o fine-tune 13.75 7.75 NYT10 w/ fine-tune 78.05 46.38 SKE21 w/o fine-tune 41.30 34.17 SKE21 w/ fine-tune 72.56 57.42</p><p>Table <ref type="table">1</ref>: Precision and recall when extracting multiple relational triples by a large language model. We only consider the complex sentences that contain more than 7 triples. The model used is Vicuna-13B for NYT10 and Qwen-7B for SKE21.</p><p>Hence, the model collaboration methods that amalgamate the strengths of small models and LLMs are a natural consideration for addressing the multiple relational triple extractions. However, as previously mentioned, traditional small models can easily generate incorrect entity pairs when dealing with complex sentences. If these results are directly incorporated into the extraction process, such as providing them to the LLMs as part of the prompt, it can easily mislead the LLMs and compromise extraction precision. Motivated by the above considerations, we propose an evaluation-filtering model based on the transformer architecture to generate candidate relational entity pairs and construct an LLMs-based relational triple extraction framework in conjunction with this model. This model has the following characteristics: First, the model works at the token level, enabling the evaluation of candidate entity pairs represented with arbitrary tokens, so that it can accurately extract the positive entity pairs and tolerate noisy candidates. Second, our model can be easily integrated into the process of LLM-based relation triple extraction as a plug-in, significantly enhancing the extraction recall rate. The model can also be seamlessly combined with traditional extraction models to improve the precision. In sum-mary, our main contributions are as follows:</p><p>• First, we construct an extraction framework that integrates both the small models and LLMs. This framework provides the filtered positive entity pairs to the LLMs as part of the prompt, thereby guiding the model to consider more entity pairs and assign proper relations to them.</p><p>• Second, we propose a fast and robust evaluation model that can be used to effectively filter wrong extracted results and generate positive entity pairs. It is loosely coupled with the extraction process and can be injected into any small model and LLM-based method to enhance the recall and F1 score of results.</p><p>• Third, we conduct extensive experiments to show that the proposed method can successfully enhance the performance of LLMs in relational triple extraction, particularly in terms of the recall rate. Additionally, supplementary experiments also indicate that the evaluationfiltering method can boost extraction precision when applied to traditional small models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Large Language Models for Relational Triple Extraction</head><p>Large Language Models (LLMs) have gained widespread attention due to their strong ability for various NLP tasks. In addition to the robust GPT series <ref type="bibr" target="#b3">(Brown et al., 2020;</ref><ref type="bibr" target="#b23">OpenAI, 2023)</ref>, open-source LLMs have been also widely studied and applied, including Llama series <ref type="bibr">(Touvron et al., 2023a,b)</ref>, Qwen <ref type="bibr" target="#b2">(Bai et al., 2023)</ref> and Vicuna <ref type="bibr">(Zheng et al., 2023)</ref>. Recent studies on LLMs suggest that they perform well in a variety of downstream tasks, even when provided with only a few examples as instructions <ref type="bibr" target="#b0">(Agrawal et al., 2022;</ref><ref type="bibr" target="#b13">Jeblick et al., 2023)</ref>. In extraction-related tasks, some works show that with proper prompting, ChatGPT can achieve comparable performance with the supervised methods on zero-shot or few-shot settings of extraction tasks <ref type="bibr" target="#b35">(Wei et al., 2023;</ref><ref type="bibr" target="#b10">Gao et al., 2023;</ref><ref type="bibr" target="#b29">Tang et al., 2023)</ref>. For open-source LLMs, previous work shows Flan-T5 <ref type="bibr">(Chung et al., 2022)</ref> can yield outstanding performance by supervising and fine-tuning and suggests LLMs should be a standard baseline for relation extractions <ref type="bibr" target="#b32">(Wadhwa et al., 2023)</ref>. However, these studies did not specifically consider the model's extraction ability on complex sentences containing multiple relational triples. Furthermore, the manual evaluation of the results was not as rigorous as exact matching, and most of these studies focus on chatGPT and do not consider various open-source LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model Collaboration in the Era of Large Language Models</head><p>Current methods of model collaboration involving large language models can be primarily categorized into three types. First, the output results of the small model are utilized as a component of the overall framework to assist the LLMs to perform better on downstream tasks <ref type="bibr" target="#b38">(Xu et al., 2023;</ref><ref type="bibr" target="#b18">Leviathan et al., 2023)</ref>. Second, large and small models are collaboratively trained based on task data to efficiently utilize the unlabeled data and minimize the bias of models <ref type="bibr" target="#b17">(Lang et al., 2022)</ref>. Third, ensembling multiple prompts or multiple LLMs to achieve more stable output results, as well as improved generalization performance <ref type="bibr" target="#b1">(Allingham et al., 2023;</ref><ref type="bibr" target="#b15">Jiang et al., 2023)</ref>.</p><p>In the field of relational triple extraction, research based on traditional models is relatively comprehensive, and some novel and effective multi-step or joint methods are proposed to extract multiple triples <ref type="bibr" target="#b19">(Li et al., 2019;</ref><ref type="bibr" target="#b36">Wei et al., 2020;</ref><ref type="bibr" target="#b33">Yu et al., 2020;</ref><ref type="bibr" target="#b37">Xie et al., 2021)</ref>. For example, RERE <ref type="bibr" target="#b37">(Xie et al., 2021)</ref> carefully compares different types of multi-step settings and shows that the relationthen-entity extraction paradigm exhibits a good performance since it suffers less from the problem of data imbalance, which is often encountered in relational triple extraction tasks. However, these methods cannot fully solve complex relational triple extraction tasks. Inspired by this, we propose to design an evaluation model and integrate this small model as a plug-in within the extraction framework based on LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Solution Framework</head><p>Our LLM-based relational triple extraction framework comprises two stages (see Figure <ref type="figure" target="#fig_1">2</ref>). In the first stage, the LLMs directly extract triples from sentences according to the provided instructions. Subsequently, in the second stage, we design an "evaluation-filtering" method, which extracts the positive entity pairs by our evaluation model and uses prompts to inform the LLMs that "these entity pairs may have certain relations in the relations list". These candidate pairs will be provided to the LLMs along with the instructions and the firststage extraction results. LLMs will further scrutinize these candidates and assign appropriate relations based on their language comprehension capabilities, thereby achieving comprehensive and accurate extraction results. An example of the whole workflow is shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basic Idea of Evaluation Model</head><p>The evaluation model (bottom-right part of Figure <ref type="figure" target="#fig_1">2</ref>) uses a sentence (a list containing N tokens) as the input and outputs a token pair evaluation matrix (N * N ). Each element in the matrix is an evaluation score for a token pair. The evaluation score of a token pair t i and t j is used to compose the evaluation score of an entity pair (s, o), where s contains t i and o contains t j . Obviously, for an input sentence, no matter how many candidate pairs are to be evaluated, only one inference is needed to get the evaluation matrix. Our goal is to build such a model that scores candidate entity pairs based on the sentence from which the triples are extracted, as Problem 1 shows.</p><p>Clearly, this evaluation model could be used as a filter, removing the extracted candidate entity pairs with low scores while retaining those with high scores. After this filtering process, we can obtain a set of precise and complete positive samples (i.e., truly related entity pairs), which can then be supplied to LLMs as prompts to facilitate highprecision extraction of multiple relational triples.</p><p>Problem 1 (Evaluation of entity pairs) Given a sentence T and a candidate entity pair set C, the evaluation model outputs a score F (s, o) for each pair (s, o) ∈ C.</p><p>Moreover, in order to overcome the noisy entity problem, we use token-level representation to support any possible entities in the sentence.</p><p>Rationality for providing entity pairs Note that we only evaluate entity pairs (s, o) and provide them to LLMs, ignoring the predicate p. The rationality of ignoring the predicate is as follows.  First, we find that in most real datasets, the entity pairs are more accurate than predicates in a labeled sample 1 . Second, the model structure based on entity pairs evaluation is more straightforward. It only needs to generate one evaluation matrix for a sentence, while the evaluation model for entire (s, p, o) triples requires to generate k matrices, where k represents the number of relations contained within the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rationality of token based representation</head><p>Our model aims at evaluating any candidate entity pairs 1 E.g., in the NYT11 training set, there are 20% wrong triples, but only 6% wrong entity pairs. in the sentences. However, extracting the entity span accurately is still a problem <ref type="bibr" target="#b8">(Dixit and Al-Onaizan, 2019;</ref><ref type="bibr" target="#b14">Ji et al., 2020)</ref>. For example, "Gates and Steve" might be wrongly identified as an entity (in Figure <ref type="figure" target="#fig_3">4</ref>). Thus, it is necessary to evaluate the candidate "entities" represented by arbitrary tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self Labeling</head><p>The evaluation model has to distinguish between correct entity pairs and wrong entity pairs, which is a binary classification task, thus we need positive and negative training samples. In original extraction datasets, a sentence is labeled with some triples, which correspond to some entity pairs with one of the target relations. Obviously, these entity pairs are positive samples (y = 1). Then, it is important to obtain negative samples. We generate negative samples with the following assumption:</p><p>Assumption 1 If a labeled sentence contains multiple triples, which involve multiple entities, then the unlabeled entity pairs are negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rationality of Assumption 1</head><p>Assumption 1 will generate a false negative entity pair (e 1 , e 2 ) only when all the four conditions are satisfied ("*" means any relations or entities):</p><p>• (e1, * , e2) is mentioned in the sentence.</p><p>• Any triples (e1, * , e2) are not labeled in the sentence. However, it is seldom the case that the four conditions are simultaneously met. The false negative case means that an annotator (no matter whether it is distant supervision via knowledge base or hand annotation via human) labels other triples in a sentence for both e 1 and e 2 , but only misses the relation between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token-level labeling</head><p>The above process generates entity pair samples, and the labeled token pairs can be simply and effectively obtained. For example, in Figure <ref type="figure" target="#fig_3">4</ref>, we simply split the negative entity pair (Microsoft, Steve Jobs) into token pairs (Microsoft, Steve) and (Microsoft, Jobs), which are labeled as negative token pairs. This process not only increases the number of training pairs, but also enables the model to evaluate unseen or wrong entities, or even any token sequence.</p><p>Note that, for any other token pairs in the sentence (e.g. (founders, Microsoft)), they are not labeled as negative or positive (y = 0). They will be masked in the training process of our evaluation model since we have no information about whether they are positive or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Model Structure</head><p>Following the Transformer architecture, our evaluation model adopts a BERT-based encoder and an attention-like 2-dim decoder (as shown in the right part of Figure <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Encoder</head><p>We use a regular Transformer model as our encoder. Specifically, we use BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> for English and RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> for Chinese. They have the same network structure.</p><p>More formally, for an input sentence with N tokens [t 1 , t 2 , ..., t N ], where t 1 = [CLS] and t N = [SEP ] are fixed special tokens, the BERT encoder converts these tokens into hidden vectors [h 1 , h 2 , ..., h N ], where each h i is a d 1 -dimension vector. In the BERT-base structure, d 1 = 768.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">2-dim Decoder</head><p>For an input sentence with N tokens, the BERTbased encoder encodes the tokens into N vectors [h 1 , h 2 , ..., h N ]. Then, following the computation of the attention matrix in Transformers, we use a onehead self-attention to compute the 2-dim attention matrix as the output of the decoder. In detail, we first use two linear layers to convert the vectors h i to d 2 -dimension vectors q i and k i :</p><formula xml:id="formula_0">qi = W (q) hi + b q , ki = W (k) hi + b k , (1)</formula><p>where W and b are trainable parameters of the linear layers, and d 2 = 64. Then, we compute their scaled dot-product attention as the output:</p><formula xml:id="formula_1">Aij = q T i kj/ √ d2.<label>(2)</label></formula><p>As proposed by Roformer <ref type="bibr" target="#b27">(Su et al., 2021)</ref>, it is advantageous to add relative position embeddings (RoPE) before computing the attention output. The relative position embeddings R i are realized by constructing sine and cosine functions that satisfy R T i R j = R j-i , we refer the readers to <ref type="bibr" target="#b27">(Su et al., 2021)</ref> for technical details. The intuition is that when encoding positional information R i and R j at position i and j, the output attention will naturally contain the relative positional information. The final form of the attention output is:</p><formula xml:id="formula_2">A ′ ij = (Riqi) T (Rjkj)/ √ d2 = q T i Rj-ikj/ √ d2.<label>(3)</label></formula><p>Recall that this decoder (without the relative position embeddings) is only a part of regular one-head self-attention, although it has O(N 2 ) outputs, its computational cost is smaller than the Transformerbased encoder. Hence, the cost of training such an evaluation model is lower than a Transformerbased extraction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>Since our task is a classification task with positive and negative labels, we use the binary crossentropy loss function to train our evaluation model: where y ij is the label of the token pair (t i , t j ), and σ is the sigmoid function. Note that, our task is not a pure binary classification task, since there are many unlabeled token pairs in our task. Thus, the positive and negative examples are not complementary. In the implementation of the loss, we ignore the part of unlabeled pairs (i.e. y = 0).</p><formula xml:id="formula_3">L = - i,j:y ij =1 log (σ(A ′ ij )) - i,j:y ij =-1 log (1 -σ(A ′ ij )),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Candidate Pairs Evaluation</head><p>After training such an evaluation model, we adapt the model to an existing extraction method to obtain better extraction results. We score each candidate extracted pairs (s, o), where s = [t sst , t sst+1 , ..., t s ed ] and o = [t ost , t ost+1 , ..., t o ed ] are sub-token-sequences in the given sentence.</p><p>Recall that, our evaluation model outputs a token pair evaluation matrix A ′ . Based on this matrix, we compute the score between s and o by the mean of the matching scores of their tokens:</p><formula xml:id="formula_4">F (s, o) = s ed k=s st o ed l=o st A ′ kl (s ed -sst + 1)(o ed -ost + 1)<label>(5)</label></formula><p>where s st and o st are the indexes of the first element of token lists s and o, s ed and o ed are the indexes of the last element of s and o.</p><p>Finally, only (s, o) satisfying F (s, o) &gt; 0 will be added to the result. Note that, we only need to predict the matrix A ′ once for each sentence, no matter how many triples of this sentence should be evaluated, thus the evaluation process is efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Parameter-Efficient Fine-Tuning for LLMs</head><p>For multiple relational triple extractions, employing instruction-tuning or in-context learning (ICL) to guide LLMs, as is done for general tasks, often yields unsatisfactory results. This is because LLMs possess strong generalization capabilities and language comprehension, leading them to inexactly recognize the span of entities or relations. For instance, they may extract predicates not presented in the predicate list, or consider book titles as part of an entity, even when their extraction range is explicitly limited through prompts. Consequently, parameter fine-tuning is necessary to adapt the model to the corresponding datasets and to potentially non-natural language representations of predicates (e.g. NYT10 dataset).</p><p>In this paper, we mainly adopt the LoRA technology <ref type="bibr" target="#b12">(Hu et al., 2021)</ref>. LoRA is a parameterefficient fine-tuning (PEFT) method. It freezes the large-scale parameters of a pre-trained model and simulates parameter changes through low-rank decomposition of the matrix, thereby adapting the model to downstream tasks with small-scale parameter adjustments. Compared to full-parameter fine-tuning, this method is more time-efficient and requires less computing resources and storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Instruction Template</head><p>To better guide LLMs in performing multiple relational triple extraction tasks, we design an instruction template that explicitly includes the task description, the restricted range of extracted predicates, the output format, and other requirements. We also explicitly require the model to extract as many relation triples as possible. Additionally, for the original large model without PEFT, a complete input-output example can also be placed after the instruction template for in-context learning.</p><p>After the evaluation model extracts candidate entity pairs, these candidates will be provided to the LLMs as part of the prompt, along with the aforementioned instructions and the first-stage extraction results, to guide the model in completing the extraction. See Appendix C for detailed prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our methods on several public and downloadable complex extraction datasets, including NYT series <ref type="bibr" target="#b25">(Riedel et al., 2010;</ref><ref type="bibr" target="#b28">Takanobu et al., 2019)</ref>, Wiki-KBP <ref type="bibr" target="#b20">(Ling and Weld, 2012)</ref>, SKE21 <ref type="bibr" target="#b37">(Xie et al., 2021)</ref> and HacRED <ref type="bibr" target="#b4">(Cheng et al., 2021)</ref>, which are challenging for many ex-</p><p>SKE21 HacRED NYT10 NYT11-HRL WikiKBP Prec. Reca. F1 Prec. Reca. F1 Prec. Reca. F1 Prec. Reca. F1 Prec. Reca. F1 Qwen-7b (w/o peft) 17.59 26.18 21.04 5.72 6.10 5.90 8.18 6.12 7.00 8.08 16.19 10.78 6.64 19.50 9.91 +Ours 42.21 40.14 41.15 19.56 15.06 17.02 10.07 6.75 8.08 15.87 24.13 19.14 9.02 18.00 12.02 #Triples&gt;=t 31.53 29.77 30.63 6.04 4.37 5.07 12.89 2.96 4.81 5.63 10.67 7.37 7.46 12.82 9.43 +Ours, #Triples&gt;=t 54.62 39.10 45.57 21.43 13.98 16.93 13.13 5.20 7.45 12.50 16.67 14.29 8.47 12.82 10.20 Qwen-7b (w/ peft) 50.34 70.16 58.62 44.51 41.47 42.94 66.30 56.27 60.88 56.48 58.01 57.24 42.54 48.50 45.33 +Ours 48.17 77.90 59.53 42.89 49.95 46.15 65.54 75.37 70.11 56.95 67.24 61.67 39.58 56.00 46.38 #Triples&gt;=t 63.27 65.40 64.31 51.54 39.77 44.89 81.49 43.30 56.55 76.74 44.00 55.93 34.78 20.51 25.81 +Ours, #Triples&gt;=t 61.36 73.84 67.02 50.84 47.37 49.04 78.26 66.53 71.92 60.94 52.00 56.12 35.00 35.90 35.44 Llama-13b (w/ peft) 33.65 24.05 28.05 12.89 9.28 10.79 18.05 54.90 27.17 13.85 52.93 21.95 18.84 76.00 30.19 +Ours 30.16 36.06 32.84 27.98 15.52 19.96 19.04 49.78 27.54 24.13 65.92 35.32 20.96 76.00 32.86 #Triples&gt;=t 37.08 21.35 27.10 17.03 8.33 11.19 36.68 47.60 41.43 38.83 53.33 44.94 20.22 46.16 28.12 +Ours, #Triples&gt;=t 44.50 29.17 35.24 31.35 13.67 19.04 51.38 56.35 53.75 59.09 52.00 55.32 32.00 61.54 42.10 Vicuna-13b (w/ peft) 69.30 51.75 59.25 34.36 33.35 33.85 71.02 62.89 66.71 32.92 56.36 41.57 17.06 18.00 17.52 +Ours 68.20 67.96 68.08 53.49 40.88 46.34 60.98 65.86 63.33 34.20 61.37 43.93 37.45 47.00 41.69 #Triples&gt;=t 84.55 29.62 43.88 45.86 30.97 36.97 76.29 43.53 55.43 55.41 54.67 55.03 30.00 23.08 26.09 +Ours, #Triples&gt;=t 74.59 60.90 67.05 61.98 38.68 47.63 70.40 51.76 59.66 59.49 62.67 61.04 41.38 30.77 35.29</p><p>Table <ref type="table">3</ref>: The main extraction results. The "w/ peft" means that parameter-efficient fine-tuning (LoRA) of base LLMs is done before triples extracting, based on a part of the train set (about 800 sentences).</p><p>Better exact match F1 scores are marked bold. The threshold t is 2 for WikiKBP and NYT11-HRL since the most complex sentence only contains 4 triples in these datasets, and is 5 for other datasets.</p><p>traction methods. Table <ref type="table" target="#tab_1">2</ref> shows their statistics and shows that sentences in these datasets often have more than one triple. A brief introduction to these datasets is provided in Appendix A.</p><p>For the NYT series, the relations in the original data are labeled in structured formats (e.g. /location/location/contains), which is not easily comprehensible to large models. Therefore, before conducting experiments, we converted these relations into natural language with similar meanings(e.g. location contains) to enhance the model's understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparing Methods and Metrics</head><p>We apply our method to some recently popular LLMs such as Qwen-7B, Vicuna-13B, and Llama-13B, and compare the triple extraction performance of our framework including evaluation-filtering with the base large models. To better assess the effectiveness of our method in complex scenarios, we have also specifically calculated the metrics in cases involving multiple triples.</p><p>Additionally, we also apply our methods to pretrained-language-model-based approaches including CasRel <ref type="bibr" target="#b36">(Wei et al., 2020)</ref>, TPLinker <ref type="bibr" target="#b33">(Wang et al., 2020)</ref>, and RERE <ref type="bibr" target="#b37">(Xie et al., 2021)</ref>, to examine the effectiveness of the evaluation model and evaluate its improvement over small models.</p><p>We report standard precision (Prec.), recall (Reca.), and F1 scores for all the experiments. We mainly focus on the exact match result, which is also the main consideration of current extraction methods. Note that, some triples extracted by the model may be deemed errors when calculating metrics due to the synonyms or the addition of certain symbols. These could also be considered as correct results by manual evaluation. However, em-ploying exact matching makes the evaluation and comparison of results more rigorous and credible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness of Our Method</head><p>The results in Table <ref type="table">3</ref> demonstrate that with the assistance of our evaluation-filtering model, the triple extraction results of various LLMs on different Chinese and English datasets have been significantly enhanced. By using the filtered candidate pairs as prompts, compared to the basic LLMs, the recall rate in the multiple relational triple extractions task can be stably and significantly improved (more than 10%) in most cases, with only a risk of slightly reduced precision. In fact, the precision of models that include evaluation-filtering will also be improved in many cases.</p><p>We specifically focused on relational triple extraction in more complex scenarios, setting a minimum number of triples for each dataset (2 for WikiKBP and NYT11-HRL, 5 for others), and only considering sentences containing more than this number of triples to assess the model's extraction effect on them. The results indicate that when a sentence contains a substantial number of triples, the direct application of LLMs to extract relational triples based on instructions often yields poor results, irrespective of whether fine-tuning on labeled data. Notably, the recall in most cases is significantly lower than the precision. In contrast, on complex sentences (number of triples&gt;=t) in various datasets, and the most complex dataset HacRED, our evaluation-filtering method can significantly enhance the recall of the extraction results, while also improving precision in most cases.</p><p>In Figure <ref type="figure">5</ref>, we use the NYT10 dataset, categorizing the test sentences based on the number of triples they contain, and utilize the Qwen-7B</p><p>NYT10 NYT10-HRL SKE21 HacRED Prec. Reca. F1 Prec. Reca. F1 Prec. Reca. F1 Prec. Reca. F1 TPLinker 84.96 89.66 87.25 74.31 61.06 67.04 72.73 77.94 75.24 54.64 61.21 57.74 + Ours 86.87 89.36 88.10 74.79 60.86 67.11 81.31 77.63 79.43 61.78 59.04 60.38 CasRel 83.82 87.63 85.69 70.25 65.11 67.58 84.24 67.50 74.95 62.62 34.62 44.59 + Ours 88.23 87.40 87.81 72.35 64.88 68.41 84.89 67.42 75.15 69.48 34.18 45.82 RERE 81.28 89.16 85.04 68.66 63.77 66.12 81.01 82.15 81.58 46.42 61.37 52.86 + Ours 87.03 88.80 87.90 71.32 63.61 67.42 83.44 81.68 82.55 69.92 59.37 64.21</p><p>Table 4: The main evaluation results of different small models. We only report the results for sentences with at least 50 tokens. Best exact match F1 scores are marked bold. model to extract triples from sentences of different complexity levels. The results show that as the number of triples within a sentence increases, our model demonstrates a progressively noticeable improvement in the recall of relational triple extraction results, compared to the base model. Moreover, it can maintain the F1 score of the results at a relatively high level.</p><p>This suggests that our method is particularly effective for extracting multiple relational triples from complex sentences, and it can sustain a high level of precision of results.</p><p>In addition, the results of the small extraction model in Table <ref type="table">4</ref> show that our method achieves a large precision improvement with a small recall decline, which leads to a better F1 score. This indicates that our evaluation model can accurately and reliably obtain candidate pairs, which can be applied to the traditional small extraction model to improve the performance of multiple relational triple extraction.</p><p>Figure <ref type="figure">5</ref>: Recall and F1-score curve of Qwen-7B (w/ peft) on NYT10, with and without our evaluationfiltering method. Minimum # of triples means we only consider sentences that contain a number of triples greater than this value. Note that the coordinates do not start from 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Further Analysis</head><p>In Table <ref type="table" target="#tab_6">5</ref>, we try three ablation settings. First, we remove the second stage of the framework, that is, the LLMs extraction part after receiving the candidate pairs prompt. Instead, we incorporated an additional relation classification model prior to the evaluation model. In other words, we use a small extraction model (RERE <ref type="bibr" target="#b37">(Xie et al., 2021)</ref>) to extract triples, which are then filtered according to the evaluation model. The filtered triples are combined with the first-stage LLMs' extraction results as the final results. The results indicate that the omission of the LLMs in the second stage leads to a decrease in the precision and F1 score of triple extraction results. Therefore, a large model in the second stage is still necessary for judgment and relation identification.</p><p>Second, we remove the first stage of the framework, that is, when inputting instructions and original sentences, we also provide the LLMs evaluation-filtering prompt, which will strictly limit the scope of triple extraction to the candidates provided by the evaluation model. The results show that our model can still enhance the recall rate of multiple triple extraction, but less effectively compared to the complete framework. This could be attributed to the presence of positive entity pairs that the evaluation model fails to recognize. However, without stringent restrictions, LLMs are capable of identifying and retaining these results.</p><p>Third, we remove the filtering step in the framework, that is, directly provide all entity pairs recognized by the entity-extraction model as prompts to the LLMs. The results show that the precision and F1 score of extraction results significantly decrease. This suggests that our evaluation-filtering method is indispensable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an evaluation model that can act as a filter to assess and identify entity pairs that have relations, thereby providing highprecision candidates for the subsequent extraction process.</p><p>We incorporate this evaluation model into our proposed evaluation-filtering framework for LLMs multiple relation triple extraction. The candidates filtered by the evaluation model are integrated into the extraction process of LLMs in the form of prompts. This effectively addresses the issue of low recall rate in triple extraction tasks performed by LLMs, without diminishing precision.</p><p>The experimental results that derived from multiple LLMs and datasets validate the effectiveness and completeness of our framework. Additionally, we confirm that our evaluation model can also be implemented in traditional small extraction models to enhance their precision and F1 score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: (a) Illustration of multiple relational triple extraction by LLMs, based on ChatGPT or Vicuna-13B. Both models are given appropriate instructions, limited predicates list and asked to extract as many as possible. (b) Compelling LLM to generate more triples results in repetitive outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model framework. On the bottom left is an arbitrary entity-extraction model. On the bottom right is our evaluation model, which outputs a token pair scoring matrix.</figDesc><graphic coords="4,93.29,62.81,408.19,243.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the workflow of our Evaluation-Filtering method.</figDesc><graphic coords="4,72.00,366.87,218.27,145.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: This sentence contains 6 entity pairs, but only 2 pairs are positive.</figDesc><graphic coords="5,138.65,62.81,317.47,53.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The statistics of complex sentences of testing datasets. |T | means the number of tokens in the sentences. |T | &gt;= x only reports results for sentences with at least x tokens. avgE, avgR denote the average numbers of labeled entities, labeled triples in the sentence, respectively. #sen denotes the number of sentences.</figDesc><table><row><cell>Dataset</cell><cell cols="16">|T |&gt;=0 avgE avgR #sen avgE avgR #sen avgE avgR #sen avgE avgR #sen avgE avgR #sen |T |&gt;=30 |T |&gt;=50 |T |&gt;=70 |T |&gt;=100</cell></row><row><cell>NYT10</cell><cell></cell><cell>2.2</cell><cell>1.7</cell><cell>5000</cell><cell>2.2</cell><cell>1.8</cell><cell>4091</cell><cell>2.2</cell><cell>1.8</cell><cell>1798</cell><cell>2.3</cell><cell>1.9</cell><cell>441</cell><cell>2.3</cell><cell>2.1</cell><cell>51</cell></row><row><cell cols="2">NYT11-HRL</cell><cell>2.0</cell><cell>1.0</cell><cell>369</cell><cell>2.0</cell><cell>1.0</cell><cell>283</cell><cell>2.0</cell><cell>1.0</cell><cell>120</cell><cell>2.0</cell><cell>1.0</cell><cell>28</cell><cell>2.0</cell><cell>1.0</cell><cell>3</cell></row><row><cell>SKE21</cell><cell></cell><cell>3.3</cell><cell>2.4</cell><cell>1150</cell><cell>3.5</cell><cell>2.6</cell><cell>901</cell><cell>3.9</cell><cell>3.0</cell><cell>423</cell><cell>3.9</cell><cell>3.0</cell><cell>202</cell><cell>4.0</cell><cell>3.2</cell><cell>80</cell></row><row><cell>WikiKBP</cell><cell></cell><cell>2.1</cell><cell>1.1</cell><cell>182</cell><cell>2.2</cell><cell>1.2</cell><cell>98</cell><cell>2.1</cell><cell>1.0</cell><cell>25</cell><cell>2.3</cell><cell>1.2</cell><cell>6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Dataset</cell><cell cols="16">|T |&gt;=50 avgE avgR #sen avgE avgR #sen avgE avgR #sen avgE avgR #sen avgE avgR #sen |T |&gt;=100 |T |&gt;=150 |T |&gt;=200 |T |&gt;=250</cell></row><row><cell>HacRED</cell><cell>7.1</cell><cell>7.4</cell><cell></cell><cell>1500</cell><cell>7.4</cell><cell>7.7</cell><cell>1372</cell><cell>8.2</cell><cell>8.8</cell><cell>1012</cell><cell>9.1</cell><cell>10.0</cell><cell>693</cell><cell>10.2</cell><cell>11.4</cell><cell>410</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ours 68.32 74.46 71.26 60.76 60.95 60.86 w/o stage 2 64.69 75.01 69.47 53.85 62.23 57.73 w/o stage 1 72.77 68.86 70.76 63.33 50.67 56.30 w/o pairs filtering 65.12 66.04 65.57 58.86 59.05 58.95 Llama-13b + Ours 53.79 28.06 36.88 52.37 60.00 55.92 w/o stage 2 40.23 41.19 40.70 36.38 62.20 45.91 w/o stage 1 63.11 25.06 35.87 68.11 52.00 58.97 w/o pairs filtering 44.75 27.25 33.87 37.76 49.33 42.77The ablation experiments results of different LLMs. We only report the results for sentences containing at least t triples. Best exact match F1 scores are marked bold.</figDesc><table><row><cell>Models (w/ peft)</cell><cell>SKE21 (t=7)</cell><cell></cell><cell cols="2">NYT11-HRL (t=3)</cell></row><row><cell></cell><cell>Prec. Reca.</cell><cell>F1</cell><cell>Prec. Reca.</cell><cell>F1</cell></row><row><cell>Qwen-7b + Vicuna-13b + Ours</cell><cell cols="4">71.49 56.83 63.33 58.82 57.14 57.97</cell></row><row><cell>w/o stage 2</cell><cell cols="4">65.26 59.90 62.92 44.00 61.33 51.24</cell></row><row><cell>w/o stage 1</cell><cell cols="4">92.76 36.06 51.93 70.91 41.67 52.49</cell></row><row><cell>w/o pairs filtering</cell><cell cols="4">63.97 57.67 60.66 32.38 55.18 40.82</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/qiaojiim/hacred</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://ai.baidu.com/broad/download?</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">Chinese NSF Youth Fund</rs> (No. <rs type="grantNumber">62102095</rs>), <rs type="funder">Major Research Plan</rs> (No. <rs type="grantNumber">92270121</rs>), and <rs type="funder">Shanghai Science and Technology Innovation Action Plan</rs> (No.<rs type="grantNumber">21511100401</rs>).The computations in this research were performed using the CFFF platform of <rs type="institution">Fudan University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HyXzGuc">
					<idno type="grant-number">62102095</idno>
				</org>
				<org type="funding" xml:id="_rczftAu">
					<idno type="grant-number">92270121</idno>
				</org>
				<org type="funding" xml:id="_X2SYJEC">
					<idno type="grant-number">21511100401</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Extraction Performance Despite the effectiveness of our model, the overall extraction results may still miss some correct triples and contain errors. On the one hand, a small amount of related entity pairs are not correctly evaluated by the evaluation model. On the other hand, it is difficult for LLMs to completely avoid errors or omissions in the second stage, although we prompt them to pick the correct candidate pairs and recheck the original results. Subsequent research could explore the optimization of the evaluation model, as well as further improvements in the extraction precision and recall of the model collaboration approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of Our Method</head><p>Our framework involves multiple components and requires the LLMs to perform extraction twice. Our method is more complicated and more time-consuming with the direct application of LLMs, and its inference time roughly doubled. To obtain stable effect improvement when dealing with complex sentences, our method is more suitable, while for simple extraction tasks, we suggest single-stage direct extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Introduction</head><p>NYT series NYT is based on the articles in New York Times. There are many derived datasets with better labeling. NYT10 <ref type="bibr" target="#b25">(Riedel et al., 2010)</ref> and NYT11 <ref type="bibr" target="#b11">(Hoffmann et al., 2011)</ref> label the complete entities. Moreover, NYT10-HRL and NYT11-HRL <ref type="bibr" target="#b28">(Takanobu et al., 2019)</ref> are better versions that are processed by optimizing the relation labels.</p><p>HacRED HacRED <ref type="bibr" target="#b4">(Cheng et al., 2021)</ref> 2 is a novel challenging extraction dataset. It analyzes the performance gap between popular datasets and practical applications, and carefully selects and designs more hard cases. HacRED consists of 65,225 relational facts annotated from 9,231 wiki documents with sufficient and diverse hard cases, which poses a very high challenge to many current complex extraction methods. SKE21 SKE19 3 is published by Baidu, and is currently the largest dataset available for complex relational triple extraction. Since its testing set is unpublished, and there are some errors in the validation set, a version named SKE21 is published by <ref type="bibr" target="#b37">Xie et al. (2021)</ref>. The testing set of SKE21 is carefully manually relabeled and contains 1,150 sentences and 2,765 annotated triples.</p><p>Wiki-KBP Wiki-KBP <ref type="bibr" target="#b20">(Ling and Weld, 2012</ref>) is based on the articles in Wikipedia. There're 1.5M sentences in training set which are automatically labeled using distant supervision and handcrafted patterns by <ref type="bibr" target="#b21">(Liu et al., 2017)</ref>, and the test set contains 289 sentences selected by the author of <ref type="bibr" target="#b24">(Ren et al., 2017)</ref> from the manual annotations in 2013 KBP slot filling results <ref type="bibr" target="#b9">(Ellis et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Details</head><p>Our experiments are conducted on two A800 GPUs. All deep models, including the LLMs and the evaluation model, are fine-tuned or implemented using the PyTorch framework. We employed AdamW optimizer as the optimizer. For the evaluation model, we first initialize the model with bert-base-cased and chinese-roberta-wwmext respectively, then train 20 epochs in English corpus task, and 40 epochs in Chinese. For the fine-tuning of LLMs, we randomly select 1500 items from the train set for each dataset and train 30 epochs. Our codes and hyper-parameters can be found at <ref type="url" target="https://github.com/Ding-Papa/Evaluating-filtering-coling24">https://github.com/Ding-Papa/  Evaluating-filtering-coling24</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dataset=sked</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instruction Template</head><p>Here we provide the instruction templates that guide the LLMs for relational triple extraction. First is the template for directly using the LLMs to perform extraction, i.e., the first stage of our method. Template for the first stage: Pre-define the following relation list r, please extract all triples containing the above relations from the given sentence S. Note that the relation name of the triple must be selected from the above list, and other relations not listed are not considered. Please output according to the specified format: [{"s": subject1, "o": object1, "p": relation1}, {"s": subject2, "o": object2, "p": relation2},...] (Optional) Here are some examples: ... Now given the following input, please complete the extracting task. Please output as many triples as possible that meet the requirements. Input:</p><p>S i , r i</p><p>In the second stage, the input of the LLMs consists of the first-stage extraction results and candidate pairs extracted by the evaluation model. The LLMs are prompted to recheck the original results, assign relations to the appropriate candidate entity pairs, and output the final extracted triples to complete the extraction. Template for the second stage: Pre-define the following relation list r. We want to extract all triples containing the above relations from the given sentence S. Here are the original extraction results A. Now we claim that the entity pairs that may be related in the above sentence are (s 1 , o 1 ), (s 2 , o 2 ), ... Please check the original results and fill in the missing triples, remove the wrong triples and output the final results. Constraints and output format are the same as stage 1. Please output according to the specified format.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot clinical information extractors</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Hegselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.12689</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple zero-shot prompt weighting technique to improve prompt ensembling in textimage models</title>
		<author>
			<persName><forename type="first">James</forename><surname>Urquhart Allingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Michael W Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><forename type="middle">Zhe</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="547" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HacRED: A large-scale relation extraction dataset toward hard cases in practical applications</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.249</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2819" to="2831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Insrl: A multi-view learning framework fusing multiple information sources for distantly-supervised relation extraction</title>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09370</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m">Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Span-level model for relation extraction</title>
		<author>
			<persName><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Linguistic resources for 2013 knowledge base population evaluations</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuansong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the feasibility of chatgpt for event extraction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03836</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Jeblick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balthasar</forename><surname>Schachtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Dexl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Mittermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">Theresa</forename><surname>Stüber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johanna</forename><surname>Topalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Wesp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Oliver Sabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ricke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Radiology</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with attentionbased span-specific and contextual semantic representations</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
	<note>Online International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LLM-blender: Ensembling large language models with pairwise ranking and generative fusion</title>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.792</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="14165" to="14178" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Surface pattern-enhanced relation extraction with global constraints</title>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4509" to="4540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-training improves prompt-based learning for large language models</title>
		<author>
			<persName><forename type="first">Hunter</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><forename type="middle">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="11985" to="12003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast inference from transformers via speculative decoding</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Leviathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="19274" to="19286" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entity-relation extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Heterogeneous supervision for relation extraction: A representation learning approach</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00166</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Tarek F Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ZS4IE: A toolkit for zero-shot information extraction with simple verbalizations</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoling</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-demo.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations</meeting>
		<imprint>
			<publisher>Seattle, Washington + Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hierarchical framework for relation extraction with reinforcement learning</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiexi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7072" to="7079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Does synthetic data generation of llms help clinical text mining</title>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04360</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">2023b. Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting relation extraction in the era of large language models</title>
		<author>
			<persName><forename type="first">Somin</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.868</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15566" to="15589" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TPLinker: Single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1572" to="1582" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10205</idno>
		<title level="m">Zero-shot information extraction via chatting with chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel cascade binary tagging framework for relational triple extraction</title>
		<author>
			<persName><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1476" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting the negative data of distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3572" to="3581" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08848</idno>
		<title level="m">Small models are valuable plug-ins for large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECAI</title>
		<meeting>ECAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zero-shot temporal relation extraction with ChatGPT</title>
		<author>
			<persName><forename type="first">Chenhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.bionlp-1.7</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="92" to="102" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05685</idno>
		<title level="m">Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
