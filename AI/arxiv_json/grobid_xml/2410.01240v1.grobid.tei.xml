<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic deductive coding in discourse analysis: an application of large language models in learning analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lishan</forename><surname>Zhang</surname></persName>
							<email>lishan.zhang3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Education</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
							<email>han.wu@mails.ccnu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Artificial Intelligence in Education</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoshan</forename><surname>Huang</surname></persName>
							<email>xiaoshan.huang@mail.mcgill.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Educational and Counselling Psychology</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country>Canada;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tengfei</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Artificial Intelligence in Education</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanxiang</forename><surname>Du</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Western Washington University</orgName>
								<address>
									<settlement>Washington</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic deductive coding in discourse analysis: an application of large language models in learning analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">206F0429DC307C31D7C92F06603739D3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language model</term>
					<term>discourse analysis</term>
					<term>deductive coding</term>
					<term>AI in education</term>
					<term>human-AI collaboration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deductive coding is a common discourse analysis method widely used by learning science and learning analytics researchers for understanding teaching and learning interactions. It often requires researchers to manually label all discourses to be analyzed according to a theoretically guided coding scheme, which is timeconsuming and labor-intensive. The emergence of large language models such as GPT has opened a new avenue for automatic deductive coding to overcome the limitations of traditional deductive coding. To evaluate the usefulness of large language models in automatic deductive coding, we employed three different classification methods driven by different artificial intelligence technologies, including the traditional text classification method with text feature engineering, BERT-like pretrained language model and GPT-like pretrained large language model (LLM). We applied these methods to two different datasets and explored the potential of GPT and prompt engineering in automatic deductive coding. By analyzing and comparing the accuracy and Kappa values of these three classification methods, we found that GPT with prompt engineering outperformed the other two methods on both datasets with limited number of training samples. By providing detailed prompt structures, the reported work demonstrated how large language models can be used in the implementation of automatic deductive coding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Discourse analysis investigates the functional use of language to perform actions and construct identities, focusing on the meaning conveyed rather than the structural aspects or surface features of the language <ref type="bibr" target="#b10">(Hjelm, 2021)</ref>. Discourse analysis can be accomplished based on text, spoken language and recordings. In learning sciences, it is used to understand teaching and learning interactions in class or after-class tutoring <ref type="bibr" target="#b24">(Rosé, 2017)</ref>, which provides valuable insights for scaffolding and facilitating learning process <ref type="bibr" target="#b31">(Wang, Yang, Wen, Koedinger, &amp; Rosé, 2015;</ref><ref type="bibr" target="#b17">Meyer, 2023)</ref>.</p><p>One common method for discourse analysis is deductive coding, which can transform qualitative analysis into quantitative analysis. In deductive coding, researchers first code each segment of discourses according to a predefined coding schema and then apply any statistical methods to analyze the coded results. This made discourse analysis easy to follow and more reproducible. However, the conduction of deductive coding is quite timeconsuming because researchers need to human label all discourses to be analyzed according to theory-informed coding schema.</p><p>To overcome the disadvantage of deductive coding, researchers started using natural language processing technologies to automate the coding process, thereby saving precious research time <ref type="bibr" target="#b23">(Rosé et al., 2008)</ref>. This shift towards automated deductive coding not only addresses the time constraints but also enhances efficiency in data analysis. We consider such methods of using natural language processing to code discourse as automatic deductive coding.</p><p>This analysis technique facilitated mining the sentiments embedded in the discussion discourses. Researchers are able track the learning sentiments embedded in huge discussion data with the help of natural language processing technology. Additionally, beyond the immediate benefit of saving research time, developing and refining such technology serves as a foundational step towards the creation of dialog-based intelligent tutoring agents <ref type="bibr" target="#b25">(Rosé &amp; Ferschke, 2016)</ref>.</p><p>From a technical perspective, automatic deductive coding shares similarities with automatic grading student's answers to open-response questions, which have been studied extensively <ref type="bibr" target="#b36">(Zhang, Huang, Yang, Yu, &amp; Zhuang, 2022)</ref>. They both aim to classify students' plain text into several predefined classes. This is a classical text classification task in natural language processing. The classes can be either coding labels for deductive coding tasks or grades for auto-grading tasks. Automatic grading has been studied for decades. With the advent of advanced natural language processing methods such as LSTM and BERT, auto-grading accuracy has significantly improved. In contrast, while learning analytics researchers use text classification techniques in emotion and sentiment analysis, these techniques do not seem to have a widespread impact on traditional discourse analysis. Researchers still often need to manually code all text instead of referring to automatic coding or text classification techniques. This is probably because such text coding tasks often cannot provide enough training data.</p><p>The recent achievement in large language models, particularly GPT made by OpenAI, has opened up a new way for automatic deductive coding. GPT demonstrates remarkable performance on general classification tasks with few examples, known as few-shot learning, or even without any samples, which are called as zero-shot learning. Some recent studies have begun to explore the use of LLM like GPT for tasks related to qualitative discourse analysis, specifically focusing on deductive coding. However, while these studies have illustrated LLM's potential, limitations remain, especially concerning the LLM's advantages comparing to other machine learning methods and the different design strategies of prompts.</p><p>Unlike previous research that relied primarily on expert-developed codebooks, we explore the integration of finetuning techniques and retrieval-augmented generation to enhance the model's performance in deductive coding. By introducing these advanced methods, we aim to improve the adaptability of LLM in qualitative research, reducing reliance on rigid codebooks and providing more flexibility in coding diverse data sets. Moreover, our comparison across traditional text classification, BERT-like models, and GPT-based LLM offers new insights into the relative performance of these models in different qualitative data environments, highlighting areas where LLM can outperform of complement existing methods.</p><p>By conducting the exploration and classification methods comparison, the study aims for answering the following two research questions:</p><p>(1) How do the three classification methods perform with the given two data sets? (2) How much improvements can we make by integrating the LLMs related techniques such as fine tuning and RAG?</p><p>The rest of the paper first briefly reviewed the existing works regarding sentiment analysis and auto-grading. These are the two fields where text classification algorithms have well proved their successes. Then we described two different data sets of our experiments. In the third, we introduced the three different text classification approaches with the emphasis in GPT approach. In the fourth, we reported the results with all the different settings. In the last, we concluded with remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Emotion and sentiment analysis</head><p>In the field of learning analytics, text classification technology is often employed for the analysis of students' emotions in participatory learning processes. Participatory learning, different from rigid teacher-controlled instruction, underscores the active involvement of students in collaborative learning processes. The key to successful participatory learning is to create positive experiences that enhance children's cognitive engagement and awareness <ref type="bibr" target="#b30">(Vartiainen, Tedre, &amp; Valtonen, 2020)</ref>. Emotions refers to a multi-componential construct of psychological subsystems, including affective, cognitive, motivational, expressive, and peripheral physiological processes <ref type="bibr" target="#b19">(Pekrun, 2006)</ref>. With the support of advanced technology, research on emotions and their role in cognitive processes within technology-rich learning environments has been gaining more attention (e.g., <ref type="bibr" target="#b12">Huang, Huang, &amp; Lajoie, 2022)</ref>. In technology-supported learning contexts, emotions can be expressed through emotional tones in the form of verbal or textual output, which refers to the vocal expression or dialogue of emotion that conveys a student's affective states <ref type="bibr" target="#b1">(Chang et al., 2023;</ref><ref type="bibr" target="#b13">Ishii, Reyes, &amp; Kitayama, 2003)</ref>. Emotions and sentiments interpreted from expressed emotional tones are associated with students' engagement, social interactions, and knowledge-sharing behaviors <ref type="bibr" target="#b5">(Dang-Xuan et al., 2017;</ref><ref type="bibr" target="#b18">Näykki et al., 2014;</ref><ref type="bibr" target="#b22">Rapisarda, 2002)</ref>, thereby shaping learners' overall learning experience. Automatically detecting learners' emotions form the foundation for understanding what keeps their learning experience positive and how to maintain such experiences. Therefore, it is important to automatically assess students' emotions and sentiments in real-time learning procedure with the assistance of machine learning and large language models. <ref type="bibr" target="#b14">Kastrati et al. (2020)</ref> devised a weakly supervised aspect-based sentiment analysis framework. Given student comments, they employed Convolutional Neural Network (CNN) for sentiment classification and Long Short Term Memory (LSTM) for aspect category recognition. Experiments were conducted using a substantial real-world education dataset, encompassing approximately 105K students' reviews gathered from Coursera, along with a dataset comprising 5,989 students' feedback in traditional classroom settings. CNN sentiment classification was applied for the binary classification of aspect sentiment, achieving 82.1% F1 score, while LSTM aspect category recognition was employed for identifying four aspect categories, achieving 86.3% F1 score. <ref type="bibr" target="#b4">Dahiya, Mohta and Jain (2020)</ref> performed sentiment and emotion analysis on textual messages with emoticons. Employing a CNN model, they trained a classifier to categorize 29,939 unique statements which are acquired from Kaggle into six distinct emotions and gave an average accuracy of 72.9%. <ref type="bibr" target="#b15">Klünder, Horstmann and Karras (2020)</ref> integrate sentiment analysis with tradition natural language processing techniques to automate sentiment classification in text-based communication. They utilized three machine learning methods-random forest, Support Vector Machine (SVM), and Naive Bayesto categorize each text segment as positive, neutral, or negative. The efficacy of this approach is substantiated through an industrial case study in software development. The case study comprises 1,947 messages extracted from a group chat within the Zulip communication tool, encompassing a total of 7,070 sentences. The ultimate classification results, reaching an accuracy of 62.97%, exhibit a level of effectiveness comparable to human ratings <ref type="bibr" target="#b15">(Klünder et al., 2020)</ref>.</p><p>Generally, datasets used for sentiment analysis are large and need a lot of human labeling for training. Even traditional methods such as random forest and SVM can sometimes satisfy the requirements. However, certain limitations may exist, such as the inability to integrate contextual information from the dialogue for assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Automatic grading for open-response questions</head><p>The grading of open-response questions is a critical aspect of educational assessments, requiring a balance between subjective evaluation and the need for efficient, scalable, and consistent grading methods. Grading for open-response questions involves the assessment of students' answers to questions that require free-form responses, as opposed to multiple-choice or other closed-ended formats. This context often demands more flexible and subjective grading methods in educational and examination settings. So traditional approach involves manual grading by educators or domain experts. To save repetitive human works and support personalized learning <ref type="bibr" target="#b7">(Erickson &amp; Botelho, 2021)</ref>, recent researches have focused on the development of automatic grading methods with machine learning and natural language processing technologies. <ref type="bibr" target="#b8">Erickson et al. (2020)</ref> employed tree-based machine learning approaches, including random forest and XGBoost, as well as deep learning methodologies such as LSTM and the Rasch Model, to assess and analyze open-response questions in mathematics. The dataset utilized in their study comprised a total of 141,612 student responses to 2,042 unique problems from 25,069 students. <ref type="bibr" target="#b34">Wilson et al (2022)</ref> conducted a comparative analysis employing three machine learning models including logistic regression, random forest, and K nearest neighbors. These algorithms are used to classify 2,450 student responses to open-ended questions in the Physical Measurement Questionnaire into four classes. <ref type="bibr" target="#b36">Zhang et al (2022)</ref> utilized the continuous bag-of-words model (CBOW) to integrate the domaingeneral and domain-specific information in the process of feature engineering. Then they built the classifier model using LSTM and evaluated it with 7 reading comprehension questions with over 16,000 labeled student answers.</p><p>Compared with other traditional automatic grading models, their proposed model significantly improved the automatic grading performance on semi-open-ended questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">LLM-based models for deductive coding</head><p>Building on the advancements in emotion and sentiment analysis, as well as the development of automatic grading systems for open-response questions, there has been growing interest in applying LLM to other complex tasks such as deductive coding in qualitative research. To offer new possibilities for handling large-scale qualitative data sets more efficiently, researchers have begun exploring how these models can assist or automate parts of this process. <ref type="bibr" target="#b35">Xiao et al (2023)</ref> find that combing LLM with expert-drafted codebooks achieves fair to substantial agreements with expert-coded results in deductive coding. Their study utilized an expert codebook to construct prompts, and although it provided transparency and explicit control, it limited the performance of the model. <ref type="bibr" target="#b28">Tai et al (2023)</ref> proposed a methodology using LLM to support traditional deductive coding in qualitative research. They compared the performance of a large language model with traditional human coding in identifying five conceptual codes in three narrative texts, providing a systematic and reliable platform for code identification and offering a means of avoiding analysis misalignment. They also pointed that there is a lack of validated research examining the use of LLM in qualitative analysis. <ref type="bibr" target="#b2">Chew et al (2023)</ref> provide a holistic approach for performing deductive coding with LLM, and aims to assess the effectiveness of GPT-3.5 across a range of deductive coding tasks through an in-depth case study and empirical evaluation on four publicly available datasets. The study noted several limitations, including the need for extensive prompt engineering and the assessment of coding performance across a wide variety of LLM. <ref type="bibr" target="#b11">Hou et al (2024)</ref> explored the use of LLM to assist in deductive coding of social annotation data, achieving fair to substantial agreement with human raters in context-independent dimensions and moderate agreement in context-dependent dimensions. They claimed that there were still some challenges of including original text in the prompt engineering or fine-tuning models for context-dependent dimensions.</p><p>In summary, existing works regarding text classification adoption in education mainly focus on sentiment analysis in participatory learning and automatic grading on student answers. Both of these works required considerable large data set for training the supervised machine learning models, so that satisfactory results can be achieved. Only a few studies have attempted to use LLM for deductive coding in content analysis. Our study aims to further verify that generalist foundation models such as GPT and their combination with other technologies can be helpful in deductive coding tasks with few labeled items. In particular, we took two different datasets to conduct our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>To conduct our study and evaluate the three automatic coding techniques, we used two datasets that were obtained from a Chinese poetry appreciation literary course at a university in central China. The course was open for science and engineering students, aiming to improve their literacy ability and cultivate their aesthetic ability. The course adopted a face-to-face collaborative teaching model and the classroom students were divided into groups. Some preclass or in-class tasks require students to complete through a collaborative annotation platform developed by our research team. Before class, the teacher assigned reading tasks, requiring students to read learning materials on the platform and highlight and annotate the materials. This formed the annotation dataset. During class, students used the chat section of the platform to interact within groups based on in-class tasks and collaborate to solve problems. This formed the discussion dataset. We described the two datasets and the coding schema in detail below.</p><p>Each dataset was split into training and testing sets using an 8:2 ratio. The training set was to train the machine learning models or tune the prompts. The testing set was to evaluate the performance of different automatic coding techniques. In this way, all the automatic coding models used the exact same dataset for model calibration and evaluation. Note that the GPT approach only used several items in the training set and the other machine learning approaches used the entire training set. However, such settings formed fair comparisons for all the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Annotation dataset</head><p>In the fall semester of 2021, we collected a dataset comprising 607 student annotations. This dataset was divided into a training set containing 484 pieces of data and a testing set containing 123 pieces. Seventy-three students participated in the course, representing diverse majors such as artificial intelligence, history, psychology, physics, etc.</p><p>Students were asked to read and annotate a Chinese article titled "The Image of Plum in Ancient Chinese Literature" on a reading annotation system developed by our research team. The annotation reading interface of the system is shown in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1</head><p>The annotation reading interface of the platform After collecting the data, two researchers manually analyzed and coded the annotation data according to the cognitive engagement classification scheme adapted from <ref type="bibr" target="#b3">Chi and Wylie (2014)</ref>. After randomly selecting 200 items from the annotation data, the level of agreement between the two researchers was measured at 0.752 Kappa, indicating good reliability. Cognitive engagement of the annotation data was analyzed manually based on the coding scheme of Table <ref type="table" target="#tab_0">1</ref>. "In the ancient poems I've studied before, there were "red beans" symbolizing love and "broken willows" conveying homesickness. Now, I've come to realize that "plum blossoms" can also represent friendship, love, and hometown sentiments."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Discussion dataset</head><p>The discussion dataset, comprising a total of 404 pieces of discussion data, was gathered in the fall semester of 2022. This dataset was subsequently divided into a training set containing 320 pieces of data and a testing set containing 84 pieces. Seventy-two students, voluntarily enrolled in the course, participated in this dataset. The students had an average age of 20 and came from various non-literary majors. The students were grouped based on the Kolb Learning Style Questionnaire <ref type="bibr" target="#b16">(Manolis, Burns, Assudani, &amp; Chinta, 2013)</ref>, resulting in a total of 10 groups with 6-7 students in each.</p><p>Students worked in groups to select one of the five poets and analyzed their representative works of chrysanthemum poems through text communication using the same system. The discussion interface of the system is shown in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2</head><p>The discussion interface of the platform Following data collection, two researchers conducted manual coding and analysis of the discussion data. The coding scheme, a modified iteration of Chi's cognitive engagement framework <ref type="bibr" target="#b3">(Chi &amp; Wylie, 2014)</ref>, was tailored to students' discussion tasks and learning characteristics. In the initial phase, two researchers independently selected data from two groups for coding, amounting to a total of 84 instances, achieving an inter-rater reliability of 0.69. Following this, the researchers refined the coding framework and deliberated on the initially coded data. Through negotiation and consensus-building, they proceeded with a second round of pre-coding. In this subsequent round, two groups, comprising a total of 69 instances, were once again randomly chosen, resulting in an enhanced interrater reliability of 0.80. The cognitive engagement coding framework employed by researchers during the manual analysis of the discussion data is detailed in Table <ref type="table" target="#tab_1">2</ref>. "No way, didn't he later still have confidence in joining the court?"</p><p>The three automatic coding techniques were evaluated by following the procedure shown in Figure <ref type="figure">3</ref>. The manually coded results were divided into training and testing datasets. The three automatic classification methods, including the traditional text classification method, BERT-like pretrained language model and GPT-like pretrained language, were applied to classify the annotation and discussion data. The classification methods are detail described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3</head><p>The experiment procedures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>This section briefly introduced the three different approaches of text classification for automatic deductive coding. In this study, we considered two different types of discourses. One is students reading annotation, the other is student discussion dialog. Since that the purpose of the comparison is to explore the power of large language models, we respectively selected one representative model for traditional machine learning and BERT-like methods. The rest of the section described these two models and how we used GPT on deductive discourse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Traditional machine learning method</head><p>We selected Random Forest (RF) as the representative model for traditional machine learning method, because RF has proved to be well performed in many related tasks <ref type="bibr" target="#b26">(Schonlau &amp; Zou, 2020)</ref>. Figure <ref type="figure">4</ref> illustrates the process of using traditional machine learning method for classification. The very first step is to conduct word segmentation to transform each student sentence into a set of words. We used jieba library to perform this step. Note that such a transformation would lose the sequential information of the original sentence as well as the grammatical structures <ref type="bibr" target="#b20">(Qader, Ameen, &amp; Ahmed, 2019)</ref>. After the sentences were split into sets of words, the frequency of each word can be calculated and the corresponding frequencies of the words used as the inputs of Random Forest. This kind of feature engineering approach is called as Bag-of-Words (BoW). Random Forest is essentially a group of decision trees. Each decision node of the decision trees was comprised of word frequency and a threshold. We used RandomForestClassifier in scikit-learn to perform the implementation. There were two hyper-parameters of this algorithm, which were estimator and random feed. Estimator defines the upper bound of the number of decision trees in the forest and the random feed decides initial values of the parameters. The estimators was set to 100 and the random feed was set to 42.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>Process of Random Forest classification method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">BERT-like pretrained model for classification</head><p>For BERT-like pretrained models, we used RoBERTa, an advanced version of BERT, enhanced through training on a larger corpus <ref type="bibr" target="#b27">(Shaheen, Wohlgenannt, &amp; Filtz, 2020)</ref>. Notably, <ref type="bibr" target="#b21">Qasim et al (2022)</ref> demonstrated the superior performance of the RoBERTa model in various classification tasks compared to other pre-trained language models. However, they also highlighted that RoBERTa exhibits stronger linguistic bias.</p><p>Figure <ref type="figure">5</ref> shows the process of using RoBERTa for classification. The initial step involves loading the dataset and tokenizing it using RoBERTa's tokenizer. This process converts the text into numerical representations suitable for machine learning. In specific, we utilized the chinese_roberta_wwm_ext_pytorch pre-trained model. Different pooling strategies, including CLS pooling, mean pooling, and max pooling, are implemented to extract features from the hidden states of the RoBERTa model. These strategies contribute to capturing essential information from the input sequences. A linear classifier head is incorporated into the model to translate the extracted features into target categories. The model is trained using the Adam optimizer with a learning rate of 2e-5 and a weight decay of 1e-4. The training is conducted over 5 epochs. A batch size of 16 is employed during the training process. The maximum sequence length for tokenization is set to 200. Sentences exceeding this length are truncated, while shorter sentences are padded with spaces to ensure uniform input dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process of RoBERTa classification method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">GPT-like pretrained model for classification</head><p>Despite the proliferation of various generative artificial intelligence models, GPT developed by OpenAI stands out as the most representative and proficient in multiple aspects. In many cases, GPT can perform well with only the instructions given in the prompts. To better instruct GPT, researchers often need to go through a process called as prompt engineering. Prompt engineering is the means by which GPT are programmed using prompts <ref type="bibr" target="#b33">(White et al., 2023)</ref>. Few-shot learning and chain of thought (CoT) are two well known and useful techniques in prompt engineering. Few-shot learning aims to enable models to learn and adapt to new tasks from a limited number of samples <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>, while CoT achieves this by breaking texts into continuous segments and utilizing them as model inputs <ref type="bibr" target="#b32">(Wei et al., 2022)</ref>. Recently, another technique is called as retrieval-augmented generation (RAG) is has emerged that can integrate domain knowledge into general large language models, thereby improving the quality and effectiveness of the generated results <ref type="bibr" target="#b9">(Gao et al., 2023)</ref>. Besides prompting engineering, fine-tuning is another method of calibrating the large language models to fit the downstream tasks <ref type="bibr" target="#b29">(Touvron et al., 2023)</ref>. But this method required much more labeled samples for training the models.</p><p>In this study, we considered all the techniques mentioned above, including few-shot learning, CoT, RAG and finetuned. In addition, we combined traditional natural language processing techniques with GPT for further improvement.</p><p>In terms of the foundation model, this study mainly employed GPT-4, recognized as one of the most powerful generative AI models. The fine-tuned foundation model was GPT 3.5 because OpenAI did not support fine-tuning GPT-4. To optimize the prompts, we used GPT4's playground for prompt formulation and refinement. When we were satisfied in the playground, we used the final prompt we got to fabricate the backend code, which went through all the discourse data and called OpenAI's API to process it. We used four different settings in the GPT approach to explore the usefulness of the techniques. Because the dataset we used was in Chinese, all prompts were also written in Chinese in our actual program no matter the setting. We translated them into English for ease of reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Prompt only</head><p>In this method, we exclusively employed prompts and GPT-4 for deductive coding. During the prompt tuning process in the playground, we drafted a prompt based on the dataset characteristics. Through iterative refinement, our final prompt consisted of the five parts below (a full version sample prompt is attached in the appendix):  【Introduction to the Course Background】  【Issuance of Instructions】  【Detailed Introduction to Encoding Rules】  【Output Structure and Examples】  【Input data】 At first, the prompt introduced the background of the course where the student discourse was generated. Then the prompt described what GPT needed to do (i.e. conduction of deductive coding). In the third, the prompt provided the detailed codebook. In the following, the prompt described the desired format of the output. Several coding examples were also provided here as the implementation of few-shot learning techniques. In specific, we manually selected three representative student discourse examples and told GPT the corresponding human labels. In the last part of the prompt, we gave the input data that needs to be coded. This method was only used for the annotation dataset. So, the input data comprised the annotation along with its highlighting text.</p><p>Regarding GPT settings, we used the gpt-4-1106-preview version with a temperature value of 0, a maximum text length of 4096, frequency_penalty set to 0, and presence_penalty set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Finetuning</head><p>We performed fine-tuning based on the GPT-3.5-Turbo model. The basic idea of fine-tuning is to use many labeled examples as the training data to calibrate the foundation model, so that the fine-tuned model can adapt to the specific downstream task, which is deductive coding in our case. As the documents of GPT suggested, around 100 trainning samples would be sufficient for the fine-tuning task of GPT. This method was applied to both annotation and discussion datasets.</p><p>For the annotation dataset, we used the same prompts as those in the previous setting (i.e. prompt only). We used 90 entries to build fine-tuned model and evaluated its performance. The training epoch was set to 3.</p><p>For the discussion dataset, because we had 10 sets of dialogs generated by 10 groups of students with 404 dialog turns in total, we asked GPT to consider the dialog turns independently without the context and did fine-tuning. We used 100 entries to build fine-tuned model. The training epoch was set to 3 as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Prompt + traditional NLP</head><p>As <ref type="bibr" target="#b6">Do et al (2024)</ref> suggested, the overall performance can be improved by integrating prompt based LLMs and traditional NLP technologies. So, we did such combinations in our automatic deductive coding as well. In specific, we built a reference database including the original reading material and the corresponding online reference information. During the process of automatic coding, instead of handling over the entire task to GPT, we did similar sentence checking at first. It means that for each input sentence of students' annotations, we wrote program to find out the most similar sentence in the reference database. The similarity between two sentences is defined as the follows:</p><formula xml:id="formula_0">, = =1 × =1 2 × =1 2 # 1</formula><p>If the similarity score between a student's annotation and the most similar sentence in the reference database exceeded a threshold, the program coded the annotation as "A", otherwise, the program asked GPT to make further judgement. The structure of the prompt for the further judgement remained the same, except for the absence of the introduction to the code "A".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Prompt with context knowledge + traditional NLP</head><p>At last, we further included context knowledge in the prompt. We used different strategies for the two different datasets. We used RAG with the reference database mentioned in the previous section for the annotation dataset. We injected the full dialog context for the discussion dataset. We respectively described the two strategies in the following.</p><p>For the annotation dataset, we retrieved two relevant sentences of the input annotation from the reference database based on their sentence similarities. We used the retrieved information to augment the generated results of GPT, in which retrieval augmented generation (RAG) was implemented. Note that the traditional NLP technique was used again in the calculation of sentence similarity. Different from the prompts introduced previously, prompts here were constructed dynamically based on the reference sentences. The contents of the prompts were also slightly different because we asked GPT to break the deductive coding process into two steps inspired by CoT. In specific, GPT needed to first consider the relation of the student's annotation to the two reference sentences, then make the coding decision. Figure <ref type="figure">6</ref>  The workflow of automatic deductive coding for annotation data For the discussion dataset, we included the entire dialog of the discussion group as the context knowledge and asked GPT to code each dialog turn. GPT was instructed to consider the surrounding dialog turns while determining the encoding categories. The framework of the classification prompt for this dataset is as follows:  【Introduction to the Course Background】  【Issuance of Instructions】  【Detailed Introduction to Encoding Rules】  【Output Structure and Examples】  Student dialogs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation metrics</head><p>In this section, we described the evaluation metrics employed to assess the performance of the automatic deductive coding approaches. We used accuracy, precision, recall, F1 score, and Cohen's kappa to evaluate and compare the approaches. The first four metrics are widely used in computational-related journals and the last one is used widely in psychology journals.</p><p>Accuracy represents the ratio of correctly predicted instances to the total instances in the dataset. It provides an overall measure of the model's correctness. Precision is the ratio of correctly predicted positive observations to the total predicted positives. It measures the accuracy of positive predictions. Recall, also known as sensitivity or true positive rate, is the ratio of correctly predicted positive observations to the actual positives in the dataset. It assesses the model's ability to capture all relevant instances. The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives. Cohen's Kappa is a statistic that measures the agreement between the predicted and actual classifications, considering the possibility of the agreement occurring by chance. It corrects for the chance agreement inherent in accuracy. High accuracy, precision, recall, and F1 score values all indicate effective classification performance. Kappa values indicate the amount of agreement between the predicted results and the ground truth. Values close to 1 indicate substantial agreement, while 0 suggests no agreement. This comprehensive set of evaluation metrics enables a thorough analysis of the classification methods, shedding light on their efficiency in handling annotation and discussion datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussion</head><p>In this section, we reported the results of the three different approaches for automatic deductive coding and make a discussion on the results. For each approach, we reported the coding results of annotation and discussion datasets respectively. We made a summary of the three approaches by the end of this section. Among the three approaches, we focused on the GPT-based one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Traditional machine learning (random forest)</head><p>For the annotation dataset, the random forest classification method achieved an overall accuracy of 0.56. This accuracy underscores a moderate yet promising success in accurately assigning coded annotations to their respective categories. The Kappa value was found to be 0.28. We calculated precision, recall, and F1-score for each code. The results were reported in Table <ref type="table">3</ref>.</p><p>Table 3 Performance Metrics for Annotation Data precision recall f1-score support A 0.56 0.33 0.42 30 C1 0.55 0.89 0.68 53 C2 0.63 0.30 0.41 40 Macro avg 0.58 0.51 0.50 123 Weighted avg 0.58 0.56 0.53 123</p><p>As for the discussion dataset, the random forest classification method exhibited a lower overall classification accuracy of 0.48. The Kappa value for the discussion data was 0.32, indicating slight agreement beyond chance. This discrepancy in accuracy between the two datasets emphasizes the method's varying performance when applied to different types of data. Similarly, precision, recall and F1-score were reported in Table <ref type="table">4</ref>.</p><p>Table 4 Performance Metrics for Discussion Data precision recall f1-score support M 1.00 0.62 0.76 13 P 1.00 0.25 0.40 20 A 0.32 1.00 0.49 12 C 0.45 0.54 0.49 28 I 0.00 0.00 0.00 11 Macro avg 0.56 0.48 0.43 84 Weighted avg 0.59 0.48 0.45 84</p><p>The results clearly showed that the random forest classifier did not provide good performance in general and had quite different performance on different coding categories. This approach exhibited an especially bad performance in the Interaction (I) coding category in the discussion dataset. This was too surprising because this approach simply treated all discourses as bags of words and made deductive coding decisions solely based on the frequencies of occurrences of these words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">BERT-based (RoBERTa)</head><p>For the annotation dataset, the overall accuracy was determined to be 0.59, with a Kappa value of 0.36. Precision, recall, F1-score for each coding category was reported in Table <ref type="table" target="#tab_4">5</ref>. As a more advanced NLP technique, RoBERTa demonstrated improved performance in both annotation and discussion datasets. However, this approach seemed to produce biased results more easily. For example, the trained automatic coder did not identify any M or I code. Probably because its computational model was complex and easy to be overfitted by a small amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">GPT-based</head><p>For the annotation dataset, we ran four experiments with the automatic coding methods including prompt-only, finetuning, prompt +NLP, and prompt with context + NLP. We reported the four results in the following.</p><p>Results of prompt-only method. The overall accuracy was quite low, at 0.37. Given that it was essentially a threefold classification problem, the performance was just a little bit better than chance. The Kappa value was only 0.005. So, using prompt alone was not enough for accomplishing our deductive coding task. Table <ref type="table">7</ref> provides more details of the results for each coding category.</p><p>Table 7 Annotation classification metrics of prompt only experiment precision recall f1-score support A 0.29 0.17 0.21 30 C1 0.42 0.45 0.44 53 C2 0.33 0.40 0.36 40 Macro avg 0.35 0.34 0.34 123 Weighted avg 0.36 0.37 0.36 123</p><p>Results of fine-tuning method. The performance of automatic deductive coding is significantly improved when the model after finetuned, even with a weaker GPT-3.5-Turbo version. The accuracy was improved to 0.54 with a Kappa of 0.28. The precision, recall and F1-score for each coding category were detailed in Table <ref type="table" target="#tab_6">8</ref>. Results of prompt+NLP method. When the active coding category was identified by the calculation of sentence similarities, the overall performance of prompt + NLP was improved compared with the prompt-only method. The overall accuracy was 0.46 and the Kappa was 0.164. Table <ref type="table">9</ref> provides the performance details for each coding category. We also combined NLP technique here with the fine-tuned model. In a result, the overall accuracy was 0.63 and the Kappa was 0.43.</p><p>Table 9 Annotation classification metrics with NLP method precision recall f1-score support A 0.85 0.73 0.79 30 C1 0.45 0.49 0.47 53 C2 0.23 0.23 0.23 40 Macro avg 0.51 0.48 0.49 123 Weighted avg 0.47 0.46 0.47 123</p><p>Results of prompt with context + NLP method. The performance of automatic deductive coding achieved the best result when the context and NLP were both involved. The overall accuracy was 0.71 and the Kappa was 0.54. Detailed performance for each coding category was presented in Table <ref type="table" target="#tab_0">10</ref>.</p><p>Table 10 Annotation classification metrics using GPT-4 precision recall f1-score support A 0.85 0.73 0.79 30 C1 0.68 0.83 0.75 53 C2 0.66 0.53 0.58 40 Macro avg 0.73 0.70 0.70 123 Weighted avg 0.71 0.71 0.70 123</p><p>For the discussion dataset, we ran two experiments with the automatic coding methods including fine-tuning and prompt with context + NLP.</p><p>Results of fine-tuning method. Remind that the fine-tuned model considered each turn in the dialogs independently. Without such context information, the corresponding automatic coding performance can still achieve an accuracy at 0.73 and the Kappa was 0.64. The detailed results of precision, recall and F1 score for each coding category were described in Table <ref type="table" target="#tab_9">11</ref>. Table 12 Discussion classification metrics using GPT4 precision recall f1-score support M 1.00 0.62 0.76 13 P 0.83 0.95 0.88 20 A 0.80 1.00 0.89 12 C 0.67 0.86 0.75 28 I 1.00 0.18 0.31 11 Macro avg 0.86 0.72 0.72 84 Weighted avg 0.82 0.77 0.75 84</p><p>Based on the reported results above, the same method had quite different performances on the two different datasets.</p><p>It is probably because that writing annotation itself is more complex than discussing to solve in-class tasks within groups. Students usually do not think too much before writing down their opinions and can even have many casual talks during discussions. In addition, contextual information in discussion is easy to obtain. It is the discourses of the sibling turns of the dialogs. In contrast, while students are making reading annotations, students need to first select and highlight one or two sentences that they think are interesting in the reading material. Then, the students may integrate the highlighted information with their own thoughts to make the annotations. Some students may even web search for outside knowledge to make annotations of better qualities. Indeed, some students tend to simply copy and paste the information they searched. We found out we could easily identify these cases via sentence similarity calculation, which is a widely used NLP technique.</p><p>Due to the nature of the complexity of annotation coding, we ran four experiments with different methods on the annotation dataset. We summarized and illustrated the performances of four different settings in Figure <ref type="figure">7</ref>. We can notice that fine-tuning, the combination of NLP, and providing context information all benefit the coding performance. So, it seems necessary to integrate all suitable techniques to dynamic construct prompts for automatic deductive coding instead of only relying on LLMs with static prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7</head><p>Comparison of Classification Metrics for Annotation Data across All Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2:</head><p>The power of prompt techniques for automatic deductive coding We need to note that the GPT-based approach is not just plug-and-play for automatic deductive coding. Indeed, ChatGPT provides an easy-to-use interface for all. Even people without any programming experience can use ChatGPT to accomplish some sophisticated tasks like article summarization and data analysis. However, as we showed in the results of the different methods of the GPT-based approach, the performance can be significantly improved when techniques such as RAG and CoT are included because GPT needs contextual information to make more accurate decisions. We also showed that we may need to figure out some rules with traditional NLP techniques like discourse similarity calculation to identify some discourse code, instead of having GPT taking over all the tasks. The reasons behind such integration are probably because we as human experts know more contextual information and guidance than LLMs, and we should describe them as much as possible by using all kinds of LLMs-related techniques such as RAG, CoT, few-shots and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In conclusion, this study aimed to assess and compare three distinct text classification methods applied to automatic deductive coding. The methods we adopted encompassed traditional text classification with feature engineering, BERT-like pre-trained language models, and GPT-like pretrained language models, representing generative language models.</p><p>The traditional text classification method, employing feature engineering struggled to adapt to the case of studentgenerated content in both annotation and discussion datasets. On the other hand, the BERT-like model exhibited improved accuracy, leveraging its contextual understanding of language. However, its reliance on tokenized input and the need for substantial data size and computational resources limit its practicality. The standout performer in our study was the GPT-based approach. This approach showcased remarkable adaptability and effectiveness in classifying both annotation and discussion data, outperforming other methods in terms of accuracy and Kappa values. The generative language model, with its inherent ability to consider word order and context, demonstrated promising results even with a limited dataset.</p><p>The comparison highlighted the potential of generative language models. The GPT-like model, in particular, presents a compelling avenue for further exploration in educational technology, showcasing promising results in the context of student participatory learning. As the field evolves, leveraging such models could bring more efficient and accurate ways to assess and engage with student-generated content, ultimately enhancing the quality of educational processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 6</figDesc><graphic coords="12,99.00,286.56,414.00,151.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="5,98.52,220.32,415.08,153.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="6,135.36,289.32,341.40,219.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="8,139.20,70.92,333.84,286.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="9,196.92,70.92,218.16,299.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="17,87.88,69.03,436.30,246.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The coding scheme of annotation data</figDesc><table><row><cell>Code</cell><cell>Behaviors</cell><cell>Descriptions</cell><cell>Exemplar</cell></row><row><cell>A</cell><cell>Copy</cell><cell>Highlight and directly/selectively</cell><cell>"During the Six Dynasties period, plum blossoms</cell></row><row><cell></cell><cell></cell><cell>copy ideas from material.</cell><cell>served as symbols of friendship, love, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell>hometown sentiments, yet they had not yet freed</cell></row><row><cell></cell><cell></cell><cell></cell><cell>themselves from the metaphorical expressions of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>comparison."</cell></row><row><cell>C1</cell><cell cols="2">Construction Make inferences, generalizations</cell><cell>"I believe this is inseparable from the</cell></row><row><cell></cell><cell></cell><cell>or summaries based on the</cell><cell>characteristics of plum blossoms. Blooming in the</cell></row><row><cell></cell><cell></cell><cell>highlighted content</cell><cell>harsh winter, preceding the myriad flowers, they</cell></row><row><cell></cell><cell></cell><cell></cell><cell>stand alone in ushering in spring. This resilience in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>blooming independently in the cold resonates with</cell></row><row><cell></cell><cell></cell><cell></cell><cell>many poets who have faced adversity in their careers</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and lives, yet refuse to conform to worldly</cell></row><row><cell></cell><cell></cell><cell></cell><cell>impurities."</cell></row><row><cell>C2</cell><cell>Integration</cell><cell>Highlight, and integrate other</cell><cell></cell></row><row><cell></cell><cell></cell><cell>information in the material or</cell><cell></cell></row><row><cell></cell><cell></cell><cell>other materials for comparison</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and connection, etc.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>The coding scheme of discussion dataThe term "wild jackals offering to the moon" refers to the first solar term during the Frost's Descent period, where jackals ceremoniously present their prey before consuming it. Ancient people believed this behavior to be a form of wild jackals performing a ritual to the moon."</figDesc><table><row><cell>Code</cell><cell>Descriptions</cell><cell>Exemplar</cell></row><row><cell>M</cell><cell>Assigning, coordinating, and</cell><cell>"Sure, let's start with the translation, shall we?"</cell></row><row><cell></cell><cell>supervising tasks.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>For the discussion dataset, the automatic coding results indicate an overall accuracy of 0.67, accompanied by a Kappa value of 0.54, showcasing a robust performance. The class-specific coding results are presented in Table6.</figDesc><table><row><cell cols="3">Annotation classification metrics using RoBERTa</cell><cell></cell><cell></cell></row><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>f1-score</cell><cell>support</cell></row><row><cell>A</cell><cell>0.64</cell><cell>0.47</cell><cell>0.54</cell><cell>30</cell></row><row><cell>C1</cell><cell>0.66</cell><cell>0.75</cell><cell>0.70</cell><cell>53</cell></row><row><cell>C2</cell><cell>0.47</cell><cell>0.47</cell><cell>0.48</cell><cell>40</cell></row><row><cell>Macro avg</cell><cell>0.59</cell><cell>0.57</cell><cell>0.57</cell><cell>123</cell></row><row><cell>Weighted avg</cell><cell>0.59</cell><cell>0.59</cell><cell>0.59</cell><cell>123</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Discussion classification metrics using RoBERTa</cell><cell></cell><cell></cell></row><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>f1-score</cell><cell>support</cell></row><row><cell>M</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>13</cell></row><row><cell>P</cell><cell>0.90</cell><cell>0.95</cell><cell>0.93</cell><cell>20</cell></row><row><cell>A</cell><cell>0.92</cell><cell>0.92</cell><cell>0.92</cell><cell>12</cell></row><row><cell>C</cell><cell>0.51</cell><cell>0.93</cell><cell>0.66</cell><cell>28</cell></row><row><cell>I</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>11</cell></row><row><cell>Macro avg</cell><cell>0.47</cell><cell>0.56</cell><cell>0.50</cell><cell>84</cell></row><row><cell>Weighted avg</cell><cell>0.52</cell><cell>0.67</cell><cell>0.57</cell><cell>84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Results</figDesc><table><row><cell cols="4">of the fine-tuning model for annotation classification of prompt only</cell><cell></cell></row><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>f1-score</cell><cell>support</cell></row><row><cell>A</cell><cell>0.61</cell><cell>0.67</cell><cell>0.63</cell><cell>30</cell></row><row><cell>C1</cell><cell>0.59</cell><cell>0.64</cell><cell>0.61</cell><cell>53</cell></row><row><cell>C2</cell><cell>0.38</cell><cell>0.30</cell><cell>0.33</cell><cell>40</cell></row><row><cell>Macro avg</cell><cell>0.52</cell><cell>0.54</cell><cell>0.53</cell><cell>123</cell></row><row><cell>Weighted avg</cell><cell>0.52</cell><cell>0.54</cell><cell>0.53</cell><cell>123</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11</head><label>11</label><figDesc>Results of the fine-tuning model for five-class classification of discussion dataWhen the entire dialog was included the prompt, the accuracy of automatic deductive coding reached 0.77, with a Kappa value of 0.72. This overall performance is acceptable for interagreement. The detailed results for each coding category is presented in Table12.</figDesc><table><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>f1-score</cell><cell>support</cell></row><row><cell>M</cell><cell>0.67</cell><cell>0.92</cell><cell>0.77</cell><cell>13</cell></row><row><cell>P</cell><cell>0.90</cell><cell>0.90</cell><cell>0.90</cell><cell>20</cell></row><row><cell>A</cell><cell>0.77</cell><cell>0.83</cell><cell>0.80</cell><cell>12</cell></row><row><cell>C</cell><cell>0.64</cell><cell>0.75</cell><cell>0.69</cell><cell>28</cell></row><row><cell>I</cell><cell>1.00</cell><cell>0.00</cell><cell>0.00</cell><cell>11</cell></row><row><cell>Macro avg</cell><cell>0.79</cell><cell>0.68</cell><cell>0.63</cell><cell>84</cell></row><row><cell>Weighted avg</cell><cell>0.77</cell><cell>0.73</cell><cell>0.68</cell><cell>84</cell></row><row><cell cols="2">Results of prompt with context + NLP.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Summary of the results</head><p>In this section, we summarized the results of all three automatic deductive coding approaches on annotation and discussion datasets. Because we used several different methods in the GPT-based approach, we selected the bestperformed one with context information and NLP integrated. The Kappa and the accuracies of the three approaches are listed in Table <ref type="table">13</ref> and Table <ref type="table">14</ref> respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1: Performance of the three classification methods</head><p>Randon Forest is the earliest AI algorithm for automatic discourse analysis among the three approaches and GPT is the most cutting-edge one. In general, the more advanced AI approaches produce better performance. Another thing that needs to be noted is that the GPT-based approach, except for the fine-tuned method, only used several labeled samples for writing up the prompt, although we allocated hundreds of labeled training samples for the sake of fair comparison. So, the biggest advantage of the GPT-based approach is not its higher accuracy and kappa, but the low requirement of training samples. This advantage can make the GPT-based approach have value in practice.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotional tones of voice affect the acoustics and perception of Mandarin tones</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plos One</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">283635</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bollenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14924</idno>
		<title level="m">LLM-assisted content analysis: Using large language models to support deductive coding</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The ICAP Framework: Linking Cognitive Engagement to Active Learning Outcomes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wylie</surname></persName>
		</author>
		<idno type="DOI">10.1080/00461520.2014.965823</idno>
	</analytic>
	<monogr>
		<title level="j">Educational Psychologist</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text classification based behavioural analysis of whatsapp chats</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the 5th International Conference on Communication and Electronics Systems (ICCES)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020. June</date>
			<biblScope unit="page" from="717" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An investigation of influentials and the role of sentiment in political communication on Twitter during election periods. Paper presented at Social Media and Election Campaigns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dang-Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stieglitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wladarsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neuberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
			<biblScope unit="page" from="168" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic Prompt Selection for Large Language Models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02717</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botelho</surname></persName>
		</author>
		<title level="m">International Conference on Educational Data Mining</title>
		<imprint>
			<date type="published" when="2021-01">2021. January</date>
		</imprint>
	</monogr>
	<note>Is it fair? Automated open response grading</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The automated grading of student open responses in mathematics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcateer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varatharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge</title>
		<meeting>the Tenth International Conference on Learning Analytics &amp; Knowledge</meeting>
		<imprint>
			<date type="published" when="2020-03">2020, March</date>
			<biblScope unit="page" from="615" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.10997</idno>
		<title level="m">Retrieval-augmented generation for large language models: A survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Discourse analysis. Paper presented at the Routledge Handbook of Research Methods in the Study of Religion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hjelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Routledge</publisher>
			<biblScope unit="page" from="229" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prompt-based and Fine-tuned GPT Models for Context-Dependent and-Independent Deductive Coding in Social Annotation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Learning Analytics and Knowledge Conference</title>
		<meeting>the 14th Learning Analytics and Knowledge Conference</meeting>
		<imprint>
			<date type="published" when="2024-03">2024. March</date>
			<biblScope unit="page" from="518" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring teachers&apos; emotional experience in a TPACK development task</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Technology Research and Development</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1283" to="1303" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spontaneous attention to word content versus emotional tone: Differences among three cultures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kitayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised framework for aspect-based sentiment analysis on students&apos; reviews of MOOCs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kastrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="106799" to="106810" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying the mood of a software development team by analyzing text-based communication in chats with machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klünder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Horstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Centered Software Engineering: 8th IFIP WG 13.2 International Working Conference, HCSE 2020</title>
		<meeting><address><addrLine>Eindhoven, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020-11-30">2020. November 30-December 2, 2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="133" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessing experiential learning styles: A methodological reconstruction and validation of the Kolb Learning Style Inventory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Manolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Assudani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chinta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Using instructional discourse analysis to study the scaffolding of student self-regulation. Paper presented at Using Qualitative Methods to Enrich Understandings of Self-regulated Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Meyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Routledge</publisher>
			<biblScope unit="page" from="17" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Socio-emotional conflict in collaborative learning-A processoriented case study in a higher education context</title>
		<author>
			<persName><forename type="first">P</forename><surname>Näykki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Järvelä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Järvenoja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Educational Research</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pekrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="315" to="341" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of bag of words; importance, implementation, applications, and challenges</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Qader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ameen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Engineering Conference (IEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06">2019. June. 2019</date>
			<biblScope unit="page" from="200" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Bangyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alqarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ali Almazroi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3498123</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The impact of emotional intelligence on work team cohesiveness and performance</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Rapisarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Organizational Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="379" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing collaborative learning processes automatically: Exploiting the advances of computational linguistics in computer-supported collaborative learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer-supported Collaborative Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="237" to="271" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A social spin on language analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">545</biblScope>
			<biblScope unit="issue">7653</biblScope>
			<biblScope unit="page" from="166" to="167" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Technology support for discussion based learning: From computer supported collaborative learning to the future of massive open online courses</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ferschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="660" to="678" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The random forest algorithm for statistical learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Stata Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="29" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Large scale legal text classification using transformer models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wohlgenannt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Filtz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12871</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Use of large language models to aid analysis of textual data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Sitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Fankhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Chicas-Mosier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Monteith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Soumya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prajjwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shruti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning machine learning with very young children: Who is teaching whom?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vartiainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tedre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Valtonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Child-computer Interaction</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">100182</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Investigating How Student&apos;s Cognitive Behavior in MOOC Discussion Forums Affect Learning Gains</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Paper presented at International Educational Data Mining Society</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spencher-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11382</idno>
		<title level="m">A prompt pattern catalog to enhance prompt engineering with chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classification of open-ended responses to a research-based assessment using natural language processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Physics Education Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10141</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdelghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion proceedings of the 28th international conference on intelligent user interfaces</title>
		<imprint>
			<date type="published" when="2023-03">2023. March</date>
			<biblScope unit="page" from="75" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An automatic short-answer grading model for semi-open-ended questions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive Learning Environments</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
