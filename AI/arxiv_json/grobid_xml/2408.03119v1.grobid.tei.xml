<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating the Translation Performance of Large Language Models Based on Euas-20</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-06">6 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Software</orgName>
								<orgName type="institution">Zhengzhou University of Light Industry</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Software</orgName>
								<orgName type="institution">Zhengzhou University of Light Industry</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating the Translation Performance of Large Language Models Based on Euas-20</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-06">6 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2DE0E47E1BA44FE689990D622A638B44</idno>
					<idno type="arXiv">arXiv:2408.03119v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language models</term>
					<term>machine translation</term>
					<term>data set</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The application and performance of Large Language Models (LLMs) on translation performance has become an important research direction and practical achievement in the field of modern natural language processing. In the recent emergence of large language models (LLMs), e.g., GPT-3 and GPT-4 , their translation performance on Zero-shot can be compared to that of powerful fully supervised machine translation systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. However, the massive corpus used for training big language models is usually dominated by monolingual data, in which the English corpus is dominant, while the proportion of corpus in other languages is relatively small <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Under this data distribution, whether the big language models can effectively model the correspondence between different languages and learn reliable translation knowledge is a great concern for researchers <ref type="bibr" target="#b6">[7]</ref>. Models may face challenges in handling translation tasks between these languages. Therefore, we evaluate the popular large language models currently available in the market to acquire a better perception of the translation performance of large language models.</p><p>In this paper, the translation ability of Large Language Models (LLMs) is investigated by answering two questions, 1) What is the translation ability of LLMs?2) Factors affecting the translation ability of LLMs?3) What are the translation results of the LLMs?</p><p>In response to the first question, we evaluate several popular LLMs: English language-centric LLMs including Llama2 <ref type="bibr" target="#b7">[8]</ref> , Falcon <ref type="bibr" target="#b8">[9]</ref>, Vicuna <ref type="bibr" target="#b9">[10]</ref> , Mistral <ref type="bibr" target="#b10">[11]</ref> and multilingual LLMs including Bloom and Bloomz <ref type="bibr" target="#b11">[12]</ref> , Gemma <ref type="bibr" target="#b12">[13]</ref> . In order to prevent data leakage and get more accurate results, we constructed a dataset Euas-20(A representative selection of 20 languages). LLMs translate other languages into Chinese and English respectively. The results show a significant improvement in the multilingual translation ability of LLMs. This improvement is not only reflected in the increase of the model's parameters, but also due to the model's improvement in training data and methods. We compare the translation results of LLMs on different languages. We find that there is a significant difference in the translation performance of LLMs on different languages, and LLMs perform better than Chinese when translating into English. For languages similar to English, LLMs also demonstrate better translation performance. Meanwhile, we find that LLMs also have translation ability on zero-resource languages. This suggests that the large language models have some generalisation ability and are able to establish correspondences between different languages in the absence of direct training data.</p><p>To address the second question, we analysed the LLMs by collecting information from their corpora. We find that a high-quality and diverse corpus can significantly improve the translation performance of LLMs. Multi-language and multi-domain training data can not only enhance the generalisation ability of the model, but also improve its effectiveness in different languages and different domains.</p><p>To address the third question, we have analysed the translation results of LLMs in various aspects.The translation results of LLMs are subject to some illusory problems, and their fluent output may mislead the users and make it difficult for them to identify the errors in the translation. In addition, LLMs tend to choose the most appropriate translation words in translation tasks by analysing a variety of factors such as semantics, fluency and culture. We also found that when LLMs met words that they had not encountered during model training, the models could not understand or process them accurately due to the lack of training on these words.</p><p>The purpose of this paper is to review and analyse the performance of the current large language model on translation tasks, to explore whether the large language model can effectively model the correspondence between different languages and the factors affecting translation, for the reference of researchers and developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Language Models</head><p>Large Language Models (LLMs) have made significant progress in translation performance. Based on deep learning, especially the Transformer architecture <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> , these Large Language Models have learned rich linguistic knowledge by pre-training on a large amount of textual data, thus achieving excellent performance in various downstream tasks.</p><p>The training process of a large language model is divided into two main phases. The first is the pre-training phase, in which the model learns unsupervised on large-scale textual data to master the basic structure and lexical usage of the language. The goal of this phase is for the model to learn a generic language representation. Next is the fine-tuning phase, in which the pre-trained model is subjected to supervised learning on a specific translation task using a bilingual parallel corpus to equip it with the ability to translate specific language pairs. The big language models support multiple languages, demonstrating the ability to generalize across languages. However, the training data of the big language models are dominated by the English corpus, with a smaller proportion of data in other languages, and this unbalanced data distribution poses a severe test for the performance of the models in a multilingual environment. Researchers are actively exploring ways to address these issues in order to further improve the performance of large language models in translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Translation</head><p>Machine Translation (MT) is a technology that uses computers to automatically translate text from one language to another. In recent years, with the rapid development of artificial intelligence and natural language processing technology, especially the emergence of large language models (e.g., OpenAI's GPT series and Google's BERT), the ability of machine translation has been significantly improved.</p><p>Modern machine translation systems mainly rely on Neural Machine Translation (NMT) technology <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> . NMT utilises deep learning and neural network models, and is able to efficiently capture and process complex relationships between source and target languages through encoder-decoder architectures and self-attention mechanisms. Compared with traditional rule-based methods and statistical machine translation (SMT) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, NMT performs better in terms of translation accuracy, fluency, and context understanding.</p><p>Machine translation, as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. However, machine translation still faces challenges, including translation of low-resource languages and maintaining coherence and fluency of translation in long texts <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>In order to evaluate the real translation capabilities of large language models, we constructed a dataset called Euas-20. This dataset contains twenty representative languages (Table <ref type="table" target="#tab_0">1</ref>), covering a large part of the global population, while demonstrating a diverse background of writing systems and language families. We have selected a number of important languages that not only have a large number of speakers, but also include some languages that are considered underresourced in the research community. With this diverse dataset, we are able to comprehensively evaluate the translation performance of the large language models in different language contexts, and thus gain a more accurate understanding of their performance in real-world applications. languages we use in our work are listed in Table <ref type="table" target="#tab_0">1</ref>. Referring to the information provided in <ref type="bibr" target="#b21">Goyal et al. (2022)</ref> <ref type="bibr" target="#b21">[22]</ref>, we populated the table. For each language, we show the ISO code, language name, language grouping, alphabet and resource level.</p><p>We selected about twenty domains such as medicine, science, art, education, environment, finance, entertainment, sports, politics, agriculture, etc. to ensure a wider coverage of the dataset. After that, we designed a prompt (Fig. <ref type="figure" target="#fig_0">1</ref>) and fed it into ChatGPT, allowing it to act as a data annotator and generate sentences according to specified rules. In each domain, ChatGPT generated fifty sentences, including different sentence types such as declarative, interrogative and exclamatory sentences. We deleted sentences with high similarity and repeated the process, eventually selecting about fifty different sentences in each domain and constructing a document containing one thousand Chinese sentences.</p><p>Next, we used Google Translate to translate this Chinese document into other target languages to build a complete dataset. In this way, we ensure that the dataset is diverse and representative, and we are able to more comprehensively evaluate the translation capabilities of large language models across different domains and languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LLMs</head><p>We evaluated the translation capabilities of nine currently popular LLMSs: fal-con7b, mistral-7b, Llama-2-7b-hf, bloom-7b1, bloomz-7b1-mt, Meta-Llama-3-8B, mpt-7b, vicuna-7b, and gemma-7b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Methods</head><p>Focusing on Chinese and English, through a prompt (Fig. <ref type="figure" target="#fig_1">2</ref>), 'source-sentence' stands for the original sentence and 'target-sentence' stands for the target sentence, and the original sentence is input to the LLMs by the command (Translate the following sentence from 'source-language' to 'target-language' and The 'target-language' translation is), so that the LLMs can translate and output the target sentence under Zero-Shot learning. In this way, various languages in the dataset are translated into Chinese and English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Indicators</head><p>Evaluation metrics are an important measure of translation quality. We adopt commonly used automatic evaluation metrics including BLEU <ref type="bibr" target="#b22">[23]</ref>, which calculates translation accuracy by comparing the n-gram overlap between candidate and reference translations, which is the traditional method for assessing the quality of machine translation.</p><p>In addition, we also consider the emerging metric COMET <ref type="bibr" target="#b23">[24]</ref>, which is designed to learn to predict human judgements of machine translation quality, and which better reflects subjective human assessments of translation quality. By combining these evaluation metrics, we are able to assess the translation performance of large-scale language models in a more comprehensive way, ensuring the accuracy and reliability of the assessment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Testing of machine translation for LLMs</head><p>In this section, we report the results of the translation of LLMs (Fig. <ref type="figure" target="#fig_2">3</ref>) and analyse the translation performance of LLMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Continuous Improvement of Translation Ability of LLMs</head><p>In recent years, the multilingual translation capability of Large Language Models (LLMs) has been significantly improved. Even under Zero-Sample Learning (Zero-Shot) conditions, LLMs still exhibit good translation performance in most translation directions, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. Based on the scores of LLMs on different languages , we can find that the translation ability of LLMs has gradually improved, especially the recent LLMs have reached new heights in terms of translation performance. For example, Llama-3-8B significantly outperforms the previous Llama-2-7B, and vicuna-7B outperforms Llama-2-7B. Overall, Llama-3-8B performs the best among all the LLMs evaluated, and it obtains the highest BLEU and COMET scores in most translation directions, showing its superior translation capabilities.</p><p>This progress is not only reflected in the increase of the model's parameters, but also due to the model's improvement in training data and methodology. Llama-3-8B uses larger and higher quality multilingual datasets during training, and adopts more advanced training algorithms, which enable it to maintain a high level of translation quality even when dealing with complex and rare language pairs. At the same time, the model's architectural optimisation and inference speed have also been improved, making Llama-3-8B not only more accurate but also more efficient in practical applications.</p><p>In addition, Llama-3-8B and other advanced LLMs demonstrate a high degree of flexibility and adaptability in coping with multilingual text comprehension and generation tasks. These models can play an important role in cross-cultural and cross-linguistic communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation performance of LLMSs across languages</head><p>The translation performance of large-scale language models (LLMs) varies significantly across languages. Typically, LLMs translate well on high-resource languages, but have relatively poor translation performance on low-and mediumresource languages. We find that LLMs perform particularly well when translating into English and relatively poorly when translating into Chinese. For languages similar to English, LLMs also demonstrate better translation perfor- mance. For example, LLMs generally achieve better translation results in the Indo-European Romance and Germanic languages.</p><p>For some languages that are more different from English, such as the Tai-Kadai languages, LLMs produce very poor translation results. This uneven translation performance is mainly due to the differences in the training data, where the high volume and quality of data for high-resource languages make the models perform better on these languages. On the other hand, low and medium resource languages are difficult for the model to learn enough linguistic features due to the scarcity of data, resulting in unsatisfactory translation results.</p><p>Nevertheless, LLMs show some translation ability even on zero-resource languages. This suggests that the large language models possess some generalisation ability and are able to establish correspondences between different languages in the absence of direct training data. Behind this ability is the fact that the models have learnt common features and structures between languages through largescale multilingual training, so that they can still translate reasonably well in the face of new language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of corpus on the translation performance of LLMs</head><p>By analysing pre-training data and corpora of large-scale language models (LLMs), we can investigate the relationship between translation performance and corpus size and category. By collecting LLMs with training data sizes (Table <ref type="table" target="#tab_1">2</ref>), we find that the larger the pre-training data size, the better the translation performance of the LLMs.For example, Llama-3-8B and Gemma-7B outperform other models overall. This suggests that rich training data is one of the key factors to improve the translation ability of the models. Most of the pre-training corpora of the big language models are Englishcentric, which on the one hand makes the models perform extremely well in English language ability; on the other hand, it also leads to their weaker ability in non-English languages. This English-centric corpus configuration improves the efficiency of the model in handling English-related tasks, but the model's performance appears to be insufficient when dealing with other languages, especially low-and medium-resource languages. Models trained in multiple languages have achieved better results in translation than LLMs limited to one or a few languages. For example, the translation performance of Gemma-7B is significantly better than that of Falcon-7B.Meanwhile, when translating languages, multilingual models tend to have better translation ability for languages that have been pre-trained than for languages that have not been pre-trained. For example, the corpus share of bloom-7b1 and bloomz-7b1-mt (Fig. <ref type="figure" target="#fig_4">5</ref>) has better translation ability for pre-trained languages. This suggests that multi-language training can effectively enhance the model's translation capability, enabling it to better handle translation tasks between various language pairs. From these observations, it can be found that a high-quality and diverse corpus can significantly improve the translation performance of LLMs. Multilanguage and multi-domain training data can not only enhance the generalisation ability of the model, but also improve its effectiveness in different languages and different domains. Therefore, in order to meet the translation needs of various languages, future LLMs should make full use of diverse corpora in the pretraining process and continuously increase the proportion of data from low-and medium-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Illusions in the translation of LLMs</head><p>Neural Machine Translation (NMT) is a task that translates a source language into a target language through inference and relies on parallel data samples used for training. Compared to Statistical Machine Translation (SMT), the output of NMT is usually very fluent, with a quality close to the human level. However, this poses a potential problem: when NMT hallucinates (i.e., generates inaccurate or spurious translations), its smooth output may mislead users and make it difficult for them to identify errors in the translation.</p><p>By analysing the translation results of LLMs, we classified the hallucinations in NMT as two categories <ref type="bibr" target="#b24">[25]</ref>, intrinsic and extrinsic hallucinations. Intrinsic Illusion: Incorrect information is included in the translation that does not match what is in the source text. An example of such an illusion is '不太了解 ', which negates '了解多少 ' in the source text. Extrinsic illusions: the translation produces additional content that does not exist in the source text.'我忘了带手机 ' is an example of illusory content because it is added without any apparent connection to the input.</p><p>Table 3. Illusions in the translation of LLMs Category Source Correct Translation Hallucinatory Translation Intrinsic Excuse me, how much do you 请问，你对这项技 非常抱歉，我们这么说是 know about this technology? 术的了解有多少？ 因为我们不太了解这种技术。 Extrinsic Excuse me, how much do you 请问，你对这项技 对不起，我忘了带手机了。 know about this technology? 术的了解有多少？</p><p>The results show that as the pre-trained corpus continues to grow, the pretrained model becomes more and more effective in generating faithful summaries of human assessments. By comparing the translation results of Gemma-7B and Falcon-7B, more intrinsic illusions are generated for monolingual models; while for multilingual models, this is less frequent. Also, we found that nouns are the most hallucinated words, and verbs also account for a certain number of hallucinations. In addition, LLMS tend to be more prone to intrinsic hallucinations when confronted with untrained language. Therefore, it is crucial to improve the accuracy and reliability of machine translation. By continuously improving our models, enhancing the diversity and quality of our datasets, and using more advanced evaluation metrics to detect and reduce illusions in translation, we can mitigate these risks and provide more secure and reliable translation services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Translation words that LLMs tend to choose in translation tasks</head><p>This section explores the translation words that Large Language Models tend to choose in translation tasks and the reasons behind them. Through previous analyses of the translation results of Gemma-7B and Falcon-7B, we found that LLMs to choose common word collocations in the target language during the translation process. This not only improves the naturalness of the translation, but also makes it more in line with the usage habits of the target language. For example, 'make a decision' in English is often translated as '做决定' instead of '制造决定' in Chinese because the former is a common collocation in Chinese and is more in line with the language conventions. This is because the former is a common collocation in Chinese, which is more in line with the language convention.</p><p>In addition, we also found that the model selects those words that are closest in meaning to be translated by deep understanding of the original and the translated text. For example, when translating the English word 'computer' into Chinese, the model chooses '电脑' instead of '计算机' because '电脑' is a common collocation in modern Chinese. 'is more commonly used and semantically accurate in modern Chinese.</p><p>LLMs tend to choose the most appropriate translation words in translation tasks by comprehensively analysing various factors such as semantics, fluency and culture. This approach not only improves the accuracy and naturalness of the translation, but also makes the translation result more in line with the usage habits of the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Phenomenon of unregistered words</head><p>Out-of-vocabulary words (OOV words), refer to words that have not been encountered during model training. These words may be new terms, technical terms, foreign language vocabulary, or recently emerged buzzwords. We found that due to the lack of training on these words, the model cannot understand or process them accurately. We choose the translation results of Gemma-7B and Falcon-7B as representative.</p><p>For monolingual models, when the model is confronted with words that have not been trained across languages, such as 'madilim na bagay' (dark matter) in Filipino, the model will ignore or mistranslate them to other nouns. For multilingual models, even if the model has been trained cross-linguistically, when the model encounters a new word like 'schadenfreude' (a German word that refers to the emotion of taking pleasure in someone else's misfortunes), it may not be able to correctly understand the meaning because the word has never appeared in its training data. ever appeared in its training data. As a result, the model will choose to ignore it, not translate it or translate it incorrectly.</p><p>In the future, LLMs need to increase their training data to cover a wider range of vocabulary, as well as dynamically expand the model's vocabulary by using external resources such as vocabularies or online data sources; to reduce this phenomenon and to improve the translation ability of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In the field of large language model translation capability evaluation, there have been a large number of related studies devoted to exploring the translation performance of different models on multiple language pairs and text types. <ref type="bibr" target="#b25">Bang et al. (2023)</ref>  <ref type="bibr" target="#b25">[26]</ref>and <ref type="bibr" target="#b26">Hendy et al. (2023)</ref> <ref type="bibr" target="#b26">[27]</ref> evaluated ChatGPT on 13 and 18 languages, respectively; Zhu et al. (2023) <ref type="bibr" target="#b27">[28]</ref> evaluated the translation capability of four popular large language models, XGLM, BLOOMZ, OPT, and ChatGPT, on 102 languages, on 202 directions. 202 directions The multilingual translation capabilities of four popular large language models, XGLM, BLOOMZ, OPT and ChatGPT, were evaluated.</p><p>In this paper, 20 representative languages are selected to evaluate nine current mainstream large-scale language models. The evaluation focuses on Chinese and English, but covers a wide range of other languages as well. Multilingual translations are performed with these models and the results are compared with a state-of-the-art translation engine (Google Translate) in order to comprehensively evaluate the translation capabilities and performance of these large language models. The aim of the study is to determine the performance of these models in different linguistic contexts, as well as their usability and accuracy in real translation tasks.</p><p>Despite the significant progress made by large-scale language models on translation tasks, a number of challenges remain. For example, there is still room for improvement in the model's ability to handle low-resource languages and diverse texts. Future research directions include improving the evaluation metrics, optimising the model structure and enhancing the training methods to further improve the performance and generalisation of large language models on translation tasks. These improvements will not only help to enhance the model's translation accuracy, but also enhance its adaptability in dealing with complex and diverse language environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, a dataset called Euas-20 is constructed and nine popular large language models (LLMs) are evaluated using this dataset. The evaluation process focuses on Chinese and English, and compares the translation performance of these models and their translation capabilities on various languages through translations in 20 languages. Also, the paper analyses the impact of pre-training data and corpus on the translation performance of large language models.The translation results of the LLMs were analysed in various ways.</p><p>The results show that although the translation performance of LLMs is improving, with Llama-3 performing particularly well, far exceeding other models, the translation ability of these models on different languages is still very unbalanced. Especially when dealing with low-resource languages, they still face great challenges. In addition, a high-quality and diverse corpus plays a significant role in improving the translation performance of large language models.</p><p>Future research needs to continue to explore how to enhance the translation capabilities of LLMs on more languages to achieve more balanced and comprehensive translation performance. This includes improving the model structure, optimising training methods, and extending and enhancing the quality and diversity of the corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Prompt 1</figDesc><graphic coords="5,221.22,115.83,172.92,206.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Prompt 2</figDesc><graphic coords="6,221.22,115.83,172.92,189.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. BLEU and COMET scores for nine LLMs translations centred on English and Chinese.</figDesc><graphic coords="7,134.77,115.84,345.83,216.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Translation performance (BLEU) of LLMS on our evaluated languages, 'xx-en' and 'xx-zh' denote translation from other languages to English and Chinese, respectively.</figDesc><graphic coords="8,134.77,115.83,345.83,222.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Corpus share of LLMs</figDesc><graphic coords="9,221.22,406.29,172.91,115.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Language</figDesc><table><row><cell cols="2">iso Language Language grouping</cell><cell>Script</cell><cell>Resource Level</cell></row><row><cell>ar arabic</cell><cell>arabic</cell><cell>Arabic</cell><cell>Medium</cell></row><row><cell>zh chinese</cell><cell>Sino-Tibetan</cell><cell>Han</cell><cell>Medium</cell></row><row><cell>da danish</cell><cell>Germanic</cell><cell>Latin</cell><cell>Medium</cell></row><row><cell>en english</cell><cell>Germanic</cell><cell>Latin</cell><cell>High</cell></row><row><cell>fr french</cell><cell>Romance</cell><cell>Latin</cell><cell>High</cell></row><row><cell>de german</cell><cell>Germanic</cell><cell>Latin</cell><cell>High</cell></row><row><cell>el greek</cell><cell>Hellenic</cell><cell>Greek</cell><cell>Medium</cell></row><row><cell>hi hindi</cell><cell>Indo-Aryan</cell><cell cols="2">Devanagari Medium</cell></row><row><cell cols="2">is icelandic Germanic</cell><cell>Latin</cell><cell>Medium</cell></row><row><cell>it italian</cell><cell>Romance</cell><cell>Latin</cell><cell>High</cell></row><row><cell cols="2">ja japanese Japonic</cell><cell cols="2">Kanji; Kana Medium</cell></row><row><cell>ko korean</cell><cell>Koreanic</cell><cell>Hangul</cell><cell>Medium</cell></row><row><cell cols="2">nl nederlands Indo-European</cell><cell>Latin</cell><cell>Medium</cell></row><row><cell>no norsk</cell><cell>Indo-European</cell><cell>Latin</cell><cell>Medium</cell></row><row><cell cols="2">pt portuguese Romance</cell><cell>Latin</cell><cell>High</cell></row><row><cell>ru russian</cell><cell>Slavic</cell><cell>Cyrillic</cell><cell>High</cell></row><row><cell>es spanish</cell><cell>Romance</cell><cell>Latin</cell><cell>High</cell></row><row><cell>tl tagalog</cell><cell>Malayo-Polynesian</cell><cell>Latin</cell><cell>Low</cell></row><row><cell>th thai</cell><cell cols="2">Sino-Tibetan+Kra-Dai Thai</cell><cell>Medium</cell></row><row><cell cols="2">vi vietnamese Vietic</cell><cell>Latin</cell><cell>Medium</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Training volume of LLMs</figDesc><table><row><cell>LLM</cell><cell>Token</cell></row><row><cell>Gemma 7b</cell><cell>6 trillion</cell></row><row><cell>Llama-2-7b-hf</cell><cell>2 trillion</cell></row><row><cell>mpt-7b</cell><cell>1 trillion</cell></row><row><cell cols="2">Meta-Llama-3-8B 15 trillion</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.087451</idno>
		<title level="m">Is chatgpt a good translator? a preliminary study</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07423</idno>
		<title level="m">Chatgpt mt: Competitive for high-(but not low-) resource languages</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptive machine translation with large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Moslem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13294</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards robust in-context learning for machine translation with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</title>
		<meeting>the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="16619" to="16629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised parallel sentences of machine translation for asian language pairs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining parallel sentences from internet with multi-view knowledge distillation for low-resource language pairs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="209" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FEDS-ICL: enhancing translation ability and efficiency of large language model by optimizing demonstration selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2024.103825</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2024.103825" />
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103825" to="102024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Falcon-40b: an open large language model with state-of-the-art performance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alshamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Goffinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Launay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Malartic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10755" to="10773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<title level="m">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Gemma: Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficiently exploring large language models for document-level machine translation with in-context learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07081</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tjunlp: System description for the wmt23 literary task in chinese to english translation direction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Machine Translation</title>
		<meeting>the Eighth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phrase-based statistical machine translation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KI 2002: Advances in Artificial Intelligence: 25th Annual German Conference on AI, KI 2002 Aachen</title>
		<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">September 16-20, 2002 Proceedings 25. 2002</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
		<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<title level="m">Six challenges for neural machine translation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The flores-101 evaluation benchmark for lowresource and multilingual machine translation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="522" to="538" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Comet: A neural framework for mt evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Farinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.09025" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detecting hallucinated content in conditional neural sequence generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2011.02593" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<title level="m">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelrehim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Afify</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Awadalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.09210</idno>
		<title level="m">How good are gpt models at machine translation? a comprehensive evaluation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04675</idno>
		<title level="m">Multilingual machine translation with large language models: Empirical results and analysis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
