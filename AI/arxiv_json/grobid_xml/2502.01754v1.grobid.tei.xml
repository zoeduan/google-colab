<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Large Language Models via Coupled Token Generation</title>
				<funder ref="#_DZQfj85">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-03">3 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nina</forename><surname>Corvelo Benz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stratis</forename><surname>Tsirtsis</surname></persName>
							<email>stsirtsis@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eleni</forename><surname>Straitouri</surname></persName>
							<email>estraitouri@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivi</forename><surname>Chatzi</surname></persName>
							<email>ichatzi@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Artola</forename><surname>Ander</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suhas</forename><surname>Velasco</surname></persName>
							<email>avelasco@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Thejaswi</surname></persName>
							<email>thejaswi@mpi-sws.org</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Gomez-Rodriguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
								<address>
									<settlement>Kaiserslautern</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Large Language Models via Coupled Token Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-03">3 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">A72B6986B2A815679CD15CA6362779FF</idno>
					<idno type="arXiv">arXiv:2502.01754v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation.</p><p>1 Tokens are the units that make up sentences and paragraphs, e.g., (sub-)words, numbers, and special end-of-sequence tokens.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the most celebrated aspects of state of the art large language models (LLMs) is that they can solve open-ended, complex tasks across many different application domains such as coding, healthcare and scientific discovery <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. However, this is crucially what also makes the evaluation and comparison of LLMs very challenging-it is very difficult, if not impossible, to create a single benchmark. As a consequence, in recent years, there has been a flurry of papers introducing different benchmarks <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. In fact, one of the flagship conferences in machine learning has even created a separate datasets and benchmarks track!</p><p>In this context, it is somehow surprising that, in comparison, there has been a paucity of work understanding, measuring or controlling for the different sources of uncertainty present in the evaluations and comparisons of LLMs based on these benchmarks <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. In our work, we focus on one source of uncertainty that has been particularly overlooked, the uncertainty in the outputs of the LLMs under comparison.</p><p>Given an input prompt, LLMs generate a sequence of tokens 1 as output using an autoregressive process <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. At each time step, they first use a neural network to map the prompt and the (partial) sequence of tokens Figure <ref type="figure">1</ref>: Example of coupled autoregressive generation for Llama 1B and Llama 8B. Boxes represent endogenous random variables and circles represent exogenous random variables. The value of each endogenous variable is given by a function of the values of its ancestors in the causal graph, as defined by Eq. 1. The value of the coupled noise variable U 1 (purple) is sampled independently from a given distribution P U , and it determines the stochastic state of the samplers used by both Llama 1B and Llama 8B during the generation of token T 1 . different to ours; they augment a single LLM with the ability to reason counterfactually about alternatives to its own outputs if individual tokens had been different. Our work also shares technical elements with a recent work by Ravfogel et al. <ref type="bibr" target="#b34">[35]</ref>, which develops a causal model to generate counterfactual strings resulting from interventions within (the network of) an LLM. However, their work does not study counterfactual generation for the purposes of model evaluation. In this context, it is also worth pointing out that the specific class of causal models used in the aforementioned works and our work, called the Gumbel-max structural causal model <ref type="bibr" target="#b35">[36]</ref>, has also been used to enable counterfactual reasoning in Markov decision processes <ref type="bibr" target="#b36">[37]</ref>, temporal point processes <ref type="bibr" target="#b37">[38]</ref>, and expert predictions <ref type="bibr" target="#b38">[39]</ref>.</p><p>Our work also builds upon the rapidly increasing literature on evaluation and comparison of LLMs <ref type="bibr" target="#b39">[40]</ref>. Within this literature, LLMs are evaluated and compared using: (i) benchmark datasets with manually hand-crafted inputs and ground-truth outputs <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> and (ii) the level of alignment with human preferences, as elicited by means of pairwise comparisons <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. However, it has become increasingly clear that oftentimes rankings derived from benchmark datasets do not match those derived from human preferences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b40">41]</ref>. Within the literature on ranking LLMs from pairwise comparisons, most studies use the Elo rating system <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>, originally introduced for chess tournaments <ref type="bibr" target="#b46">[47]</ref>. However, Elo-based rankings are sensitive to the order of pairwise comparisons, as newer comparisons have more weight than older ones, which leads to unstable rankings <ref type="bibr" target="#b20">[21]</ref>. To address this limitation, several studies have instead used the Bradley-Terry model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, which weighs pairwise comparisons equally regardless of their order. Nevertheless, both the Elo rating system and the Bradley-Terry model have faced criticism, as pairwise comparisons often fail to satisfy the fundamental axiom of transitivity, upon which both approaches rely <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref>. Recently, several studies have used the win-rate <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, which weighs comparisons equally regardless of their order and does not require the transitivity assumption. In our work, we focus on win-rates. However, we believe that it may be possible to extend our theoretical and empirical results to rankings based on Elo ratings and the Bradley-Terry model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Causal Model for Coupled Autoregressive Generation</head><p>Let V denote a vocabulary (set) of tokens, including an end-of-sequence token ⊥, V * = V ∪ V 2 ∪ • • • ∪ V K be the set of sequences of tokens up to length K, and ∅ be the empty token. <ref type="foot" target="#foot_2">4</ref> An LLM m ∈ M takes as input a prompt sequence s q ∈ V * and responds with an output sequence s ∈ V * , generated using an autoregressive process. At each time step i ∈ [K] of the process, the LLM first takes as input the concatenation of the prompt sequence s q and the (partial) output sequence s i-1 , and generates a distribution over tokens d i ∈ ∆(V ). Then, it samples the next token t i ∼ d i from the distribution d i and creates the output sequence s i = s i-1 • t i , where • denotes the concatenation of a token or sequence with another sequence. If t i = ⊥, it terminates and returns s = s i , otherwise, it continues to the next step i + 1 in the generation. Once the process is completed, the output sequence s is assigned a score r, which is subsequently used for model evaluation.</p><p>Following the recent work by Chatzi et al. <ref type="bibr" target="#b33">[34]</ref>, we augment the above autoregressive process using a structural causal model (SCM) <ref type="bibr">[49,</ref><ref type="bibr" target="#b48">50]</ref>, which we denote as C. The SCM C is defined by the following structural equations:<ref type="foot" target="#foot_3">foot_3</ref> </p><formula xml:id="formula_0">S 0 = S q , D i = f D (S i-1 , M ) if last(S i-1 ) ̸ = ⊥, P ∅ otherwise , T i = f T (D i , U i ) if D i ̸ = P ∅ , ∅ otherwise , S i = S i-1 • T i , S = S K , and R = f R (S, Z).<label>(1)</label></formula><p>In the above equations, M, S q , U = (U i ) i∈{1,...,K} , and Z are independent exogenous random variables, with M ∼ P M , S q ∼ P Q , U i ∼ P U , and Z ∼ P Z . Moreover, f D , f T and f R are given functions, P ∅ denotes the point mass distribution on ∅, and last(S i-1 ) denotes the last token of the sequence S i-1 . Here, the function f D maps an input sequence S i-1 to a distribution D i for the next token, using the architecture and network weights of the LLM M , the function f T and distribution P U specify the sampling mechanism that is used to sample the next token at each step of the generation process, following the distribution D i , and the function f R and distribution P Z specify the exact scoring process by which the score R is assigned to an output sequence S during the evaluation of the LLM M . Throughout the paper, we focus on sampling mechanisms that satisfy counterfactual stability <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37</ref>]an intuitive form of consistency between the next token T i , its distribution D i , and the corresponding noise variable U i . <ref type="foot" target="#foot_4">6</ref> Moreover, we allow the score R to be observable or unobservable, and its semantic meaning and support of its distribution to vary depending on the evaluation protocol. For example, in multiple-choice questions <ref type="bibr" target="#b50">[52]</ref>, R ∈ {0, 1} may represent whether an LLM outputs a correct (R = 1) or an incorrect (R = 0) response. In pairwise comparisons <ref type="bibr" target="#b15">[16]</ref>, R ∈ R + may represent the level of user's satisfaction with the response provided by an LLM. In this context, the noise variable Z models any potential sources of uncertainty in the scoring process, e.g., uncertainty in users' preferences <ref type="bibr" target="#b51">[53]</ref><ref type="bibr" target="#b52">[54]</ref><ref type="bibr" target="#b53">[55]</ref>.</p><p>Building upon the above causal model, we can now formally express what it means to sample (and evaluate) output sequences by different LLMs using the same source of randomness,<ref type="foot" target="#foot_5">foot_5</ref> a process we refer to as coupled autoregressive generation. Consider a specific model m, a prompt s q , and fixed noise values u and z. It is easy to see that specifying these values is sufficient to (deterministically) specify and compute the exact value of the output sequence S and its score R using the autoregressive generation and scoring process given by Eq. 1. Then, we can formally express the coupled output sequences by two models m and m ′ and their corresponding scores as the result of interventions do(M = m) and do(M = m ′ ), respectively, where the do(•) operator forcibly sets the value of M while keeping the prompt s q and the noise values u, z fixed <ref type="bibr" target="#b54">[56]</ref>. In what follows, we denote the respective scores R m (u, s q , z) and R m ′ (u, s q , z), following standard notation <ref type="bibr">[49]</ref>. For an illustration of coupled autoregressive generation against independent autoregressive generation-the vanilla generation approach-refer to Figure <ref type="figure">1</ref>.</p><p>In practice, one run of coupled autoregressive generation consists of two or more runs of autoregressive generation with the same prompt s q and noise values u and z, one per LLM. 8 From a causal perspective, we can view these runs as realizations of possible worlds where everything is equal except for the (architecture and network weights of the) LLM. Or we can also view one of these runs as a realization of the factual world and the other runs as realizations of different counterfactual worlds. Consequently, this lends support to attribute any difference in the scores R m (u, s q , z) across models m ∈ M to the models' architectures and weights rather than the randomness in their autoregressive generation processes. In the following sections, we will investigate both theoretically and empirically the differences between coupled and independent autoregressive generation in the context of evaluations based on benchmark datasets and pairwise comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation based on benchmark datasets</head><p>In this section, we focus on the evaluation and comparison of LLMs based on benchmark datasets, e.g., multiple-choice questions <ref type="bibr" target="#b50">[52]</ref>, and theoretically investigate under which conditions coupled autoregressive generation requires fewer samples than independent autoregressive generation to reliably estimate the competitive advantage of one LLM over another.</p><p>Given a benchmark dataset characterized by an input prompt distribution P Q , for each prompt s q ∼ P Q , let c(s q ) ⊂ V * denote the set of correct output sequences. 9 In what follows, for ease of exposition, we consider binary scores R m (u, s q ) = 1 {S m (u, s q ) ∈ c(s q )} ∈ {0, 1}, where S m (u, s q ) denotes the output sequence of a model m given a prompt s q under a realized sequence of noise values u and 1{•} is the indicator function. 10  The standard approach to compare the performance of any pair of LLMs m, m ′ ∈ M using a benchmark dataset reduces to estimating the difference in their expected score, i.e.,</p><formula xml:id="formula_1">E U ∼P U ,U ′ ∼P U ,Sq∼P Q [R m (U ↑ , S q ) -R m ′ (U ↑ ′ , S q )],<label>(2)</label></formula><p>Independent generation</p><p>where note that we use different noise variables U and U ′ for each LLM because, in the standard approach, each LLM generates outputs to each query independently (i.e., using independent autoregressive generation). At first, one may think that, in this context, coupled autoregressive generation will not be helpful. Under coupled autoregressive generation, the difference in the expected score adopts the following form:</p><formula xml:id="formula_2">E U ∼P U ,Sq∼P Q [R m (U ↑ , S q ) -R m ′ (U ↑ , S q )].<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coupled generation</head><p>Therefore, based on the linearity of expectation and the fact that, under independent generation, both U and U ′ are sampled from the same distribution P U , it is easy to see that Eqs. 2 and 3 are equivalent. However, as we will show next, coupled autoregressive generation allows us to reliably estimate the difference in the two LLMs' scores from finite samples faster. More formally, we first start by characterizing the relation between the variances of the difference of scores between LLMs using the following proposition: 11   Proposition 1 For any pair of LLMs m, m ′ ∈ M, it holds that</p><formula xml:id="formula_3">Var[R m (U , S q ) -R m ′ (U ′ , S q )] = Var[R m (U , S q ) -R m ′ (U , S q )] + 2 • Cov[R m (U , S q ), R m ′ (U , S q )]<label>(4)</label></formula><p>8 In practice, we may not always have control over the noise value z (e.g., when the scoring process is performed by an end user). However, even in such cases, we can still implement coupled autoregressive generation if the scoring processes occur simultaneously for each run, such as in pairwise comparisons. 9 In multiple-choice questions, c(sq) may consist of all sequences that include the correct choice. 10 Our theoretical results can be extended to real-valued scores in a bounded interval. 11 All proofs can be found in Appendix B.</p><p>This result immediately implies that, if the scores achieved by the LLMs under comparison are positively correlated, i.e., the LLMs tend to generate a (in-)correct output sequence on the same prompts under the same noise values, then the variance of the difference in scores is lower under coupled generation than under independent generation, and thus we can expect a reduction in the sample size required to obtain equivalent estimation errors. In what follows, we will analyze two canonical settings in which this condition holds and, in Section 5, we will provide empirical evidence that, in a well-known benchmark dataset, this condition also holds.</p><p>In the first canonical setting, the correct response to each prompt is one of two given single-token sequences, the LLMs m and m ′ under comparison always output a response that is either of these two sequences, and the sampling mechanism used by the LLMs satisfies counterfactual stability. While this setting may seem restrictive, it is found in real-world scenarios. For example, think of true/false questions (or multiple-choice questions with two options) and evaluation protocols in which the LLMs are explicitly instructed to always output true/false (or one of the two options) via their system prompt. <ref type="foot" target="#foot_6">12</ref> The following proposition shows that the variance of the difference in scores is lower under coupled autoregressive generation: Proposition 2 Consider a benchmark dataset such that c(s q ) ⊊ {t 1 , t 2 } for all s q ∼ P Q , where t 1 and t 2 are two single-token sequences. Let m and m ′ be two LLMs that assign positive probability to the sequences t 1 and t 2 and zero probability to any other sequence. If the sampling mechanism defined by f T and P U satisfies counterfactual stability, then, it holds that</p><formula xml:id="formula_4">Var[R m (U , S q ) -R m ′ (U ′ , S q )] &gt; Var[R m (U , S q ) -R m ′ (U , S q )].<label>(5)</label></formula><p>In the second canonical setting, the correct response to each prompt is a single-token sequence, the LLMs m and m ′ under comparison always output a single-token response, and the sampling mechanism used by the LLMs is given by the Gumbel-Max SCM <ref type="foot" target="#foot_7">13</ref> . Similarly as in the first canonical setting, this second setting is also found in real-world scenarios, particularly taking into account that the default categorical sampler in the library PyTorch <ref type="bibr" target="#b49">[51]</ref> implements the Gumbel-Max SCM. The following proposition shows that, as long as the model m ′ is similar enough to m, the variance of the difference in scores is lower under coupled generation: Proposition 3 Consider a benchmark dataset such that |c(s q )| = 1 for all s q ∼ P Q . Let m be an LLM that assigns positive probability to every single-token sequence and zero probability to any other sequence. If the sampling mechanism defined by f T and P U is given by the Gumbel-Max SCM, then, there exists a constant ε(m) &gt; 0 such that, for every LLM m ′ that assigns positive probability to every single-token sequence and zero probability to any other sequence and satisfies</p><formula xml:id="formula_5">d(m, m ′ ) = sup sq ∥f D (s q , m) -f D (s q , m ′ )∥ ∞ &lt; ε(m), it holds that Var[R m (U , S q ) -R m ′ (U ′ , S q )] &gt; Var[R m (U , S q ) -R m ′ (U , S q )].</formula><p>Based on the above proposition, we hypothesize that coupled autoregressive generation will reduce the number of samples required to reliably compare the performance of LLMs whenever these are sufficiently similar, e.g., whenever we compare fine-tuned or quantized versions of the same pre-trained LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation based on pairwise comparisons</head><p>In this section, we focus on the evaluation and comparison of LLMs according to their level of alignment with human preferences, as elicited by pairwise comparisons between outputs of different LLMs to the same prompts. Such an evaluation protocol has become particularly popular to evaluate and compare LLMs in open-ended, complex tasks in which, in contrast to benchmark datasets, there are no structured ground-truth outputs. In what follows, we provably show that, perhaps surprisingly, different LLMs may compare differently under coupled autoregressive generation and under independent autoregressive generation.</p><p>One of the standard approaches to evaluate and compare different LLMs according to their level of alignment with human pairwise preferences reduces to estimating the win-rate achieved by each LLM m against any other LLM m ′ ̸ = m, i.e.,<ref type="foot" target="#foot_8">foot_8</ref> </p><formula xml:id="formula_6">E U ∼P U ,U ′ ∼P U ,Sq∼P Q [1{R m (U ↑ , S q ) &gt; R m ′ (U ↑ ′ , S q )}]<label>(6)</label></formula><p>Independent generation</p><p>where 1{R m (u, s q ) &gt; R m ′ (u, s q )} = 1 (0) means that, for prompt s q and realized sequence of noise values u, the output of m is (not) preferred over the output of m ′ . <ref type="foot" target="#foot_9">15</ref>Here, similarly as in Eq. 2 in the evaluation based on benchmark datasets, we use different noise variables U and U ′ because, in this standard approach, each LLM generates outputs to each prompt independently (i.e., using independent autoregressive generation). Conversely, under coupled autoregressive generation, the win-rate adopts the following form:</p><formula xml:id="formula_7">E U ∼P U ,Sq∼P Q [1{R m (U ↑ , S q ) &gt; R m ′ (U ↑ , S q )}]<label>(7)</label></formula><p>Coupled generation</p><p>However, in contrast with the comparison of the expected difference in scores under independent and coupled autoregressive generation in the evaluation based on benchmark datasets, we cannot directly claim that Eqs. 6 and 7 are equivalent because the win-rate is non-linear with respect to R m (u, s q ) and R m ′ (u ′ , s q ). In what follows, we will further analyze the difference between win-rates in two canonical settings similar to those we used in Section 3.</p><p>In the first canonical setting, for each prompt, the response can only be one of two given single-token sequences and one of these sequences is preferred over the other by the user. Further, the LLMs under comparison always output one of them as a response and the sampling mechanism used by the LLMs satisfies counterfactual stability. Then, we can compute the win-rates achieved by each LLM m against any other LLM m ′ ̸ = m under independent and coupled autoregressive generation using the following proposition: Proposition 4 Given a fixed prompt s q ∼ P Q , assume that f R (s + ) &gt; f R (s -) for s + = s q •t + and s -= s q •t -, where t + and t -are single-token sequences. Further, assume that the LLMs m and m ′ respond t + with probability p m and p m ′ , respectively, and t -with probability 1p m and 1p m ′ , and the sampling mechanism defined by f T and P U satisfies counterfactual stability. Without loss of generality, assume p m ′ &gt; p m . Then, under coupled autoregressive generation, we have that</p><formula xml:id="formula_8">E U ∼P U [1{R m (U , s q ) &gt; R m ′ (U , s q )}] = 0, E U ∼P U [1{R m (U , s q ) &lt; R m ′ (U , s q )}] = p m ′ -p m .<label>(8)</label></formula><p>Conversely, under independent autoregressive generation, we have that</p><formula xml:id="formula_9">E U ,U ′ ∼P U [1{R m (U , s q ) &gt; R m ′ (U ′ , s q )}] = p m (1 -p m ′ ), E U ,U ′ ∼P U [1{R m (U , s q ) &lt; R m ′ (U ′ , s q )}] = p m ′ (1 -p m ) (9)</formula><p>From the above proposition, we can readily conclude that, in general, the win-rates do differ under independent and coupled autoregressive generation. Nevertheless, we may be tempted to conclude that, for ranking LLMs, this difference appears inconsequential because, for each fixed prompt s q , we have that</p><formula xml:id="formula_10">E U ∼P U [1{R m (U , s q ) &lt; R m ′ (U , s q )}] -E U ∼P U [1{R m (U , s q ) &gt; R m ′ (U , s q )}] = E U ,U ′ ∼P U [1{R m (U , s q ) &lt; R m ′ (U ′ , s q )}] -E U ,U ′ ∼P U [1{R m (U , s q ) &gt; R m ′ (U ′ , s q )}].</formula><p>However, whenever one needs to rank more than two LLMs, the difference in win-rates can be actually consequential-the rankings derived from the win-rates can be different under independent and coupled autoregressive generation, as illustrated by the following simple example. Consider we are given three LLMs m 1 , m 2 , and m 3 , and we need to rank them according to the average win-rate they achieve against each other on two input prompts q and q ′ , each with a preferred single-token response out of two single-token responses. Assume that the probability that each LLM outputs the preferred single-token response for q and q ′ is given by the table of the example introduced in Section 1. Under independent autoregressive generation, the average win-rates of m 1 , m 2 and m 3 are 0.1545, 0.15675 and 0.16225, respectively. Therefore, m 3 is ranked at the top, followed by m 2 , and m 1 is ranked last. In contrast, under coupled autoregressive generation, the average win-rates of m 1 , m 2 and m 3 are 0.0525, 0.0225, and 0.03, respectively, and thus m 1 is ranked at the top, followed by m 3 , and m 2 is ranked last. <ref type="foot" target="#foot_10">16</ref> Interestingly, the ranking obtained under coupled autoregressive generation aligns with the ranking obtained in Section 1 using the average accuracy of each LLM. More crucially, this case illustrates how rankings obtained using coupled and independent autoregressive generation can differ, leading to opposite conclusions regarding the LLMs' performance.</p><p>In the second canonical setting, for each prompt, the response can be one of any single-token sequences, and each of the sequences may provide a different level of user's satisfaction (i.e., achieve a different score). Further, the LLMs under comparison always output one of them as a response and the sampling mechanism used by the LLMs is given by the Gumbel-Max SCM. The following proposition shows that the number of ties between an LLM m and any other sufficiently similar LLM m ′ ̸ = m are higher under coupled autoregressive generation than under independent autoregressive generation:</p><formula xml:id="formula_11">Proposition 5 Given a fixed prompt s q ∼ P Q , assume, without loss of generality, that f R (s q • t 1 ) ≥ f R (s q • t 2 ) ≥ . . . ≥ f R (s q • t |V | ).</formula><p>Let m be an LLM that assigns positive probability to every single-token sequence and zero probability to any other sequence. If the sampling mechanism defined by f T and P U is given by the Gumbel-Max SCM, then, there exists a constant ε(m) &gt; 0 such that, for every LLM m ′ that assigns positive probability to every single-token sequence and zero probability to any other sequence and satisfies</p><formula xml:id="formula_12">d(m, m ′ ) = sup sq ∥f D (s q , m) -f D (s q , m ′ )∥ ∞ &lt; ε(m), it holds that E U ∼P U [1{R m (U , s q ) = R m ′ (U , s q )}] &gt; E U ,U ′ ∼P U [1{R m (U , s q ) = R m ′ (U ′ , s q )}].</formula><p>The above proposition implies that the win-rates under independent and coupled autoregressive generation are different and, similarly as in the first canonical setting, rankings derived from the win-rates may differ under independent and coupled autoregressive generation. We investigate this further in our experiments in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate several large language models from the Llama family under coupled and independent autoregressive generation using: (i) the benchmark dataset MMLU <ref type="bibr" target="#b50">[52]</ref> and (ii) pairwise comparisons between outputs of the LLMs when prompted using open-ended questions from the LMSYS Chatbot Arena platform <ref type="bibr" target="#b55">[57]</ref>. In all our experiments, the LLMs use an implementation of the Gumbel-Max SCM <ref type="bibr" target="#b33">[34]</ref> as a sampler both under coupled and independent autoregressive generation. For details on hardware, datasets and models used for experiments, refer to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on the MMLU dataset</head><p>In this section, we compare three LLMs of different sizes, namely, Llama-3.1-8B-Instruct and Llama-3.2-{1B, 3B}-Instruct, using the MMLU benchmark dataset <ref type="bibr" target="#b50">[52]</ref>, which comprise 14,042 multiple choice questions covering 52 knowledge areas. Recall that our theoretical results in Section 3 suggest that coupled  shows the absolute error in the estimation of the expected difference between the scores of the LLMs against the number of samples; for each point on the x-axis, we perform 1,000 sub-samplings and shaded areas correspond to 95% confidence intervals. Across all panels, we use all questions from the knowledge area "college computer science" of MMLU. We obtained qualitatively similar results for other knowledge areas (refer to Appendix D).</p><p>autoregressive generation requires fewer samples than independent generation to reliably estimate the competitive advantage of one LLM against another in certain canonical settings. Here, our goal is to empirically investigate to what extent these results generalize to evaluations based on the MMLU dataset.</p><p>Experimental setup. In our experiments, for each multiple choice question in the MMLU benchmark dataset, we provide the question itself together with the available options (4 for each question, indexed from A to D) as an input prompt to the LLMs. Further, we instruct the LLMs to generate an output sequence comprising only the index of the selected option through a system prompt-refer to Appendix C for the exact prompt. To evaluate the outputs provided by each LLM, we use a binary score R ∈ {0, 1}, which indicates whether the LLM output is the (single) correct (R = 1) or incorrect (R = 0) answer of the given options. To obtain reliable conclusions, we experiment with each multiple choice question 10 times, each time using a (different) random seed to generate the Gumbel noise variables used by the sampler. Due to space constraints, in what follows, we compare Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct on the knowledge area "college computer science". In Appendix D, we provide further results on other knowledge areas and other pairs of LLMs.</p><p>Results. Figures <ref type="figure" target="#fig_1">2a</ref> and <ref type="figure" target="#fig_1">2b</ref> show that the scores of the LLMs are positively correlated under coupled generation and thus the variance of the difference in scores is lower under coupled generation than under independent, in agreement with Proposition 1. Further, we compute the error in the estimation of the expected difference in scores resulting from using the two approaches as a function of the available sample size. To this end, we first estimate the expected score difference using 1,000 samples and consider this as (a proxy of) the ground truth. Then, we compute the absolute estimation error achieved by independent and coupled generation while sub-sampling the original samples across various sample sizes. Figure <ref type="figure" target="#fig_1">2c</ref> summarizes the results, which show that, as expected from our theoretical analysis, a lower variance of the difference in scores under coupled generation leads to a reduction in the number of samples required to achieve equivalent error in the estimation of the expected difference between the scores of the LLMs. Perhaps surprisingly, we find that this reduction can, in practice, be quite large. For example, to achieve an estimation error of ≈0.034, coupled generation needs 40% fewer samples than independent generation. Each empirical win-rate is computed using pairwise comparisons between the outputs to 500 questions with 10 (different) random seeds under both coupled and independent generation. The error bars correspond to 95% confidence intervals. For each pair of empirical win-rates under coupled and independent generation, we conduct a two-tailed z-test, to test the null hypothesis that the empirical win-rates are the same; ( * * * * , * * * ) indicate p-values (&lt; 0.0001, &lt; 0.001). We obtain qualitatively similar results for other LLMs (refer to Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on the LMSYS-Chat-1M Dataset</head><p>In this section, we compare the same three LLMs as in the previous section as well as three quantized variants 17 , namely, Llama-3.1-8B-Instruct-{AWQ-INT4, bnb-4bit, bnb-8bit}, using pairwise comparisons between their outputs by a strong LLM, when prompted with open-ended questions from the LMSYS Chatbot Arena platform <ref type="bibr" target="#b55">[57]</ref>. Similarly as in the previous section, here, our goal is to investigate to what extent the theoretical results derived in Section 4, which show that the win-rates under coupled and independent autoregressive generation are different in certain canonical settings, generalize.</p><p>Experimental setup. We experiment with 500 questions from the LMSYS-Chat-1M dataset <ref type="bibr" target="#b56">[58]</ref>. We provide the question itself as an input prompt to the LLMs, and instruct them to generate a concise response as an output through a system prompt. Further, similarly as elsewhere <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">59]</ref>, we use a strong LLM, namely, GPT-4o-2024-11-20, as a judge. More specifically, for each question and pair of outputs provided by two different LLMs, we prompt the judge to respond which of the two outputs it prefers, but allowing the judge to declare a tie-for the exact prompts we use, refer to Appendix C. Given these pairwise comparisons, to evaluate the outputs provided by each LLM, we use the win-rate achieved by each LLM against each other. To obtain reliable conclusions, similarly as in the previous section, we repeat each experiment 10 times, each time using a (different) random seed to generate the Gumbel noise variables used by the Gumbel-Max SCM.</p><p>Results. We find that the empirical win-rate of each LLM against any other LLM is generally lower under coupled generation than under independent generation, as shown in Figure <ref type="figure" target="#fig_2">3</ref> for Llama-3.1-8B-Instruct-bnb-8bit and Figure <ref type="figure" target="#fig_4">6</ref> in Appendix E for other LLMs. Moreover, whenever the LLMs under comparison are sufficiently similar, the difference between win-rates is statistically significant, suggesting that our theoretical results may generalize beyond the canonical setting discussed in Section 4. We hypothesize that this is partially due to an increase in the number of ties under coupled autoregressive generation. For example, for Llama-3.1-8B-Instruct-bnb-8bit, we observe a 24%, 11%, 15% increase in the number of ties in the pairwise comparisons against Llama-3.1-8B-Instruct, Llama-3.1-8B-Instruct-bnb-4bit, and Llama-3.1-8B-Instruct-AWQ-INT4. Remarkably, the difference in empirical win-rates leads to differences in the rankings derived from the average win-rates, as shown in Table <ref type="table">1</ref>. Under independent generation, the average win-rates achieved by Llama-3.1-8B-Instruct and Llama-3.1-8B-Instruct-bnb-8bit are 17 Refer to Appendix C for more details on the quantized variants.</p><p>Coupled Independent LLM Rank Avg. win-rate Rank Avg. win-rate 8B 1 0.3670 ±0.0020 1 0.3863 ±0.0020 bnb-8bit 2 0.3562 ±0.0020 1 0.3825 ±0.0020 bnb-4bit 3 0.3339 ±0.0020 3 0.3463 ±0.0020 AWQ-INT4 4 0.3164 ±0.0019 4 0.3310 ±0.0019 3B 5 0.2787 ±0.0019 5 0.2828 ±0.0019 1B 6 0.1650 ±0.0015 6 0.1664 ±0.0015</p><p>Table <ref type="table">1</ref>: Average win-rate and ranking of each LLM on questions from the LMSYS-Chat-1M dataset. To estimate the average win-rate of each LLM, along with 95% confidence intervals, we use the pairwise comparisons between the outputs of all pairs of LLMs using all 500 questions with 10 (different) random seeds under both coupled and independent generation. To derive the rankings, for each LLM, we choose the lowest ranking provided by the method of Chatzi et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>statistically indistinguishable and thus they are both ranked at the top. However, under coupled generation, Llama-3.1-8B-Instruct has a competitive advantage against Llama-3.1-8B-Instruct-bnb-8bit, and it is ranked at the top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Limitations</head><p>In this section, we discuss several aspects of our work, which we believe are important to consider and may serve as a basis for future research.</p><p>Model assumptions. Our theoretical analysis of coupled autoregressive generation focuses on sampling mechanisms that satisfy counterfactual stability <ref type="bibr" target="#b35">[36]</ref>. Although counterfactual stability has been shown to be a desirable property for causal mechanisms in SCMs and, more specifically, for causal mechanisms used for sampling in LLMs <ref type="bibr" target="#b33">[34]</ref>, counterfactual stability may not always be appropriate and should be justified by domain specific knowledge <ref type="bibr" target="#b58">[60]</ref>. In this context, it is also worth mentioning that the Gumbel-Max SCM is not the only SCM that satisfies counterfactual stability <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b59">61]</ref>. Therefore, it would be interesting to understand the sensitivity of coupled autoregressive generation to this specific choice of SCM as well as extending our theoretical analysis to sampling mechanisms satisfying other alternative properties <ref type="bibr" target="#b60">[62]</ref>.</p><p>Practical considerations. Our experimental results and theoretical analysis suggest that coupled autoregressive generation is most advantageous over independent autoregressive generation whenever the LLMs under comparison are sufficiently close in terms of their next-token distributions. Motivated by this observation, it would be important to identify which parts of the LLM development pipeline (e.g., the LLMs' architectures, training data, or fine-tuning process) lead, in practice, to sufficiently small changes in the next-token distributions for coupled autoregressive generation to be most beneficial.</p><p>Our causal model for coupled autoregressive generation assumes that the LLMs under comparison share the same vocabulary. However, in practice, this may not hold since models use different tokenizers-different families of tokenizers may even use different low-level representations for tokens that appear to be the same at the string level. <ref type="foot" target="#foot_11">18</ref> One could think of naively lifting this assumption by merging the vocabularies of different LLMs, however, we empirically found that, using this strategy, different LLMs end up using different tokens (and thus noise values) to generate the same responses and thus coupled autoregressive generation provides significantly lower gains. Extending our causal model for coupled autoregressive generation to LLMs with different tokenizers is an interesting, albeit challenging, direction for future work.</p><p>Evaluation. We have conducted experiments using LLMs from the Llama family, namely Llama-3.1-8B-Instruct and Llama-3.2-{1B, 3B}-Instruct, and quantized versions thereof. It would be interesting to conduct experiments with LLMs from other families and also consider fine-tuned versions of them to understand how coupled autoregressive generation behaves in different settings. Furthermore, we have experimented with (i) a single benchmark dataset (i.e., MMLU) and (ii) a single dataset of prompts for pairwise comparisons (i.e., LMSYS Chatbot Arena), where we have used a strong LLM as a judge (i.e., <ref type="bibr">GPT-4o-2024-11-20)</ref> and win-rate as an evaluation metric. To better understand the benefits of coupled autoregressive generation, it would be important to experiment with additional datasets, pairwise comparisons made by humans, and additional evaluation metrics based on, e.g., the Elo rating system <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref> and the Bradley-Terry model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we have introduced a causal model of coupled autoregressive generation that enables the evaluation and comparison of different LLMs under the same source of randomness. In several canonical settings, we have shown that, in evaluations based on benchmark datasets, coupled autoregressive generation can provably reduce the number of samples required to reliably compare the performance of LLMs and, in evaluations based on pairwise comparisons, it can provably lead to different and, perhaps more intuitive, rankings of LLMs in comparison with independent autoregressive generation. Lastly, we have empirically demonstrated that our theoretical results generalize to several state of the art LLMs and datasets commonly used for the evaluation and ranking of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Formal Definition of Counterfactual Stability</head><p>Counterfactual stability is a desirable property of SCMs <ref type="bibr" target="#b35">[36]</ref> that has previously been used in the context of autoregressive generation of LLMs <ref type="bibr" target="#b33">[34]</ref>. In the following, we provide its formal definition along with a simple example to explain the intuition behind it. Throughout this section, P C ; do(•) denotes the probability of the interventional distribution entailed by an SCM C under an intervention do(•). Moreover, P C | ⋆ ; do(•) denotes the probability of the counterfactual distribution entailed by an SCM C under an intervention do(•) given that an observed event ⋆ has already occurred. Definition 1 A sampling mechanism defined by f T and P U satisfies counterfactual stability if for all LLMs m, m ′ ∈ M, i ∈ {1, 2, . . . , K} and tokens t 1 , t 2 ∈ V with t 1 ̸ = t 2 , the condition</p><formula xml:id="formula_13">P C ; do(M =m ′ ) [T i = t 1 | D i ] P C ; do(M =m) [T i = t 1 | D i ] ≥ P C ; do(M =m ′ ) [T i = t 2 | D i ] P C ; do(M =m) [T i = t 2 | D i ]<label>(10)</label></formula><formula xml:id="formula_14">implies that P C | Di,M =m,Ti=t1 ; do(M =m ′ ) [T i = t 2 ] = 0.</formula><p>The property of counterfactual stability has an intuitive interpretation that can be best understood via a simple example. Assume that the vocabulary contains 2 tokens "A" and "B" and, using LLM m, the next-token distribution at a time step i assigns values 0.6, 0.4 to the two tokens, respectively. Moreover, the realized noise value u i is such that the token "A" is sampled. Now, consider that, while keeping the noise value u i fixed, we change the LLM to m ′ , resulting in a next-token distribution that assigns values 0.7, 0.3 to the two tokens, respectively. Counterfactual stability ensures that, since the noise value u i led to "A" being sampled under m at 0.6 to 0.4 odds, the same value cannot lead to "B" being sampled under m ′ where its relative odds are lower (i.e., 0.3 to 0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Proposition 1</head><p>We can rewrite the variance of the difference in scores under independent generation in terms of the variance of the difference in scores under coupled generation as follows:</p><formula xml:id="formula_15">Var[R m (U , S q ) -R m ′ (U ′ , S q )]] = Var[R m (U , S q ) -R m ′ (U , S q ) + R m ′ (U , S q ) -R m ′ (U ′ , S q )]] = Var[R m (U , S q ) -R m ′ (U , S q )] + Var[R m ′ (U , S q ) -R m ′ (U ′ , S q )] + 2 • Cov[R m (U , S q ) -R m ′ (U , S q ), R m ′ (U , S q ) -R m ′ (U ′ , S q )].</formula><p>For the variance of the difference in scores for the same LLM under independent noise values, we have that</p><formula xml:id="formula_16">Var[R m ′ (U , S q ) -R m ′ (U ′ , S q )] (a) = E[(R m ′ (U , S q ) -R m ′ (U ′ , S q )) 2 ] -E[R m ′ (U , S q ) -R m ′ (U ′ , S q )] 2 (b) = E[R m ′ (U , S q ) 2 -2 • R m ′ (U , S q )R m ′ (U ′ , S q ) + R m ′ (U ′ , S q ) 2 ] (c) = 2 • E[R m ′ (U , S q ) 2 ] -2 • E[R m ′ (U , S q )R m ′ (U ′ , S q )],</formula><p>where (a) holds by the definition of variance, (b) is due to the subtraction term being 0, and (c) is due to the linearity of expectation. Further, for the covariance of the difference in scores under independent generation and the difference in scores under coupled generation, we have that</p><formula xml:id="formula_17">Cov[R m (U , S q ) -R m ′ (U , S q ), R m ′ (U , S q ) -R m ′ (U ′ , S q )] (a) = E[(R m (U , S q ) -R m ′ (U , S q )) • (R m ′ (U , S q ) -R m ′ (U ′ , S q ))] -E[R m (U , S q ) -R m ′ (U , S q )] • E[R m ′ (U , S q ) -R m ′ (U ′ , S q )] (b) = E[R m (U , S q )R m ′ (U , S q )] -E[R m (U , S q )R m ′ (U ′ , S q )] -E[R m ′ (U , S q )R m ′ (U , S q )] + E[R m ′ (U , S q )R m ′ (U ′ , S q )] (c) = Cov[R m (U , S q ), R m ′ (U , S q )] -E[R m ′ (U , S q ) 2 ] + E[R m ′ (U , S q )R m ′ (U ′ , S q )]]</formula><p>where (a) and (c) hold by the definition of covariance and (b) is due to the last term being zero and by the expansion of the first term. Putting all the above results together, it follows that</p><formula xml:id="formula_18">Var[R m (U , S q ) -R m ′ (U ′ , S q )]] = Var[R m (U , S q ) -R m ′ (U , S q )] + 2 • Cov [R m (U , S q ), R m ′ (U , S q )] + 2 • E[R m ′ (U , S q )R m ′ (U ′ , S q )] -2 • E[R m ′ (U , S q ) 2 ] + 2 • E[R m ′ (U , S q ) 2 ] -2 • E[R m ′ (U , S q )R m ′ (U ′ , S q )] = Var[R m (U , S q ) -R m ′ (U , S q )] + 2 • Cov[R m (U , S q ), R m ′ (U , S q )]</formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Proposition 2</head><p>Due to Proposition 1, to show that Eq. 5 holds, it suffices to show that the covariance between the scores of the different LLMs under coupled generation is non-negative, i.e., Cov[R m (U , S q ), R m ′ (U , S q )] ≥ 0.</p><p>To this end, we first rewrite the covariance as</p><formula xml:id="formula_19">Cov[R m (U , S q ), R m ′ (U , S q )] = P [R m (U , S q ) = 1, R m ′ (U , S q ) = 1]-P [R m (U , S q ) = 1]•P [R m ′ (U , S q ) = 1] = sq P [S q = s q ] • (P [R m (U , s q ) = 1, R m ′ (U , s q ) = 1] -P [R m (U , s q ) = 1] • P [R m ′ (U , s q ) = 1])<label>(11)</label></formula><p>Next, we note that the event R m (U , s q ) = 1 is equivalent to LLM m sampling the ground truth token for prompt s q . Without loss of generality, assume t 1 is the ground truth token, i.e., c(s q ) = t 1 . Then, since only tokens {t 1 , t 2 } have positive probability under m and m ′ , it must hold that either (i) one LLM assigns a greater probability to t 1 and the other LLM assigns a greater probability to t 2 , or (ii) both LLMs assign the same probabilities. Further, since the sampling mechanism defined by f T and P U satisfies counterfactual stability, we have that the condition in Eq. 10 holds in both (i) and (ii) and, under coupled generation, the LLM with greater (or equal) probability for t 1 will always sample t 1 when the LLM with lower (or equal) probability does. This implies that</p><formula xml:id="formula_20">P [R m (U , s q ) = 1, R m ′ (U , s q ) = 1] = min{P [R m (U , s q ) = 1], P [R m ′ (U , s q ) = 1]}<label>(12)</label></formula><p>Finally, since it holds that</p><formula xml:id="formula_21">min{P [R m (U , s q ) = 1], P [R m ′ (U , s q ) = 1]} ≥ P [R m (U , s q ) = 1]P [R m ′ (U , s q ) = 1]<label>(13)</label></formula><p>because P [R m (U , s q ) = 1] ∈ (0, 1) and P [R m ′ (U , s q ) = 1] ∈ (0, 1) by assumption, we can conclude from Eq. 11 that</p><formula xml:id="formula_22">Cov[R m (U , S q ), R m ′ (U , S q )] &gt; 0.<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Proposition 3</head><p>Using Proposition 1, we have that</p><formula xml:id="formula_23">Cov[R m (U , S q ), R m ′ (U , S q )] = E[R m (U , S q ) • R m ′ (U , S q )] -E[R m (U , S q )] • E[R m ′ (U , S q )] = P [R m (U , S q ) = 1, R m ′ (U , S q ) = 1] (i) -P [R m (U , S q ) = 1) • P [R m ′ (U , S q ) = 1]<label>(ii)</label></formula><p>.</p><p>In the remainder of the proof, we will bound each term (i) and (ii) separately and, since |c(s q )| = 1 for all s q ∼ P Q , assume without loss of generality that the correct token is single-token sequence t 1 .</p><p>To bound the term (ii), first note that, using the definition of the Gumbel-Max SCM, we have that, for each k ∈ {2, . . . , |V |}, it holds that</p><formula xml:id="formula_24">R m (U , s q ) = 1 ⇐⇒ U 1 + log([f D (s q , m)] t1 ) ≥ U k + log([f D (s q , m)] t k ), R m ′ (U , s q ) = 1 ⇐⇒ U 1 + log([f D (s q , m ′ )] t1 ) ≥ U k + log([f D (s q , m ′ )] t k ).</formula><p>Next, let ε * &gt; 0 be an arbitrary constant that we will determine later such that</p><formula xml:id="formula_25">| log([f D (S q , m)] t k ) -log([f D (S q , m ′ )] t k )| ≤ ε * ,<label>(15)</label></formula><p>and note that since, by assumption, D t k &gt; 0 for all k ∈ {1, . . . , |V |}, any bound on the absolute difference of log-probabilities</p><formula xml:id="formula_26">| log([f D (S q , m)] t k ) -log([f D (S q , m ′ )] t k )| uniformly implies a bound on the difference of probabilities |[f D (S q , m)] t k -[f D (S q , m ′ )] t k | and vice versa.</formula><p>For simplicity, we prove the result in the log-domain. Now, using the bound defined by Eq. 15, we have that</p><formula xml:id="formula_27">k̸ =1 {U 1 + log([f D (S q , m ′ )] t1 ) ≥ U k + log([f D (S q , m ′ )] t k )} ⊂ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) + ε * ≥ U k + log([f D (S q , m)] t k ) -ε * } ,</formula><p>and we can then bound the term (ii) as follows:</p><formula xml:id="formula_28">P [R m (U , S q ) = 1] • P [R m ′ (U , S q ) = 1] = P [∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k )}] × P [∩ k̸ =1 {U 1 + log([f D (S q , m ′ )] t1 ) ≥ U k + log([f D (S q , m ′ )] t k )}] ≤ P [∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k )}] × P [∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) + ε * ≥ U k + log([f D (S q , m)] t k ) -ε * }].</formula><p>To bound the term (i), first note that, using the bound defined by Eq. 15, we have that</p><formula xml:id="formula_29">k̸ =1 {U 1 + log([f D (S q , m ′ )] t1 ) ≥ U k + log([f D (S q , m ′ )] t k )} ⊃ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) -ε * ≥ U k + log([f D (S q , m)] t k ) + ε * } .</formula><p>Thus, we can bound the term (i) as follows:</p><formula xml:id="formula_30">P [R m (U , S q ) = 1, R m ′ (U , S q ) = 1] = P ∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k )} ∩ {U 1 + log([f D (S q , m ′ )] t1 ) ≥ U k + log([f D (S q , m ′ )] t k )} ≥ P ∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k )} ∩ {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k ) + 2ε * } (a) = sq P [S q = s q ] • P [∩ k̸ =1 {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k ) + 2ε * }],</formula><p>where (a) follows from the fact that</p><formula xml:id="formula_31">{U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k ) + 2ε * } ⊂ {U 1 + log([f D (S q , m)] t1 ) ≥ U k + log([f D (S q , m)] t k )} .</formula><p>Now, note that, for k ∈ {2, . . . , |V |}, the variable</p><formula xml:id="formula_32">X k ≡ U 1 -U k ∼ Logistic(0, 1) (for k = 1, define X k ≡ 0).</formula><p>Therefore, we can rewrite the bound for (i) as</p><formula xml:id="formula_33">P [R m (U , S q ) = 1, R m ′ (U , S q ) = 1] ≥ sq P [S q = s q ] • k̸ =1 •P [{X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t1 ) + 2ε * }]</formula><p>and we can rewrite the bound for (ii) as</p><formula xml:id="formula_34">P [R m (U , S q ) = 1]P [R m ′ (U , S q ) = 1] ≤ sq P [S q = s q ] •    k̸ =1 P [{X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t k ) -2ε * }]    × P [∩ k̸ =1 {X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t k )}].</formula><p>As a consequence, to prove that P</p><formula xml:id="formula_35">[R m (U , S q ) = 1, R m ′ (U , S q ) = 1] &gt; P [R m (U , S q ) = 1]P [R m ′ (U , S q ) = 1], it suffices to show that sq P [S q = s q ] k̸ =1 •P [{X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t1 ) + 2ε * }] &gt; sq P [S q = s q ] k̸ =1 •P [{X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t1 ) -2ε * }] × P [∩ k̸ =1 {X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t1 )}] (16)</formula><p>To do so, note that Eq. 16 holds trivially for ε * = 0 since</p><formula xml:id="formula_36">P [∩ k̸ =1 {X k ≥ log([f D (S q , m)] t k ) -log([f D (S q , m)] t1 )}] &lt; 1,</formula><p>which is a fixed term independent of m ′ . Since all terms in Eq. 16 are continuous in ε * , there exists ε * (m) &gt; 0, possibly dependent of m but independent of m ′ , such that Eq. 16 holds if</p><formula xml:id="formula_37">sup sq ∥log(f D (s q , m)) -log(f D (s q , m ′ ))∥ ∞ &lt; ε * (m).</formula><p>Since by assumption D t &gt; 0 for all t ∈ V , there exists ε(m) &gt; 0 in probability space such that Eq. 16 holds if</p><formula xml:id="formula_38">sup sq ∥f D (s q , m) -f D (s q , m ′ )∥ ∞ &lt; ε(m).</formula><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Proposition 4</head><p>Under coupled autoregressive generation, if the LLM m samples the preferred token t + , then the LLM m ′ must also sample t + because t + is more likely under m ′ than under m and the sampling mechanism defined by f T and P U satisfies counterfactual stability. This implies that the win-rate achieved by m against m ′ is</p><formula xml:id="formula_39">E U ∼P U [1{R m (U , s q ) &gt; R m ′ (U , s q )}] = P [f T (f D (s q , m), U ) = t + , f T (f D (s q , m ′ ), U ) = t -] = 0<label>(17)</label></formula><p>and that</p><formula xml:id="formula_40">P [f T (f D (s q , m), U ) = t + , f T (f D (s q , m ′ ), U ) = t + ] = P [f T (f D (s q , m), U ) = t + ] = p m .<label>(18)</label></formula><p>Using the same reasoning, if the LLM m ′ samples the non-preferred token t -, then, m must also sample t - because t -is more likely under m than under m ′ . This implies that</p><formula xml:id="formula_41">P [f T (f D (s q , m), U ) = t -, f T (f D (s q , m ′ ), U ) = t -] = P [f T (f D (s q , m ′ ), U ) = t -] = 1 -p m ′<label>(19)</label></formula><p>Then, from Eq. 18 and Eq. 19, we can conclude that</p><formula xml:id="formula_42">E U ∼P U [1{R m (U , s q ) = R m ′ (U , s q )}] = p m + (1 -p m ′ )<label>(20)</label></formula><p>Finally, from Eq. 17 and Eq. 20, we can conclude that the win-rate achieved by m ′ against m is</p><formula xml:id="formula_43">E U ∼P U [1{R m (U , s q ) &lt; R m ′ (U , s q )}] = 1 -E U ∼P U [1{R m (U , s q ) &gt; R m ′ (U , s q )}] -E U ∼P U [1{R m (U , s q ) = R m ′ (U , s q )}] = p m ′ -p m .</formula><p>Under independent autoregressive generation, the LLMs m and m ′ sample tokens independently from each other, i.e., f T (f D (s q , m), U ) ⊥ f T (f D (s q , m ′ ), U ′ ). Thus, we can factorize all joint probabilities when computing the win-rates and obtain</p><formula xml:id="formula_44">E U ,U ′ ∼P U [1{R m (U , s q ) &gt; R m ′ (U ′ , s q )}] = P [f T (f D (s q , m), U ) = t + ] • P [f T (f D (s q , m ′ ), U ′ ) = t -] = p m • (1 -p m ′ ) and E U ,U ′ ∼P U [1{R m (U , s q ) &lt; R m ′ (U ′ , s q )}] = p m ′ • (1 -p m ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Proof of Proposition 5</head><p>We follow the notations and technique of Proposition 3. Fix query s q and consider first the case of independent autoregressive generation. Since each LLM can only assign a non-zero probability to single-token sequences, we have:</p><formula xml:id="formula_45">P [R m (U , s q ) = R m ′ (U ′ , s q )] = |V | k=1 P [f T (f D (s q , m), U ) = t k ]P [f T (f D (s q , m ′ ), U ) = t k ] &lt; |V | k=1 P [f T (f D (s q , m), U ) = t k ],</formula><p>In the case of coupled autoregressive generation, since</p><formula xml:id="formula_46">P [{f T (f D (s q , m), U ) = t k } ∩ {f T (f D (s q , m), U ) = t j }] = 0, i ̸ = j,</formula><p>we obtain:</p><formula xml:id="formula_47">P [R m (U , s q ) = R m ′ (U , s q )] = P [∪ i {f T (f D (s q , m), U ) = t k , f T (f D (s q , m ′ ), U ) = t k }] = k P [{f T (f D (s q , m), U ) = t k , f T (f D (s q , m ′ ), U ) = t k }] = k P [f T (f D (s q , m), U ) = t k ]P [f T (f D (s q , m ′ ), U ) = t k |f T (f D (s q , m), U ) = t k ].</formula><p>We now follow <ref type="bibr" target="#b61">[63]</ref> and expand the posterior Gumbels, P</p><formula xml:id="formula_48">[f T (f D (s q , m ′ ), U ) = t k |f T (f D (s q , m), U ) = t k ],</formula><p>as truncated Gumbel distributions. In particular, we leverage the fact that</p><formula xml:id="formula_49">max t∈V {U t + log([f D (s q , •)] t )} ∼ Gumbel(0, 1),<label>(21)</label></formula><p>and that a Gumbel distribution, with parameter log(θ), truncated at b ∼ Gumbel(0, 1) can be sampled as</p><formula xml:id="formula_50">-log(exp(-b) -log(η)/θ), η ∼ U (0, 1).<label>(22)</label></formula><p>Furthermore, by assumption, D t k &gt; 0 for all k ∈ {1, . . . , |V |}, so that any bound on the absolute difference of log-probabilities | log([f D (s q , m)] t k )log([f D (s q , m ′ )] t k )| uniformly implies a bound on the difference of probabilities |[f D (s q , m)] t k -[f D (s q , m ′ )] t k | and vice versa. Using the bound</p><formula xml:id="formula_51">| log([f D (s q , m)] t k ) -log([f D (s q , m ′ )] t k )| ≤ ε *</formula><p>and the Gumbel properties in Eq. 21 and Eq. 22, we obtain: </p><formula xml:id="formula_52">P [R m (U , s q ) = R m ′ (U , s q )] = k P [f T (f D (s q , m), U ) = t k ] × P k log([f D (s 1 , m ′ )] t k ) -log([f D (s 1 , m)] t k ) -log(-log(η k )) ≥ log([f D (s 1 , m ′ )]</formula><p>where η k ∼ U(0, 1) are independently distributed uniform random variables. Now, note that the claim holds for ε * = 0 since, in that case, we have that</p><formula xml:id="formula_54">P k {-log(-log(η k )) ≥ -log(-log(η k ) -log(η k )/[f D (s 1 , m ′ )] t k )} = 1,</formula><p>using that x →log(x) is strictly decreasing. Since all terms in Eq. 23 are continuous in ε * , there exists ε * (m) &gt; 0, possibly dependent on m but independent of m ′ , such that</p><formula xml:id="formula_55">P [R m (U , s q ) = R m ′ (U , s q )] &gt; P [R m (U , s q ) = R m ′ (U ′ , s q )]<label>(24)</label></formula><p>holds if sup sq ∥log(f D (s q , m))log(f D (s q , m ′ ))∥ ∞ &lt; ε * (m).</p><p>Since by assumption D t &gt; 0 for all t ∈ V , there exists ε(m) &gt; 0 in probability space such that Eq. 24 holds if sup sq ∥f D (s q , m)f D (s q , m ′ )∥ ∞ &lt; ε(m).</p><p>This concludes the proof.</p><p>B.6 Calculation of average win-rates in the example used in Sections 1 and 4</p><p>In this section, we provide detailed calculations of the win-rates for the example in Sections 1 and 4. Recall that in this example, we are given three LLMs m 1 , m 2 and m 3 , and we need to rank them according to their ability to answer correctly two types of input prompts, q and q ′ , picked uniformly at random. We assume that the true probability that each LLM answers correctly each type of input prompt is given by: m 1 m 2 m 3 q p 1 = 0.4 p 2 = 0.48 p 3 = 0.5 q ′ p ′ 1 = 1 p ′ 2 = 0.9 p ′ 3 = 0.89</p><p>Using Proposition 4, the win-rates under independent autoregressive generation are given, for each LLM m k , by:</p><formula xml:id="formula_56">1 2 j̸ =k E U ,U ′ ∼P U ,Sq∼P Q [1{R m k (U , S q ) &gt; R mj (U ′ , S q )}] = j̸ =k p k (1 -p j ) + j̸ =k p ′ k (1 -p ′ j ) 4 . (<label>25</label></formula><formula xml:id="formula_57">)</formula><p>Substituting the numerical values we obtain:</p><formula xml:id="formula_58">1 2 j̸ =1 E U ,U ′ ∼P U ,Sq∼P Q [1{R m1 (U , S q ) &gt; R mj (U ′ , S q )}] = 0.1545, 1 2 j̸ =2 E U ,U ′ ∼P U ,Sq∼P Q [1{R m2 (U , S q ) &gt; R mj (U ′ , S q )}] = 0.15675, 1 2 j̸ =3 E U ,U ′ ∼P U ,Sq∼P Q [1{R m3 (U , S q ) &gt; R mj (U ′ , S q )}] = 0.16225<label>(26)</label></formula><p>Similarly, using Proposition 4, the win-rates using coupled autoregressive generation can be written, for each LLM m k , as:</p><formula xml:id="formula_59">1 2 j̸ =k E U ∼P U ,Sq∼P Q [1{R m k (U , S q ) &gt; R mj (U , S q )}] = j̸ =k (p k -p j ) + + j̸ =k (p ′ k -p ′ j ) + 4 ,<label>(27)</label></formula><p>where (•) + = max(0, •) denotes the positive part. Substituting the numerical values we obtain:</p><formula xml:id="formula_60">1 2 j̸ =1 E U ∼P U ,Sq∼P Q [1{R m1 (U , S q ) &gt; R mj (U , S q )}] = 0.0525, 1 2 j̸ =2 E U ∼P U ,Sq∼P Q [1{R m2 (U , S q ) &gt; R mj (U , S q )}] = 0.0225, 1<label>2</label></formula><p>j̸ =3 E U ∼P U ,Sq∼P Q [1{R m3 (U , S q ) &gt; R mj (U , S q )}] = 0.03. Empirical estimate of the win-rate under coupled autoregressive generation as given by Eq. 7 and under independent generation generation as given by Eq. 6. Each empirical win-rate is computed using pairwise comparisons between the outputs of each LLM and any other LLM over 500 questions with 10 (different) random seeds. The error bars correspond to 95% confidence intervals. For each pair of empirical win-rates, we conduct a two-tailed test, to test the hypothesis that the empirical win-rates are the same; ( * * * * , * * * , * * , * ) indicate p-values (&lt; 0.0001, &lt; 0.001, &lt; 0.01, &lt; 0.05), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results on the LMSYS-Chat-1M Dataset</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Estimation error vs. # samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct on multiplechoice questions from the MMLU dataset. Panel (a) shows the kernel density estimate (KDE) of the covariance between the scores of the two LLMs on each question under coupled generation; the dashed line corresponds to the average value. Panel (b)shows the KDE of the variance of the difference between the scores of the LLMs on each question under coupled and independent generation; the highlighted point corresponds to the median value. Panel (c) shows the absolute error in the estimation of the expected difference between the scores of the LLMs against the number of samples; for each point on the x-axis, we perform 1,000 sub-samplings and shaded areas correspond to 95% confidence intervals. Across all panels, we use all questions from the knowledge area "college computer science" of MMLU. We obtained qualitatively similar results for other knowledge areas (refer to Appendix D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Empirical win-rate of Llama-3.1-8B-Instruct-bnb-8bit against any other LLM on questions from the LMSYS-Chat-1M dataset. Each empirical win-rate is computed using pairwise comparisons between the outputs to 500 questions with 10 (different) random seeds under both coupled and independent generation. The error bars correspond to 95% confidence intervals. For each pair of empirical win-rates under coupled and independent generation, we conduct a two-tailed z-test, to test the null hypothesis that the empirical win-rates are the same; ( * * * * , * * * ) indicate p-values (&lt; 0.0001, &lt; 0.001). We obtain qualitatively similar results for other LLMs (refer to Appendix E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>tj )log([f D (s 1 , m)] tj )log(-log(η k )log(η j )/[f D (s 1 , m ′ )] tj ) ≥ k P [f T (f D (s q , m), U ) = t k ] × P ∩ k {log(-log(η k )) ≥ -2ε *log(-log(η k )log(η j )/[f D (s 1 , m ′ )] tj )}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Empirical win-rate of each LLM against other LLMs on questions from the LMSYS-Chat-1M dataset. Empirical estimate of the win-rate under coupled autoregressive generation as given by Eq. 7 and under independent generation generation as given by Eq. 6. Each empirical win-rate is computed using pairwise comparisons between the outputs of each LLM and any other LLM over 500 questions with 10 (different) random seeds. The error bars correspond to 95% confidence intervals. For each pair of empirical win-rates, we conduct a two-tailed test, to test the hypothesis that the empirical win-rates are the same; ( * * * * , * * * , * * , * ) indicate p-values (&lt; 0.0001, &lt; 0.001, &lt; 0.01, &lt; 0.05), respectively.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>If an LLM is forced to output tokens deterministically, multiple lines of evidence suggest that its performance worsens<ref type="bibr" target="#b32">[33]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Refer to Appendix B.6 for the detailed calculation of the average win-rates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Here, V j denotes the set of all sequences of length j that can be constructed from the tokens in V . We restrict our attention to sequences of finite length (≤ K) because, in practice, the context window of LLMs is finite.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We use capital letters to denote random variables and lowercase letters to denote their realizations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>The default categorical sampler in PyTorch<ref type="bibr" target="#b49">[51]</ref>, one of the most popular libraries used by state of the art LLMs, is an implementation of the Gumbel-Max SCM<ref type="bibr" target="#b35">[36]</ref>, which satisfies counterfactual stability. For a formal definition of counterfactual stability, refer to Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>In our work, we implicitly assume that different LLMs share the same vocabulary V , however, in practice, this may not hold if the LLMs use different tokenizers. Refer to Section 6 for further discussion on this point.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>Here, our goal is to illustrate that there exist natural conditions under which coupled autoregressive generation is provably beneficial in comparison to independent autoregressive generation. However, in practice, in this canonical setting, one could directly use the LLMs' probabilities for the two tokens in each prompt to estimate the average difference of scores exactly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>The Gumbel-Max SCM is defined as f T (D i , U i ) = argmax t∈V {log (D i,t ) + U i,t }, where U i,t ∼ Gumbel(0, 1) are i.i.d. noise variables associated with each token<ref type="bibr" target="#b33">[34]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>We believe that our theoretical results can be extended to other popular performance metrics based on the Elo rating system<ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref> and the Bradley-Terry model<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>, as discussed in Section 6.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9"><p>For simplicity, we assume that human preferences are deterministic and thus Rm(u, sq, z) = Rm(u, sq). We lift this assumption in our experiments in Section 5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10"><p>Refer to Appendix B.6 for the detailed calculation of the average win-rates.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_11"><p>For example, certain tokenizers represent spaces between words with the unicode character U+2581, while others use U+0120.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. <rs type="person">Gomez-Rodriguez</rs> acknowledges support from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement No. <rs type="grantNumber">945719</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DZQfj85">
					<idno type="grant-number">945719</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Details</head><p>Hardware setup. Our experiments are executed on a compute server equipped with 2 × Intel Xeon Gold 5317 CPU, 1,024 GB main memory, and 2 × A100 Nvidia Tesla GPU (80 GB, Ampere Architecture). In each experiment a single Nvidia A100 GPU is used.</p><p>Datasets. As a benchmark dataset, we use Measuring Massive Multitask Language Understanding dataset (MMLU) <ref type="bibr" target="#b50">[52]</ref> consisting of 14,042 questions covering 52 diverse knowledge areas with each question offering four possible choices indexed from A to D, and a ground-truth answer. For pairwise comparison tasks, we use the first 500 questions from the LMSYS-Chat-1M dataset <ref type="bibr" target="#b56">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models.</head><p>In our experiments, we use Llama-3.1-8B-Instruct, its quantized variants Llama-3.1-8B-Instruct-{AWQ-INT4, bnb-4bit, bnb-8bit} and Llama-3.2-{1B, 3B}-Instruct models. The models are obtained from Hugging Face, and the quantised LLM variants Llama-3.1-8B-Instruct-{bnb-4bit, bnb-8bit} are built using the bitsandbytes library <ref type="bibr" target="#b62">[64]</ref>. Prompts. To instruct LLMs for generating output, we use the system prompt in Table <ref type="table">3</ref> for the MMLU dataset and Table <ref type="table">4</ref> for the LMSYS-Chat-1M dataset. Further, to perform pairwise comparisons of outputs of different LLMs, we use the system prompt in Table <ref type="table">2</ref>, which is adapted from <ref type="bibr" target="#b15">[16]</ref>, to prompt the strong LLM.</p><p>System: Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. Your job is to evaluate which assistant's answer is better. When evaluating the assistants' answers, compare both assistants' answers. You must identify and correct any mistakes or inaccurate information. Then consider if the assistant's answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive. Then consider the creativity and novelty of the assistant's answers when needed. Finally, identify any missing important information in the assistants' answers that would be beneficial to include when responding to the user prompt. do not provide any justification or explanation for your response. You must output only one of the following choices as your final verdict: 'A' if the response of assistant A is better 'B' if the response of assistant B is better 'Tie' if the responses are tied Table <ref type="table">2</ref>: System prompt used for obtaining pairwise preferences using GPT-4o-2024-11-20 as the judge.</p><p>System: You will be given multiple choice questions. Please reply with a single character 'A', 'B', 'C', or 'D' only. DO NOT explain your reply.  for each point on the x-axis, we perform 1,000 sub-samplings and shaded areas correspond to 95% confidence intervals. We observe qualitatively similar results for other knowledge areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experimental Results on the MMLU Dataset</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of Artificial General Intelligence: Early experiments with GPT-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming</title>
		<author>
			<persName><forename type="first">Hussein</forename><surname>Mozannar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems</title>
		<meeting>the Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AI-Generated Medical Advice-GPT and Beyond</title>
		<author>
			<persName><forename type="first">Claudia</forename><forename type="middle">E</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Medical Association</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1349" to="1350" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mathematical Discoveries from Program Search with Large Language Models</title>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadamin</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">S</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengming</forename><surname>Ellenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Fawzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">625</biblScope>
			<biblScope unit="issue">7995</biblScope>
			<biblScope unit="page" from="468" to="475" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srulik</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maged</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Al-Shaibani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finetuned Language Models are Zero-Shot Learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations. ICLR</title>
		<meeting>the International Conference on Learning Representations. ICLR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-Task Generalization via Natural Language Crowdsourcing Instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3470" to="3487" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Alexander Cosgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Acosta-Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">Arad</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Jue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><forename type="middle">S</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Andrew Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Koreeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Holistic Evaluation of Language Models. Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Roberts</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07">Jul 2023</date>
			<biblScope unit="page" from="22631" to="22648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring Massive Multitask Language Understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations. ICLR</title>
		<meeting>the International Conference on Learning Representations. ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-Instruct: Aligning Language Models with Self-Generated Instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training Language Models to Follow Instructions with Human Feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.12966</idno>
		<title level="m">Aligning Large Language Models with Human: A Survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chatbot arena: an open platform for evaluating LLMs by human preference</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><forename type="middle">N</forename><surname>Angelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banghua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford Alpaca: An instruction-following LLaMA model</title>
		<imprint>
			<date type="published" when="2023-05">2023. May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, data track</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="46595" to="46623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative Judge for Evaluating Alignment</title>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Run-Ze</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations. ICLR</title>
		<meeting>the International Conference on Learning Representations. ICLR</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations</title>
		<author>
			<persName><forename type="first">Ruosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teerth</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elo uncovered: Robustness and best practices in language model evaluation</title>
		<author>
			<persName><forename type="first">Meriem</forename><surname>Boubdir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beyza</forename><surname>Ermis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large Language Models Encode Clinical Knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Gamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakr</forename><surname>Babiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman ;</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Semturs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7972</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2023-07">July 2023</date>
			<pubPlace>Blaise Agüera y Arcas, Dale Webster,</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adding error bars to evals: A statistical approach to language model evaluations</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.00640</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Lovish</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aaditya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><surname>Koyejo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.10229</idno>
		<title level="m">Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. Quantifying variance in evaluation benchmarks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The Llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.naacl-long.20</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.naacl-long.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Boyeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Anastasios N Angelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07008</idno>
		<title level="m">AutoEval Done Right: Using Synthetic Data for Model Evaluation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prediction-powered ranking of large language models</title>
		<author>
			<persName><forename type="first">Ivi</forename><surname>Chatzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Straitouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Thejaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">Gomez</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Limits to scalable evaluation at the frontier: LLM as judge won&apos;t beat twice the data</title>
		<author>
			<persName><forename type="first">Florian</forename><forename type="middle">E</forename><surname>Dorner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><forename type="middle">Y</forename><surname>Nastl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13341</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odellia</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Perlitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilach</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Yehudai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.09569</idno>
		<title level="m">Justrank: Benchmarking LLM judges for system ranking</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Ivi</forename><surname>Chatzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">Corvelo</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Straitouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Tsirtsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.17027</idno>
		<title level="m">Counterfactual token generation in large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Counterfactual generation from language models</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anej</forename><surname>Svete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vésteinn</forename><surname>Snaebjarnarson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.07180</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Counterfactual off-policy evaluation with Gumbel-Max structural causal models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Oberst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4881" to="4890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Counterfactual explanations in sequential decision making under uncertainty</title>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Tsirtsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Counterfactual temporal point processes</title>
		<author>
			<persName><forename type="first">Kimia</forename><surname>Noorbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Counterfactual inference of second opinions</title>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Corvelo</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez Gomez-Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Survey on Evaluation of Large Language Models</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<title level="m">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</title>
		<imprint>
			<date type="published" when="2023-05">2023. May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00861</idno>
		<title level="m">General Language Assistant as a Laboratory for Alignment</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">QLoRA: Efficient Finetuning of Quantized LLMs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="10088" to="10115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akbir</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Ruis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><surname>Chatarena</surname></persName>
		</author>
		<ptr target="https://github.com/chatarena/chatarena" />
		<title level="m">Multi-Agent Language Game Environments for Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</title>
		<author>
			<persName><forename type="first">Yen-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on NLP for Conversational AI</title>
		<meeting>the Workshop on NLP for Conversational AI</meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Arpad</surname></persName>
		</author>
		<author>
			<persName><surname>Elo</surname></persName>
		</author>
		<title level="m">The USCF Rating System: Its Development, Theory, and Applications</title>
		<imprint>
			<publisher>United States Chess Federation</publisher>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the Limitations of the Elo, Real-World Games are Transitive, Not Additive</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Gidel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2905" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A law of comparative judgment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0070288</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Milton</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Individual choice behavior</title>
		<author>
			<persName><forename type="first">Luce</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A probabilistic calculus of actions</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Annual Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings</title>
		<author>
			<persName><surname>Lmsys</surname></persName>
		</author>
		<ptr target="https://lmsys.org/,2023.Online" />
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Lmsys-chat-1m: A large-scale real-world LLM conversation dataset</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixue</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05579</idno>
		<title level="m">LLMs-as-judges: A comprehensive survey on LLM-based evaluation methods</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Counterfactual analysis in dynamic latent state models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Haugh</surname></persName>
		</author>
		<author>
			<persName><surname>Singal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning generalized gumbel-max causal mechanisms</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lorberbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Daniel D Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamir</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Estimating categorical counterfactuals via deep twin networks</title>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Vlontzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciarán M</forename><surname>Gilligan-Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A review of the Gumbel-Max trick and its extensions for discrete stochasticity in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Iris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Huijben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><forename type="middle">B</forename><surname>Kool</surname></persName>
		</author>
		<author>
			<persName><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ruud</surname></persName>
		</author>
		<author>
			<persName><surname>Van Sloun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3157042</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1353" to="1371" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<ptr target="https://huggingface.co/docs/bitsandbytes/main/en/index" />
		<title level="m">Bits and Bytes Foundation. Bits and bytes quantisation library</title>
		<imprint>
			<date type="published" when="2024-01-28">2024. 28 Jan 2025</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
