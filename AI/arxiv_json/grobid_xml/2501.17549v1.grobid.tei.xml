<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QUERY-AWARE LEARNABLE GRAPH POOLING TO-KENS AS PROMPT FOR LARGE LANGUAGE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-29">29 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wooyoung</forename><surname>Kim</surname></persName>
							<email>wkim@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="laboratory">Smart Systems Lab</orgName>
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<postCode>03722</postCode>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Byungyoon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="laboratory">Smart Systems Lab</orgName>
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<postCode>03722</postCode>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wooju</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="laboratory">Smart Systems Lab</orgName>
								<orgName type="institution">Yonsei University Seoul</orgName>
								<address>
									<postCode>03722</postCode>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QUERY-AWARE LEARNABLE GRAPH POOLING TO-KENS AS PROMPT FOR LARGE LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-29">29 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">4B36399A542B87E3B34D336EA677131A</idno>
					<idno type="arXiv">arXiv:2501.17549v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-structured data plays a vital role in numerous domains, such as social networks, citation networks, commonsense reasoning graphs and knowledge graphs. While graph neural networks have been employed for graph processing, recent advancements have explored integrating large language models for graph-based tasks. In this paper, we propose a novel approach named Learnable Graph Pooling Token (LGPT), which addresses the limitations of the scalability issues in node-level projection and information loss in graph-level projection.</p><p>LGPT enables flexible and efficient graph representation by introducing learnable parameters that act as tokens in large language models, balancing fine-grained and global graph information. Additionally, we investigate an Early Query Fusion technique, which fuses query context before constructing the graph representation, leading to more effective graph embeddings. Our method achieves a 4.13% performance improvement on the GraphQA benchmark without training the large language model, demonstrating significant gains in handling complex textual-attributed graph data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A graph is a data structure composed of nodes and edges that represent the relationships between those nodes. Graphs are essential for representing complex relational situations in the real world. For example, social networks <ref type="bibr" target="#b18">(Li et al., 2024;</ref><ref type="bibr" target="#b21">Myers et al., 2014)</ref> like X (Twitter) and urban networks, citation networks <ref type="bibr" target="#b11">(Hu et al., 2020)</ref> in academic fields that represent authorship, affiliations, and citations, protein and molecular graphs <ref type="bibr" target="#b3">(Cao et al., 2023)</ref> for depicting complex molecular interactions, commonsense reasoning graphs like ConceptNet <ref type="bibr" target="#b29">(Speer et al., 2017)</ref>, and knowledge graphs such as Wikidata <ref type="bibr" target="#b34">(Vrandečić &amp; Krötzsch, 2014</ref>) that store various facts. Traditionally, graph data has been processed using handcrafted feature extraction methods like Katz Index and PageRank <ref type="bibr" target="#b13">(Katz, 1953;</ref><ref type="bibr" target="#b22">Page, 1999)</ref>. However, with the recent advancements in deep learning, Graph Neural Networks (GNNs) such as GCN, GAT, and Graph Transformer have become widely researched for processing graphs <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b33">Veličković et al., 2017;</ref><ref type="bibr" target="#b28">Shi et al., 2020)</ref>.</p><p>Meanwhile, the field of Natural Language Processing (NLP) has experienced a revolutionary shift with the advent of Large Language Models (LLMs). These models, pre-trained on massive text datasets, possess general problem-solving abilities <ref type="bibr" target="#b5">(Chung et al., 2024)</ref>. Recently, it has been reported that LLMs can also understand the structural information of graphs and solve graph tasks <ref type="bibr" target="#b8">(Fatemi et al., 2023;</ref><ref type="bibr">Wang et al., 2024b;</ref><ref type="bibr" target="#b4">Chai et al., 2023)</ref>. The combination of LLMs and graphs is particularly useful in Text-Attributed Graphs (TAGs), where each node and edge contains textual features. One of the simplest approaches is to transform graph information into text and feed it into the LLM through knowledge-augmented prompting, as demonstrated by methods like <ref type="bibr" target="#b0">Baek et al. (2023)</ref> and <ref type="bibr" target="#b38">Wu et al. (2023)</ref>. However, graphs contain highly complex structural information, making it difficult to convert them into text. Moreover, the performance varies significantly depending on how the graph is textualized, and the optimal text encoding method is still unknown <ref type="bibr" target="#b8">(Fatemi et al., 2023)</ref>. To overcome these limitations, <ref type="bibr" target="#b24">Perozzi et al. (2024)</ref> has achieved significant performance improvements by embedding graph data using GNNs and projecting it into the word embedding space of LLMs through contin-Preprint uous prompting. Furthermore, <ref type="bibr" target="#b31">Tian et al. (2024)</ref> proposed a technique that distills query-related information during the interaction between graph and query via cross-modality pooling. This continuous prompting method for graphs can be categorized into node-level projection and graph-level projection <ref type="bibr" target="#b26">(Ren et al., 2024)</ref>. Node-level projection passes the information of all nodes, obtained through the GNN, to the LLM and is used in tasks such as node classification or link prediction, which require fine-grained structural information. Graph-level projection compresses node representations into a single vector and passes it to the LLM, which is useful in tasks like graph classification that require global graph information.</p><p>However, both approaches have limitations. In node-level projection, each node representation is treated as a token by the LLM. Since graphs tend to grow exponentially, this method lacks scalability given the limited prompt length of LLM. Even if a model, like <ref type="bibr" target="#b2">Beltagy et al. (2020)</ref>, can process extremely long prompts, the computational cost becomes prohibitive. Graph-level projection, where all node information is pooled into a single vector and passed to the LLM, avoids the scalability issue. However, converting a graph with complex context into a single vector results in information loss <ref type="bibr" target="#b1">(Bahdanau, 2014)</ref>. Given that LLMs must process a graph with vast amounts of information as a single token, this is inevitable.</p><p>To address these limitations, we propose a new concept named Learnable Graph Pooling Token (LGPT). This allows graph information to be represented by n learnable parameters, which are passed to the LLM as n tokens. This approach resolves both the computational issue of node-level projection and the information loss problem in graph-level projection. Additionally, we investigate an early query fusion method and deal with the limitations of a late query fusion method. While cross-modality pooling as the late query fusion in <ref type="bibr" target="#b31">Tian et al. (2024)</ref> combines query context with graph embeddings, it does so after the graph is encoded. In contrast, we propose an approach that integrates query context before constructing the graph representation, thereby offering a more effective graph embedding method that takes the query context into account.</p><p>Our main contribution is as follows:</p><p>• We propose a novel concept of Learnable Graph Pooling Token (LGPT), which enables balanced projection between node-level and graph-level projection. As a result, we achieved more than a 4.13% improvement in performance on the GraphQA benchmark dataset without LLM training.</p><p>• We explore a method to integrate the early query fusion method during the graph embedding process. Through experiments, we demonstrate that incorporating query context before constructing the node embeddings of the graph leads to greater performance improvements than combining it afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>2.1 LLM AS GRAPH ENCODER Fatemi et al. (2023) and Wang et al. (2024b) demonstrated that encoding graphs into various textual forms allows LLMs to solve graph-centric tasks. Additionally, Wang et al. (2024a) advanced Chain of Thought (CoT) <ref type="bibr" target="#b37">(Wei et al., 2022)</ref> into a graph-suitable format by adding the instruction "Let's construct a graph with the nodes and edges first" enabling LLMs to map graph information into conceptual space. However, these approaches all have the limitation of processing graphs at the text level. Since graphs contain complex relational information, converting them into text loses a lot of structural knowledge. To overcome these limitations, <ref type="bibr" target="#b24">Perozzi et al. (2024)</ref>; <ref type="bibr" target="#b31">Tian et al. (2024)</ref> have emerged that embed graphs using GNNs and integrate them with LLMs. Notably, <ref type="bibr" target="#b9">He et al. (2024)</ref> achieved significant performance improvements by using both textual graphs and GNN embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LEARNABLE POOLING METHOD</head><p>Sum and Mean Pooling have traditionally been used as readout functions to create graph embeddings from node embeddings in GNNs. However, they suffer from scalability issues that arise when dealing with graphs that have a varying number of nodes, an inability to emphasize important nodes and information loss in compressing node information into a single vector. To address these limitations, learnable pooling methods that incorporate learnable parameters have been explored. <ref type="bibr" target="#b42">Ying et al. (2018)</ref> introduced hierarchical pooling by applying soft clustering to reflect the hierarchical structure of graphs. Also, <ref type="bibr" target="#b16">Lee et al. (2019)</ref> proposed a learnable pooling method by incorporating an attention mechanism to capture more information from important nodes. Nevertheless, these approaches still face the risk of information loss as they condense numerous node embeddings into a single graph embedding vector <ref type="bibr" target="#b1">(Bahdanau, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">QUERY AWARE GRAPH REPRESENTATION</head><p>In <ref type="bibr" target="#b31">Tian et al. (2024)</ref>, a method was introduced to combine graph and query representations as cross modality pooling using a cross-attention mechanism. While this merges the information from the graph and the query, it has the limitation of being a Late Fusion approach, as the graph and query information are encoded independently before being combined. In <ref type="bibr" target="#b39">Yasunaga et al. (2021)</ref>, a virtual query node is created within the graph to perform graph encoding that is dependent on the meaning of the query. This approach is effectively used in Early Fusion for query-aware graph representation, as seen in its connection to instruction nodes in models like <ref type="bibr" target="#b43">Zhang et al. (2022)</ref> and <ref type="bibr" target="#b40">Yasunaga et al. (2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROBLEM STATEMENT</head><p>We address the problem of Textual Attributed Graph Question Answering (Graph QA) by combining a graph encoder with a large language model. In Graph QA tasks, a query q and a graph G which is provided as external knowledge related to the query are given. Our goal is to generate the optimal answer a * for q by utilizing the information contained in G.</p><formula xml:id="formula_0">a * = arg max a p(a|q, G)<label>(1)</label></formula><p>In this context, G is a text-attributed graph, where both nodes and the edges are associated with textual attributes. Formally, G is defined as G = {V, L, {x n } n∈V , {x l } l∈L }, where V and L represent the sets of nodes (vertices) and edges (links).</p><p>x n and x l denote the textual attributes of the nodes and edges.</p><p>The p(a|q, G) is composed of a Graph Retriever p θ (S|q, G) and an Answer Generator p ϕ (a|q, S) where S is the sub-graph which related with q <ref type="bibr" target="#b23">(Peng et al., 2024)</ref>. In this paper, we borrow <ref type="bibr" target="#b9">He et al. (2024)</ref> results for the graph retriever and focus on optimizing p ϕ (a|q, S) by redefining the problem.</p><formula xml:id="formula_1">p(a|q, G) = p θ (S|q, G)p ϕ (a|q, S) (2) ≈ p ϕ (a|q, S) (3) 3.2 QUERY AWARE LEARNABLE GRAPH POOLING TOKENS 3.2.1 OVERVIEW</formula><p>The process of p ϕ (a|q, S) is divided into three main components as shown in Figure <ref type="figure" target="#fig_0">1</ref>. First, the given sub-graph S is transformed into a textual graph via a discrete prompt template T . Then it is processed by word embedding layer WE of the LLM. Second, the graph S is converted into graph embeddings through a graph encoder GE ψ which has learnable parameters ψ.</p><formula xml:id="formula_2">E text = WE(T (S)) where, E text ∈ R |text|×d (4) E S = GE ψ (S) where, E S ∈ R n×d<label>(5)</label></formula><p>The discrete prompt embedding E text and the continuous prompt embedding E S are concatenated and input into LLM along with the query WE(q) which is processed by WE. Our objective is to optimize the word distribution of the predicted answer a, aligning it with the word distribution of the optimal answer a * . To this end, both the LLM and WE are frozen in their pre-trained states,  <ref type="bibr">(2024)</ref>. Graph Token <ref type="bibr" target="#b24">(Perozzi et al., 2024)</ref> generates node embeddings from the given graph S using a GNN encoder and applies mean pooling to deliver the graph information to the LLM. G-Retriever <ref type="bibr" target="#b9">(He et al., 2024)</ref> follows the same process but differs in that it transforms the given graph S into a textual graph and feeds it into the LLM along with the additional information. Our approach builds on G-Retriever by incorporating LGPT and an Early Query Fusion Module (Red Box).</p><p>while we focus on optimizing the GE ψ . In equations ( <ref type="formula">4</ref>), ( <ref type="formula" target="#formula_2">5</ref>) and ( <ref type="formula">6</ref>), d represents the dimension of the embedding vector, |text| and |q| refer to the number of tokens in the tokenized text and query. n refers to the number of learnable graph pooling tokens, which is described in section 3.2.3.</p><formula xml:id="formula_3">p ϕ (a|q, S) = LLM([E S ; E text ; WE(q)])</formula><p>where,</p><formula xml:id="formula_4">[E S ; E text ; WE(q)] ∈ R (n+|text|+|q|)×d (6) 3.2.2 EARLY QUERY FUSION</formula><p>The total amount of information I total contained in the given graph S, has exponential complexity because a graph represents relationships between nodes. However, for our goal, it is not necessary to utilize all of the whole information. Instead, only the information relevant to the query I query which is much smaller than I total , is sufficient. Note that, I total ≫ I query .</p><p>In <ref type="bibr" target="#b31">Tian et al. (2024)</ref>, the approach first embeds the graph with exponential complexity and then filters I query through cross-modality pooling. This leads to ineffective, as the entire graph with exponential complexity is encoded first. In contrast, we enhance the effectivity of information representation by adopting an early fusion method, where query information is fused before the graph embedding is generated. To achieve this, the query is embedded in the graph embedding space as a virtual query node n q using text encoder TextEnc <ref type="bibr" target="#b39">(Yasunaga et al., 2021)</ref>.</p><formula xml:id="formula_5">n q = TextEnc(q) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>The query node n q connects all nodes in the graph S. It performs message passing using GNN query , resulting in the graph S q that incorporates the query node embedding n ′ q and the original node embeddings. Subsequently, GNN graph is employed to encode the original relational information of the graph S q .</p><formula xml:id="formula_6">{n ′ q } ∪ {x n } n ∈ nodes of Sq = GNN query (S, n q ) (8) {x n } n ∈ nodes of Sg = GNN graph (S q ) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">LEARNABLE GRAPH POOLING TOKENS (LGPT)</head><p>There are two main approaches to prompting with a graph encoder. The first approach involves passing all node embeddings to the LLM, while the second approach uses a readout function to transform node embeddings into single graph embedding, which is then passed to the LLM. In node-level prompting, each node is treated as a token by the LLM, but as the number of nodes increases, this method becomes impractical due to scalability issues. On the other hand, in graphlevel prompting, methods such as mean pooling, DiffPool <ref type="bibr" target="#b42">(Ying et al., 2018)</ref> and SAGPool <ref type="bibr" target="#b16">(Lee et al., 2019)</ref> are used to compress node embeddings into a single vector, which is then provided to the LLM. However, this requires encoding all the graph information into a single vector, increasing the risk of information loss <ref type="bibr" target="#b1">(Bahdanau, 2014)</ref>.</p><p>To address this issue, we propose a novel pooling method named Learnable Graph Pooling Tokens (LGPT).</p><p>LGPT employees n learnable parameters with the same dimension as the node embeddings, which are fully connected to all the nodes in the given graph S g . After that, message passing is performed through a GNN pool process, resulting in S p and graph representation</p><formula xml:id="formula_7">n tokens {g ′ 1 • • • g ′ n }</formula><p>. This method reduces the risk of information loss compared to previous approaches that represented the graph using only a single vector. Finally, processed LGPT {g</p><formula xml:id="formula_8">′ 1 • • • g ′ n }</formula><p>is transformed into E S , the input format for the LLM, through a projection layer Proj composed of Multi-Layer Perceptron (MLP).</p><p>LGPT :</p><formula xml:id="formula_9">{g 1 , • • • , g n } (10) {g ′ 1 • • • g ′ n } ∪ {x n } n ∈ nodes of Sp = GNN pool (S g , {g 1 , • • • , g n })<label>(11)</label></formula><formula xml:id="formula_10">E S = Proj({g ′ 1 • • • g ′ n })<label>(12)</label></formula><p>The reason LGPT works effectively is that it conceptually combines two learnable pooling methods. <ref type="bibr" target="#b42">Ying et al. (2018)</ref> performs pooling hierarchically through soft clustering, where a node can be assigned to multiple clusters. In our method, since all nodes are connected to learnable tokens and perform message passing for pooling, each LGPT token can be seen as a soft cluster, making our approach conceptually aligned with soft clustering in <ref type="bibr" target="#b42">Ying et al. (2018)</ref>. Additionally, by using Graph Transformer <ref type="bibr" target="#b28">(Shi et al., 2020)</ref> as the GNN architecture, our method operates similarly to <ref type="bibr" target="#b16">Lee et al. (2019)</ref>, which employs the self-attention mechanism. In essence, our method works well because it conceptually borrows from both <ref type="bibr" target="#b42">Ying et al. (2018)</ref> and <ref type="bibr" target="#b16">Lee et al. (2019)</ref>. However, the key difference from these methods is that, instead of pooling into a single graph embedding, our approach uses multiple learnable tokens for pooling, thereby reducing information loss.</p><p>In summary, the proposed method consists of two main components: Early Query Fusion and the Learnable Graph Pooling Tokens. Learnable parameters of GNN query and GNN graph are required for Early Query Fusion while learnable parameters of GNN pool and LGPT are necessary for the graph pooling.</p><formula xml:id="formula_11">ψ = parameters of {GNN query , GNN graph , GNN pool , LGPT, Proj}<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ANALYSIS OF TIME COMPLEXITY</head><p>Let k denote the number of nodes, t the number of prompt text tokens, g the number of GNN layers, and n the number of LGPTs. The time complexity of the proposed Graph Encoder, which employs three GNNs, is thus O(3g(n + k)). In comparison, the time complexity of both G-Retriever and GraphToken is O(gk). Given that n ≪ k, the time complexity of our method aligns with that of other baseline graph encoders, remaining at O(gk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>The computational time complexity for the LLM component is determined by the self-attention mechanism and is proportional to the square of the prompt length. In our model, the prompt consists of t + n tokens, whereas GraphToken and G-Retriever process t + 1 tokens. We set k = 8 to ensure n ≪ t, thereby maintaining the time complexity of LLM computation in our model at O(t<ref type="foot" target="#foot_1">foot_1</ref> ), consistent with that of other baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>4.1 EXPERIMENT SETUP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DATASETS</head><p>For our experiments, we used the GraphQA benchmark dataset (Table <ref type="table" target="#tab_1">1</ref>) released by <ref type="bibr" target="#b9">He et al. (2024)</ref>. The experiments are conducted using the QA data and the corresponding graphs provided. This dataset consists of three sub-QA tasks as follows. The datasets are divided into train, validation and test subsets using a 6:2:2 ratio.</p><p>• ExplaGraphs <ref type="bibr" target="#b27">(Saha et al., 2021)</ref>: The dataset is a commonsense reasoning dataset composed of 2,766 graphs for stance prediction in debates. The task is evaluating whether two arguments support or not to use the information from the given graph. The evaluation metric used is accuracy. • SceneGraphs: The SceneGraphs dataset, derived from GQA (Hudson &amp; Manning, 2019), contains 100,000 scene graphs detailing objects, attributes and relationships within images. It challenges users with tasks requiring spatial understanding and multi-step inference to answer open-ended questions based on scene graph descriptions, evaluated on accuracy. • WebQSP <ref type="bibr" target="#b41">(Yih et al., 2016;</ref><ref type="bibr" target="#b20">Luo et al., 2023)</ref>: WebQSP is a large-scale knowledge Graph QA dataset with 4,737 questions, requiring multi-hop reasoning to answer. It uses a subset of Freebase, containing facts within 2-hops of the entities in the questions. The task is evaluated using the Hits@1 metric to measure the precision of the top answer. LGPT is discussed in the following section 4.4. All experiments were conducted on a single Nvidia A6000 48GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS</head><p>We compared the results of our approach with various methods that rely on prompting without training LLM. First, as discrete prompt baselines without prompt module training (Inference Only),</p><p>Figure <ref type="figure">2</ref>: Inference Only Method Details. Zero-CoT <ref type="bibr" target="#b15">(Kojima et al., 2022)</ref> adds the prompt "Let's think step by step" utilizing the core concept of Chain of Thought <ref type="bibr" target="#b37">(Wei et al., 2022)</ref>, to enable LLMs to generate reasoning processes automatically. CoT-BAG <ref type="bibr">(Wang et al., 2024a)</ref> adapts this for graph tasks by modifying the prompt to "Let's construct a graph with the nodes and edges first". On the other hand, KAPING <ref type="bibr" target="#b0">(Baek et al., 2023)</ref> prompted the information of the given graph as linearized triples. we used zero-shot, Zero-CoT <ref type="bibr" target="#b15">(Kojima et al., 2022)</ref>, CoT-BAG <ref type="bibr">(Wang et al., 2024a)</ref>, KAPING <ref type="bibr" target="#b0">(Baek et al., 2023)</ref>. Also, for a fair comparison with our method, we included methods with trained prompt module training (Frozen LLM w/ PT), such as Prompt Tuning <ref type="bibr" target="#b17">(Lester et al., 2021)</ref>, Graph Token <ref type="bibr" target="#b24">(Perozzi et al., 2024)</ref> and G-Retriever <ref type="bibr" target="#b9">(He et al., 2024)</ref>. Details of each method are described in Figure <ref type="figure" target="#fig_0">1</ref> and <ref type="figure">2</ref>.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows the main results. The prompt module with GNN consistently shows better performance than the only inference setting. This suggests that the structural representation of the graph is more appropriately encoded through GNN embeddings. The lower performance of Prompt Tuning, which only trains learnable parameters without GNNs, compared to Graph Token and G-Retriever, further supports it. When comparing Ours to G-Retriever with all settings identical except for Early Query Fusion and LGPT, our approach achieves performance improvements ranging from 2.38% to 5.77%, with an average improvement of 4.13% across the three datasets. The improvement indicates that Early Query Fusion and LGPT further enhance performance by ensuring that query-specific information is integrated early, reducing the risk of information loss.</p><p>We conducted ablation studies to analyze the individual effects and interaction of Early Query Fusion and LGPT. Table <ref type="table" target="#tab_3">3</ref> shows the results. When applying Early Fusion, we observed an average performance improvement of 2.88%. Although there is a 0.22 performance drop on the WebQSP dataset, the reported standard deviation of G-Retriever's performance due to random seed variation is 1.21, suggesting that the performance drop is not statistically significant <ref type="bibr" target="#b9">(He et al., 2024)</ref>. Additionally, applying LGPT results in an average performance improvement of 3.87%, indicating that Preprint the traditional pooling method using a single vector incurs information loss and our method offers an effective alternative. Finally, when both methods are applied together, they exhibit a positive interaction, achieving an average performance improvement of 4.13%.</p><p>In the previous experiments, we reported results by training only the prompt module, without training the LLM, to independently analyze the effects of the proposed method. Additionally, we conduct experiments where both the LLM and the prompt module are trained together. To efficiently train the LLM, we employ a Low-Rank Adaption (LoRA) <ref type="bibr" target="#b10">(Hu et al., 2021)</ref>. The trainable parameters, including the prompt module, accounted for only 1.82% of the total parameters.</p><p>The experimental results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. Our approach shows even greater effectiveness when training the LLM with LoRA. Compared to all baselines, our method, which used LoRA for training the LLM, achieved the highest performance improvements. On average, it demonstrated an 86.78% performance improvement over the non-trained LLM and an 11.48% improvement over the LLM trained with LoRA. Additionally, it outperforms G-Retriever with LoRA, which trained both the LLM and the prompt module, by 3.54%. This indicates that our prompting method with training LLM by using LoRA is highly effective in conveying graph information to the LLM and proves to be superior to other prompting methods.</p><p>4.3 EFFECT OF EARLY QUERY FUSION <ref type="bibr" target="#b30">Sun et al. (2018)</ref> reported that in the process of knowledge enhancement, Early Fusion, where information is combined during the representation creation phase, results in greater performance improvements compared to Late Fusion, where embeddings are combined after they have been independently generated. On the other hand, <ref type="bibr" target="#b31">Tian et al. (2024)</ref> employed late fusion when combining query and graph information, in which the fusion process only occurs after the graph information has been fully encoded.</p><p>In this paper, we adopted Early Query Fusion, where query information is integrated before the graph embeddings are generated. To validate the effectiveness of this approach, we conducted experiments comparing early fusion and late fusion methods. For late fusion, we used the cross modality pooling technique from <ref type="bibr" target="#b31">Tian et al. (2024)</ref>, while for Early Fusion, we applied the Early Query Fusion strategy proposed in this work.</p><p>The results present in Table <ref type="table" target="#tab_4">4</ref> show the differences between the two methods. In the case that is applied mean pooling as the readout function, late fusion performance decreases compared to when query fusion is not used. This suggests that combining fully processed embeddings from different modalities may work as noise or lead to inefficient information integration. On the other hand, early query fusion shows slight performance improvements, indicating that integrating query information earlier in the process allows for better representation and information fusion within the graph structure. Moreover, when both Early Fusion and Late Fusion are applied together, a greater average performance improvement is observed.</p><p>Even when applying LGPT in the readout function, early fusion results in greater performance improvements compared to late fusion. Similar to the case with mean pooling, applying late fusion leads to a slight performance decrease. Moreover, combining both methods also results in a per-  formance drop. The key takeaway from these experiments is that Early Query Fusion is a more suitable and effective approach for integrating query information with graph structures compared to the traditional Late Fusion method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PERFORMANCE COMPARISON OF THE NUMBER OF LGPT</head><p>In this section, we compared the model's performance on the SceneGraph dataset by varying the number of LGPT from 1, 8, to 32. The results are shown in Figure <ref type="figure" target="#fig_2">4</ref> and regardless of the Query Fusion method, using 8 LGPTs consistently outperforms using just 1 LGPT. This suggests, as mentioned earlier, that encoding the complex information of a graph into a single vector leads to information loss. Specifically, when compressing all the graph information into a single vector, important relationships and characteristics may not be fully captured, resulting in degraded performance. LGPTs yielded the highest performance in both methods, reaching the maximum score for Early Fusion and Late Fusion. However, performance did not improve further when increasing the number of LGPTs to 32, suggesting that beyond a certain point, additional LGPTs do not contribute to further performance gains.</p><p>Notably, except for the Late Fusion method, using 8 LGPTs outperform using 32 LGPTs. As the number of learnable parameters increases, the search space during training also expands and having more parameters does not necessarily lead to better performance. Our experimental results support this observation, showing that too many learnable parameters can result in overfitting or information redundancy, which ultimately hinders performance.</p><p>However, in the case of the Late Fusion method, performance improves as the number of LGPT increases. This can be attributed to the fact that, in Late Fusion, LGPT is directly involved in the cross-attention operations between the graph and query information. In this context, a greater number of LGPTs allows for richer information exchange, leading to performance gains.</p><p>Overall, this experiment highlights the importance of carefully selecting the number of LGPTs.</p><p>Increasing the number of parameters beyond a certain threshold does not always guarantee performance improvements. In particular, using 8 LGPTs consistently achieves the best performance across different Fusion methods, suggesting that this number strikes a good balance between performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we introduced a novel approach, the Learnable Graph Pooling Token (LGPT), which addresses the challenges of graph representation for text-attributed graph question answering tasks.</p><p>Our method bridges the gap between node-level and graph-level projections by representing graph information with learnable parameters passed as tokens to large language models. This approach mitigates both the scalability issues inherent in node-level projections and the information loss in graph-level projections. Additionally, we proposed an Early Query Fusion technique, which incorporates query information during the graph embedding process, ensuring that query-specific details are embedded into the graph representation before it is constructed. This method demonstrates significant performance improvements over previous approaches using late query fusion.</p><p>Through extensive experimentation on the GraphQA benchmark, our approach consistently outperformed existing methods, achieving an average improvement of 4.13% over the baseline model without training LLM. The combination of LGPT and Early Query Fusion proved to be highly effective in addressing the complexities of textual-attributed graphs while ensuring scalable and efficient graph representation without training large language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Proposed Method. Our approach is similar to Perozzi et al. (2024); He et al.(2024). Graph Token<ref type="bibr" target="#b24">(Perozzi et al., 2024)</ref> generates node embeddings from the given graph S using a GNN encoder and applies mean pooling to deliver the graph information to the LLM. G-Retriever<ref type="bibr" target="#b9">(He et al., 2024)</ref> follows the same process but differs in that it transforms the given graph S into a textual graph and feeds it into the LLM along with the additional information. Our approach builds on G-Retriever by incorporating LGPT and an Early Query Fusion Module (Red Box).</figDesc><graphic coords="4,108.00,81.86,396.01,287.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The red bars represent the case where both the LLM and the prompt module were trained using LoRA, while the blue bars represent the case where only the prompt module was trained, and the gray bars represent inference only. Training the LLM using LoRA alongside the prompt module resulted in a significant performance improvement. Additionally, even when training the LLM, our approach, which combines LGPT and the Early Query Fusion Module, demonstrated superior QA performance compared to G-Retriever.</figDesc><graphic coords="9,108.00,81.86,396.02,236.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance Comparison of the number of LGPT The figure presents the performance comparison between Early Fusion and Late Fusion approaches, with varying numbers of Learnable Graph Pooling Tokens (LGPT). The experimental results indicate that using 8LGPTs yielded the highest performance in both methods, reaching the maximum score for Early Fusion and Late Fusion. However, performance did not improve further when increasing the number of LGPTs to 32, suggesting that beyond a certain point, additional LGPTs do not contribute to further performance gains.</figDesc><graphic coords="10,147.60,81.86,316.80,124.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of GraphQA benchmark Dataset<ref type="bibr" target="#b9">He et al. (2024)</ref>.</figDesc><table><row><cell>Dataset</cell><cell>ExplaGraphs</cell><cell>SceneGraphs</cell><cell>WebQSP</cell></row><row><cell># Graph</cell><cell>2766</cell><cell>100000</cell><cell>4737</cell></row><row><cell>Avg. # Nodes</cell><cell>5.17</cell><cell>19.13</cell><cell>1370.89</cell></row><row><cell>Avg. # Edges</cell><cell>4.25</cell><cell>68.44</cell><cell>4252.37</cell></row><row><cell cols="2">Node Attribute Commonsense Concepts</cell><cell>Object Attributes (e.g. color, shape)</cell><cell>Entities in KG (Freebase)</cell></row><row><cell cols="4">Edge Attribute Commonsense Relations Relations (e.g. actions, spatial relations) Relations in KG (Freebase)</cell></row><row><cell>Task</cell><cell>Commonsense reasoning</cell><cell>Scene graph QA</cell><cell>Knowledge Graph QA</cell></row><row><cell cols="2">4.1.2 IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell></row><row><cell cols="4">We set our implementation details to be consistent with He et al. (2024) such as discrete prompt</cell></row><row><cell cols="4">template T to textualize the given graph and how to retrieve graph S from G. We used LLaMa2-</cell></row><row><cell cols="4">7b 1 (Touvron et al., 2023) with 4-bit NormalFloat quantization (Dettmers et al., 2021; 2024) as LLM</cell></row><row><cell cols="4">and Setence Transformer 2 (Reimers, 2019) as TextEnc. For GNN architecture we employed a Graph</cell></row><row><cell cols="4">Transformer (Shi et al., 2020) with four layers. The model was trained to minimize Cross Entropy</cell></row><row><cell cols="4">Loss for each label's token using the AdamW (Loshchilov, 2017) optimizer with learning late of</cell></row><row><cell cols="4">1e-4. The number of LGPT was set to 8 and the performance comparison based on the number of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main Results. The table compares the experimental results of various methods, including Inference Only and Frozen LLM w/ PT. Our model (Ours) demonstrated the highest performance, surpassing both Inference Only and Frozen LLM w/ PT approaches. Compared to G-Retriever, our model shows performance improvements ranging from 2.38% to 5.77%, with an average improvement of 4.13%.</figDesc><table><row><cell></cell><cell></cell><cell cols="5"># of Prompt Tokens Expla Graphs SceneGraphs WebQSP Average</cell></row><row><cell></cell><cell>Zero-Shot</cell><cell>-</cell><cell>56.50</cell><cell>39.74</cell><cell>41.06</cell><cell>45.77</cell></row><row><cell>Inference Only</cell><cell>Zero-CoT CoT-BAG</cell><cell>--</cell><cell>57.04 57.94</cell><cell>52.60 56.80</cell><cell>51.30 39.60</cell><cell>53.65 51.45</cell></row><row><cell></cell><cell>KAPING</cell><cell>-</cell><cell>62.27</cell><cell>43.75</cell><cell>52.64</cell><cell>52.89</cell></row><row><cell></cell><cell>Prompt Tuning</cell><cell>10</cell><cell>57.63</cell><cell>63.41</cell><cell>48.34</cell><cell>56.46</cell></row><row><cell></cell><cell>Graph Token</cell><cell>1</cell><cell>85.08</cell><cell>49.03</cell><cell>57.05</cell><cell>63.72</cell></row><row><cell>Frozen LLM w/ PT</cell><cell>G-Retriever</cell><cell>1</cell><cell>85.16</cell><cell>81.31</cell><cell>70.49</cell><cell>78.99</cell></row><row><cell></cell><cell>Ours ∆ G-Retriever</cell><cell>8</cell><cell>90.07 +5.77%</cell><cell>84.50 +3.92%</cell><cell>72.17 +2.38%</cell><cell>82.25 +4.13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Result of Ablation Studies. The table shows the results of analyzing the individual effects of Early Query Fusion and Learnable Graph Pooling Tokens (LGPT). Applying Early Query Fusion resulted in an average performance improvement of 2.88%, while LGPT contributed an average improvement of 3.87%. When both methods are applied together, additional performance gains are observed, leading to a total improvement of 4.13% compared to G-Retriever.</figDesc><table><row><cell></cell><cell cols="5"># of Prompt Tokens Expla Graphs SceneGraphs WebQSP Average</cell></row><row><cell>G-Retriever</cell><cell>1</cell><cell>85.16</cell><cell>81.31</cell><cell>70.49</cell><cell>78.99</cell></row><row><cell>with Early Query Fusion ∆ G-Retriever</cell><cell>1</cell><cell>89.53 +5.13%</cell><cell>83.98 +3.28%</cell><cell>70.27 -0.31%</cell><cell>81.26 +2.88%</cell></row><row><cell>with LGPT ∆ G-Retriever</cell><cell>8</cell><cell>88.98 +4.49%</cell><cell>85.05 +4.60%</cell><cell>72.11 +2.30%</cell><cell>82.05 +3.87%</cell></row><row><cell>with LGPT and Early Query Fusion ∆ G-Retriever</cell><cell>8</cell><cell>90.07 +5.77%</cell><cell>84.5 +3.92%</cell><cell>72.17 +2.38%</cell><cell>82.25 +4.13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of Query Fusion Method. The table compares the results of Early Fusion and Late Fusion methods. Applying Early Fusion leads to performance improvements, while Late Fusion results in a decrease in performance. When both methods are applied together, further performance gains are observed than when late fusion is only applied, indicating that Early Fusion is a more effective approach than Late Fusion.</figDesc><table><row><cell>Readout</cell><cell cols="7"># of Tokens Early Fusion Late Fusion Expla Graphs SceneGraphs WebQSP Average</cell></row><row><cell></cell><cell></cell><cell>X</cell><cell>X</cell><cell>89.71</cell><cell>82.99</cell><cell>69.47</cell><cell>80.72</cell></row><row><cell>Mean Pooling</cell><cell>1</cell><cell>X O</cell><cell>O X</cell><cell>83.75 89.53</cell><cell>81.20 83.98</cell><cell>70.51 70.27</cell><cell>78.49 81.26</cell></row><row><cell></cell><cell></cell><cell>O</cell><cell>O</cell><cell>90.97</cell><cell>84.77</cell><cell>69.90</cell><cell>81.88</cell></row><row><cell></cell><cell></cell><cell>X</cell><cell>X</cell><cell>88.98</cell><cell>85.05</cell><cell>72.11</cell><cell>82.05</cell></row><row><cell>LGPT</cell><cell>8</cell><cell>X O</cell><cell>O X</cell><cell>87.36 90.07</cell><cell>84.35 84.50</cell><cell>71.56 72.17</cell><cell>81.09 82.25</cell></row><row><cell></cell><cell></cell><cell>O</cell><cell>O</cell><cell>88.62</cell><cell>85.19</cell><cell>70.70</cell><cell>81.50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/meta-llama/Llama-2-7b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/sentence-transformers/all-roberta-large-v11</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge-augmented language model prompting for zero-shot knowledge graph question answering</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04136</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery</title>
		<author>
			<persName><forename type="first">He</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16208</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Graphllm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05845</idno>
		<title level="m">Boosting graph reasoning ability of large language model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02861</idno>
		<title level="m">8-bit optimizers via block-wise quantization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Talk like a graph: Encoding graphs for large language models</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04560</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">G-retriever: Retrieval-augmented generation for textual graph understanding and question answering</title>
		<author>
			<persName><forename type="first">Xiaoxin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07630</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Urbangpt: Spatio-temporal large language models</title>
		<author>
			<persName><forename type="first">Zhonghang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5351" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01061</idno>
		<title level="m">Reasoning on graphs: Faithful and interpretable large language model reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information network or social network? the structure of the twitter follow graph</title>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on world wide web</title>
		<meeting>the 23rd international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph retrieval-augmented generation: A survey</title>
		<author>
			<persName><forename type="first">Boci</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohe</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuntao</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.08921</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Let your graph do the talking: Encoding structured data for llms</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.05862</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Reimers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of large language models for graphs</title>
		<author>
			<persName><forename type="first">Xubin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6616" to="6626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Explagraphs: An explanation graph generation task for structured commonsense reasoning</title>
		<author>
			<persName><forename type="first">Swarnadeep</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07644</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00782</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural prompting with large language models</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panpan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="19080" to="19088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Can language models solve graph problems in natural language?</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Can language models solve graph problems in natural language?</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Retrieve-rewriteanswer: A kg-to-text enhanced llms framework for knowledge graph question answering</title>
		<author>
			<persName><forename type="first">Yike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.11206</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Qa-gnn: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06378</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep bidirectional language-knowledge graph pretraining</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="37309" to="37323" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Greaselm: Graph reasoning enhanced language models for question answering</title>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08860</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
