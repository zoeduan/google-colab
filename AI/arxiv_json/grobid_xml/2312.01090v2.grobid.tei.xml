<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF GENERATED WARGAME AI: DOUBLE LAYER AGENT TASK PLANNING BASED ON LARGE LANGUAGE MODEL A PREPRINT</title>
				<funder ref="#_bpWNwqk">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-12-19">December 19, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
							<email>sunyuxiang@nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
							<email>junjiezhao@smail.nju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Checheng</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xianzhong</forename><surname>Zhou</surname></persName>
							<email>zhouxz@nju.edu.cn</email>
							<idno type="ORCID">0000-0000-0000-0000</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University Nanjing</orgName>
								<address>
									<postCode>210023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University Nanjing</orgName>
								<address>
									<postCode>210023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University Nanjing</orgName>
								<address>
									<postCode>210023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Intelligence Science and Technology</orgName>
								<orgName type="institution">Nanjing University Nanjing</orgName>
								<address>
									<postCode>210023</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Management and Engineering</orgName>
								<orgName type="institution">Nanjing University Nanjing</orgName>
								<address>
									<postCode>210023</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SELF GENERATED WARGAME AI: DOUBLE LAYER AGENT TASK PLANNING BASED ON LARGE LANGUAGE MODEL A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-19">December 19, 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">3B18E9BA6339EC842CC732221C2670E4</idno>
					<idno type="arXiv">arXiv:2312.01090v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>Large language models</term>
					<term>Agents</term>
					<term>Generative AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The large language models represented by ChatGPT have a disruptive impact on the field of artificial intelligence. But it mainly focuses on natural language processing, speech recognition, machine learning and natural language understanding. This paper innovatively applies the large language model to the field of intelligent decision-making, places the large language model in the decisionmaking center, and constructs an agent architecture with the large language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the large language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since ChatGPT was officially launched on November 30, 2022, it has quickly become one of the most popular intelligent Chatbot <ref type="bibr" target="#b0">Van Dis et al. [2023]</ref>, <ref type="bibr" target="#b1">Stokel-Walker and Van Noorden [2023]</ref>. Since its inception, ChatGPT has been applied in multiple fields such as code correction Surameery and Shakor <ref type="bibr">[2023]</ref>, public health <ref type="bibr">Som S [2023]</ref>, and global warming Biswas <ref type="bibr">[2023]</ref>. In July 2023, OpenAI released the Code Interpreter plugin, further enhancing ChatGPT's data parsing capabilities and addressing the natural weaknesses of large language models in mathematics and language. These developments have provided new inspiration for improving the intelligence and generalization of AI in the field of intelligent wargame simulation, that is, using ChatGPT self generated AI to make intelligent decisions in war games.</p><p>Although the development and application of rule AI and data-driven AI <ref type="bibr" target="#b5">Cheng et al. [2021]</ref>is the starting point in the field of intelligent wargame, data-driven AI has gradually become a research hotspot in recent years, in which Reinforcement learning AI has made a series of breakthroughs. In terms of data-driven AI, Liu Man, Zhang Hongjun, and others have designed a wargame decision-making framework that balances rules and data <ref type="bibr">Liu et al. [2020a]</ref>. In terms of Reinforcement learning AI, Li Chen's team from Nanjing University of Science and Technology designed a multi-agent decision-making method under the Actor Critical framework and achieved good intelligence <ref type="bibr" target="#b7">Chen et al. [2021]</ref>. Xu Jiale, Zhang Haidong, and others designed a CNN based strategy learning model to improve the accuracy of wargame situation prediction <ref type="bibr" target="#b8">Xu et al. [2022]</ref>. Tencent's AI Lab used Deep reinforcement learning to achieve game confrontation in the King's Glory game, and defeated professional players <ref type="bibr" target="#b9">Ye et al. [2020]</ref>, <ref type="bibr" target="#b10">Chen et al. [2020]</ref>. In a word, with the deepening of the combination of deep learning, Reinforcement learning and intelligent wargame, the intelligence of agents has been continuously improved <ref type="bibr" target="#b11">Mnih et al. [2015]</ref>, <ref type="bibr" target="#b12">Silver et al. [2016]</ref>, <ref type="bibr" target="#b13">Vinyals et al. [2019]</ref>, <ref type="bibr">Liu et al. [2020b]</ref>.</p><p>Although rule AI does not require a long period of training, due to its limitations in rules, the upper limit of intelligence level is difficult to break through the upper limit of rules; While data-driven AI and Reinforcement learning AI improve their intelligence and flexibility by processing large amounts of data through Reinforcement learning algorithms, their interpretability is poor, and it is difficult to achieve model migration under scenario and capture point changes <ref type="bibr" target="#b19">SUN et al. [2022]</ref>, <ref type="bibr" target="#b16">Wurman et al. [2022]</ref>, <ref type="bibr" target="#b17">Schrittwieser et al. [2020]</ref>, <ref type="bibr" target="#b18">Silver et al. [2018]</ref>. Therefore, improving the intelligence and generalization of AI in the field of intelligent wargame becomes the focus of further research.</p><p>Moreover, the decision-making of adversarial games is complex and continuous. In order to make decisions more intelligent and generalized, this article focuses on introducing a self generated AI wargame architecture based on a large language model. Create a decision-making mechanism that involves multiple generative agents interacting, mimicking human behavior and generating interpretable, credible, and highly generalizable game adversarial intelligent decisions. The core work of this article is as follows:</p><p>1. The self generated AI wargame architecture is an intelligent agent architecture centered on a large language model. This architecture consists of multiple generative agents, each with its own large language model (this article uses ChatGPT as the driving tool). These intelligent agents can communicate and cooperate with each other through reflective and memory streams, and jointly make decisions. By talking to each other, they can share information, analyze the situation, and make inferences and decisions based on the conversation content.</p><p>2. Build a two-layer agent task planning model, targeting strategic agents and tactical agents to plan tasks for the game confrontation process. Strategic agents describe specific situations observed by all current agents. Planning refers to task allocation and execution based on all observed situational information. The tactical agent only focuses on the situation observed by a single agent chess piece and executes related tasks according to the strategic planning agent. However, tactical agents can also have their own judgments and feedback based on the prompts issued by strategic agents.</p><p>3. Taking wargame as the experimental platform, the experiment shows that the intelligent decision-making ability of the large language model is significantly stronger than reinforcement learning AI and rule AI, and the intelligence, understandability, and generalization are all better. And through experiments. Meanwhile, research has found that providing prior knowledge from experts in the field specifically to large language models can significantly improve their intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Wargame AI Architecture</head><p>In the war chess environment, we have realized the confrontation between six red chessmen and five blue chessmen as Figure <ref type="figure">1</ref> shows. The red and blue chessmen in different clusters have different semantic interaction information, which is generated through ChatGPT. Adjust the Task Plan, Clear the Enemy's Firepower Points, and then Support the Squad CP: TK_A and TK_B, you are required to provide fire support to our Squad. TK_A: CP, the enemy defense is strong, should eliminate the enemy's firepower points first and then support the infantry. CP: Agreed. TK_A and TK_B, clear the enemy's firepower points first and then support our Squad. Squad: IFV, please suppress the armored forces. IFV: Understood, I will do my best to destroy it. Request Fire Suppression … … Figure 1: Specific interaction of self generated wargame AI in the environment Individual Situational Awareness Battle Situation Combat Memory Flow Command Experience Flow Extracted Combat Memory Extracted Command Experience Tactical Agent Combat Operation Strategic Agent adjust Overall Situational Awareness Battle Situation extract Task Plan To implement the decision-making mechanism mentioned above, we have developed an agent architecture consisting of three main components: a memory stream for storing and allocating buffers and generating batches; a reflection stream for using batches as prompts for the large language model to understand its role in the decision-making process; and a task planning stream for synthesizing higher-level reasoning from batches to enable the agent to integrate situational information and make better pre-battle plans. The agent architecture is designed to store, synthesize, and apply past battlefield experience to enable the large language model to generate trustworthy decisions.</p><p>3 Generative Wargame AI Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wargame Agent Interaction</head><p>In the architecture described above, the wargame agents obtain situational information and interact with each other in natural language to maintain collaboration. Each agent describe their action in natural language, such as "red agent 1 is preparing to capture the control point and moving along the road", "blue agent 2 is preparing to aim at the enemy target 1". Then the sentence is translated into specific actions and directly affect the wargame environment. In the meanwhile, all the actions and movements will be displayed as a series of number symbols which appear above each avatar to provide abstract representation of actions. To achieve this, the architecture utilizes a language model to translate language into actions, while representing a concise symbol above each wargame to represent ChatGPT's suggestions for actions that this agent should take. For example, "red agent 1 is preparing to capture the control point" is displayed as "!" appearing above the wargame, while "red agent 1 is preparing to aim at the enemy" is displayed as "→".</p><p>In this environment, agents communicate with each other in natural language which is fully understandable by humans.</p><p>They obtain the situational information of other operators and environment from the semantics of sentences. Here is a sample of an agent communicating with another. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composing model</head><p>The generative wargame AI aims to provide a novel decision-making framework for intelligent decision-making in wargame environment. Compared to the traditional rule-based AI, data-driven AI , or the reinforcement learning AI, our architecture utilizes ChatGPT for intelligent decision-making and interaction with the wargame environment. Generative wargame AI takes the current environment and past experiences as inputs, and produces output in the form of generated actions.</p><p>Generated actions can be described as these steps: the architecture provides the large language model with well-trained prompts, the language model chooses the numbers which encompass all the actions to take according to prompts, the agents take actions with the corresponding number. The innovation of the architecture lies in the combination of the large language model with retrieval of relevant information and the utilization of prompts to adjust the output of language model.</p><p>Building upon the foundation of the architecture, we constructed a dual-layer agent system including the strategic agent and tactical agent. Strategic agent obtains all the information regarding the states of both its own sites and the observed opponents as input, then it combines this with the overall environment and input as prompts to generate a macro-level tactics intelligent task planning flow. And strategic agent assigns tasks to the tactics agent in the form of prompts, and tactics agent, based on its own states, provides modification suggestions and reasons for modifying. Then strategic agent keeps replanning according to these suggestions until all the tactics agents are not going to provide furthermore suggestions.</p><p>Certainly, the strategic and tactics agents still face a lot of challenges even with the use of the state-of-the-art large language model like GPT-4 LLM(Large Language Model). Since extensive generation of events and memories generated by the two agents, the most critical challenge in this architecture lies in generating the most relevant memory fragments while retrieving and synthesizing relevant data from the memory stream. Therefore, this article attempts to reduce computational power and memory requirements, and uses GPT-4 LLM for strategic agent decision-making, facilitating overall strategic input and expert knowledge document input. For tactical agents, this article uses GPT-3.5 LLM for decision-making, as tactical agents can interact and provide feedback on results in turn, reducing computational power and memory requirements without affecting intelligence</p><p>Memory Stream As the central component of the architecture, the memory stream directly influences the efficiency and accuracy of decision-making. The entire memory stream is a list of memory objects, with each object consisting of a natural language description, a creation timestamp and a recent access timestamp. The fundamental element in memory list is observation, which contains all the situational information observed by agents. Due to the presence of the fog of the war, the battlefield environment doesn't allow for complete knowledge and awareness. The common information observed by an agent in a particular state is subject to certain limitations, which includes individual actions, actions taken by our own side's agent and the actions taken by opponent agents within our visible range.</p><p>Examples: Observation 1: agent observes its own side's agent approaching the control point and trying to control it.</p><p>Observation 2: agent observes its opposed agent approaching urban residential area and trying to shoot.</p><p>We construct a retrieval function within the entire memory stream architecture and utilize it to extract observations from the historical experiences of the agents, providing a foundation for generating reasonable prompts and enable the language model to produce rational decision. The retrieval function can be selective, with the form of prioritizing the extraction of recently observations, important nodes set before and relevant memory to produce effective outcome.</p><p>Recency assigns a higher score to the observation added recently, in which case the agent prioritizes the memory information generated by the recent several steps. To account for the influence of time factors, we implement a time decay coefficient to calculate the score.</p><p>Importance categorizes the data within the memory stream into regular memories and core memories, and allocating higher scores to the core memories generated by agents. For example, a red agent moving towards the left and approaching the road can be categorized as a regular memory while a red agent approaching the control point and eliminating a blue agent can be classified as a core memory. In this architecture, we ask the language model to directly output the importance integer scores in a range from 1 to 10, in which case 1 means the purely common memory like moving on the road while 10 means the most important core memory like seizing the control point or shooting successfully. The specific implementation process can be described as follows: retrieving the corresponding memory from the memory stream to form a prompt, allowing the agent to generate importance scores accordingly and storing them back to the memory stream.</p><p>Example: Memory: the red agent one is seizing the control point.</p><p>Importance score: 8 Relevance assigns a higher score to those object that are relevant to the current situation because of the presence of relevance between different memory objects. For example, a red agent arrives at the road and approaching the control point at a higher speed, this memory has a strong correlation with the red agent's seizing the We use this score to comprehensively determine the prompts that should be extracted, and instruct the agent to generate corresponding and reasonable action-plannings based on these prompts.</p><p>Reflection Stream However, the observational performance of memory flow has limitations in the decision-making process in practical wargame environment. Reasoning based on raw observation is not efficient enough to allow a large language model to generate high-level decision results. It is necessary to infer and generate advanced reasoning semantics through the observation of information and the planning of actions. This paper defines this reasoning process as a higher-level memory flow, referred to as reflection. It is essentially a higher-level and more abstract thinking process. The reflection flow is generated together with the memory flow, but the generation of reflection is distinguished by the retrieval function in the preceding memory flow. When the score in the retrieval function exceeds a certain threshold, reflection is triggered. This reflective process involves a higher-level abstraction and understanding of previously observed information. It is essentially a combination of observed semantics and planned semantics generated through prompts, and it is generated periodically, providing the wargame agent with reasoning semantics.</p><p>The first step of reflection is to raise questions and clarify the reflective process based on the previous experience flow of wargame agent. For example, the blue agent is approaching the road and accelerating towards the control point.</p><p>The planning suggests that the red agent should reach hexagon 1403 and shoot the blue agent at that point. From this, reflective semantics are generated: The blue agent poses a significant threat and may create a disadvantageous situation for the red agent in this confrontation.</p><p>The reflection process allows the agent to reflect not only on their current observation but on other reflection. As a result, memories generated by the agent can be divided into different levels under the reflection mechanism, allowing for more accurate decision-making at an abstract level.</p><p>Task Planning Stream Strategic agent, based on the current situation observed by all the agents of our side, describes it as a prompt following a specific format: &lt; Summary, Observations, Planning &gt;. The Summary aims to convert the current situation from visual to semantic information <ref type="bibr" target="#b19">Sun et al. [2022]</ref>.</p><p>Observations describe specific circumstances observed by all agents, further enrich semantic information based on the summary. Planning involves task allocation and execution based on the observed situation. Example: Summary: our 10 agents are moving towards the control point and have identified 3 blue agents. Observations: blue agent 1 is nearing the control point.</p><p>Planning: red agent 1 -3 will prioritize engaging blue agent 1, while agents 4 -10 will quickly move towards the control point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ChatGPT + Wargaming Business Process</head><p>The whole core process is to transform the situation image information in the wargame simulation into semantic information, which includes description information and situation information, and this information is sent to the wargame agent in the form of prompt, and then the agent feedback the corresponding planning semantics, which is goal oriented. The planning semantics are then transformed into action sequences (such as 1, 2, 3, 4, ..10, where numbers represent specific actions. Alternatively, they are transformed into corresponding actions such as attack, defense, evasion, acceleration, shooting, left movement, etc.), which affect the environment and generate new environments. These actions are then recycled back to the starting situation image and converted into semantics.</p><p>On this basis, in order to reduce computational power and memory requirements, and improve operational efficiency, this article allows strategic agents to use GPT-4 LLM and tactical agents to use GPT-3.5 LLM. Compared to using GPT-4 or GPT-3.5 LLM entirely, this can comprehensively improve the intelligence of intelligent decision-making without requiring too much computing power and memory space. Firstly, input expert prior knowledge documents into the strategic agent for learning through GPT-4 LLM, and then provide appropriate prompt inputs to enable the strategic agent to make decisions through GPT-4 LLM and convert them into action outputs that affect the wargame environment. The strategic agent then sends corresponding instructions to each tactical agent for execution. The tactical agent provides feedback on whether the task is suitable for the current agent through GPT-3.5 LLM combined with appropriate prompts, and provides the recommended execution results to the strategic agent for adjustment.</p><p>Strategic agent: Based on the task planning flow, the strategic agent synthesizes states 1 to 10 and provides a task planning sequence, which is the action that each wargame should take in the step allocation;</p><p>Tactical agent: The tactical agent receives task planning and provides modification suggestions and reasons for the assigned tasks based on its own state;</p><p>The strategic agent plans again based on the modification suggestions until all tactical agents no longer provide modification suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Verification of Simulation Experimental Environment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Environment Display</head><p>This paper verifies the above established large language model through simulation experiments. The simulation platform is a wargame simulation platform, which can conduct game confrontation between red and blue sides. Both red and blue sides can use intelligent algorithms to make decisions and execute actions <ref type="bibr" target="#b20">Sun et al. [2020]</ref>. The basic adversarial rule is that the red and blue sides compete for the middle control point (red flag), and the party who first reaches the control point wins. Or if one party is completely destroyed by the other party, the other party wins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Advantages of large language model over reinforcement learning intelligent decision-making</head><p>In the previous experiments, we mainly made decisions through rule AI and reinforcement learning AI. For the first time, this work used the large language model to make decisions for agents, and it was verified on this platform. Interestingly, this work found that there is a large difference between large language models and Reinforcement learning. First, large language models or trained large language models can make decisions without waiting for the convergence of training, and can directly obtain high intelligence. Reinforcement learning algorithms often need a lot of training to gradually adapt to a new task. At the same time, compared with the reinforcement learning algorithm, the decision making using the large language model can directly achieve excellent intelligence in multiple different tasks, and does not need to re-train for different tasks, which is of high value for practical applications.</p><p>This article proposes two algorithms, GWA algorithm and GWAE algorithm. The GWA algorithm adopts the composition model proposed in this article and utilizes ChatGPT for decision-making in large language models. GWAE inputs expert experience on the basis of GWA. This paper inputs expert experience of Military simulation in the form of a document. See the appendix for the document.</p><formula xml:id="formula_0">Method Missions Kill Goal Survive GWAE 298±11 10504±64 4238±28 GWA 332±9 9106±99 5102±33 RNM-PPO 745±9 9102±141 4985±44 PPO 850±19 7804±44 5068±38 PK-DQN 792±14 7732±60 5026±53 DQN 1285±7 6948±161 5154±57</formula><p>Table 1: Scores of different algorithms for three tasks: kill, get goal, and survive.</p><p>0 25 50 75 100 125 150 175 200 Episodes 0 20 40 60 80 100 Cumulative Winrates Line Plot for Cumulative Winrate of All Algorithms Over Episodes GWAE -Ours GWA -Ours RNM-PPO PPO PK-DQN DQN The winning rate of GWA algorithm is generally higher than that of Reinforcement learning algorithm, including RNM-PPO, PPO, PK-DQN, DQN. After inputting expert prior knowledge documents for GWA algorithm, the intelligence of GWAE algorithm is significantly improved on the basis of the original GWA.</p><p>The experiment compares the GWAE algorithm and GWA algorithm proposed in this paper, and compares the winning rate of the algorithm proposed in this paper with the RNM-PPO <ref type="bibr" target="#b21">Xue et al. [2023]</ref>, PPO, PK-DQN <ref type="bibr" target="#b20">Sun et al. [2020]</ref> and DQN algorithms. Through Figure <ref type="figure" target="#fig_7">6</ref>, Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref>, it can be found that the intelligence of reinforcement learning algorithms DQN, PK-DQN, PPO, and RNM-PPO is enhanced in turn. However, the GWA algorithm that uses the large language model to make decisions is better than the reinforcement learning algorithm. Only the RNM-PPO  0 20 40 60 80 GWAE -Ours_mean_winrate 0 20 40 60 80 GWA -Ours_mean_winrate 0 20 40 60 RNM-PPO_mean_winrate 0 20 40 60 PPO_mean_winrate 0 20 40 60 PK-DQN_mean_winrate 0 50 GWAE -Ours_mean_winrate 0 20 40 DQN_mean_winrate 0 25 50 75 GWA -Ours_mean_winrate 0 25 50 75 RNM-PPO_mean_winrate 0 20 40 60 PPO_mean_winrate 0 20 40 60 PK-DQN_mean_winrate 0 20 40 DQN_mean_winrate algorithm is closer to GWA. If expert experience documents are fed to GWA, the GWAE algorithm's victory rate will be significantly improved. Overall, the overall winning rate of the algorithm proposed in this paper is significantly better than the previous classical reinforcement learning algorithm. Also on the premise of fixed blue intelligence and fixed reasoning scenarios, the overall effect of intelligent decision-making using the large language model is also relatively stable, the overall victory rate fluctuates relatively little. For different typical task scenarios, GWA and GWAE exhibits outstanding performance, especially in the task of scoring, the performance of GWA and GWAE is significantly better than that of classical reinforcement learning algorithms. These two algorithms have demonstrated the potential value of large language models in the field of intelligent decision-making by making appropriate decisions for different task planning scenarios without undergoing extensive training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work innovatively applies the large language model to intelligent decision-making, and verifies the feasibility of the large language model for decision-making in the wargame platform. Compared with the intelligent decision-making of Reinforcement learning, this paper finds that the large language model has obvious advantages. Firstly, the large language model for decision-making has strong adaptability in practical game confrontations due to sufficient training in advance. There is no need to wait to restart training, and it has strong intelligence and generalization for different tasks.</p><p>Secondly, the intelligence shown by the large language model is obviously stronger than the general Reinforcement learning algorithm, which proves the great potential of the large language model in decision-making. Finally, this article also found through experiments that there is a significant correlation between the intelligence of large language models and prompt. If there is a more suitable prompt, its displayed intelligence is significantly improved. Of course, the work of this article is still an initial exploration of the large language model, and there is still much work to be innovated in the future, such as the attempt of the large language model in different scenarios, and the use of the large language model in more complex game adversarial environments to further enhance the intelligence of the adversarial blue, in order to test the intelligence level of the large language model. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategic Agent:</head><p>Red tank 1 Move_to: 16, 14 Reason: Tank 1 is moved from the closest corner to the target, while avoiding the direct attack of blue tank 2. Red tank 2 Move_to: 17, 17 Reason: Tank 2 is moved as close to the target as possible while avoiding the possible fire range of blue tanks 2 and 10. Red tank 3 Move_to: 18, 19 Reason: Tank 3 is moved towards the target position, trying to stay out of the range of blue tanks 2 and 10. Red tank 4 Move_to: 15, 17 Reason: Tank 4 is moved to a location that can fend off the firepower of blue tanks 2 and 10 and quickly reach the target when needed. Red tank 5 Move_to: 16, 19 Reason: Tank 5 is trying to get close to the target, while maintaining a safe distance to avoid the firepower of blue tanks 2 and 10. Red tank 6 Move_to: 17, 19 Reason: Tank 6 is heading towards the target, keeping a distance as much as possible to avoid the firepower of blue tanks 2 and 10. Red tank 7 Move_to: 16, 17 Reason: Tank 7 is maintained in a good position towards the target, keeping itself out of the direct range of blue tanks 2 and 10. Red tank 8 Move_to: 19, 18 Reason: Tank 8 is moved towards the target, trying to stay out of the range of blue tanks 2 and 10. Red tank 9 Move_to: 20, 17 Reason: Tank 9 is moving forward to the target, avoiding the fire range of blue tanks 2 and 10. Red tank 10 Move_to <ref type="bibr">: 20, 16</ref> Reason: Tank 10 is trying to get as close to the target as possible, keeping a safe distance to avoid the firepower of blue tanks 2 and 10.</p><p>These moves aim to position each tank towards the target point and to keep each tank at as varied locations as possible to avoid the concentrated firepower from the blue team. This strategy aims to get the tanks to the target point as quickly as possible and to avoid enemy firepower where possible. As Red tank 1, the command "Red tank 1 Hide" given by the command center seems somewhat unreasonable from my perspective.</p><p>"Hide" implies that I should avoid blue tank 2's attacks, which may lead to distancing myself from the target control point. As an attacking force, my priority should be to approach the target and engage in combat at the right moment, rather than evading the battle.</p><p>My ultimate goal is to capture control point <ref type="bibr">[12,</ref><ref type="bibr">24]</ref>, and currently, blue tank 2 (located at <ref type="bibr">[16,</ref><ref type="bibr">40]</ref>) is closer to the control point. If I choose to hide, it will further increase the distance between me and the target, which is not conducive to reaching the objective quickly.</p><p>In the last step, blue tank 5, 7, and 8 launched indirect attacks at <ref type="bibr">[17,</ref><ref type="bibr">7]</ref>, and blue tank 10 also moved near <ref type="bibr">[17,</ref><ref type="bibr">41]</ref>. Given that blue tank 5, 7, and 8 launched indirect attacks, I have reason to believe that the area around <ref type="bibr">[17,</ref><ref type="bibr">7]</ref> might not be the position of blue tank 2. Therefore, avoiding direct confrontation with blue tank 2 is not the most urgent task at the moment.</p><p>In conclusion, from my perspective, I believe that "Red tank 1 Hide" is not the most reasonable command. Instead, I should choose to advance towards the control point <ref type="bibr">[12,</ref><ref type="bibr">24]</ref>, adopt a suitable strategy to avoid direct engagement with blue tank 2, and consider how to coordinate with the movements of other friendly tanks to achieve victory. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>, transport the Squad to the front line and remain vigilant. APC: CP, we are rapidly transporting the Squad to the front line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The relationship between strategic and tactics agent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Double Layer Agent Task Planning Decision Framework Based on Large Language Model</figDesc><graphic coords="4,72.00,267.14,468.00,359.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Experimental Simulation Environment Display</figDesc><graphic coords="6,71.07,72.00,468.96,251.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Line Plot for Winrate of All Algorithms Over Episodes. The winning rate of GWA algorithm is generally higher than that of Reinforcement learning algorithm, including RNM-PPO, PPO, PK-DQN, DQN. After inputting expert prior knowledge documents for GWA algorithm, the intelligence of GWAE algorithm is significantly improved on the basis of the original GWA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Cumulative Mean Winrate of All Algorithms Over Episodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Violin Plot for Mean Winrate of All Algorithms Over Episodes. The GWAE algorithm and GWA algorithm have a higher winning rate and are more stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Scatter plot of the average winning rate of all algorithms. The results of comparing the overall winning rates of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and DQN algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Strategic Agent Prompt Feedback Display</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Tactical Agent Prompt Feedback Display</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61876079</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bpWNwqk">
					<idno type="grant-number">61876079</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Large Language Model Prompt</head><p>In this experiment, it was found that the intelligence of the large language model is closely related to prompt. Two different prompts were used for guidance in this experience, and the results returned after prompt input are shown in the figure. The figure <ref type="figure">9</ref> shows the response results of the strategic agent feedback. The figure <ref type="figure">10</ref> shows the response results of tactical agent feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Prior Knowledge Document</head><p>This article feeds professional domain knowledge documents into the proposed GWA algorithm, forming the GWAE algorithm. Document link: <ref type="url" target="https://github.com/sunyuxiang926/wargame/blob/b0118cd0">https://github.com/sunyuxiang926/wargame/blob/b0118cd0</ref> 1fa46f2790a89895336df81519d91707/files/theses/explanat ion_of_domain_expert_knowledge.pdf</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chatgpt: five priorities for research</title>
		<author>
			<persName><forename type="first">Eva Am</forename><surname>Van Dis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudi</forename><forename type="middle">L</forename><surname>Bockting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="224" to="226" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What chatgpt and generative ai mean for science</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stokel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Van Noorden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="214" to="216" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Use chat gpt to solve programming bugs</title>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Nigar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">Y</forename><surname>Surameery</surname></persName>
		</author>
		<author>
			<persName><surname>Shakor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology &amp; Computer Engineering (IJITC)</title>
		<idno type="ISSN">2455-5290</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="17" to="22" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Role of chat gpt in public health</title>
		<author>
			<persName><forename type="first">Biswas</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="868" to="869" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Potential use of chat gpt in global warming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1126" to="1127" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge traction and data-driven wargame ai design and key technologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Eng. Electron. Technol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2911" to="2917" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decision method for tactical level military chess entity operations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control and Decision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2977" to="2985" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-agent decision-making method under the actor-critic framework and its application in wargames</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Yanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Yongliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tiande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems engineering and electronic technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tactical maneuver strategy learning of land war based on convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of System Simulation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2181</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mastering complex control in moba games with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Deheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6672" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards playing full moba games with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Guibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuhao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="621" to="632" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of deep reinforcement learning algorithm and its application</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="314" to="326" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Review of intelligent games: Implications of game ai for combat inference</title>
		<author>
			<persName><forename type="first">Sun</forename><surname>Yuxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yihui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xinlei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Journal of Intelligent Science and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Outracing champion gran turismo drivers with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Peter R Wurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kawamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Macglashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Devlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">602</biblScope>
			<biblScope unit="issue">7896</biblScope>
			<biblScope unit="page" from="223" to="228" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intelligent decision-making and human language communication based on deep reinforcement learning in a wargame environment</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="201" to="214" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Research and implementation of intelligent decision based on a priori knowledge and dqn algorithms in wargame environment</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1668</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-attribute decision-making in wargames leveraging the entropy-weight method in conjunction with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
