<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Advanced Visual Reasoning Ability of Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-21">21 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dongnan</forename><surname>Liu</surname></persName>
							<email>dongnan.liu@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tengfei</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidong</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Advanced Visual Reasoning Ability of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-21">21 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">7EDF0B9AE9A2FB098DC03267154FBD43</idno>
					<idno type="arXiv">arXiv:2409.13980v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in Vision-Language (VL) research have sparked new benchmarks for complex visual reasoning, challenging models' advanced reasoning ability. Traditional Vision-Language Models (VLMs) perform well in visual perception tasks while struggling with complex reasoning scenarios. Conversely, Large Language Models (LLMs) demonstrate robust text reasoning capabilities; however, they lack visual acuity. To bridge this gap, we propose Complex Visual Reasoning Large Language Models (CVR-LLM), capitalizing on VLMs' visual perception proficiency and LLMs' extensive reasoning capability. Unlike recent multimodal large language models (MLLMs) that require a projection layer, our approach transforms images into detailed, context-aware descriptions using an iterative self-refinement loop and leverages LLMs' text knowledge for accurate predictions without extra training. We also introduce a novel multimodal in-context learning (ICL) methodology to enhance LLMs' contextual understanding and reasoning. Additionally, we introduce Chain-of-Comparison (CoC), a step-by-step comparison technique enabling contrasting various aspects of predictions. Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The concept of complex visual reasoning was introduced with Visual Commonsense Reasoning (VCR) dataset <ref type="bibr" target="#b47">(Zellers et al., 2019)</ref> in 2019, which tests models' ability to understand visual content as well as commonsense cognition. However, the development in this field has remained relatively subdued, primarily due to Vision-Language Models' (VLMs) limitations in incorporating commonsense knowledge <ref type="bibr" target="#b14">(Gan et al., 2022)</ref>. Recent years have seen significant advancements in complex linguistic reasoning tasks <ref type="bibr" target="#b11">(Cobbe et al., 2021;</ref><ref type="bibr" target="#b43">Wei et al., 2022)</ref> due to the emerging GPT3 <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>, LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref>, and Vicuna <ref type="bibr" target="#b10">(Chiang et al., 2023)</ref>. This leap forward has triggered a renewed interest in the complex visual reasoning area, exploring how visual perception can enhance linguistic inference and potentially overcome previous hurdles <ref type="bibr" target="#b14">(Gan et al., 2022)</ref>. It has led to innovative benchmarks focusing on various aspects: commonsense reasoning -WinoGAViL <ref type="bibr" target="#b2">(Bitton et al., 2022)</ref>, compositionality -Winoground <ref type="bibr" target="#b39">(Thrush et al., 2022)</ref>, weird image explanation -Whoops <ref type="bibr" target="#b3">(Bitton-Guetta et al., 2023)</ref>, and humor understanding -NYCCC <ref type="bibr" target="#b15">(Hessel et al., 2022)</ref>. These tasks demand models not only accurately interpret image content, but also integrate knowledge from daily experiences, general commonsense, cultural context, and humor sense. For example, a synthetic image, as shown in Whoop's example in Figure <ref type="figure">1</ref> of "The portrait of the Mona Lisa depicts a stern male face." contradicts the cultural context, as the famous painting Mona Lisa depicts a female face.</p><p>In this paper, we introduce a novel method named Complex Visual Reasoning Large Language Models (CVR-LLM), based on the "VLMs + LLMs" concept. Recent multimodal large language models (MLLMs) like LLaVA <ref type="bibr" target="#b24">(Liu et al., 2024</ref><ref type="bibr">(Liu et al., , 2023a) )</ref> and MiniGPT4 <ref type="bibr" target="#b53">(Zhu et al., 2023;</ref><ref type="bibr" target="#b6">Chen et al., 2023)</ref> have proven effective in many VL tasks. However, these models are resource-intensive, relying on millions of image-text pairs for projection layer learning. To overcome this limitation, our approach leverages the visual perception strengths of VLMs to translate images into context-aware image descriptions (CaID) via an inference-only, dual-loop self-refinement process that incorporates feedback from LLMs. These detailed descriptions enhance the LLMs' inference process, transforming multi-modal tasks into simpler single-modal challenges and streamlining the overall process. In addition, we develop a unique multi-modal in-Figure <ref type="figure">1</ref>: Five distinct examples from diverse datasets in the complex visual reasoning field <ref type="bibr" target="#b3">(Bitton-Guetta et al., 2023)</ref> challenge AI models' ability of complex reasoning in different aspects such as general commonsense.</p><p>context learning (ICL) approach named Complex Visual Reasoning ICL (CVR-ICL), which enhances the reasoning capacities of LLMs within a range of complex multi-modal environments. Figure <ref type="figure" target="#fig_0">2</ref> provides an illustration of how our CVR-LLM is applied to the Winoground task. It describes the images as appropriate sentences via CaID and utilizes the sophisticated reasoning and ICL abilities of LLMs through CVR-ICL for more accurate predictions.</p><p>Our research stands as the pioneering study to explore such a broad array of benchmarks <ref type="bibr">(WinoGAViL, Winoground, Whoops, VCR, and NYCCC)</ref>, proposing a paradigm centred on the "VLM+LLM" concept for addressing complex visual reasoning tasks. Experimental results show that CVR-LLM achieves SOTA performance across all five tasks. Further ablation studies and comparative analyses reveal the effectiveness of each module and the superiority of our method over previous approaches. Particularly in comparative analysis, we introduce the Chain-of-Comparison (CoC) technique, inspired by "Chain-of-Thought" and utilizing GPT4 <ref type="bibr">(Achiam et al., 2023)</ref>, to address the limitations of conventional metrics in evaluating abstract concepts. CoC provides a nuanced analysis by systematically dissecting and quantitatively contrasting various facets of the results for a comprehensive evaluation.</p><p>Our contributions are summarized as follows: (1) We present the first comprehensive study across all complex visual reasoning tasks, including Wino-GAViL, Winoground, Whoops, VCR, and NYCCC.</p><p>(2) We design a context-aware image description generation method and a specific in-context learning strategy<ref type="foot" target="#foot_0">foot_0</ref> , to enhance the advanced visual reasoning ability of LLMs to multi-modal complex visual reasoning tasks. (3) We further introduce Chain-of-Comparsion, a novel GPT4-based comparison technique inspired by "Chain-of-Thought" filling the gaps of traditional metrics in abstract concept evaluation. (4) Experimental results show that our approach surpasses current SOTA models in a range of complex visual reasoning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reasoning Research in Vision-Language Domain</head><p>In recent years, multi-modal reasoning research has significantly advanced. Beyond the complex visual reasoning benchmarks discussed in Section 1, many studies focus on the reasoning process itself, such as chain-of-thought <ref type="bibr" target="#b19">(Kojima et al., 2022;</ref><ref type="bibr" target="#b37">Shaikh et al., 2022)</ref> or reasoning modules <ref type="bibr">(Zhou et al., 2023b;</ref><ref type="bibr" target="#b17">Jiang et al., 2023)</ref>, which are crucial for enhancing AI models' analytical capabilities and performance. For instance, <ref type="bibr">Liu et al. (2023b)</ref> introduced a modality-aligned thought chain reasoning framework to incorporate explicit reasoning into task-oriented dialogue generation, improv- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large Language Models for Vision-Language Analysis</head><p>The past two years have seen an unprecedented surge in the development and application of LLMs <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr">Touvron et al., 2023a;</ref><ref type="bibr" target="#b10">Chiang et al., 2023)</ref> across diverse fields. LLMs have garnered acclaim for their robust capabilities, including advanced analytical prowess <ref type="bibr" target="#b19">(Kojima et al., 2022)</ref>, extensive text-level knowledge <ref type="bibr">(Naveed et al., 2023)</ref> and superior understanding ability <ref type="bibr" target="#b5">(Chang et al., 2023)</ref>. Furthermore, they are equipped with two powerful mechanisms: chain-of-thought <ref type="bibr" target="#b19">(Kojima et al., 2022)</ref> and in-context learning <ref type="bibr">(Liu et al., 2021a)</ref>, which significantly augment their effectiveness and performance in specialized tasks <ref type="bibr">(Naveed et al., 2023)</ref>. For example, Muraoka et al. (2023) developed a cross-lingual model trained alongside a crosslingual LLM, leveraging LLMs' capabilities across languages. Lan et al. (2023) proposed reasoning question prompts for Visual Question Answering (VQA) tasks, unlocking LLMs' potential in zeroshot learning. Additionally, Yang et al. (2023) introduced SODA, a system that integrates LLMs with explainable AI to assist marketers with data interpretation, enhancing human-AI collaboration. Zhong et al. (2023) used knowledge distillation to imbue the SUR-adapter with LLMs' semantic understanding and reasoning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we introduce the CVR-LLM framework, highlighting its innovative process for generating context-aware image descriptions (CaID) as well as its complex visual reasoning in-context learning (CVR-ICL) strategy. Initially, we explain the CaID generation process, which differs from traditional image captioning by using a selfrefinement loop with feedback from Large Language Models (LLMs) to produce accurate and contextually relevant descriptions (Section 3.1). Subsequently, we present the CVR-ICL approach (Section 3.2), which enhances LLMs' contextual understanding and reasoning by assessing relevant cases and selecting suitable complex multi-modal demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context-Aware Image Description</head><p>Pre-trained VLMs <ref type="bibr" target="#b21">(Li et al., 2023;</ref><ref type="bibr" target="#b1">Alayrac et al., 2022)</ref> have demonstrated their proficiency in generating detailed image captions on benchmarks such as MSCOCO <ref type="bibr" target="#b8">(Chen et al., 2015)</ref>. However, while these captions may accurately reflect visual content, they are not customized for complex visual reasoning scenarios. Recently, the trend of multi-modal instruction-following agents like miniGPT4 <ref type="bibr" target="#b53">(Zhu et al., 2023;</ref><ref type="bibr" target="#b6">Chen et al., 2023)</ref> and LLaVA <ref type="bibr" target="#b24">(Liu et al., 2024</ref><ref type="bibr">(Liu et al., , 2023a))</ref>, integrating opensource LLMs <ref type="bibr" target="#b10">(Chiang et al., 2023;</ref><ref type="bibr">Touvron et al., 2023b)</ref> with pre-trained vision encoders <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020;</ref><ref type="bibr">Liu et al., 2021b)</ref> to create a MLLM, has become very popular. The effectiveness of these models is heavily reliant on tuning with vast amounts of VL instruction data, which is generated by powerful LLMs like ChatGPT (Ope-nAI, 2022) and GPT4 <ref type="bibr">(Achiam et al., 2023)</ref>. While promising, their reliance on extensive VL instruc- tion data for tuning requires the substantial resource and time investment. In this work, we introduce a more efficient method for generating context-aware image descriptions, which depends on the inference process and leverages task-specific information and feedback from LLMs to craft better prompts, guiding the caption generation process more effectively.</p><p>Our CaID framework optimizes the process of creating context-aware image descriptions through a dual-loop self-refinement approach, as shown in Figure <ref type="figure" target="#fig_1">3</ref>. Initially, it leverages task-specific details and LLM insights to craft precise image prompts. These initial prompts are designed to distill essential task-related information, guiding the captioner in producing descriptions that not only cover image content but are also deeply aligned with the task's requirements. Specifically, given a task specific text description t with an image i (for processes involving multiple images, we approach each image sequentially), the generation of initial context-aware image descriptions can be described as follows:</p><formula xml:id="formula_0">d init = C(i, L(t)),<label>(1)</label></formula><p>where d init is the initial generated context-aware image description. C is the image-to-text captioner, transfering the image into the description. L is the LLM, encapsulating crucial task-related text information t (e.g. requirements, questions, cue words) into feature prompts.</p><p>In the second loop, our approach is crafted to encapsulate essential task-related details as well as LLMs' feedback, enhancing description generation with LLMs' vast knowledge. Specifically, it merges initial descriptions with task specifics and CVR-ICL examples into a task-focused prompt, guiding LLMs to make more precise predictions.</p><p>These predictions are then treated as pseudo labels, asking LLMs to design further inquiries for deeper insights around them. In this way, we build up a feedback reflection between LLM prediction and context-aware caption, enhancing the richness and accuracy of the content produced. The textual feedback is then leveraged to refine the image prompts, providing deep insights that inform and guide the generation of nuanced image descriptions. The revised context-aware image descriptions can be described as follows:</p><formula xml:id="formula_1">d revised = C(i, L(t, Q(p))),<label>(2)</label></formula><p>where d revised is the revised generated contextaware image description. Q is the further query from LLM. p is the prediction from LLM according to the generated task prompt. Q(p) is the text feedback for updating image prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complex Visual Reasoning ICL</head><p>LLMs are renowned for their exceptional incontext learning capabilities, especially with taskspecific examples. The optimal in-context exemplars enable LLMs to leverage their background knowledge for more precise outcomes. However, most of the research works <ref type="bibr">(Liu et al., 2021a;</ref><ref type="bibr" target="#b38">Sorensen et al., 2022)</ref> have primarily focused on the text-centric domain, with few works <ref type="bibr" target="#b1">(Alayrac et al., 2022;</ref><ref type="bibr" target="#b48">Zhao et al., 2023)</ref> exploring multimodal in-context learning for VL tasks. Our approach, unlike prior methods focused solely on text similarity in NLP, such as the kNN-augmented in-context example selection (KATE), integrates multi-modal factors, thereby enriching the discipline with a fresh perspective. Furthermore, it is also different from MMICL <ref type="bibr" target="#b48">(Zhao et al., 2023)</ref> in the multi-modal domain, which employs a vision prompt generator for image-to-visual embedding conversion and merges these with text embeddings as a union measurement factor. Complex visual reasoning tasks demand models capable of selecting in-context examples from a multi-modal domain, leveraging extensive background knowledge and information within it <ref type="bibr" target="#b48">(Zhao et al., 2023)</ref>. However, our CVR-LLM is grounded in LLMs, which are inherently text-based, leading to a gap between textual and multi-modal domains. Directly applying a text-based kNN clustering method could result in the loss of important multi-modal information. On the other hand, using multi-modal information for retrieval might ignore essential context-aware information within our generated image descriptions. To address this, we propose the complex visual reasoning ICL, which aims to select in-context examples for LLMs by effectively integrating both text and multi-modal components. This dual analysis enables our LLM to more effectively select contextually relevant examples, ensuring a balanced integration of text and multimodal insights for enhanced in-context learning. Figure <ref type="figure" target="#fig_2">4</ref> illustrates the framework of our CVR-ICL strategy. Specifically, given a task t with an image i, we initially convert the image into a description d, which enables the task to be applicable not only in multi-modal domains but also in text-only scenarios. Then, we employ a multi-modal encoder f m and a text encoder f t to transform inputs from the multi-modal domain and the text domain into vector representations as follows:</p><formula xml:id="formula_2">x m = f m (t, i), (3a) x t = f t (t, d),<label>(3b)</label></formula><p>where x m is the vector representation in the multimodal domain. x t is the vector representation in the text domain.</p><p>Upon transforming each example into two distinct vector forms, we compute the cosine similarity score to identify and select the examples that are most relevant. Considering a target sample in test set and the ith example in the training set, the similarity calculation process can be expressed as follows:</p><formula xml:id="formula_3">s m = f c (x m , x ith m ),<label>(4a)</label></formula><formula xml:id="formula_4">s t = f c (x t , x ith t ), (4b) s = s m + s t ,<label>(4c)</label></formula><p>where s m is the similarity score between the target sample and ith example in dataset on the multimodal domain, s t is the similarity score between the target sample and ith example in dataset on the text domain. s is the final similarity score. f c is the cosine similarity function. Finally, the top-k cases with the highest s are selected as the in-context examples, aimed at boosting the contextual understanding and prediction accuracy of the LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Metrics</head><p>To evaluate the effectiveness of our proposed method, we conduct a comprehensive test in complex visual reasoning areas. Our evaluation included WinoGAViL (4373 samples), Winoground (400 samples), Whoops (500 samples), VCR (2653 out of over 26k samples, selecting a random 10%), and NYCCC (528 samples), providing a broad assessment of our approach's capabilities. In the terms of metrics, we adhered to the evaluation methods provided by these datasets, ensuring a fair assessment of our method's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For the basic captioner in context-aware image description (Section 3.1), we choose the BLIP2-flant5xxl <ref type="bibr" target="#b21">(Li et al., 2023)</ref> as our baseline. For CVR-ICL phase (Section 3.2), we employ BM25 <ref type="bibr" target="#b36">(Robertson et al., 1995)</ref> and BLIP2 multi-embedding <ref type="bibr" target="#b21">(Li et al., 2023)</ref> to encode text and multi-modal inputs, respectively. It is important to note that the ICL example results are derived from LLM inference without using actual annotations to prevent data leakage. For our LLMs, we choose three </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to State-of-the-Arts</head><p>In this section, we evaluate our proposed CVR-LLM against various models across a range of complex visual reasoning tasks, including Wino-GAViL, Winoground, Whoops, VCR, and NYCCC. These models fall into two categories: VLMs <ref type="bibr" target="#b18">(Kim et al., 2021;</ref><ref type="bibr" target="#b35">Radford et al., 2021;</ref><ref type="bibr" target="#b13">Gan et al., 2020;</ref><ref type="bibr" target="#b21">Li et al., 2023)</ref> and MLLMs <ref type="bibr" target="#b24">(Liu et al., 2024</ref><ref type="bibr">(Liu et al., , 2023a;;</ref><ref type="bibr" target="#b53">Zhu et al., 2023;</ref><ref type="bibr" target="#b6">Chen et al., 2023)</ref>. Notably, MLLMs like LLaVA and MiniGPT4 struggle with tasks involving multiple images, making their performance data unavailable for WinoGAViL and Winoground.</p><p>Table <ref type="table" target="#tab_1">1</ref> showcases our method's superiority across five tasks, eclipsing both VLMs and LMMs. For example, our CVR-LLM Llama3 significantly surpasses the SOTA model BLIP2 by achieving an 88.7% accuracy (+17.1 improvement) in SWOW setting on the WinoGAViL benchmarks. Similarly, it outperforms the SOTA model MiniGPT4 with a 62.0% accuracy (+13.8 improvement) on the GPT4 rate <ref type="bibr" target="#b3">(Bitton-Guetta et al., 2023)</ref> for Whoops tasks, underscoring our framework's advanced performance. Additionally, our method performs well on three LLM-based categories, demonstrating robust generation abilities with consistent performance. This highlights the versatility and adaptability of our model, ensuring high-quality results across various complex visual reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this section, we examine the individual contributions of the components within our framework CVR-LLM GP T 4 . As demonstrated in Table <ref type="table">2</ref>, we present an ablation study that quantifies the performance impact of each module across various datasets. The experimental findings suggest that the CVR-ICL module significantly boosts the inference performance of LLMs compared to using contextaware image descriptions alone, with the exception of the NYCCC dataset (It may be due to NYCCC's focus on humor, where precise descriptions are more critical). This highlights the CVR-ICL module's effectiveness in enhancing LLM capabilities across various tasks. In addition, our comprehensive method, CVR-LLM, which integrates both context-aware descriptions and CVR-ICL, achieves a substantial enhancement in performance relative to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Context-Aware Image Description vs General Image Caption In this section, we investigate CaID's impact at an abstract level and design a novel method to quantitatively demonstrate the semantic gap between context-aware image descriptions and general image captions (Note that the performance impact has been shown in Table <ref type="table">2</ref>). Figure <ref type="figure" target="#fig_3">5</ref> provides two examples comparing contextaware image descriptions with general image captions and our goal is to determine whether contextaware descriptions offer more contextually relevant information to aid LLMs in decision-making. Un-</p><p>Module WinoGAViL Winoground Whoops VCR NYCCC 5/6 10/12 SWOW Text Image Group GPT4 Rate Q-&gt;A QA-&gt;R Q-&gt;AR Match acc. CrowdAcc NYAcc Base 60.0 58.3 78.4 28.7 26.2 16.0 36.4 38.0 37.0 21.3 41.8 41.3 46.0 Base+CaID 63.5 62.0 73.7 31.5 30.0 19.7 54.6 43.9 44.2 22.9 51.5 48.7 53.6 Base+CVR-ICL 69.8 66.1 80.9 39.0 29.2 22.0 60.6 48.8 49.2 25.8 48.0 47.6 52.9 CVR-LLM GP T 4 73.4 73.2 86.5 43.5 35.0 26.5 62.0 54.3 52.9 30.4 60.6 57.4 63.1</p><p>Table <ref type="table">2</ref>: The ablation study of our CVR-LLM on five complex visual reasoning tasks. "Base" represents using the general image captions and GPT4 to complete these tasks. "Base+CaID" means using the context-aware image descriptions instead of the general image captions and GPT4 to test the performance. "Base+CVR-ICL" represents using general image captions and GPT4 with our designed CVR-ICL learning methods. like traditional sentence evaluations that rely on annotations to compute metrics like BLEU <ref type="bibr" target="#b34">(Papineni et al., 2002)</ref> and CIDEr <ref type="bibr" target="#b42">(Vedantam et al., 2015)</ref>, we lack direct measures to assess the contextual relevance of sentences. To address this, we use GPT4 <ref type="bibr">(Achiam et al., 2023)</ref> to evaluate the relative effectiveness between two kinds of expressions with the prompt: "Evaluate the equivalence of the following two options for the task XXX. Option A: XXX; Option B: XXX. Please return True if Option B is better than Option A in answering questions; return False if the opposite is true; return Equal if they are the same for the question.". Additionally, inspired by the concept of chain-of-thought (CoT) <ref type="bibr" target="#b43">(Wei et al., 2022)</ref>, we propose a novel comparison chain-of-comparison (CoC), which implements a step-by-step analysis to evaluate the effectiveness. This method involves a comprehensive four-step analysis protocol, depicted in Figure <ref type="figure" target="#fig_4">6</ref>. It follows a series of cognitive steps that our brains undertake to make sense of information, particularly when engaging with complex problems.</p><p>Figure <ref type="figure" target="#fig_5">7</ref> shows the results of directly employing GPT4 to compare the effectiveness of general im-  age captions with our image descriptions in the specific scenario of answering task-related questions. Furthermore, Table <ref type="table" target="#tab_4">3</ref> presents the performance derived from utilizing GPT4 to conduct a detailed, step-by-step analytical assessment of effectiveness. These empirical results indicate that our approach yields image descriptions with enhanced contextual relevance, thereby significantly aiding LLMs in the decision-making process, particularly on the WinoGAViL and Whoops datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Visual Reasoning ICL vs Other ICL</head><p>The CVR-ICL is designed to optimize the selection of in-context exemplars within a multi-modal environment, thereby enhancing the reasoning abilities of LLMs. This innovative method is contrasted with three alternative configurations: Random In-Context Learning (RICL) <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>, KATE <ref type="bibr">(Liu et al., 2021a)</ref>, and Multi-modal Similar In-Context Learning (MMICL) <ref type="bibr" target="#b48">(Zhao et al., 2023)</ref>. To ensure a fair comparison, we utilized general image captions across all models to test performance WinoGAViL Caption Better 6.0 4.3 8.3 5.0 5.9 Description Better 75.3 76.0 71.3 76.7 74.8 Equal 18.7 19.7 20.3 18.3 19.3 Winoground Caption Better 24.0 24.0 29.0 27.0 26 Description Better 59.0 56.0 59.0 56.0 57.5 Equal 17.0 20.0 12.0 17.0 16.5 Whoops Caption Better 27.0 13.0 14.0 13.0 16.7 Description Better 71.0 80.0 76.0 75.0 75.5 Equal 2.0 7.0 10.0 12.0 7.7 VCR Caption Better 24.3 32.5 30.1 28.6 28.9 Description Better 53.5 45.4 50.6 52.7 50.5 Equal 22.2 22.1 19.3 18.7 20.6 NYCCC Caption Better 18.6 15.8 17.4 19.1 17.7 Description Better 58.5 62.3 60.4 61.0 60.5 Equal 22.9 21.9 22.2 19.9 21.8 Table 4: The performance of using different ICL methods on different datasets.</p><p>for eliminating the effect of our context-aware image descriptions. As demonstrated in Table <ref type="table">4</ref>, our CVR-ICL outperforms other ICL methods, demonstrating its adeptness at integrating and leveraging both textual and multi-modal domains to select the most contextually appropriate exemplars.</p><p>Case Number Selection in Complex Visual Reasoning ICL Figure <ref type="figure" target="#fig_6">8</ref> illustrates the influence of varying case numbers in the CVR-ICL on the performance of our proposed CVR-LLM method. The experimental results suggest a trend where the model's performance initially improves with an increase in case numbers, exhibits fluctuations at higher numbers, and eventually declines as the case number becomes excessively large. This pattern suggests that the optimal selection for the number of cases is four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Qualitative Results</head><p>To showcase the capabilities of our approach, we present qualitative results in Figure <ref type="figure" target="#fig_7">9</ref>. It illustrates how LLMs leverage contextual information to ask more relevant and insightful questions tailored the specific tasks. For instance, when provided with an image of the chess piece, the LLMs might ask  "What does the chess piece look like?". Subsequently, the captioner model generates contextually appropriate descriptions, such as "A chess piece that looks like a unicorn.". This synergy enhances the LLM's decision-making process, making it more precise and context-aware. More detailed qualitative results with corresponding prompts and CVR-ICL examples are illustrated in Appendix A.1 and Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose CVR-LLM, an innovative approach for complex visual reasoning tasks. This method boosts LLMs' understanding of visual content for complex reasoning via context-aware image descriptions. We also develop a multi-modal in-context learning technique, enhancing LLMs' reasoning skills at both image and text levels. Experimental results show that CVR-LLM sets new benchmarks across multiple complex visual reasoning tasks. We also introduce a nuanced GPT4 based analysis technique Chain-of-Comparison to automatically break down and contrast among various aspects of generated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitation</head><p>Although our approach achieves SOTA performance across a wide range of complex visual reasoning benchmarks, it still has two notable limitations. First, compared to the MLLMs that can perform end-to-end inference directly, our approach operates as an LLM-agent-driven framework. This involves VLMs generating context-aware image descriptions, followed by the LLM performing inference with ICL to predict the answer. While this twostep process enhances contextual understanding and reasoning, it may significantly increase time consumption compared to direct end-to-end inference models. Second, despite its overall strong performance and generalization ability, our approach still lags behind GPT4V in some tasks. Figure <ref type="figure" target="#fig_8">10</ref> shows that our CVR-LLM can surpass GPT4V in SWOW setting in dataset but fall short in others. Our future work will focus on refining the integration between VLMs and LLMs components and enhancing the model's efficiency and accuracy across a broader spectrum of complex visual reasoning challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Qualitative Results with Corresponding Prompt</p><p>Section 5 only illustrates the simplified process of our Context-aware Image Description (CaID) generation. Here, we delve into more details about the generation process and the corresponding prompts. Figure <ref type="figure" target="#fig_9">11</ref> provides an example of the CaID generation process applied to the VCR <ref type="bibr" target="#b47">(Zellers et al., 2019</ref>) task. In this example, the initial input consists of an image showing several individuals, with two of them (Person1 and Person4) holding guns. The associated question is: "Why do Person1 and Person4 have guns?" with multiple-choice options such as "1) They are soldiers. 2) Person1 and Per-son4 are robbing a hotel room. 3) They are cattle thieves. 4) They are about to shoot someone.". The CaID process begins by generating a detailed description of the image. The captioner model produces an initial caption: "An image of a man in a suit with a gun and another in a suit with a gun.". This caption, while descriptive, lacks the context needed to answer the specific question posed. To address this, our system prompts the LLM with a scenario where it acts as a questioner for the image caption model. The LLM is instructed to generate a follow-up question to gather crucial information for answer prediction. The prompt guides the LLM to consider specific details such as the appearance and pose of the individuals. In this case, the LLM generates the question: "What is the appearance of Person1 and Person4?". This question is designed to extract more contextually relevant details from the image captioner. The captioner then provides a refined description: "Per-son1 is wearing a suit with a gun and Person4 is wearing a suit with a gun.". This additional information helps to better understand the scene and narrows down the possible answers to the original question. This detailed process highlights how our system leverages both multi-modal and textual information to generate precise and contextually relevant descriptions, ultimately improving the performance on complex visual reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Qualitative CVR-ICL Examples</head><p>Section 3.2 only illustrates the mechanism of our CVR-ICL. Here, we explain more details about its implementation. Figures 12 showcases one example of our CVR-ICL on the WinoGAViL <ref type="bibr" target="#b2">(Bitton et al., 2022)</ref>. To accurately calculate similarity scores using the cosine similarity function, we utilize BM25 <ref type="bibr" target="#b36">(Robertson et al., 1995)</ref> for text encoding and BLIP2 multi-embedding <ref type="bibr" target="#b21">(Li et al., 2023)</ref> for multi-modal inputs. As illustrated in Figure <ref type="figure" target="#fig_10">12</ref>, the process begins with encoding both the test and training prompts through multi-modal and textbased encoders. For instance, a test case from WinoGAViL might contain the question "Select two pictures most related to clouds?" along with images of a foggy river, a cloud of sand on a beach, and other related scenes. At the beginning, the multi-modal encoder processes these images as well as the question and generates multimodal-level embeddings. Simultaneously, we convert these images into context-aware image descriptions and translate the entire case into text form. The textbased encoder then generates corresponding textlevel embeddings. Next, we calculate the individual cosine similarity scores in both the multi-modal and text domains. The final similarity score, which determines the most relevant cases, is calculated in a balanced manner as S = S 1 + S 2 . These scores are then sorted, and the top-k most similar cases are selected as in-context learning examples. This dual- encoding and similarity scoring approach ensures that we capture the nuanced relationships between multi-modal inputs and text, thereby enhancing the accuracy and relevance of our in-context learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Comparative Analysis with Fine-tuned Models</head><p>In this section, we explore the impact of fine-tuning strategy on performance in complex visual reasoning tasks. Since some tasks in the complex visual reasoning field are initially designed in the supervised setting, we are curious whether our approach can also perform better with the help of real annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 More Explanation about Our CoC</head><p>The Chain-of-Comparison (CoC) is designed to qualitatively analyze the semantic contribution of context-aware image descriptions against general image captions. It is inspired by the popular idea of Chain-of-Thought, which implements a step-bystep analysis to evaluate effectiveness. Figure <ref type="figure" target="#fig_11">13</ref> shows an example from the Whoops dataset, comparing the semantic gap between a general caption "An airplane prepares to take off" (Option A) and our context-aware image description "An airplane is taking off from a highway in the middle of the Our CoC prompt asks the LLM to analyze the semantic contribution through four steps: Initial Perception, Recognizing Incongruity, Contextual Analysis, and Linking to the Question. This process mimics the human brain's analytical process. We directly ask the LLM to compare the contributions of the two options and determine which is better.</p><p>For instance, in the Initial Perception step, the LLM identifies Option B as superior because it is highly unusual and immediately striking, as airplanes typically do not take off from highways, especially in desert environments. This scenario is much more unusual and striking compared to the routine scenario of Option A, which merely depicts an airplane preparing to take off at an airport. During the Contextual Analysis step, Option B is again favored. The LLM explains that contextually, the scenario raises questions about why an airplane is using a highway in a desert for takeoff, which is not standard practice and could imply unusual circumstances or emergencies. Option A, in contrast, has nothing contextually strange about an airplane preparing for takeoff in a typical airport setting. Finally, in the Linking to the Question step, the LLM determines that Option B provides a clearer connection to the concept of weirdness through its unconventional and striking situation. Option A does not inherently link to weirdness, as it describes a routine occurrence in aviation.</p><p>This example demonstrates how our CoC framework effectively breaks down and evaluates the semantic contributions of different types of image descriptions, highlighting the advantages of context-aware image descriptions in complex vi-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of our CVR-LLM works on the Winoground dataset. Our method transfers images into context-aware image descriptions through CaID and leverages the sophisticated reasoning and ICL abilities of LLMs with the CVR-ICL module, offering a more precise answer. ing contextual understanding and effectiveness. Lv et al. (2023) proposed a counterfactual crossmodality reasoning method for better video moment localization. Zhou et al. (2023a) developed a multi-step reasoning probability transfer mechanism to improve multi-label interaction classifications. Yu et al. (2023) presented a hierarchical reasoning network to consolidate multi-level interactive cues, from coarse to fine-grained details, enhancing Human-Object Interaction (HOI) representations.</figDesc><graphic coords="3,70.87,59.53,430.77,126.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework overview of CaID. It is designed to transfer images into contextualized descriptions, bypassing the need for direct multi-modal fusion and leveraging LLMs' extensive knowledge for more accurate predictions.</figDesc><graphic coords="4,71.69,70.87,213.90,124.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The generic diagram of our proposed CVR-ICL approach. The dual analysis enables our approach to more effectively select contextually relevant examples from text and multi-modal domains.</figDesc><graphic coords="5,94.45,59.54,403.60,198.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two examples from WinoGAViL compare context-aware image descriptions with general image captions. WinoGAViL is designed to ask the model to select the image that best matches the cue word.</figDesc><graphic coords="7,85.87,183.06,185.54,196.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The illustration of how to use GPT4 for stepby-step comparsion.</figDesc><graphic coords="7,315.69,183.07,196.44,124.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hypothesis verification with GPT4, which demonstrates the effectiveness of our CaID against general image captions.</figDesc><graphic coords="7,309.14,349.91,209.52,80.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The different case numbers in CVR-ICL and corresponding performance.</figDesc><graphic coords="8,339.70,48.19,148.42,134.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two qualitative results from Whoops illustrating the capabilities of our approach. Whoops is designed to ask the model to explain what makes images weird.</figDesc><graphic coords="8,317.87,213.99,192.05,197.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The comparison of our CVR-LLM against GPT-4V.</figDesc><graphic coords="9,80.42,422.52,196.43,81.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The detailed illustration of our CaID process on VCR. Best viewed by zooming in.</figDesc><graphic coords="12,310.24,70.87,207.33,315.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The detailed illustration of our CVR-ICL on WinoGAViL. Best viewed by zooming in.</figDesc><graphic coords="13,80.84,70.87,430.84,295.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The detailed illustration of our CoC on Whoops. Best viewed by zooming in.</figDesc><graphic coords="14,137.54,70.87,317.47,422.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,82.14,59.53,430.87,254.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparison of our CVR-LLM with popular VLMs and MM LLMs on five complex visual reasoning tasks. Notably, MLLMs like LLaVA and MiniGPT4 exhibit limitations in handling tasks involving multiple images or computing image-text similarity scores, resulting in their performance being unavailable for tasks like WinoGAViL and Winoground.popular LLMs as inference models for generation tests including: Llama3-8B (Meta, 2024) for CVR-LLM Llama3 , GPT3.5 (OpenAI, 2023) for CVR-LLM GP T 3.5 , and GPT4(Achiam et al., 2023)  for CVR-LLM GP T 4 . Performance comparisons are conducted directly on the test set without any finetuning, as WinoGAViL, Winoground, and NYCC datasets are exclusively for testing purposes.</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell cols="11">WinoGAViL 5/6 10/12 SWOW Text Image Group GPT4 Rate Q-&gt;A QA-&gt;R Match acc. CrowdAcc Winoground Whoops VCR NYCCC</cell></row><row><cell></cell><cell>ViLT (2021)</cell><cell cols="2">55.0 52.0</cell><cell>59.0</cell><cell cols="2">34.7 14.0</cell><cell>9.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">CLIP ViT-L/14 (2021) 47.0 15.0</cell><cell>66.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.6</cell><cell>55.8</cell></row><row><cell>VLM</cell><cell>UNITER (2020) ViLLA (2020)</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell cols="2">38.0 14.0 37.0 13.2</cell><cell>10.5 11.0</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>-48.1</cell><cell>-47.0</cell></row><row><cell></cell><cell>BLIP (2022)</cell><cell cols="2">54.6 45.0</cell><cell>66.5</cell><cell cols="2">46.5 27.7</cell><cell>24.2</cell><cell>22.0</cell><cell>29.2</cell><cell>27.5</cell><cell>58.7</cell><cell>58.1</cell></row><row><cell></cell><cell>BLIP2 (2023)</cell><cell cols="2">49.3 38.8</cell><cell>71.6</cell><cell cols="2">44.0 26.0</cell><cell>23.5</cell><cell>31.0</cell><cell>24.5</cell><cell>25.6</cell><cell>58.3</cell><cell>56.7</cell></row><row><cell></cell><cell>LLaVA 1.0 (2024)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.0</cell><cell>28.3</cell><cell>40.0</cell><cell>55.8</cell><cell>53.1</cell></row><row><cell>MLLM</cell><cell>LLaVA 1.5 (2023a) MiniGPT4 V1 (2023)</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>42.4 44.6</cell><cell>35.1 40.6</cell><cell>44.5 47.7</cell><cell>59.3 58.5</cell><cell>56.0 55.6</cell></row><row><cell></cell><cell>MiniGPT4 V2 (2023)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.2</cell><cell>48.8</cell><cell>49.7</cell><cell>60.4</cell><cell>59.2</cell></row><row><cell></cell><cell>CVR-LLM Llama3</cell><cell cols="2">72.3 70.4</cell><cell>88.7</cell><cell cols="2">45.0 29.5</cell><cell>24.5</cell><cell>60.4</cell><cell>50.5</cell><cell>52.4</cell><cell>59.8</cell><cell>57.7</cell></row><row><cell>VLM+LLM</cell><cell>CVR-LLM GP T 3.5</cell><cell cols="2">73.4 71.6</cell><cell>83.4</cell><cell cols="2">42.7 30.5</cell><cell>23.5</cell><cell>61.2</cell><cell>51.1</cell><cell>53.4</cell><cell>59.4</cell><cell>56.8</cell></row><row><cell></cell><cell>CVR-LLM GP T 4</cell><cell cols="2">74.7 73.2</cell><cell>86.5</cell><cell cols="2">43.5 35.0</cell><cell>26.5</cell><cell>62.0</cell><cell>52.9</cell><cell>54.3</cell><cell>60.6</cell><cell>57.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The performance of using GPT4 to assess the effectiveness of two options (general image caption and our context-aware image description) based on CoC.</figDesc><table><row><cell>Dataset</cell><cell>Category</cell><cell cols="4">RICL (2020) KATE (2021a) MMICL (2023) CVR-ICL</cell></row><row><cell></cell><cell>5/6</cell><cell>64.1</cell><cell>68.6</cell><cell>66.3</cell><cell>69.8</cell></row><row><cell>WinoGAViL</cell><cell>10/12</cell><cell>61.7</cell><cell>64.1</cell><cell>62.8</cell><cell>66.1</cell></row><row><cell></cell><cell>SWOW</cell><cell>80.7</cell><cell>82.8</cell><cell>80.9</cell><cell>80.9</cell></row><row><cell></cell><cell>Text</cell><cell>35.0</cell><cell>29.5</cell><cell>27.5</cell><cell>39.0</cell></row><row><cell>Winoground</cell><cell>Image</cell><cell>22.5</cell><cell>30.0</cell><cell>25.0</cell><cell>29.2</cell></row><row><cell></cell><cell>Group</cell><cell>18.5</cell><cell>20.0</cell><cell>17.5</cell><cell>22.0</cell></row><row><cell>Whoops</cell><cell>GPT4 Rate</cell><cell>60.4</cell><cell>62.0</cell><cell>60.8</cell><cell>62.0</cell></row><row><cell></cell><cell>Q-&gt;A</cell><cell>45.1</cell><cell>48.6</cell><cell>44.0</cell><cell>48.8</cell></row><row><cell>VCR</cell><cell>QA-&gt;R</cell><cell>46.5</cell><cell>48.9</cell><cell>46.3</cell><cell>49.2</cell></row><row><cell></cell><cell>Q-&gt;AR</cell><cell>22.5</cell><cell>24.8</cell><cell>23.6</cell><cell>25.8</cell></row><row><cell></cell><cell>Match acc.</cell><cell>44.4</cell><cell>47.5</cell><cell>45.5</cell><cell>48.0</cell></row><row><cell>NYCCC</cell><cell>CrowdAcc</cell><cell>46.6</cell><cell>46.4</cell><cell>43.7</cell><cell>47.6</cell></row><row><cell></cell><cell>NYAcc</cell><cell>50.3</cell><cell>51.2</cell><cell>49.8</cell><cell>52.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>For the test-only datasets WinoGAViL and Winoground, we randomly divided them into splits of 80% training, 10% validation, and 10% testing. Due to the small number of cases in these tasks, we abandoned training LLMs to avoid catastrophic forgetting. Instead, we chose to fine-tune the captioner using the real labels and incorporated these real annotations into our CVR-ICL examples. Results shown in Table5compare our CVR-LLM's performance in zero-shot and fine-tuned settings against SOTA performances, revealing that our method maintains SOTA performance in several areas.The comparison of our CVR-LLM against SOTA performance under two kinds of settings.</figDesc><table><row><cell>Dataset</cell><cell>Category</cell><cell cols="4">Zero-shot SOTA CVR-LLM SOTA CVR-LLM Finetuned</cell></row><row><cell></cell><cell>5/6</cell><cell>55.0</cell><cell>74.7</cell><cell>54.6</cell><cell>82.8</cell></row><row><cell>WinoGAViL</cell><cell>10/12</cell><cell>52.0</cell><cell>73.2</cell><cell>47.2</cell><cell>80.8</cell></row><row><cell></cell><cell>SWOW</cell><cell>59.0</cell><cell>88.7</cell><cell>68.8</cell><cell>95.9</cell></row><row><cell></cell><cell>Text</cell><cell>46.5</cell><cell>43.5</cell><cell>47.0</cell><cell>55.0</cell></row><row><cell>Winoground</cell><cell>Image</cell><cell>27.7</cell><cell>35.0</cell><cell>42.2</cell><cell>42.5</cell></row><row><cell></cell><cell>Group</cell><cell>24.2</cell><cell>26.5</cell><cell>30.5</cell><cell>35.0</cell></row><row><cell>Whoops</cell><cell cols="2">GPT-4 Rate 31.0</cell><cell>62.0</cell><cell>71.0</cell><cell>72.0</cell></row><row><cell></cell><cell>Q-&gt;A</cell><cell>48.8</cell><cell>52.9</cell><cell>87.4</cell><cell>85.3</cell></row><row><cell>VCR</cell><cell>QA-&gt;R</cell><cell>49.7</cell><cell>54.3</cell><cell>89.6</cell><cell>87.5</cell></row><row><cell></cell><cell>Q-&gt;AR</cell><cell>28.6</cell><cell>30.4</cell><cell>78.6</cell><cell>77.1</cell></row><row><cell></cell><cell>Match acc.</cell><cell>60.4</cell><cell>60.6</cell><cell>84.5</cell><cell>80.9</cell></row><row><cell>NYCCC</cell><cell>CrowdAcc</cell><cell>59.2</cell><cell>57.4</cell><cell>73.3</cell><cell>69.6</cell></row><row><cell></cell><cell>NYAcc</cell><cell>66.5</cell><cell>63.1</cell><cell>68.2</cell><cell>65.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The project is available at: https://CVR-LLM.github.io</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sual reasoning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 The CVR-LLM Performance with LLaMA2</head><p>Table <ref type="table">1</ref> presents the results of our CVR-LLM framework using Llama3, GPT-3.5, and GPT-4 base models. Additionally, we evaluated CVR-LLM on the Llama2-13B model <ref type="bibr">(Touvron et al., 2023b)</ref>, which was also employed in LLaVA <ref type="bibr" target="#b44">(Wu et al., 2023;</ref><ref type="bibr" target="#b24">Liu et al., 2024)</ref>, to ensure a fair comparison. Table <ref type="table">6</ref> compares the performance of CVR-LLM (Llama2-based) and CVR-LLM (Llama3-based) against LLaVA versions 1.0 <ref type="bibr" target="#b24">(Liu et al., 2024)</ref> and 1.5 <ref type="bibr" target="#b44">(Wu et al., 2023)</ref> on complex reasoning tasks. The results demonstrate that while our CVR-LLM performs well on the Llama2 base model, it slightly underperform compared to Llama3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 The Parameter Setting in Equation 4c</head><p>Section 3.2 explains that our in-context learning examples are selected based on a similarity score calculated as follows:</p><p>In this section, we discuss how the parameter  influences the performance of In-Context Learning (ICL). Table <ref type="table">7</ref> presents the results for various values of  on the WinoGAViL dataset. The results indicate that  = 1 leads to the best performance of our CVR-ICL strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Comparison against Other VLM+LLM Methods</head><p>In the main paper, we compare our method with several popular end-to-end MLLMs, including LLaVA <ref type="bibr" target="#b44">(Wu et al., 2023)</ref> and MiniGPT-4 (Zhu   <ref type="bibr">et al., 2023)</ref>. Additionally, we evaluate our approach against VLM+LLM methods such as DD-CoT <ref type="bibr" target="#b49">(Zheng et al., 2023)</ref> and DIEM <ref type="bibr" target="#b16">(Jiang et al., 2024)</ref>. Table <ref type="table">8</ref> presents the comparison results of our CVR-LLM framework versus these methods. While our approach is similar to DIEM in focusing on visual information from images, it demonstrates superior performance in complex visual reasoning tasks. Instead of decomposing the image and extracting information from individual components, we utilize an iterative refinement strategy, enabling the Large Language Model (LLM) to pose more precise questions and extract highly specific, valuable information from the image.</p><p>A.8 The Performance on Multi-step Reasoning Dataset</p><p>Our CVR-LLM framework is designed for complex visual reasoning tasks, making it well-suited for multi-step reasoning datasets, such as Sci-enceQA <ref type="bibr" target="#b28">(Lu et al., 2022)</ref> and M3CoT <ref type="bibr" target="#b7">(Chen et al., 2024)</ref>. In this section, we evaluate the performance of our CVR-LLM on the M3CoT dataset to determine its effectiveness. Table <ref type="table">9</ref> presents a comparison between our CVR-LLM and other Tool-Usage methods. The results show that our approach performs well on questions related to general image content, particularly in areas like physical and social sciences. However, it faces challenges with images containing multiple elements, occasionally leading to hallucinations in detailed descriptions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lama</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florencia</forename><surname>Leoni Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janko</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Altman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Shyamal Anadkat, et al. 2023. Gpt-4 technical report</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Winogavil: Gamified association benchmark to challenge vision-and-language models</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitzan</forename><surname>Bitton Guetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26549" to="26564" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images</title>
		<author>
			<persName><forename type="first">Nitzan</forename><surname>Bitton-Guetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2616" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Minigpt-v2: large language model as a unified interface for vision-language multi-task learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09478</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">M3cot: A novel benchmark for multi-domain multi-step multi-modal chainof-thought</title>
		<author>
			<persName><forename type="first">Qiguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16473</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">2023. April 2023</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6616" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision-language pretraining: Basics, recent advances, and future trends</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="163" to="352" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06293</idno>
		<title level="m">Do androids laugh at electric sheep? humor&quot; understanding&quot; benchmarks from the new yorker caption contest</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diem: Decomposition-integration enhancing multimodal insights</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="27304" to="27313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-spectral image stitching via spatial graph reasoning</title>
		<author>
			<persName><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving zero-shot visual question answering via large language models with reasoning question prompts</title>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weining</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4389" to="4400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<title level="m">Improved baselines with visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What makes good in-context examples for gpt-3? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zheng-Jun Zha, and Qingming Huang. 2023b. Matcr: Modality-aligned thought chain reasoning for multimodal task-oriented dialogue generation</title>
		<author>
			<persName><forename type="first">Yiting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="5776" to="5785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">2021b. Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 36th Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Counterfactual cross-modality reasoning for weakly supervised video moment localization</title>
		<author>
			<persName><forename type="first">Zezhong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6539" to="6547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Llama 3 model card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer of large language model by visually-derived supervision toward low-resource languages</title>
		<author>
			<persName><forename type="first">Masayasu</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishwaranjan</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3637" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Muhammad Saqib</title>
		<author>
			<persName><forename type="first">Humza</forename><surname>Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asad</forename><surname>Ullah Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06435</idno>
	</analytic>
	<monogr>
		<title level="m">Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Introducing chatgpt</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://www.openai.com/research/gpt-3-5" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2023" to="2026" />
		</imprint>
	</monogr>
	<note>OpenAI. 2023. Gpt-3.5: Generative pre-trained transformer 3.5.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Okapi at trec-3. NIST SPECIAL PUBLICA-TION SP</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On second thought, let&apos;s not think step by step! bias and toxicity in zeroshot reasoning</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08061</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Michael Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Glenn</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">Jeffrey</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexia</forename><forename type="middle">Pauline</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11364</idno>
		<title level="m">An information-theoretic approach to prompt engineering without ground truth labels</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Winoground: Probing vision and language models for visio-linguistic compositionality</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Candace</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5238" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothe</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Visual chatgpt: Talking, drawing and editing with visual foundation models</title>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04671</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Against opacity: Explainable ai and large language models for effective digital advertising</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlo</forename><surname>Ongpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Farseev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9299" to="9305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical reasoning network with contrastive learning for few-shot human-object interaction recognition</title>
		<author>
			<persName><forename type="first">Jiale</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4260" to="4268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zefan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzheng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaikai</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07915</idno>
		<title level="m">Mmicl: Empowering vision-language model with multi-modal in-context learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ddcot: Duty-distinct chain-ofthought prompting for multimodal reasoning in language models</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5168" to="5191" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models</title>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weushao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="567" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">2023a. Learning from easy to hard pairs: Multistep reasoning network for human-object interaction detection</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengtang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="4368" to="4377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">2023b. Uncovering the unseen: Discover hidden intentions by micro-behavior graph reasoning</title>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Multimedia</title>
		<meeting>the 31st ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="page" from="6623" to="6633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
