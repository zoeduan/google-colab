<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daking</forename><surname>Rai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">George Mason University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yilun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
							<email>bailinw@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
							<email>ziyuyao@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">George Mason University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">846F96234B978C7365FABDE19641F68C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While large language models (LLMs) have demonstrated strong capability in structured prediction tasks such as semantic parsing, few amounts of research have explored the underlying mechanisms of their success. Our work studies different methods for explaining an LLM-based semantic parser and qualitatively discusses the explained model behaviors, hoping to inspire future research toward better understanding them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic parsing is a task of mapping natural language utterances to their logical forms like SQL queries or lambda expressions for database or knowledge base querying. Despite its structured prediction nature, recent work has shown that a large language model (LLM) which generates output sequentially could achieve comparable or even better performance than the traditional structured decoders <ref type="bibr" target="#b2">(Scholak, Schucher, and Bahdanau 2021)</ref>. However, why these LLMs could do well in semantic parsing is still unclear.</p><p>In this paper, we seek to provide one of the first studies toward explaining LLM-based neural semantic parsers. We use the text-to-SQL semantic parsing task <ref type="bibr" target="#b6">(Yu et al. 2018</ref>) and the UnifiedSKG model <ref type="bibr" target="#b5">(Xie et al. 2022)</ref> for a case study. We empirically explore a set of local explanation methods and quantitatively discussed the explanation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>(1) LIME <ref type="bibr" target="#b1">(Ribeiro, Singh, and Guestrin 2016)</ref> generates an explanation by training locally-faithful interpretable models with the dataset obtained by perturbing the prediction instance. (2) Shapley value measures the importance of a feature by its average marginal contribution to the prediction score. (3) Kernel SHAP <ref type="bibr" target="#b0">(Lundberg and Lee 2017)</ref> is another efficient way of estimating Shapley values by training a linear classifier. (4) LERG (Tuan et al. 2021) is a set of two approaches, LERG L and LERG S, recently adapted from LIME and Shapley value to conditioned sequence generation tasks. When applying these methods to explain an LLM-based semantic parser, we consider each output token 10% 20% 30% 40% 50% Necessity Ratio 0 20 40 60 80 100 120 50% 60% 70% 80% 90% Sufficiency Ratio 2 4 6 8 10 Sufficiency value Random LERG_L LIME Attention Shapley value Kernel SHAP LERG_S Necessity value 1: Necessity (left) and Sufficiency (right) scores when removing or keeping the top-K% important features.</p><p>as one prediction and attribute it to the input features. ( <ref type="formula">5</ref>) Attention: Prior work has revealed that attention may be interpreted as feature importance. Therefore, we also introduce an attention-based local explanation method, where the feature attribution is calculated by averaging the last layer of the multi-headed cross-attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>In our experiments, we consider the task of text-to-SQL semantic parsing where the goal is to generate a SQL query given a natural language question and the database schema (i.e., tables and columns included in the database) as input. We experiment with UnifiedSKG <ref type="bibr" target="#b5">(Xie et al. 2022)</ref>, one of the state-of-the-art models, which adopts a T5 encoderdecoder structure. 1 Following <ref type="bibr" target="#b5">Xie et al. (2022)</ref>, we train and evaluate the parser on the Spider dataset <ref type="bibr" target="#b6">(Yu et al. 2018)</ref>.</p><p>Through the experiments, we seek to answer two Research Questions (RQs): (1) Which local explanation method is the most faithful to explaining the LLM-based UnifiedSKG parser? (2) How well does the explanation align with human intuitions? To answer RQ1, we follow <ref type="bibr" target="#b4">Tuan et al. (2021)</ref> and compare different explanation methods on two metrics: (a) Sufficiency measures the perplexity when keeping only the top-K% most important features by each explanation method; the lower the better/faithful. (b) Necessity measures the perplexity change when the top-K% 1 We used the "T5 base prefix spider with cell value" version from <ref type="url" target="https://github.com/HKUNLP/UnifiedSKG">https://github.com/HKUNLP/UnifiedSKG</ref>. We did not use the T5-3B version because of the large computational demand, which we will discuss in Section . most important features are removed; the higher the better/faithful. To answer RQ2, we qualitatively discuss the most faithful explanation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Faithfulness. The results in Figure <ref type="figure">1</ref> show LERG S has the best performance as per both sufficiency and necessity metrics with Kernel SHAP having comparable performance as well. In general, we observe that Shapley value-based explanation methods have more faithful explanations than other methods. In addition, we also found that attention-based explanations are more faithful than LIME and LERG L.</p><p>Plausibility. Using LERG S as a lens, we qualitatively study how UnifiedSKG works. We define plausible explanations as those which align well with human intuition. In our study, we classify each explanation into plausible or partially plausible ones. Interestingly, we didn't find any explanation that is completely implausible. Under this setup, we investigate the four aspects listed below (Figure <ref type="figure" target="#fig_0">2</ref>): (1) Feature Attribution for (In)correct Predictions: We randomly sample 20 examples where the model makes correct and incorrect predictions, respectively. We find out that in most cases (85% for correct and 70% for incorrect), LERG S generates a plausible explanation for both types.</p><p>(2) Different Hardness Levels: We randomly sample 20 examples for each hardness level -easy, medium, hard, and extra hard, as defined by the Spider benchmark based on the SQL complexity. We observed that in most cases the model behaviors are in line with human intuitions even at the extra hard level (80%; &gt;90% for other levels).</p><p>(3) Compositional Generalization: We seek to understand whether the model attributes the output fragments to correct features compositionally when it makes correct predictions. We conducted a similar manual examination as before and observed that in most (80%) cases our model shows compositionally generalizable feature attribution. (4) In-domain vs. Out-of-domain: As the Spider training and dev sets are split by databases (which could be seen as different domains), we also manually compare the model explanations in in-domain and out-of-domain cases. We observe that for both cases (75% and 80% respectively), the generated explanations were mostly plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Future Directions</head><p>Our study has revealed several challenges and opportunities in explaining an LLM-based semantic parser: (1) Computational costs: Most local explanation methods require model inference over a large set of input perturbations, which is computationally inefficient. Future work may look into improving the attention-based explanation method, which does not rely on perturbations and hence could save much computation.</p><p>(2) Feature interaction: Traditional feature attribution does not provide information about how features (e.g., question tokens and contextual database schema items) interact with each other. Future work may uncover these interactions to gain deeper insights into how the model works.</p><p>(3) Explanation for user understanding: Current saliency maps encompass a lot of information. Future work could examine how to present the information in a concise and friendly way such that users could easily grasp the intuition of the model prediction and verify its correctness. (4) Explanation for debugging: Future work should also investigate how the local explanation results could be used to probe and debug a semantic parser, such as to improve their capability in compositional generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: UnifiedSKG generally shows plausible explanations. In (a), "concert singer" is the database name; "singer : ..." shows the table along with its columns (omitted with ellipsis); similarly for other examples. For (b), the selected column "hp" is incorrect and should be "horsepower". Blue/red indicates positive/negative importance. Darkness indicates strength.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PICARD: Parsing incrementally for constrained auto-regressive decoding from language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Value for N-Person Games</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RAND Corporation</title>
		<imprint>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Local explanation of dialogue response generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pryor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UnifiedSKG: Unifying and multi-tasking structured knowledge grounding with text-to-text language models</title>
		<author>
			<persName><forename type="first">T;</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
