<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-30">30 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<email>yang.zhou2020@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
							<email>pengfei.cao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
							<email>yubo.chen@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-30">30 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">4945EF287A4C81EEAFE5BD723A2A61A6</idno>
					<idno type="arXiv">arXiv:2308.15851v1[cs.MM]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge-based visual question answering is a very challenging and widely concerned task. Previous methods adopt the implicit knowledge in large language models (LLM) to achieve excellent results, but we argue that existing methods may suffer from biasing understanding of the image and insufficient knowledge to solve the problem. In this paper, we propose PROOFREAD -PROmpting vision language model with knOwledge From laRgE lAnguage moDel, a novel, lightweight and efficient knowledge-based VQA framework, which make the vision language model and the large language model cooperate to give full play to their respective strengths and bootstrap each other. In detail, our proposed method uses LLM to obtain knowledge explicitly, uses the vision language model which can see the image to get the knowledge answer, and introduces knowledge perceiver to filter out knowledge that is harmful for getting the correct final answer. Experimental results on two datasets prove the effectiveness of our approach. Our method outperforms all state-of-the-art methods on the A-OKVQA dataset in two settings and also achieves relatively good performance on the OKVQA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Visual question answering (VQA) <ref type="bibr" target="#b1">(Antol et al. 2015)</ref> aims to allow machines to understand images and answer free-form questions by reasoning on given images. However, in real scenarios, it is not enough to answer questions only based on the content of images. Machines also need to know some world knowledge or commonsense knowledge of the objects involved in the image. Motivated by this, knowledgebased VQA <ref type="bibr" target="#b27">(Wang et al. 2018</ref><ref type="bibr" target="#b26">(Wang et al. , 2017;;</ref><ref type="bibr" target="#b15">Marino et al. 2019;</ref><ref type="bibr" target="#b19">Schwenk et al. 2022</ref>) has received increasing interest these years, which focuses on questions that require knowledge to answer. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, to answer the question "Which American president is associated with the stuffed animal seen here?", it does not only need to recognize that the image is about teddy bears but also should know the relationship between "teddy bear" and "George Roosevelt" (George Roosevelt named the teddy bear).</p><p>Traditional methods <ref type="bibr" target="#b5">(Ding et al. 2022;</ref><ref type="bibr" target="#b14">Marino et al. 2021;</ref><ref type="bibr" target="#b31">Yu et al. 2020;</ref><ref type="bibr" target="#b29">Wu et al. 2022;</ref><ref type="bibr" target="#b35">Zhu et al. 2020</ref>) model this problem as a two-stage process, where the first stage retrieves knowledge from an explicit external knowledge resources (e.g., Wikipedia, ConceptNet <ref type="bibr">(Speer, Chin, and</ref> Question: Which American president is associated with the stuffed animal seen here? Knowledge: President Theodore Roosevelt is associated with the creation of the Teddy bear, which was named after him. Havasi 2017)), and the second stage fuses knowledge with images and questions to predict answers. However, due to the incompletion of knowledge bases and the poor generalization of these methods which are limited by the training data scale, it is difficult for them to achieve excellent performance. With the development of large language models (LLMs) <ref type="bibr" target="#b2">(Brown et al. 2020;</ref><ref type="bibr" target="#b16">Ouyang et al. 2022;</ref><ref type="bibr" target="#b7">Hoffmann et al. 2022)</ref>, in the field of natural language processing (NLP), some knowledge-intensive tasks (e.g., commonsense question answering <ref type="bibr" target="#b24">(Talmor et al. 2019)</ref>, open domain question answering <ref type="bibr" target="#b25">(Voorhees and Tice 2000)</ref>, document summarization <ref type="bibr" target="#b20">(See, Liu, and Manning 2017)</ref> and etc.) no longer rely on external knowledge bases and can still achieve human-like performance. Motivated by this, some studies <ref type="bibr" target="#b30">(Yang et al. 2022;</ref><ref type="bibr" target="#b7">Gui et al. 2022;</ref><ref type="bibr" target="#b21">Shao et al. 2023;</ref><ref type="bibr" target="#b9">Hu et al. 2022)</ref> attempt to take advantage of large amount of knowledge stored in the LLMs and its powerful reasoning ability to solve knowledge-based VQA. However, since existing LLMs are basically trained on text, the information in the images cannot be directly understood by text-only LLMs (e.g., <ref type="bibr">GPT-3 (Brown et al. 2020)</ref>, ChatGPT <ref type="bibr" target="#b16">(Ouyang et al. 2022)</ref>). In order to make up for this gap, some methods have been proposed, which can be roughly divided into two categories: prompting-based methods, and multimodal interface-based methods.</p><p>Prompting-based methods: Some methods <ref type="bibr" target="#b30">(Yang et al. 2022;</ref><ref type="bibr" target="#b21">Shao et al. 2023;</ref><ref type="bibr" target="#b9">Hu et al. 2022</ref>) try to provide information about the image in the prompt to the LLM and predict the answer. For example, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(a), PICa <ref type="bibr" target="#b30">(Yang et al. 2022</ref>) prompts a frozen GPT-3 with the caption of the image "Lots of toys on a pink blanket" to provide the information of the image. However, these methods may suffer from biasing understanding of the image, because the caption information of the image is not enough to elicit all the knowledge needed to solve the problem in the LLM. In other words, the limitation of these methods is that the LLM cannot really see the image, and information (captions or answer candidates) is always difficult to exhaust all the key content in the image. Under this condition, it is probably wrong to rely on the caption to prompt LLM to get the final answer. For instance, the caption about the image in Figure <ref type="figure" target="#fig_0">1</ref> is "Lots of toys on a pink blanket", the contribution of which is very limited to help answer the question "Which American president is associated with the stuffed animal seen here?". With the caption as the prompt, it is difficult for LLMs to have an accurate and perfect understanding of image content, and then LLMs will be confused to obtain the answer.</p><p>Multimodal interface-based methods: Some studies <ref type="bibr">(Li et al. 2023a;</ref><ref type="bibr" target="#b17">Peng et al. 2023;</ref><ref type="bibr" target="#b0">Alayrac et al. 2022</ref>) attempt to train a multimodal interface for the LLM so that they can directly see the image to obtain the answer, which is also a very important way in constructing vision language model (VLM). As Figure illustrated in 1(b), this kind of VLM often freezes the image encoder and language model, and maps the image representation to the embedding space of the language model by training a multimodal interface on multimodal data. In this framework, almost all knowledge is stored in the language model, if the language model does not contain the knowledge needed to solve the problem, then the question remains unanswerable. In fact, existing VLMs that be publicly available to access still rely on relatively small-scale language models (e.g., FlanT5 <ref type="bibr" target="#b4">(Chung et al. 2022)</ref>, OPT <ref type="bibr">(Zhang et al. 2022</ref>)), and the amount of knowledge stored in the model is limited. In summary, these methods may suffer from insufficient knowledge to solve the knowledge-based VQA problem. As shown in <ref type="bibr">Figure 1(b)</ref>, asking questions to the VLM directly will be answered incorrectly. But as shown in Figure <ref type="figure" target="#fig_0">1(c)</ref>, with the help of appropriate knowledge, the problem can be solved. And according to our sampling statistics, 43% of the error of BLIP2 <ref type="bibr">(Li et al. 2023a</ref>) on the AOKVQA dataset can be corrected by providing appropriate knowledge. This phenomenon shows that only relying on the implicit knowledge in the current VLM is not enough to answer the question.</p><p>Intuitively, in this paper, we propose PROOFREAD -PROmpting vision language model with knOwledge From laRgE lAnguage moDel, a lightweight and efficient knowledge-based VQA framework. Firstly, to avoid the bi-asing understanding of the image, PROOFREAD adopts a frozen VLM to predict the answers, which can see the image. Secondly, to alleviate the insufficient knowledge of existing VLMs, PROOFREAD leverages an LLM to make up for this gap. Apart from making up for the weakness of the above methods, PROOFREAD also has the following two advantages. Firstly, PROOFREAD bridges LLMs and VLMs without a complex training process (Only 3000 samples were selected to adjust a small number of parameters), during which the parameters of LLM and VLM are frozen. Secondly, PROOFREAD provides a paradigm for efficiently generating knowledge by large models, and proposes Knowledge Perceiver, a novel knowledge filtering mechanism, to minimize the impact of harmful knowledge. Experimental results demonstrate the effectiveness of our proposed method.</p><p>Our contribution can be summarized as follows:</p><p>• We design a novel lightweight framework PROOFREAD to bridge LLM with the vision language model, which can help VLM with the knowledge from LLM, without complex training process. • PROOFREAD can efficiently generate knowledge by LLMs. The proposed knowledge perceiver can effectively filter harmful knowledge. • We conduct extensive experiments on two public opendomain knowledge-based VQA datasets. Experimental results prove the effectiveness of our method. Our code will be publicly available soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Knowledge-based Visual Question Answering. Knowledge-based visual question answering <ref type="bibr" target="#b15">(Marino et al. 2019;</ref><ref type="bibr" target="#b19">Schwenk et al. 2022;</ref><ref type="bibr" target="#b27">Wang et al. 2018</ref><ref type="bibr" target="#b26">Wang et al. , 2017) )</ref> aims to solve questions that cannot be answered based on images alone and requires some knowledge or commonsense. Early works rely heavily on retrieving knowledge from knowledge bases and then training to get answers. Mucko <ref type="bibr" target="#b35">(Zhu et al. 2020)</ref> first proposes to depict an image by a multi-modal heterogeneous graph containing multiple layers of information based on visual, semantic, and knowledge modalities, and then answer questions. KRISP <ref type="bibr" target="#b14">(Marino et al. 2021</ref>) combines implicit knowledge in Transformer and symbolic knowledge in the knowledge bases for the knowledge-based VQA. However, due to the incompleteness of the existing knowledge resources, these methods are not able to achieve decent performance in the open domain knowledge-based VQA <ref type="bibr" target="#b15">(Marino et al. 2019;</ref><ref type="bibr" target="#b19">Schwenk et al. 2022)</ref>. Benefiting from the powerful reasoning ability and abundant knowledge of large language models(e.g., <ref type="bibr">GPT-3 (Brown et al. 2020)</ref>, ChatGPT <ref type="bibr" target="#b16">(Ouyang et al. 2022</ref>)), some studies <ref type="bibr" target="#b0">(Alayrac et al. 2022;</ref><ref type="bibr" target="#b30">Yang et al. 2022;</ref><ref type="bibr" target="#b21">Shao et al. 2023;</ref><ref type="bibr" target="#b7">Gui et al. 2022;</ref><ref type="bibr" target="#b9">Hu et al. 2022)</ref> utilize large language models as part of their methods and achieve excellent performance. PICA <ref type="bibr" target="#b30">(Yang et al. 2022)</ref> converts images in knowledge-based VQA into captions and uses GPT3 and in-context learning to get the final answer. Flamingo <ref type="bibr" target="#b0">(Alayrac et al. 2022)</ref> proposes a vision language model, which freezes the language model to save Answer the questions about the image through the Vision Language Model (M V ). Bottom Right: Knowledge Filter Module. According to the relationship between knowledge and questions, the final answer (e.g., Theodore Roosevelt) is given by Knowledge Perceiver (M f ). all language model capabilities, and trains the multimodal interface on the basis of it so that the model can cope with image input. Prophet <ref type="bibr" target="#b21">(Shao et al. 2023</ref>) uses the VQA model to get candidate answers and then leverages GPT-3 to give the final answer through in-context learning. Our method also exploits the large language model.</p><p>In-Context Learning. The large language model, especially the large language model after instruction tuning, shows a powerful few-shot learning ability, giving some input and output demonstrations to the large language model, it can generate high-quality answers without training. This training-free few-shot learning capability is called in-context learning. And this capability also exists in multimodal large language models trained on multimodal data <ref type="bibr" target="#b0">(Alayrac et al. 2022;</ref><ref type="bibr" target="#b17">Peng et al. 2023;</ref><ref type="bibr" target="#b34">Zhu et al. 2023)</ref>. Some studies <ref type="bibr" target="#b13">(Liu et al. 2022;</ref><ref type="bibr" target="#b28">Wang et al. 2022;</ref><ref type="bibr" target="#b18">Rubin, Herzig, and Berant 2022;</ref><ref type="bibr" target="#b33">Zhang, Feng, and Tan 2022;</ref><ref type="bibr">Li et al. 2023b</ref>) have shown that constructing a suitable example set can help enhance the ability of in-context learning. REINA <ref type="bibr" target="#b28">(Wang et al. 2022)</ref> retrieves samples similar to the test samples on the training set and then performs in-context learning to achieve good performance. <ref type="bibr" target="#b33">Zhang, Feng, and Tan (2022)</ref> propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. UDR <ref type="bibr">(Li et al. 2023b</ref>) proposes a unified model to retrieve demonstrations for a wide range of NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we will introduce the details of PROOF-READ framework for Knowledge-based VQA. Figure <ref type="figure" target="#fig_1">2</ref> schematically visualizes our approach, which includes three major components: (1) Answer Prediction Module, which consists of a vision language model (VLM) for answering questions about the images based on the visual, question, and knowledge; (2) Knowledge Generation Module, which generates knowledge that may be needed to answer questions; (3) Knowledge Filter Module, which aims for filtering knowledge and giving the final answer. Consequently, these three modules will be introduced in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Prediction Module</head><p>Answer Prediction Module adopts a frozen Vision Language Model(M V , VLM) as the base model for Knowledge-based VQA because VLM has a certain ability to understand instructions as well as excellent performance. Formally, define a VQA problem as P = (v, q, a), where v, q, a denotes the visual input, question about the image, and the answer to the question. The original answer for VQA can be obtained by VLM</p><formula xml:id="formula_0">âo = M V (v, q) ∈ R V ,<label>(1)</label></formula><p>where âo = {â o 1 , âo 2 , ..., âo t } is the original answer consisting of t words and R V is the answer space. And the confidence score about the original answer of M V is computed as follows:</p><formula xml:id="formula_1">p o = t i=1 p(â o i |â o 1 , ..., âo i-1 ) ∈ R,<label>(2)</label></formula><p>where p(â o i |â o 1 , ..., âo i-1 ) denotes the prediction probability of the word âo i . Relying on image information alone cannot solve knowledge-based VQA, and some knowledge needs to be acquired at this time. We design a novel Knowledge Generation Module to obtain knowledge, which will be introduced in the next section. Knowledge Generation Module generates a series piece of knowledge K = {K 1 , K 2 , ..., K n } about the problem P . Therefore, our framework combines every piece of knowledge and the question into prompts through manually designed templates and inputs them into VLM to obtain knowledge-based answers,</p><formula xml:id="formula_2">âki = M V (v, E 1 (K i , q)) ∈ R V ,<label>(3)</label></formula><p>where âk i = {â ki 1 , âki 2 , ..., âki t } is the knowledge answer consisting of t words, and E 1 (•, •) is the prompt function to combine question and knowledge. Similarly, the confidence score p ki about the knowledge answer a ki is obtained via Equation <ref type="formula" target="#formula_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Generation Module</head><p>The goal of the Knowledge Generation Module is to generate knowledge for VQA efficiently and correctly. Our framework leverages ChatGPT <ref type="bibr" target="#b16">(Ouyang et al. 2022</ref>), a language model with a large amount of parameters and strong interaction ability, to provide knowledge. However, directly informing ChatGPT questions and letting it generate relevant knowledge will lead to subjective inferences in the generated knowledge. The model often gives the guessed answer to the question first and gives the knowledge related to the answer it thinks, which results in limited knowledge generation. Besides, LLMs may suffer from the biasing understanding of the image, and the answer it gives will also be unreliable, which leads to misleading knowledge generation. Based on this, in order to generate knowledge more effectively, we set a Question Model (M Q ) to generate relevant knowledge questions about VQA Problem P , and then use a Knowledge Answer Model (M K ) to answer these questions. We decouple the generation process into a question-and-answer format. When generating knowledge questions about the problems, the subjective intention of the model in the generation process will be reduced. It is very important to build highquality demonstrations in in-context learning <ref type="bibr" target="#b2">(Brown et al. 2020)</ref> for generating knowledge questions. Our method proposes a new way to select high-quality examples. First, we construct a Demo Bank D = {d i } τ i=1 from the training set for in-context learning, which contains τ samples. Then for new samples during training or testing, demonstrations are selected from the Demo Bank to acquire knowledge.</p><p>PROOFREAD builds the Demo Bank on the training set. We first randomly select a few samples from the training set. Then ask some knowledge questions manually, and get the corresponding knowledge from Knowledge Answer Model M K . And we adopt the Vision Language Model M V to get the original answer and the knowledge answer for these samples, using which as the initialization seed of the Demo Bank.</p><p>For a testing or training sample P = (v, q, a), our framework leverages the encoder of Vision Language Model (M V ) to obtain the representation of the sample</p><formula xml:id="formula_3">r = Encoder M V (v, q) ∈ R d ,<label>(4)</label></formula><p>where d stands for hidden size. According to the representation of the sample, we will find the k most similar samples from the Demo Bank D as the demonstrations D p for generating the knowledge question, which can be formulated as</p><formula xml:id="formula_4">S QE = argT opK i∈{1,2,...,τ } r T r i ||r|| 2 ||r i || 2 ,<label>(5)</label></formula><formula xml:id="formula_5">D p = {(v i , q i , a i )|i ∈ S QE }.</formula><p>(6) Constructing these demonstrations and the current problem into a prompt, we can obtain the knowledge questions generated by Question Model M Q as</p><formula xml:id="formula_6">Q = {Q i } m i=1 = M Q (E 2 ({(C(v i ), q i )|i ∈ S QE }, (C(v), q))), (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where m is the number of generated questions, E 2 (•, •) is the prompt function to combine demonstrations and the current problem, C(•) denotes the caption function to convert the image to caption. Finally, we feed the knowledge questions into Knowledge Answer Model M K to get the knowledge for the problem</p><formula xml:id="formula_8">P K = {K i } n i=1 = M K (E 3 (Q)), (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where n is the number of knowledge, E 3 (•) is the prompt function for ask knowledge questions to M K . It is worth noting that n is not equal to m, because in the prompt, we tell the model answer in points, and each question may generate multiple pieces of knowledge.</p><p>In order to evaluate the quality of automatically generated questions and knowledge on the training set, we categorize the generated knowledge into three classes:</p><p>• Useful Knowledge, which makes up for the lack of knowledge that the model has to solve the problem. • Harmful Knowledge, which can mislead the model into giving wrong answers to otherwise solvable problems • Neutural Knowledge, which has no appreciable impact on the model's ability to solve the problem. Therefore, for a sample, the number of useful knowledge generated can be recorded as u, and the number of harmful knowledge is h</p><formula xml:id="formula_10">u = n i=1 I(â ki = a)I(â o ̸ = a),<label>(9) h</label></formula><formula xml:id="formula_11">= n i=1 I(â ki ̸ = a)I(â o = a),<label>(10)</label></formula><p>where I(•) denotes indicator function, n is the number of knowledge, a ki is the i-th knowledge answer and a o is the original answer, a is the ground truth answer. Traverse each sample on the training set, find similar samples from the Demo Bank to generate knowledge, get the answers to the problem, and calculate the u and h values. For each newly generated sample, the process to update the DEMO Bank is shown in the Algorithm 1. In order to ensure the diversity of samples in the Demo Bank and to retrieve them efficiently, each time a sample whose similarity with the generated sample is greater than a certain threshold λ is found from the Bank, the two are compared, and the excellent samples are retained.</p><p>Algorithm 1: Demo Bank Update Algorithm</p><formula xml:id="formula_12">Input: Current Demo Bank D = {di} n j=1 = {(ri, vi, qi, ai, {K j i } n j=1 , âo i , {â kj i } n j=1 ), hi, ui} t i=1 , similarity threshold λ, new sample s = (r, v, q, a, {K j } n j=1 , âo , {â kj } n j=1 , h, u) 1: for di in D do 2: if r T r i ||r|| 2 ||r i || 2 &gt; λ then 3: if h &lt;</formula><p>hi then 4: remove di from D 5: add s to D 6: else if h = hi and u &gt; ui then 7: remove di from D 8: add s to D 9: end if 10: end if 11: end for Output: New Demo Bank D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Filter Module</head><p>This module aims to filter out useful knowledge from the generated knowledge, which can also be defined as classifying knowledge into three classes (useful, harmful, and neutral), and give the final answer according to the classification of knowledge. Our method leverages XGBoost (Chen and Guestrin 2016), a gradient-boosted decision tree (GBDT), as the Knowledge Perceiver M F to classify the knowledge. For a knowledge-based VQA problem P = (v, q, a) We selected 11 relevant features to classify the knowledge, which is introduced as follows: and knowledge. can be obtained similarly by con = Ent(q, K i ) • Knowledge Confident Important. Among all the knowledge of the problem, K = {K 1 , K 2 , ..., K n }, the importance I C of a certain piece of knowledge K i is the softmax of p ki . from the perspective of knowledge confidence.</p><p>• Knowledge Visual Important. Similarly, among all the knowledge of the problem, the importance I v of a certain piece of knowledge is the softmax of s vi , from the perspective of the similarity between knowledge and image. • Knowledge Caption Important. Among all the knowledge of the problem, the importance I kc of a certain piece of knowledge is the softmax of s ci , from the perspective of the similarity between knowledge and image caption.</p><p>For each piece of knowledge during the test process, the knowledge will be classified by the knowledge perceiver, and finally, the answer with the highest vote of useful knowledge and netural knowledge will be adopted as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Experimental Settings</head><p>Datasets. We choose two public knowledge-based VQA datasets OKVQA <ref type="bibr" target="#b15">(Marino et al. 2019</ref>) and A-OKVQA <ref type="bibr" target="#b19">(Schwenk et al. 2022)</ref> to validate the effectiveness of our method. OKVQA is a large knowledge-based VQA dataset, which contains 14k question-answer pairs and 14k images from the MSCOCO dataset <ref type="bibr" target="#b12">(Lin et al. 2014)</ref>. Questions in the dataset written by annotators during construction are required to contain knowledge. A-OKVQA is currently the largest knowledge-based VQA benchmark, which is an augmented successor of OK-VQA and contains a diverse set of 25k questions requiring a broad base of knowledge to answer. For the two datasets, we randomly sample 3k samples from their training sets as training samples for building Demo Bank and training Knowledge Perceiver.</p><p>Implementation Details. We adopt frozen BLIP-2 (FlanT5XXL) <ref type="bibr">(Li et al. 2023a)</ref> as Vision Language Model to generate the answer and generate the caption of the image. For multiple-choice output, we constrain the model to only generate option words (i.e., abcd). The demonstration number k used when generating knowledge questions is set to 3. When constructing the Demo Bank, our framework traverses the training set twice. The proposed method leverages MPNET <ref type="bibr" target="#b22">(Song et al. 2020)</ref> for representing the text to compute the similarity between questions and knowledge or captions and knowledge. The entailment and contradiction score adopts the entailment model used by <ref type="bibr" target="#b8">Honovich et al. (2021)</ref>. The similarity between the image and text is calculated using EVA-CLIP <ref type="bibr" target="#b6">(Fang et al. 2022)</ref>. All experiments are conducted with NVIDIA GeForce RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with SOTA methods</head><p>As shown in Table <ref type="table" target="#tab_2">1</ref>, our method outperforms other methods on the A-OKVQA data set, whether in the setting of generating the answer directly or giving options to choose the final answer. Especially under the setting of multiple choice, our method can outperform the state-of-the-art (SOTA) methods by a large margin with 3.0% accuracy in the test set. In addition, from the Table <ref type="table" target="#tab_2">1</ref>, we notice:</p><p>(1) Our method stands out among large language modelbased methods, indicating that our method indeed combines Table 2: Overall performance comparison in OKVQA dataset. The upper part is the methods based on the knowledge base, and the lower part is the methods leveraging a large language model.</p><p>the vision language model's capability of understanding images and the knowledge of advanced large models.</p><p>(2) And it is worth mentioning that our method is based on BLIP-2 <ref type="bibr">(Li et al. 2023a)</ref>, our method outperforms BLIP-2 with 6.7% accuracy in the setting of multiple choice and 10.4% accuracy in the setting of direct answer in the test set of A-OKVQA, which validates that our framework can exploit the knowledge.</p><p>(2) Large language model-based approaches are always performing better than conventional methods. This phenomenon illustrates the superiority of large language models in answering knowledge/commonsense questions.</p><p>Similarly, as shown in Table <ref type="table">2</ref> our method also achieves relatively good performance on OKVQA. Although not exceeding all baselines, our framework training is very efficient. Only training on a small number of samples, and freezing the parameters of the vision language model, our method can achieve a similar performance to the baseline af-</p><p>Method Accuracy PROOFREAD 77.8 w/o Demo Bank 76.2 w/o Knowledge Perceiver 71.4 w/o Knowledge 70.2 Table 3: Ablation study of PROOFREAD in A-OKVQA validation set.</p><p>ter multi-modal pre-training or the baseline fully trained on the OKVQA dataset. In addition, we guess that our method does not exceed some baselines because few parameters cannot fully fit the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To verify the effectiveness of each component of PROOF-READ, we conduct ablation studies on A-OKVQA. Here we consider three settings:</p><p>• w/o Demo Bank, which use fixed manual demonstrations to generate knowledge questions. • w/o Knowledge Perveiver, which without filtering, use the answer that the model gives the highest confidence, as the final answer. • w/o Knowledge, which does not use any external knowledge (i.e., BLIP-2). It can be seen from Table <ref type="table">3</ref> that each module is effective, and then we will analyze them one by one.</p><p>Effectiveness of Demo Bank. When adopting fixed manual samples as demonstrations for generating knowledge questions, we can see that the performance has dropped by 1.6% accuracy. It shows that PROOFREAD selects samples from the updateable Demo Bank for in-context learning does improve the quality of generating knowledge questions.</p><p>Effectiveness of Knowledge Perceiver. When replacing the knowledge perceiver with the direct usage of the knowledge answer with the model's highest confidence as the final answer, performance drops dramatically (6.4% accuracy). Our analysis may be because there will be some misleading knowledge during the generating process, which may make the model mistake the original correct answer and be very confident. This can also be regarded as a phenomenon of hallucination of the model. Thus, it is very important to filter out harmful knowledge for knowledge-based VQA.</p><p>Effectiveness of Knowledge. Our framework is equivalent to the Vision Language Model (BLIP-2) if no knowledge is used at all. At this time, the performance dropped by 7.6%, which shows that the Vision Language Model is indeed lacking in knowledge. If the gap can be supplemented by a large language model, even without complex training, it can achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and Analysis</head><p>We also explore the importance of different features for filtering knowledge and the impact of different numbers of training samples on the final performance. For each example, the left side shows the image, the question, the predicted answer without using knowledge, and the predicted final answer using knowledge, while the right side shows the knowledge questions and knowledge generated according to the questions, where U, N, H represent the ground truth label for the knowledge being useful, neutral, or harmful respectively. In classification, we find that if the vision language model has low confidence in the original answer, it is often because it lacks the knowledge to solve the problem. Knowledge confidence important represents the importance of the model to obtain answers through different knowledge. The more important the knowledge, the higher the confidence that can often be obtained.</p><p>Impact of different numbers of training samples. Our method leverages part of the training set to adjust the parameters, and the results obtained with different sample numbers in the A-OKVQA validation set are shown on the right side of Figure <ref type="figure" target="#fig_4">4</ref>. Using only 1k samples, 76.7% accuracy can already be achieved, which illustrates the effectiveness of our framework for data utilization. Intuitively, as the amount of data used rises, the performance of our method also increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>To illustrate the intermediate results of our method more intuitively, we pick some examples to illustrate. Figure <ref type="figure" target="#fig_3">3</ref> shows three cases of our method on the OKVQA and A-OKVQA datasets, as well as the knowledge questions generated and the knowledge obtained (partially). In Example (a) and (b), answering the question directly by the vision language model leads to a wrong answer, due to lacking of knowledge. In Example (a), directly answering the creation time of Pepsi-Cola Company will be incorrectly answered as the creation time of Coca-Cola, probably because of the lack of knowledge of the creation time of Pepsi-Cola. With the blessing of useful knowledge, the vision language model can answer correctly. In Example (a), after getting the useful knowledge of the creation time of Pepsi, our method can answer it correctly. In both Example (a) and Example (b) our method effectively utilizes useful knowledge to get the correct final answer.</p><p>However, our method also has limitations. The introduction of knowledge may also introduce harmful information and turn the originally correct answer into a wrong one. For instance, in Example (c) our method generates some harmful knowledge and adopts the answer generated by the wrong knowledge as the final answer. At this time, it is necessary to effectively filter the knowledge. The previous experiments have verified the effectiveness of our method in filtering knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a novel and efficient framework to combine the vision language model and the large language model for knowledge-based VQA. The proposed method uses a large language model to acquire knowledge and a vision language model to answer questions. To generate highquality knowledge, we propose to use an updatable Demo Bank to pose knowledge questions. To reduce the impact of harmful knowledge, we propose to use Knowledge Perceiver to filter knowledge. In future work, we will explore the possibility of extending PROOFREAD to more knowledgeintensive tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of different approaches based on LLMs. Snowflakes represent that the module is frozen, and flames represent that the module is trainable. (a) An example of prompting-based methods, which convert images to captions fed into the LLM. (b) An example of Multimodal interface-based methods, which train a multimodal interface for large models that can understand image inputs. (c) Part of PROOFREAD (our method), the frozen VLM that leverages knowledge can answer questions correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall framework of PROOFREAD. Top: Knowledge Generation Module. Retrieve k similar examples from the Demo Bank according to the current problem (P ), and generate m questions through the in-context learning of the Question Model (M Q ). Generate n pieces of knowledge through the Knowledge Answer Model (M K ). Bottom Left: Answer Prediction Module. Answer the questions about the image through the Vision Language Model (M V ). Bottom Right: Knowledge Filter Module. According to the relationship between knowledge and questions, the final answer (e.g., Theodore Roosevelt) is given by Knowledge Perceiver (M f ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><figDesc>Original Confident. The confident score p o about the original answer a o from M V . • Knowledge Confident. The confidence score p ki about the knowledge answer a ki from M V . • Confident Gain. The difference between Knowledge Confident and Original Confident, which can be formally defined as p ki -p o . • Text Similarity. The cosine similarity s ti of the representation between the question about the image and a piece of knowledge. • Caption Similarity. Cosine similarity s ci between the image caption representation and the knowledge representation. • Image Similarity. Cosine similarity s vi between the image representation and the knowledge representation. • Entailment. The entailment score e between the question and knowledge. can be obtained by ent = Ent(q, K i ), where Ent(•, •) is the entailment model. • Contradiction. The contradiction between the question</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Three cases of PROOFREAD on OKQVA dataset and A-OKVQA dataset. For each example, the left side shows the image, the question, the predicted answer without using knowledge, and the predicted final answer using knowledge, while the right side shows the knowledge questions and knowledge generated according to the questions, where U, N, H represent the ground truth label for the knowledge being useful, neutral, or harmful respectively.</figDesc><graphic coords="7,63.18,56.85,80.01,66.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: The importance of different features for knowledge perceiver in A-OKVQA validation set. The cover rate represents the number of samples that different features can cover in a gradient-boosted decision tree (GBDT), which represents the importance of this feature for knowledge classification. Right: The effect of using different numbers of training samples for the final result in the A-OKVQA val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Overall performance (accuracy) comparison in A-OKVQA dataset.</figDesc><table><row><cell>Method</cell><cell cols="4">Direct Answer Multiple Choice val test val test</cell></row><row><cell cols="4">conventional methods</cell><cell></cell></row><row><cell>Pythia</cell><cell>25.2</cell><cell>21.9</cell><cell>49.0</cell><cell>40.1</cell></row><row><cell>ViLBERT</cell><cell>30.6</cell><cell>25.9</cell><cell>49.1</cell><cell>41.5</cell></row><row><cell>LXMERT</cell><cell>30.7</cell><cell>25.9</cell><cell>51.4</cell><cell>41.6</cell></row><row><cell>KRISP</cell><cell>33.7</cell><cell>27.1</cell><cell>51.9</cell><cell>42.2</cell></row><row><cell>GPV-2</cell><cell>48.6</cell><cell>40.7</cell><cell>60.3</cell><cell>53.7</cell></row><row><cell>BLIP-2</cell><cell>53.2</cell><cell>49.7</cell><cell>70.2</cell><cell>69.4</cell></row><row><cell cols="4">large language model-based approaches</cell><cell></cell></row><row><cell>ClipCap</cell><cell>18.1</cell><cell>15.8</cell><cell>44.0</cell><cell>43.8</cell></row><row><cell>Phrophet</cell><cell>58.2</cell><cell>55.7</cell><cell>76.4</cell><cell>73.6</cell></row><row><cell>PromptCap</cell><cell>56.3</cell><cell>59.6</cell><cell>73.2</cell><cell>73.1</cell></row><row><cell cols="2">PROOFREAD 62.2</cell><cell>60.2</cell><cell>77.8</cell><cell>76.1</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell></row><row><cell cols="4">conventional methods</cell><cell></cell></row><row><cell>Mucko</cell><cell></cell><cell></cell><cell></cell><cell>29.2</cell></row><row><cell>ConceptBERT</cell><cell></cell><cell></cell><cell></cell><cell>33.7</cell></row><row><cell>KRISP</cell><cell></cell><cell></cell><cell></cell><cell>38.9</cell></row><row><cell>MAVEx</cell><cell></cell><cell></cell><cell></cell><cell>40.3</cell></row><row><cell>TRiG</cell><cell></cell><cell></cell><cell></cell><cell>49.4</cell></row><row><cell>BLIP-2</cell><cell></cell><cell></cell><cell></cell><cell>45.9</cell></row><row><cell cols="4">large language model-based approaches</cell><cell></cell></row><row><cell>PICA</cell><cell></cell><cell></cell><cell></cell><cell>48.0</cell></row><row><cell>KAT</cell><cell></cell><cell></cell><cell></cell><cell>53.1</cell></row><row><cell>Flamingo-80B</cell><cell></cell><cell></cell><cell></cell><cell>57.8</cell></row><row><cell>Phrophet</cell><cell></cell><cell></cell><cell></cell><cell>61.1</cell></row><row><cell>PROOFREAD</cell><cell></cell><cell></cell><cell></cell><cell>57.6</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flamingo: a Visual Language Model for Few-Shot Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scaling Instruction-Finetuned Language Models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18">2022. June 18-24, 2022</date>
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno>CoRR, abs/2211.07636</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KAT: A Knowledge Augmented Transformer for Vision-and-Language</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V M</forename><surname>Ruíz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="956" to="968" />
		</imprint>
	</monogr>
	<note>and Sifre, L. 2022. Training Compute-Optimal Large Language Models. CoRR, abs/2203.15556</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">$Qˆ2$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</title>
		<author>
			<persName><forename type="first">O</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="7856" to="7870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">PromptCap: Prompt-Guided Task-Aware Image Captioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno>CoRR, abs/2211.09699</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>CoRR, abs/2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unified Demonstration Retriever for In-Context Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4644" to="4668" />
		</imprint>
	</monogr>
	<note>ACL 2023 Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06">2014. September 6-12, 2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What Makes Good In-Context Examples for GPT-3?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Vulic</surname></persName>
		</editor>
		<meeting>Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022<address><addrLine>Dublin, Ireland and Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-27">2022. May 27, 2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19">2021. June 19-25, 2021</date>
			<biblScope unit="page" from="14111" to="14121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16">2019. 2019. June 16-20, 2019</date>
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kosmos-2: Grounding Multimodal Large Language Models to the World</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2306.14824</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning To Retrieve Prompts for In-Context Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V M</forename><surname>Ruíz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="2655" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A-OKVQA: A Benchmark for Visual Question Answering Using World Knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022 -17th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-23">2022. October 23-27, 2022</date>
			<biblScope unit="volume">13668</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30">2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2023. June 18-24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MP-Net: Masked and Permuted Pre-training for Language Understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>December 6-12, 2020, virtual</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Com-monsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The TREC-8 Question Answering Track</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Language Resources and Evaluation, LREC 2000</title>
		<meeting>the Second International Conference on Language Resources and Evaluation, LREC 2000<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2000-05">2000. May -June 2, 2000</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explicit Knowledge-based Reasoning for Visual Question Answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Hen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Sierra</surname></persName>
		</editor>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. 2017. August 19-25, 2017</date>
			<biblScope unit="page" from="1290" to="1296" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FVQA: Fact-Based Visual Question Answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2413" to="2427" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3170" to="3179" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-Modal Answer Validation for Knowledge-Based VQA</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="3081" to="3089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Multimodal Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM &apos;20: The 28th ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-12">2020. October 12-16, 2020</date>
			<biblScope unit="page" from="3743" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">OPT: Open Pre-trained Transformer Language Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active Example Selection for In-Context Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates, De</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9134" to="9148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno>CoRR, abs/2304.10592</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1097" to="1103" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
