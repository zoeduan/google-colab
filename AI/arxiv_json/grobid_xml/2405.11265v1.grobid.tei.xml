<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EnviroExam: Benchmarking Environmental Science Knowledge of Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-18">18 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanqian</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongfang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Environment</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EnviroExam: Benchmarking Environmental Science Knowledge of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-18">18 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F0BA6CE475087EFF41ED8F749E1E8330</idno>
					<idno type="arXiv">arXiv:2405.11265v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Solid Waste Treatment And Resource Utilization Environmental Decision Support System Environmental Biotechnology ••• Atmospheric Pollution Control Engineering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the field of environmental science, it is crucial to have robust evaluation metrics for large language models to ensure their efficacy and accuracy. We propose EnviroExam, a comprehensive evaluation method designed to assess the knowledge of large language models in the field of environmental science. EnviroExam is based on the curricula of top international universities, covering undergraduate, master's, and doctoral courses, and includes 936 questions across 42 core courses. By conducting 0-shot and 5-shot tests on 31 open-source large language models, EnviroExam reveals the performance differences among these models in the domain of environmental science and provides detailed evaluation standards. The results show that 61.3% of the models passed the 5-shot tests, while 48.39% passed the 0shot tests. By introducing the coefficient of variation as an indicator, we evaluate the performance of mainstream open-source large language models in environmental science from multiple perspectives, providing effective criteria for selecting and fine-tuning language models in this field. Future research will involve constructing more domain-specific test sets using specialized environmental science textbooks to further enhance the accuracy and specificity of the evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the invention of the Transformer architecture in 2017 <ref type="bibr" target="#b18">[19]</ref>, the development of large language models has accelerated. In this context, the emergence of both closed-source models like ChatGPT <ref type="bibr" target="#b13">[14]</ref> and Claude <ref type="bibr" target="#b1">[2]</ref>, and open-source models like Llama <ref type="bibr" target="#b17">[18]</ref> and Qwen <ref type="bibr" target="#b2">[3]</ref>, has addressed the challenge of extracting valuable information from vast knowledge bases and, in some cases, surpassed human intelligence. To effectively leverage large language models in environmental science, establishing evaluation standards has become essential <ref type="bibr">[20; 1; 4]</ref>.</p><p>Researchers have established knowledge assessment tests for general large language models, with notable test sets including Ceval <ref type="bibr" target="#b10">[11]</ref>, MMLU <ref type="bibr" target="#b9">[10]</ref>, and HumanEval <ref type="bibr" target="#b5">[6]</ref>. These tests evaluate model capabilities across various disciplines and rank them, providing a clear understanding of their strengths and weaknesses for developers and users. However, due to the strong specialization and unique characteristics of vertical fields, these general tests often fail to cover the content of environmental science, thereby limiting the development of specialized language models in this domain.</p><p>The establishment of evaluation standards for vertical domain language models, such as Lawbench <ref type="bibr">[9; 13]</ref> and Medbench <ref type="bibr" target="#b4">[5]</ref>, demonstrates the feasibility and potential of creating such standards in specific fields. Lawbench evaluates 52 mainstream models from the perspectives of memory, understanding, and application, achieving notable results. Medbench, through a comprehensive medical competency test set, has conducted thorough evaluations of 113 models, yielding highly credible outcomes. These</p><p>Collecting undergraduate, master's, and doctoral training programs Remove redundant data and generate a core lesson list Generate exam drafts using the most powerful AIs Prompt Manual post-editing and proofreading </p><formula xml:id="formula_0">M = 1 n n i=1 s i</formula><p>Where s i is the score of a large language model on a specific test, and n is the total number of tests.</p><p>2. Calculate the standard deviation: Compute the standard deviation of all test scores relative to the mean:</p><formula xml:id="formula_1">σ = 1 n n i=1</formula><p>(s i -1) <ref type="foot" target="#foot_1">2</ref>3. Calculate the coefficient of variation (CV): The coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the relative dispersion of the scores:</p><p>CV = σ M 4. Calculate the original composite index I: The original composite index I is defined as a function of the mean score and the coefficient of variation:</p><formula xml:id="formula_2">I = M × (1 -CV), 0 ≤ CV ≤ 1 model void * 2 , CV &gt; 1</formula><p>Models We used 31 different open-source large language models with varying parameter sizes and sources for this generation. The diversity of these large language models allows for more effective testing of their performance in the field of environmental science. The parameter sizes are shown in the Table1, and detailed descriptions of the models can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Experiment Setting We used OpenCompass<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr" target="#b7">[8]</ref> (ver 2.1.0) for model evaluation and configured the 31 large language models with the following parameters: max_out_len=100, max_seq_len=4096, temperature=0.7, and top_p=0.95. We tested all models using both 0-shot and 5-shot methods.</p><p>5-shot score In the 5-shot experiment, we first provide the model with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Result</head><p>General comparison Based on the overall results of the 5-shot and 0-shot tests, the existing opensource large language models demonstrate relatively good proficiency in the field of environmental science. In the 5-shot tests, where a passing score is considered 60, 19 models passed while 12 models failed, resulting in a pass rate of 61.3%. In the 0-shot tests, 15 models passed and 16 models failed, yielding a pass rate of 48.39%. The highest scoring model in the 5-shot tests was DeepSeek-67B-Chat, with a score of 84.94. In the 0-shot tests, the highest scoring model was Llama-3-70B-Instruct, with a score of 80.47. Does few-shot prompting provide assistance? Based on the summarized score results above, we can generate the following radar charts for 5-shot and 0-shot tests. From this radar chart, we can observe that nearly one-third of the large language models did not show an improvement in scores   with additional prompts; on the contrary, their scores decreased. This may be because the models have sacrificed few-shot capabilities to enhance their 0-shot performance.</p><p>Does chain-of-thought prompting help? For most large language models, the use of Chainof-Thought (COT) prompting is effective and positive. The most significant positive effects were observed in vicuna-13b-v1.5, deepseek-67b-chat, and llama-3-8b-instruct, with scores increasing by 42.12, 41.77, and 33.82 points, respectively. However, for other large language models, the introduction of COT led to a surge in error rates when handling questions. The most significant negative impacts were seen in baichuan-13b-chat, chatglm2-6b, and gemma-7b, with scores decreasing by 59.69, 42.83, and 27.9 points, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This study establishes a multi-source evaluation standard for large language models in the field of environmental science. By incorporating the coefficient of variation as an indicator, it evaluates the performance of mainstream open-source large language models in the domain of environmental science from multiple sources. This can provide an effective set of selection criteria for choosing large language models for fine-tuning in the field of environmental science. Future research will follow the phi-3 approach, using specialized textbooks in environmental science to construct a more domain-specific dataset with a distinct environmental science focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Most of our data drafts come from GPT-4 and Claude. Given that existing large language models have been extensively trained on vast amounts of internet data, there is a risk of test data leakage. These models may have been trained on similar task formats or even the exact test data, resulting in exceptionally high scores <ref type="bibr" target="#b16">[17]</ref>. In the future, we will seek more principled methods to prevent data contamination.  DeepSeek-7B-Chat and DeepSeek-67B-Chat are advanced language models from the DeepSeek LLM series. They have been trained from scratch on a massive dataset comprising 2 trillion English and Chinese tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A List of</head><p>Gemma-7B and Gemma-2B-it are lightweight, state-of-the-art open models introduced by Google in the Gemma series. The Gemma models are well-suited for various text generation tasks, including Q&amp;A, summarization, and reasoning. They are built using the same research and technology used to create the Gemini models. Their relatively small size allows users to deploy these models in resource-limited environments.</p><p>InternLM-Chat-20B and InternLM-Chat-7B are large language models from the InternLM series. InternLM2-Chat-20B and InternLM2-Chat-7B are models from the InternLM2 series. These models are chat-oriented and tailored for practical scenarios. Compared to the InternLM series, the InternLM2 series models significantly outperform their predecessors in all aspects, particularly in reasoning, mathematics, coding, chat experience, instruction following, and creative writing, leading in performance among similarly sized open-source models, and supporting a wider range of intelligent agents and complex task multi-step tool calls.</p><p>Mistral-7B-Instruct-v0.1 and Mixtral-8x7B-Instruct-v0.1 are large language models from the Mistral series. They outperform the Llama 2 13B model in most benchmarks, approach CodeLlama 7B performance in coding tasks, while maintaining good performance in English tasks, and use Grouped Query Attention (GQA) to speed up inference.</p><p>Qwen1.5-14B-Chat and Qwen1.5-7B-Chat are test versions of large language models from the Qwen2 series. Qwen-14B-Chat and Qwen-7B-Chat are large language models from the Qwen series. These models are Transformer-based and pre-trained on large datasets. Compared to the Qwen series, the Qwen2 series models can stably support a 32K context length, support basic and chat models in multiple languages, and excel in language understanding, reasoning, and mathematics.</p><p>Vicuna-13B-v1.5, Vicuna-7B-v1.5, and Vicuna-7B-v1.5-16K are large language models from the Vicuna series, fine-tuned on user-shared conversations collected from ShareGPT. These models are primarily used for research on large language models and chatbots.</p><p>BlueLM-7B-Chat is a large-scale pre-trained language model independently developed by the Vivo AI Global Research Institute, open-sourcing a long-text base model and dialogue model supporting 32K. It was trained on a high-quality corpus with a scale of 26 trillion tokens, including Chinese, English, and a small amount of Japanese and Korean data, achieving leading results on C-Eval and CMMLU.</p><p>TigerBot-13B-Chat-V2 and TigerBot-7B-Chat-V3 are large language models from the TigerBot series. Developed on the basis of Llama-2 and BLOOM, the TigerBot series models further push the boundaries in data, training algorithms, infrastructure, and application tools. Compared to SOTA open-source models like Llama-2, our models achieve significant performance improvements, especially with a 6% increase in English and a 20% increase in Chinese. Additionally, the TigerBot model series has achieved leading performance on major academic and industrial benchmarks and leaderboards.</p><p>Llama-2-70B-Chat and Llama-2-13B-Chat are large language models from the Llama2 series developed by Meta. Llama-3-8B-Instruct and Llama-3-70B-Instruct are large language models from the Llama3 series developed by Meta. These models are optimized for dialogue use cases, outperforming many available open-source chat models on common industry benchmarks and optimized for usability and safety.</p><p>Skywork-13B is a large language model from the Skywork series. It is pre-trained on a high-quality, clean-filtered dataset of 3.2 trillion multilingual (mainly Chinese and English) and code data, with English accounting for 52.2%, Chinese for 39.6%, and code for 8%. It demonstrates the best performance among models of the same scale in various evaluations and benchmarks while ensuring strong capabilities in both Chinese and English and code proficiency.</p><p>Yi-6B-Chat is a large language model from the Yi series. Targeting bilingual language models and a 3T multilingual corpus, the Yi series models have become one of the strongest LLM models globally, showing great prospects in language understanding, common sense reasoning, and reading comprehension.</p><p>E Details of 5-shot and 0-shot evaluations for 31 LLMs</p><p>Dataset baichuan 2-13bchat baichuan -13bchat chatglm2 -6b chatglm3 -6b chatglm3 -6b-32k deepseek -7b-chat deepseek -67bchat gemma-7b gemma-2b-it internlm 2-chat-20b internlm 2-chat-7b internlm -chat-20b internlm -chat-7b mistral-7binstruct-v0.1 mixtral-8x7binstruct-v0.1 qwen1.5-14b-chat qwen1.5-7b-chat qwen-14b-chat qwen-7b-chat vicuna-13b-v1.5 vicuna-7b-v1.5 vicuna-7b-v1.5-16k bluelm-7b-chat tigerbot-13bchat-v2 tigerbot-7b-chat-v3 llama-3-8binstruct llama-3-70binstruct llama-2-70b-chat llama-2-13b-chat skywork-13b yi-6bchat environmental_science 94.44 22.22 5.56 77.78 88.89 88.89 88.89 33.33 72.22 94.44 100.00 94.44 88.89 83.33 88.89 94.44 94.44 94.44 88.89 77.78 22.22 61.11 88.89 77.78 66.67 94.44 94.44 88.89 94.44 27.78 88.89 environmental_fluid_mechanics 83.33 16.67 33.33 66.67 100.00 83.33 83.33 50.00 66.67 100.00 83.33 83.33 50.00 50.00 100.00 83.33 83.33 100.00 66.67 83.33 50.00 66.67 100.00 83.33 83.33 83.33 100.00 66.67 50.00 33.33 83.33 environmental_analytical_chemistry 100.00 16.67 16.67 66.67 66.67 83.33 83.33 66.67 66.67 66.67 100.00 83.33 50.00 0.00 100.00 83.33 83.33 66.67 83.33 50.00 16.67 100.00 100.00 50.00 33.33 66.67 100.00 50.00 66.67 50.00 66.67 principles_and_applications_of_ecology 100.00 28.57 35.71 85.71 85.71 92.86 100.00 64.29 50.00 100.00 100.00 100.00 85.71 85.71 85.71 85.71 92.86 92.86 100.00 92.86 71.43 92.86 92.86 85.71 92.86 100.00 100.00 100.00 85.71 50.00 100.00 pollution_control_microbiology 83.33 27.78 16.67 61.11 83.33 72.22 88.89 50.00 33.33 83.33 88.89 66.67 66.67 44.44 77.78 83.33 100.00 88.89 83.33 72.22 5.56 55.56 66.67 77.78 55.56 72.22 94.44 66.67 33.33 5.56 88.89 atmospheric_pollution_control_engineering 85.71 23.81 4.76 71.43 85.71 61.90 90.48 76.19 42.86 95.24 90.48 90.48 76.19 61.90 90.48 90.48 76.19 95.24 80.95 57.14 0.00 52.38 76.19 80.95 66.67 85.71 90.48 76.19 52.38 9.52 80.95 environmental_monitoring 100.00 4.55 0.00 95.45 90.91 81.82 100.00 22.73 45.45 95.45 95.45 95.45 100.00 68.18 95.45 95.45 95.45 95.45 90.91 86.36 27.27 59.09 95.45 86.36 40.91 95.45 95.45 90.91 40.91 36.36 100.00 environmental_impact_assessment 83.33 12.50 0.00 70.83 58.33 54.17 91.67 29.17 66.67 75.00 83.33 70.83 75.00 75.00 83.33 87.50 83.33 75.00 75.00 58.33 16.67 54.17 70.83 54.17 20.83 75.00 95.83 70.83 50.00 4.17 83.33 urban_water_supply_and_drainage_system_engineering 77.78 11.11 11.11 88.89 77.78 77.78 88.89 77.78 44.44 88.89 88.89 77.78 88.89 55.56 88.89 66.67 100.00 77.78 100.00 66.67 66.67 66.67 66.67 0.00 22.22 100.00 88.89 88.89 77.78 66.67 77.78 environmental_soil_science 55.56 11.11 22.22 100.00 88.89 88.89 88.89 66.67 44.44 100.00 100.00 100.00 100.00 66.67 88.89 100.00 100.00 100.00 100.00 55.56 55.56 77.78 100.00 66.67 44.44 100.00 88.89 88.89 77.78 55.56 77.78 environmental_economics 78.95 10.53 5.26 73.68 73.68 68.42 89.47 31.58 52.63 89.47 84.21 84.21 78.95 63.16 89.47 94.74 84.21 84.21 73.68 68.42 26.32 52.63 78.95 63.16 36.84 78.95 94.74 73.68 68.42 31.58 84.21 environmental_decision_support_system 88.89 38.89 16.67 94.44 100.00 100.00 94.44 88.89 83.33 94.44 100.00 94.44 94.44 100.00 100.00 100.00 100.00 100.00 100.00 100.00 27.78 88.89 94.44 94.44 77.78 100.00 94.44 94.44 100.00 55.56 100.00 solid_waste_treatment_and_resource_utilization 83.33 0.00 5.56 72.22 88.89 83.33 94.44 11.11 44.44 88.89 88.89 83.33 88.89 83.33 94.44 83.33 88.89 94.44 88.89 61.11 22.22 77.78 77.78 83.33 61.11 88.89 100.00 94.44 61.11 11.11 94.44 water_pollution_control_engineering 75.00 12.50 12.50 87.50 87.50 87.50 100.00 25.00 62.50 75.00 75.00 100.00 75.00 62.50 87.50 87.50 75.00 87.50 87.50 87.50 25.00 62.50 87.50 75.00 75.00 100.00 87.50 75.00 50.00 25.00 100.00 environmental_planning_and_management 90.00 15.00 25.00 85.00 80.00 90.00 80.00 55.00 75.00 80.00 80.00 90.00 90.00 60.00 85.00 85.00 90.00 90.00 75.00 80.00 45.00 60.00 65.00 60.00 60.00 85.00 80.00 75.00 60.00 30.00 85.00 indoor_environmental_pollution_and_control 95.65 13.04 34.78 86.96 95.65 91.30 100.00 73.91 73.91 82.61 95.65 91.30 95.65 86.96 86.96 95.65 95.65 95.65 91.30 86.96 52.17 69.57 95.65 78.26 73.91 95.65 91.30 95.65 82.61 47.83 95.65 environmental_statistics 89.47 10.53 15.79 84.21 89.47 68.42 100.00 63.16 42.11 94.74 94.74 84.21 94.74 89.47 100.00 89.47 84.21 78.95 84.21 63.16 26.32 68.42 78.95 73.68 68.42 89.47 100.00 89.47 63.16 15.79 78.95 industrial_ecology 86.36 13.64 27.27 95.45 90.91 72.73 86.36 27.27 68.18 90.91 90.91 86.36 86.36 81.82 95.45 90.91 90.91 86.36 95.45 72.73 18.18 63.64 90.91 68.18 36.36 90.91 90.91 86.36 68.18 27.27 90.91 environmental_physical_pollution_and_control 80.00 45.00 30.00 85.00 85.00 90.00 100.00 60.00 45.00 100.00 90.00 85.00 85.00 65.00 80.00 95.00 95.00 95.00 90.00 65.00 30.00 70.00 65.00 60.00 70.00 85.00 100.00 95.00 55.00 25.00 85.00 environmental_nanotechnology 100.00 42.86 0.00 92.86 85.71 85.71 100.00 57.14 92.86 100.00 92.86 92.86 92.86 92.86 100.00 92.86 100.00 92.86 92.86 78.57 50.00 78.57 92.86 71.43 78.57 100.00 92.86 92.86 92.86 21.43 100.00 specialty_wastewater_treatment 22.22 44.44 0.00 33.33 33.33 55.56 66.67 77.78 22.22 88.89 66.67 66.67 55.56 33.33 88.89 55.56 77.78 55.56 44.44 33.33 44.44 44.44 22.22 11.11 22.22 88.89 77.78 44.44 33.33 55.56 66.67 modern_biology 88.89 27.78 11.11 72.22 77.78 83.33 94.44 50.00 55.56 83.33 100.00 83.33 83.33 66.67 88.89 94.44 88.89 83.33 77.78 66.67 44.44 55.56 88.89 72.22 66.67 88.89 100.00 77.78 66.67 33.33 88.89 theory_and_technology_of_environmental_pollution_prevention_and_control 93.75 37.50 12.50 81.25 100.00 81.25 100.00 62.50 43.75 93.75 100.00 100.00 100.00 93.75 100.00 100.00 93.75 100.00 93.75 81.25 81.25 68.75 93.75 68.75 50.00 100.00 100.00 81.25 68.75 75.00 100.00 reaction_kinetics_and_reactor_design 60.00 30.00 0.00 70.00 90.00 70.00 100.00 10.00 100.00 80.00 70.00 100.00 90.00 80.00 100.00 60.00 80.00 60.00 70.00 80.00 10.00 80.00 90.00 30.00 40.00 70.00 80.00 50.00 50.00 0.00 90.00 engineering_ethics_and_environmental_ethics 100.00 56.52 56.52 86.96 95.65 82.61 95.65 56.52 73.91 100.00 100.00 86.96 95.65 86.96 95.65 95.65 86.96 91.30 91.30 86.96 39.13 82.61 78.26 91.30 60.87 95.65 95.65 95.65 95.65 21.74 91.30 sludge_treatment_disposal_and_resource_utilization 92.86 14.29 7.14 78.57 92.86 28.57 92.86 64.29 64.29 85.71 85.71 85.71 85.71 92.86 100.00 78.57 85.71 85.71 78.57 78.57 14.29 71.43 64.29 28.57 28.57 100.00 78.57 92.86 42.86 21.43 85.71 safe_disposal_and_resource_recovery_of_hazardous_waste 88.89 50.00 11.11 77.78 94.44 88.89 100.00 66.67 77.78 94.44 100.00 94.44 94.44 94.44 100.00 94.44 94.44 100.00 88.89 88.89 44.44 88.89 94.44 88.89 72.22 100.00 100.00 83.33 83.33 22.22 94.44 theory_and_technology_of_air_pollution_prevention_and_control 88.24 0.00 5.88 94.12 88.24 82.35 94.12 70.59 64.71 100.00 88.24 88.24 88.24 76.47 88.24 88.24 88.24 82.35 82.35 64.71 17.65 70.59 82.35 70.59 41.18 94.12 100.00 76.47 76.47 17.65 94.12 environmental_geographic_information_system 91.67 25.00 33.33 79.17 91.67 91.67 95.83 58.33 75.00 91.67 100.00 91.67 87.50 91.67 95.83 91.67 91.67 91.67 95.83 79.17 20.83 83.33 79.17 66.67 54.17 95.83 95.83 87.50 87.50 8.33 87.50 clean_production_and_energy_conservation_and_emissions_reduction 64.71 35.29 17.65 76.47 76.47 64.71 94.12 58.82 35.29 88.24 88.24 100.00 100.00 64.71 82.35 100.00 88.24 82.35 82.35 70.59 47.06 52.94 82.35 52.94 58.82 82.35 94.12 82.35 52.94 23.53 94.12 circulate_economy_and_industrial_ecology_methodologies 90.91 18.18 31.82 81.82 100.00 90.91 95.45 40.91 63.64 100.00 90.91 90.91 81.82 77.27 86.36 95.45 90.91 90.91 86.36 81.82 22.73 72.73 81.82 63.64 36.36 90.91 95.45 95.45 77.27 31.82 86.36 atmospheric_particulate_matter_detection_and_analysis_techniques 95.00 5.00 5.00 80.00 90.00 80.00 90.00 70.00 65.00 90.00 95.00 85.00 90.00 70.00 85.00 100.00 95.00 95.00 85.00 80.00 20.00 75.00 90.00 90.00 70.00 90.00 95.00 80.00 70.00 25.00 100.00 environmental_electrochemical_theory_and_technology 53.85 7.69 7.69 69.23 69.23 76.92 76.92 46.15 46.15 61.54 69.23 76.92 61.54 53.85 69.23 69.23 69.23 76.92 53.85 53.85 15.38 53.85 76.92 30.77 30.77 61.54 84.62 53.85 38.46 23.08 61.54 environmental_microwave_chemistry_technology 78.57 14.29 28.57 71.43 57.14 71.43 85.71 50.00 50.00 64.29 71.43 71.43 50.00 28.57 78.57 85.71 78.57 71.43 64.29 57.14 50.00 42.86 64.29 64.29 71.43 71.43 64.29 64.29 50.00 42.86 64.29 advanced_environmental_chemistry 95.00 15.00 15.00 90.00 100.00 85.00 100.00 30.00 75.00 85.00 95.00 90.00 90.00 60.00 95.00 95.00 95.00 95.00 90.00 90.00 35.00 65.00 80.00 85.00 65.00 95.00 90.00 90.00 80.00 40.00 90.00 mass_transfer_processes 56.25 25.00 25.00 75.00 68.75 62.50 87.50 68.75 56.25 68.75 81.25 56.25 81.25 50.00 62.50 56.25 75.00 68.75 68.75 56.25 18.75 62.50 75.00 31.25 43.75 81.25 87.50 62.50 50.00 18.75 56.25 carbon_neutrality_theory_and_technology 90.91 27.27 18.18 90.91 90.91 86.36 100.00 54.55 77.27 95.45 95.45 81.82 95.45 72.73 90.91 95.45 90.91 95.45 86.36 77.27 27.27 68.18 77.27 86.36 59.09 95.45 95.45 90.91 68.18 18.18 95.45 environmental_biotechnology 53.33 6.67 6.67 60.00 86.67 33.33 93.33 26.67 53.33 80.00 66.67 60.00 33.33 53.33 73.33 66.67 73.33 53.33 46.67 60.00 13.33 33.33 73.33 33.33 6.67 73.33 80.00 80.00 40.00 13.33 66.67 modern_detection_techniques 72.41 51.72 0.00 93.10 75.86 89.66 93.10 68.97 44.83 96.55 96.55 89.66 75.86 51.72 86.21 86.21 96.55 86.21 96.55 68.97 58.62 72.41 68.97 55.17 79.31 89.66 93.10 68.97 62.07 55.17 93.10 environmental_pollution_investigation_and_tracing 95.65 0.00 13.04 73.91 100.00 86.96 86.96 8.70 30.43 86.96 95.65 86.96 86.96 86.96 86.96 82.61 86.96 95.65 91.30 69.57 0.00 69.57 82.61 65.22 13.04 91.30 91.30 91.30 78.26 0.00 82.61 persistent_organic_pollutants 80.95 42.86 9.52 80.95 85.71 71.43 90.48 38.10 33.33 95.24 90.48 85.71 76.19 61.90 95.24 95.24 90.48 90.48 80.95 71.43 9.52 66.67 71.43 52.38 52.38 95.24 100.00 85.71 61.90 23.81 85.71 environmental_behavior_of_atmospheric_science_and_climate_change 88.24 23.53 23.53 82.35 88.24 100.00 88.24 52.94 52.94 88.24 94.12 94.12 82.35 82.35 94.12 94.12 88.24 100.00 82.35 88.24 17.65 52.94 100.00 82.35 41.18 94.12 94.12 94.12 76.47 23.53 88.24 mean 82.70 22.26 15.67 79.53 84.67 78.24 92.15 51.46 57.94 88.41 89.60 86.19 82.11 69.18 89.56 87.27 88.54 86.73 82.99 72.58 31.11 66.95 81.00 64.55 53.06 88.85 92.21 80.68 65.25 29.14 86.30 σ 16.09 14.87 12.52 12.11 13.31 15.13 7.21 19.92 17.14 10.20 9.85 10.58 15.43 20.07 8.71 11.68 7.92 12.08 13.19 13.50 19.12 13.62 14.28 22.01 20.47 10.24 7.76 13.98 17.48 17.68 11.24 cv 0.19 0.67 0.80 0.15 0.16 0.19 0.08 0.39 0.30 0.12 0.11 0.12 0.19 0.29 0.10 0.13 0.09 0.14 0.16 0.19 0.61 0.20 0.18 0.34 0.39 0.12 0.08 0.17 0.27 0.61 0.13 1-cv 0.81 0.33 0.20 0.85 0.84 0.81 0.92 0.61 0.70 0.88 0.89 0.88 0.81 0.71 0.90 0.87 0.91 0.86 0.84 0.81 0.39 0.80 0.82 0.66 0.61 0.88 0.92 0.83 0.73 0.39 0.87 M*(1-cv) 66.61 7.39 3.15 67.42 71.35 63.11 84.94 31.54 40.80 78.20 79.75 75.60 66.68 49.11 80.85 75.59 80.62 74.65 69.80 59.08 11.99 53.33 66.72 42.54 32.59 78.61 84.45 66.70 47.77 11.46 75.06 33 83.33 55.56 77.78 88.89 66.67 72.22 100.00 77.78 94.44 100.00 94.44 83.33 83.33 94.44 100.00 94.44 94.44 61.11 33.33 27.78 66.67 83.33 72.22 77.78 83.33 100.00 72.22 44.44 55.56 88.89 environmental_fluid_mechanics 100.00 83.33 100.00 66.67 83.33 66.67 66.67 50.00 50.00 100.00 66.67 50.00 66.67 50.00 66.67 83.33 83.33 83.33 66.67 33.33 33.33 83.33 100.00 83.33 100.00 33.33 100.00 33.33 16.67 66.67 83.33 environmental_analytical_chemistry 66.67 50.00 50.00 16.67 50.00 50.00 50.00 66.67 66.67 50.00 83.33 66.67 50.00 16.67 50.00 83.33 66.67 50.00 33.33 50.00 50.00 66.67 100.00 50.00 16.67 33.33 100.00 33.33 50.00 16.67 50.00 principles_and_applications_of_ecology 92.86 85.71 64.29 78.57 92.86 71.43 78.57 92.86 64.29 100.00 92.86 100.00 78.57 78.57 85.71 85.71 92.86 78.57 71.43 42.86 71.43 78.57 92.86 78.57 78.57 85.71 100.00 78.57 78.57 35.71 92.86 pollution_control_microbiology 77.78 83.33 44.44 83.33 83.33 33.33 50.00 72.22 50.00 77.78 94.44 61.11 55.56 50.00 72.22 72.22 88.89 83.33 72.22 16.67 11.11 50.00 72.22 66.67 50.00 11.11 88.89 61.11 16.67 16.67 88.89 atmospheric_pollution_control_engineering 85.71 66.67 52.38 61.90 80.95 28.57 42.86 80.95 47.62 85.71 90.48 85.71 80.95 47.62 76.19 80.95 80.95 76.19 57.14 9.52 9.52 42.86 76.19 61.90 57.14 85.71 90.48 38.10 28.57 9.52 80.95 environmental_monitoring 95.45 81.82 77.27 77.27 100.00 31.82 72.73 86.36 59.09 100.00 90.91 100.00 100.00 45.45 95.45 90.91 95.45 90.91 90.91 31.82 22.73 40.91 95.45 95.45 81.82 100.00 95.45 68.18 31.82 54.55 95.45 environmental_impact_assessment 70.83 79.17 41.67 75.00 58.33 12.50 70.83 58.33 79.17 62.50 75.00 70.83 83.33 62.50 75.00 66.67 70.83 70.83 66.67 29.17 4.17 58.33 70.83 70.83 45.83 62.50 91.67 66.67 25.00 29.17 87.50 urban_water_supply_and_drainage_system_engineering 88.89 55.56 66.67 88.89 66.67 55.56 88.89 88.89 55.56 77.78 88.89 100.00 88.89 55.56 66.67 77.78 88.89 100.00 77.78 66.67 66.67 33.33 88.89 77.78 66.67 66.67 88.89 66.67 22.22 22.22 66.67 environmental_soil_science 77.78 88.89 66.67 88.89 88.89 66.67 88.89 77.78 44.44 88.89 100.00 100.00 77.78 77.78 77.78 100.00 100.00 88.89 77.78 66.67 55.56 55.56 77.78 77.78 77.78 77.78 77.78 55.56 0.00 22.22 66.67 environmental_economics 78.95 78.95 52.63 73.68 73.68 63.16 89.47 73.68 52.63 68.42 89.47 78.95 73.68 68.42 89.47 89.47 78.95 84.21 68.42 36.84 31.58 57.89 89.47 57.89 52.63 63.16 89.47 52.63 68.42 47.37 73.68 environmental_decision_support_system 94.44 88.89 83.33 88.89 100.00 77.78 66.67 83.33 88.89 88.89 100.00 94.44 94.44 100.00 100.00 100.00 94.44 88.89 61.11 66.67 50.00 66.67 94.44 100.00 72.22 72.22 94.44 88.89 38.89 83.33 94.44 solid_waste_treatment_and_resource_utilization 88.89 88.89 50.00 72.22 83.33 55.56 50.00 88.89 72.22 94.44 88.89 77.78 88.89 61.11 94.44 88.89 88.89 88.89 83.33 27.78 11.11 55.56 77.78 94.44 77.78 83.33 94.44 55.56 16.67 33.33 94.44 water_pollution_control_engineering 75.00 75.00 62.50 75.00 62.50 50.00 50.00 50.00 62.50 75.00 75.00 87.50 62.50 75.00 62.50 62.50 62.50 62.50 62.50 37.50 25.00 62.50 87.50 62.50 62.50 75.00 87.50 62.50 50.00 50.00 87.50 environmental_planning_and_management 85.00 90.00 70.00 75.00 75.00 60.00 60.00 65.00 75.00 65.00 70.00 85.00 85.00 40.00 75.00 80.00 90.00 80.00 80.00 20.00 25.00 55.00 75.00 80.00 65.00 85.00 80.00 65.00 35.00 30.00 75.00 indoor_environmental_pollution_and_control 95.65 86.96 73.91 82.61 91.30 69.57 73.91 73.91 69.57 86.96 95.65 78.26 91.30 65.22 86.96 86.96 82.61 78.26 69.57 47.83 47.83 60.87 91.30 91.30 73.91 82.61 86.96 43.48 30.43 39.13 100.00 environmental_statistics 94.74 73.68 68.42 73.68 78.95 42.11 89.47 73.68 57.89 68.42 94.74 84.21 89.47 78.95 94.74 73.68 73.68 68.42 57.89 15.79 21.05 57.89 84.21 78.95 63.16 52.63 94.74 63.16 42.11 42.11 84.21 industrial_ecology 86.36 86.36 40.91 90.91 90.91 68.18 63.64 81.82 77.27 90.91 90.91 90.91 81.82 72.73 90.91 77.27 86.36 90.91 77.27 40.91 18.18 50.00 90.91 86.36 90.91 86.36 95.45 72.73 31.82 36.36 81.82 environmental_physical_pollution_and_control 75.00 80.00 80.00 80.00 85.00 50.00 65.00 85.00 70.00 85.00 90.00 75.00 70.00 75.00 85.00 90.00 90.00 85.00 80.00 30.00 35.00 55.00 65.00 75.00 75.00 80.00 100.00 70.00 45.00 25.00 90.00 environmental_nanotechnology 92.86 92.86 71.43 100.00 85.71 57.14 71.43 71.43 92.86 92.86 85.71 85.71 100.00 78.57 92.86 92.86 92.86 92.86 57.14 28.57 35.71 78.57 85.71 92.86 78.57 28.57 78.57 85.71 78.57 35.71 92.86 specialty_wastewater_treatment 55.56 55.56 44.44 66.67 44.44 77.78 33.33 33.33 33.33 55.56 66.67 66.67 44.44 33.33 55.56 44.44 33.33 22.22 11.11 44.44 44.44 33.33 33.33 33.33 44.44 55.56 66.67 33.33 33.33 11.11 55.56 modern_biology 72.22 83.33 66.67 77.78 94.44 61.11 77.78 66.67 55.56 94.44 100.00 83.33 88.89 55.56 88.89 83.33 88.89 61.11 38.89 38.89 44.44 61.11 77.78 72.22 72.22 50.00 88.89 61.11 27.78 66.67 77.78 theory_and_technology_of_environmental_pollution_prevention_and_control 87.50 81.25 81.25 81.25 93.75 56.25 50.00 75.00 93.75 56.25 93.75 100.00 93.75 87.50 87.50 100.00 93.75 81.25 68.75 75.00 68.75 68.75 87.50 87.50 81.25 93.75 93.75 68.75 31.25 43.75 87.50 reaction_kinetics_and_reactor_design 80.00 90.00 10.00 90.00 90.00 30.00 10.00 70.00 90.00 80.00 60.00 100.00 90.00 70.00 90.00 90.00 30.00 90.00 70.00 10.00 0.00 80.00 100.00 90.00 80.00 60.00 80.00 60.00 10.00 0.00 90.00 engineering_ethics_and_environmental_ethics 100.00 91.30 69.57 86.96 91.30 82.61 86.96 82.61 78.26 78.26 95.65 91.30 91.30 82.61 82.61 95.65 82.61 86.96 86.96 65.22 43.48 86.96 86.96 86.96 82.61 91.30 91.30 86.96 60.87 56.52 78.26 sludge_treatment_disposal_and_resource_utilization 85.71 78.57 50.00 71.43 85.71 21.43 42.86 71.43 57.14 85.71 78.57 78.57 71.43 71.43 85.71 78.57 57.14 78.57 42.86 14.29 28.57 50.00 64.29 64.29 64.29 71.43 85.71 42.86 7.14 28.57 71.43 safe_disposal_and_resource_recovery_of_hazardous_waste 94.44 100.00 77.78 77.78 88.89 72.22 72.22 83.33 77.78 94.44 88.89 94.44 88.89 88.89 100.00 94.44 94.44 88.89 77.78 27.78 38.89 61.11 88.89 100.00 94.44 94.44 100.00 88.89 72.22 33.33 88.89 theory_and_technology_of_air_pollution_prevention_and_control 88.24 82.35 76.47 88.24 88.24 41.18 52.94 88.24 70.59 88.24 100.00 88.24 88.24 70.59 94.12 76.47 82.35 82.35 64.71 41.18 29.41 52.94 70.59 76.47 64.71 82.35 100.00 35.29 41.18 17.65 100.00 environmental_geographic_information_system 91.67 91.67 66.67 79.17 83.33 79.17 29.17 62.50 91.67 91.67 95.83 91.67 87.50 95.83 91.67 83.33 91.67 75.00 37.50 41.67 45.83 75.00 87.50 83.33 75.00 62.50 95.83 79.17 50.00 33.33 83.33 clean_production_and_energy_conservation_and_emissions_reduction 82.35 82.35 52.94 82.35 94.12 52.94 76.47 82.35 58.82 70.59 70.59 94.12 88.24 52.94 76.47 100.00 94.12 94.12 82.35 23.53 41.18 58.82 88.24 64.71 70.59 94.12 94.12 52.94 35.29 35.29 88.24 circulate_economy_and_industrial_ecology_methodologies 72.73 90.91 81.82 77.27 90.91 59.09 54.55 86.36 63.64 77.27 86.36 90.91 86.36 77.27 86.36 86.36 81.82 90.91 86.36 22.73 40.91 68.18 77.27 72.73 68.18 68.18 90.91 45.45 45.45 63.64 77.27 atmospheric_particulate_matter_detection_and_analysis_techniques 85.00 75.00 65.00 75.00 85.00 55.00 60.00 95.00 65.00 85.00 95.00 85.00 85.00 50.00 85.00 95.00 85.00 90.00 55.00 45.00 50.00 65.00 95.00 75.00 70.00 75.00 90.00 75.00 65.00 60.00 90.00 environmental_electrochemical_theory_and_technology 69.23 69.23 61.54 61.54 76.92 46.15 23.08 61.54 53.85 61.54 61.54 76.92 76.92 61.54 76.92 69.23 53.85 61.54 61.54 15.38 23.08 53.85 69.23 46.15 53.85 30.77 76.92 61.54 38.46 30.77 61.54 environmental_microwave_chemistry_technology 78.57 57.14 50.00 64.29 50.00 50.00 35.71 64.29 35.71 57.14 64.29 64.29 71.43 42.86 71.43 78.57 71.43 57.14 71.43 42.86 42.86 21.43 64.29 42.86 28.57 14.29 57.14 42.86 35.71 28.57 71.43 advanced_environmental_chemistry 90.00 80.00 75.00 75.00 90.00 55.00 80.00 95.00 60.00 95.00 95.00 100.00 95.00 55.00 95.00 95.00 95.00 90.00 75.00 50.00 45.00 55.00 85.00 70.00 85.00 55.00 80.00 60.00 50.00 35.00 95.00 mass_transfer_processes 68.75 68.75 37.50 75.00 81.25 50.00 68.75 62.50 43.75 50.00 81.25 50.00 75.00 37.50 68.75 56.25 62.50 62.50 75.00 18.75 18.75 56.25 81.25 62.50 68.75 68.75 87.50 37.50 31.25 37.50 68.75 carbon_neutrality_theory_and_technology 90.91 72.73 72.73 81.82 90.91 59.09 59.09 72.73 59.09 95.45 95.45 90.91 90.91 81.82 86.36 95.45 86.36 95.45 77.27 18.18 27.27 54.55 86.36 72.73 68.18 77.27 95.45 59.09 27.27 50.00 95.45 environmental_biotechnology 60.00 46.67 60.00 53.33 86.67 20.00 46.67 53.33 60.00 73.33 53.33 46.67 46.67 60.00 66.67 53.33 60.00 66.67 53.33 13.33 6.67 20.00 53.33 60.00 60.00 46.67 80.00 33.33 13.33 6.67 66.67 modern_detection_techniques 86.21 89.66 68.97 82.76 75.86 44.83 82.76 93.10 62.07 89.66 96.55 86.21 82.76 51.72 86.21 96.55 86.21 89.66 58.62 55.17 55.17 37.93 75.86 62.07 82.76 86.21 93.10 58.62 58.62 24.14 93.10 environmental_pollution_investigation_and_tracing 82.61 78.26 86.96 56.52 78.26 34.78 86.96 39.13 39.13 86.96 91.30 78.26 86.96 78.26 86.96 78.26 82.61 82.61 56.52 0.00 0.00 30.43 73.91 78.26 30.43 34.78 91.30 56.52 8.70 47.83 82.61 persistent_organic_pollutants 90.48 76.19 33.33 76.19 80.95 38.10 95.24 80.95 42.86 85.71 85.71 85.71 76.19 57.14 71.43 95.24 90.48 100.00 76.19 23.81 23.81 47.62 71.43 71.43 71.43 76.19 95.24 47.62 47.62 9.52 85.71 environmental_behavior_of_atmospheric_science_and_climate_change 94.12 76.47 70.59 76.47 82.35 52.94 52.94 82.35 41.18 88.24 88.24 94.12 76.47 76.47 88.24 94.12 94.12 100.00 70.59 29.41 23.53 58.82 100.00 94.12 76.47 76.47 94.12 64.71 29.41 29.41 100.00 mean 83.63 78.97 62.65 75.80 81.97 52.77 62.83 74.35 63.02 80.80 85.88 83.43 80.58 64.78 82.24 83.86 80.96 80.56 65.95 34.49 33.21 56.51 81.35 74.30 68.03 66.99 89.59 59.17 37.40 35.73 82.94 σ 10.39 11.89 16.67 13.13 12.76 16.59 19.66 14.91 15.73 14.08 12.36 14.05 13.24 17.61 11.75 13.20 15.88 15.20 15.75 17.53 17.80 15.14 13.29 15.13 16.85 22.20 9.11 15.81 18.88 17.97 11.99 cv 0.12 0.15 0.27 0.17 0.16 0.31 0.31 0.20 0.25 0.17 0.14 0.17 0.16 0.27 0.14 0.16 0.20 0.19 0.24 0.51 0.54 0.27 0.16 0.20 0.25 0.33 0.10 0.27 0.50 0.50 0.14 1-cv 0.88 0.85 0.73 0.83 0.84 0.69 0.69 0.80 0.75 0.83 0.86 0.83 0.84 0.73 0.86 0.84 0.80 0.81 0.76 0.49 0.46 0.73 0.84 0.80 0.75 0.67 0.90 0.73 0.50 0.50 0.86 M*(1-cv) 73.24 67.08 45.98 62.68 69.21 36.18 43.17 59.44 47.29 66.72 73.52 69.37 67.34 47.17 70.48 70.66 65.08 65.35 50.20 16.96 15.41 41.37 68.07 59.17 51.17 44.79 80.47 43.35 18.52 17.75 70.95 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data collection and process workflows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The composite index scores I of 31 LLMs evaluated on EnviroExam(5-shot).</figDesc><graphic coords="5,108.00,83.32,395.99,277.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The composite index scores I of 31 LLMs evaluated on EnviroExam(0-shot).</figDesc><graphic coords="5,108.00,411.31,395.99,277.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A radar chart of the composite index scores I for 31 large language models evaluated in 5-shot and 0-shot settings on enviroexam.</figDesc><graphic coords="6,108.00,72.00,395.99,300.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>BFigure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Prompts for each subject(Document translated, original question in Chinese)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Detail scores for each subject in 5-shot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Detail scores for each subject in 0-shot</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>5 annotated examples. These examples cover different types of environmental science questions and include answers to help the model understand the structure and expected format of the responses. Next, we present the model with new test questions and ask it to generate answers based on the previously learned 5 examples. The model's generated answers are recorded and compared with the standard answers to evaluate its performance in understanding and generating new answers. This process aims to test the model's learning and generalization abilities with limited examples.Scores are shown in Figure 3. Models evaluated in this paper 0-shot score In the 0-shot experiment, we present the model with test questions without providing any annotated examples beforehand. The model is required to generate answers based solely on its pre-existing knowledge and training. The generated answers are then recorded and compared with the standard answers to evaluate the model's performance in understanding and answering new questions without prior examples. This process aims to test the model's ability to generalize and perform accurately without any specific guidance or context provided through examples.Scores are shown in Figure 4.</figDesc><table><row><cell>Model</cell><cell>Creator</cell><cell cols="2">#Parameters Access</cell></row><row><cell>baichuan2-13b-chat</cell><cell>Baichuan-ai</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>baichuan-13b-chat</cell><cell>Baichuan-ai</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>chatglm2-6b</cell><cell>THUDM</cell><cell>6B</cell><cell>Weights</cell></row><row><cell>chatglm3-6b</cell><cell>THUDM</cell><cell>6B</cell><cell>Weights</cell></row><row><cell>chatglm3-6b-32k</cell><cell>THUDM</cell><cell>6B</cell><cell>Weights</cell></row><row><cell>deepseek-7b-chat</cell><cell>Deepseek ai</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>deepseek-67b-chat</cell><cell>Deepseek ai</cell><cell>67B</cell><cell>Weights</cell></row><row><cell>gemma-7b</cell><cell>Google</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>gemma-2b-it</cell><cell>Google</cell><cell>2B</cell><cell>Weights</cell></row><row><cell>internlm2-chat-20b</cell><cell cols="2">Shanghai AI Laboratory 20B</cell><cell>Weights</cell></row><row><cell>internlm2-chat-7b</cell><cell cols="2">Shanghai AI Laboratory 7B</cell><cell>Weights</cell></row><row><cell>internlm-chat-20b</cell><cell cols="2">Shanghai AI Laboratory 20B</cell><cell>Weights</cell></row><row><cell>internlm-chat-7b</cell><cell cols="2">Shanghai AI Laboratory 7B</cell><cell>Weights</cell></row><row><cell>mistral-7b-instruct-v0.1</cell><cell>Mistralai</cell><cell>7B</cell><cell>Weights</cell></row><row><cell cols="2">mixtral-8x7b-instruct-v0.1 Mistralai</cell><cell>56B</cell><cell>Weights</cell></row><row><cell>qwen1.5-14b-chat</cell><cell>Alibaba Cloud</cell><cell>14B</cell><cell>Weights</cell></row><row><cell>qwen1.5-7b-chat</cell><cell>Alibaba Cloud</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>qwen-14b-chat</cell><cell>Alibaba Cloud</cell><cell>14B</cell><cell>Weights</cell></row><row><cell>qwen-7b-chat</cell><cell>Alibaba Cloud</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>vicuna-13b-v1.5</cell><cell>LMSYS</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>vicuna-7b-v1.5</cell><cell>LMSYS</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>vicuna-7b-v1.5-16k</cell><cell>LMSYS</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>bluelm-7b-chat</cell><cell>Vivo</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>tigerbot-13b-chat-v2</cell><cell>TigerResearch</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>tigerbot-7b-chat-v3</cell><cell>TigerResearch</cell><cell>7B</cell><cell>Weights</cell></row><row><cell>llama-3-8b-instruct</cell><cell>Meta</cell><cell>8B</cell><cell>Weights</cell></row><row><cell>llama-3-70b-instruct</cell><cell>Meta</cell><cell>70B</cell><cell>Weights</cell></row><row><cell>llama-2-70b-chat</cell><cell>Meta</cell><cell>70B</cell><cell>Weights</cell></row><row><cell>llama-2-13b-chat</cell><cell>Meta</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>skywork-13b</cell><cell>Skywork</cell><cell>13B</cell><cell>Weights</cell></row><row><cell>yi-6b-chat</cell><cell>01-ai</cell><cell>6B</cell><cell>Weights</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Environmental Science Core ClassesEnvironmental science education plan &amp; number of questions</figDesc><table><row><cell>Course Name</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use Harbin Institute of Technology as an example because its Environmental Science and Engineering program received an A+ rating in the fourth round of university evaluations<ref type="bibr" target="#b6">[7]</ref>. (An A+ rating in university discipline evaluations signifies that the program is among the top in its field nationwide.)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>When CV is greater than 1, it indicates that the relative variability of the data is very high, and the mean can no longer effectively represent the central tendency of the data<ref type="bibr" target="#b15">[16]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>OpenCompass is an open-source benchmarking platform designed to evaluate and compare the performance of large language models (LLMs) across various tasks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We utilized 4 NVIDIA A100-80G GPUs for our tests. Given that the largest model is only 70B, most computations were completed within a few hours. This research was supported by the highperformance computing platform at <rs type="institution">Harbin Institute of Technology</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Use of chatgpt: What does it mean for biology and environmental science?</title>
		<author>
			<persName><forename type="first">Evgenios</forename><surname>Agathokleous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Costas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Saitanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science of The Total Environment</title>
		<imprint>
			<biblScope unit="volume">888</biblScope>
			<biblScope unit="page">164154</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Model card and evaluations for claude models</title>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autonomous chemical research with large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Boiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Macknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabe</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">624</biblScope>
			<biblScope unit="issue">7992</biblScope>
			<biblScope unit="page" from="570" to="578" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Medbench: A large-scale chinese benchmark for evaluating medical large language models</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Evaluating large language models trained on code</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">China Academic Degrees and Graduate Education Development Center</title>
		<ptr target="https://www.cdgdc.edu.cn/dslxkpgjggb/" />
	</analytic>
	<monogr>
		<title level="m">The fourth round of discipline evaluation results</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Opencompass: A universal evaluation platform for foundation models</title>
		<author>
			<persName><forename type="first">Opencompass</forename><surname>Contributors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lawbench: Benchmarking legal knowledge of large language models</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengzhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models</title>
		<author>
			<persName><forename type="first">Yuzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhuo</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tangjun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Capabilities of large language models in control engineering: A benchmark study on gpt-4, claude 3 opus, and gemini 1.0 ultra</title>
		<author>
			<persName><forename type="first">Darioush</forename><surname>Kevian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Usman</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Havens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Dullerud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lawgpt</surname></persName>
		</author>
		<author>
			<persName><surname>Lawgpt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Chatgpt: Applications, opportunities, and threats</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performance of some estimators of relative variability</title>
		<author>
			<persName><forename type="first">Raydonal</forename><surname>Ospina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Marmolejo-Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Applied Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pretraining on the test set is all you need</title>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jun-Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Jason Ren</surname></persName>
		</author>
		<idno type="PMID">36943179</idno>
	</analytic>
	<monogr>
		<title level="j">Chatgpt and environmental research. Environmental Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="17667" to="17670" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
