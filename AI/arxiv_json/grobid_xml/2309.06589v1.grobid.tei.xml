<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do Generative Large Language Models need billions of parameters?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-12">12 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
							<email>gholami@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="department" key="dep2">Member IEEE</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marwan</forename><surname>Omar</surname></persName>
							<email>momar3@iit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The Institute of Electrical and Electronics Engineers</orgName>
								<orgName type="department" key="dep2">Member IEEE</orgName>
								<orgName type="institution">Illinois Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do Generative Large Language Models need billions of parameters?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-12">12 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">8FAC3354C01583065FB4943474C46C1D</idno>
					<idno type="arXiv">arXiv:2309.06589v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are a type of artificial intelligence (AI) model designed to understand and generate human-like text based on the context provided to them. These models have become increasingly popular in recent years, with the most notable examples being OpenAI's GPT series <ref type="bibr" target="#b28">[Radford et al., 2019]</ref>, including the latest version, GPT-4. LLMs have shown significant improvements over previous models in various natural language processing (NLP) tasks and have numerous practical applications in diverse domains. LLMs are a subcategory of deep learning models that focus on processing and generating text data. They are built using transformer architectures, which were first introduced by <ref type="bibr" target="#b33">[Vaswani et al., 2017]</ref>. and have since revolutionized the field of NLP. Transformers are characterized by their self-attention mechanisms, which enable them to process and generate text sequences in parallel rather than sequentially, as was the case with previous models like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.</p><p>Large language models are trained on vast amounts of text data, often sourced from the internet, to learn patterns, structures, and relationships within the text. This allows them to understand and generate human-like text in a variety of contexts <ref type="bibr" target="#b31">[Shoeybi et al., 2019]</ref>. The size of these models is typically measured in terms of parameters, with recent models like GPT-3 and GPT-4 boasting billions of parameters <ref type="bibr" target="#b11">[Fedus et al., 2022]</ref>.</p><p>Large language models have numerous practical applications across a wide range of domains, including but not limited to:</p><p>1. Text Generation and Completion: LLMs can generate human-like text based on a given prompt, making them valuable tools for tasks like content creation, email drafting, and code generation <ref type="bibr" target="#b7">[Brown et al., 2020]</ref>.</p><p>2. Machine Translation: LLMs have demonstrated impressive performance in translating text between languages, rivaling dedicated machine translation models <ref type="bibr" target="#b21">[Lample and Conneau, 2019]</ref>.</p><p>3. Summarization: LLMs can generate concise summaries of longer text passages, making them useful in areas like news aggregation, research paper summarization, and extracting key information from documents <ref type="bibr" target="#b25">[Liu and Lapata, 2019]</ref>.</p><p>4. Question Answering: LLMs can understand and answer questions based on a given context, making them effective tools for applications like virtual assistants, customer support, and knowledge extraction <ref type="bibr" target="#b30">[Rajpurkar et al., 2016]</ref>.</p><p>5. Sentiment Analysis: LLMs can classify text based on sentiment, enabling use cases in market research, social media monitoring, and product review analysis <ref type="bibr" target="#b37">[Zhang et al., 2018]</ref>.</p><p>6. Conversational AI: LLMs can generate contextually appropriate responses in a dialogue, making them useful for building chatbots and voice assistants <ref type="bibr" target="#b0">[Adiwardana et al., 2020]</ref>.</p><p>Despite their impressive capabilities, large language models also present several challenges related to their development, deployment, and ethical considerations:</p><p>• Computational and Energy Requirements: Training LLMs requires vast computational resources, making the process expensive and energy-intensive <ref type="bibr" target="#b32">[Strubell et al., 2019]</ref>. This limits accessibility to such models for researchers and organizations with limited resources.</p><p>• Data Bias: Since LLMs are trained on large datasets sourced from the internet, they may inadvertently learn and propagate biases present in the data <ref type="bibr" target="#b3">[Bender et al., 2021]</ref>. This can lead to biased outputs, negatively impacting certain user groups or perpetuating harmful stereotypes.</p><p>• Model Controllability: Controlling the outputs of LLMs can be challenging due to their complex and highly interconnected nature. This can generate inappropriate, harmful, or offensive content <ref type="bibr" target="#b28">[Radford et al., 2019]</ref>.</p><p>• Intellectual Property Concerns: LLMs can generate text that resembles human-written content, which raises questions about intellectual property rights and potential copyright infringement <ref type="bibr" target="#b24">[Lipton and Steinhardt, 2018]</ref>.</p><p>• Privacy Issues: As LLMs are trained on vast amounts of data, there is a possibility that they may unintentionally memorize and reveal sensitive information, presenting privacy concerns <ref type="bibr" target="#b8">[Carlini et al., 2019]</ref>.</p><p>The primary goal of this study is to explore, investigate and propose new approaches to create efficient LLMs. This objective is important for several reasons:</p><p>• Accessibility: Efficient LLMs require fewer computational resources for training and inference, making them more accessible to researchers, developers, and organizations with limited resources. This democratizes access to state-of-the-art natural language processing (NLP) technology, promoting innovation and reducing the gap between well-funded organizations and smaller players in the field.</p><p>• Energy consumption: Training and deploying LLMs can be energy-intensive due to their size and complexity <ref type="bibr" target="#b32">(Strubell et al., 2019)</ref>. Developing more efficient models can significantly reduce energy consumption and the associated environmental impact, contributing to more sustainable AI development practices.</p><p>• Cost reduction: Efficient LLMs can lower the financial costs associated with training, deployment, and maintenance. This allows organizations to allocate resources more effectively, potentially accelerating the development of new applications and services that rely on advanced NLP capabilities.</p><p>• Latency and real-time applications: Efficient LLMs can provide faster response times during inference, which is particularly important for real-time applications, such as conversational AI, virtual assistants, and other interactive systems that demand low latency.</p><p>• Edge computing: As more AI applications are being deployed on edge devices, such as smartphones and IoT devices, it is crucial to develop efficient LLMs that can operate within the constraints of limited computing power, memory, and energy resources. Efficient LLMs can enable sophisticated NLP capabilities on edge devices, expanding their potential use cases and improving user experiences.</p><p>• Scalability: Developing efficient LLMs allows for better scalability in terms of the number of users and applications supported by a given model. This is particularly relevant for cloud-based AI services that need to serve a large number of clients simultaneously, where resource efficiency directly translates into cost savings and improved performance.</p><p>• Encouraging research: Making LLMs more efficient can stimulate further research in model optimization, compression, and resource-conscious training techniques. This can lead to the discovery of novel methods that improve LLMs and other types of deep learning models, benefiting the broader AI research community. Parameter sharing can indeed help make Transformer models more efficient. Transformers are known for their large number of parameters, which can lead to high computational and memory requirements. By sharing parameters across different parts of the model, you can reduce the total number of unique parameters, leading to a smaller and more efficient model. Using parameter sharing in Transformers can be beneficial for several reasons: • Reduced model size: Transformers typically have a large number of parameters, which can result in substantial memory and storage requirements. Parameter sharing helps reduce the overall number of unique parameters, leading to a more compact model that is easier to store and deploy.</p><p>• Faster training and inference: With fewer parameters to learn and process, training and inference times can be reduced, leading to more efficient and faster models. This can be especially important when deploying models on resource-constrained devices or in situations where low-latency responses are critical.</p><p>• Improved generalization: Sharing parameters across different parts of the model can encourage the learning of more general and robust features. This can potentially lead to better generalization to unseen data or tasks, improving the model's overall performance.</p><p>• Easier optimization: With fewer parameters to optimize, the model's optimization landscape may become smoother and easier to navigate. This can lead to faster convergence during training and potentially better final model performance.</p><p>• Regularization effect: Parameter sharing can have a regularization effect by implicitly constraining the model's capacity. This can help prevent overfitting, especially when training data is limited.</p><p>• Transfer learning and multi-task learning: Parameter sharing can be used to share representations between different tasks or modalities, promoting the learning of shared features and potentially improving performance on multiple tasks simultaneously.</p><p>However, it's important to strike a balance between the benefits of parameter sharing and the potential reduction in model performance. Excessive parameter sharing can lead to a loss of expressive power, resulting in decreased performance on the given task. It is crucial to carefully consider the trade-offs and determine the appropriate level of parameter sharing that maximizes efficiency while maintaining the desired performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Natural Language Processing (NLP) has been a major area of research in Artificial Intelligence and Machine Learning since the early days of computer science <ref type="bibr" target="#b34">[Voorhees et al., 1999</ref><ref type="bibr" target="#b26">, Moldovan et al., 2000</ref><ref type="bibr" target="#b6">, Brill et al., 2002</ref><ref type="bibr" target="#b12">, Ferrucci et al., 2010</ref><ref type="bibr" target="#b15">, Gholami and Noori, 2021</ref><ref type="bibr">, 2022</ref><ref type="bibr">, Gholami et al., 2022</ref><ref type="bibr">, Gholami and Khashe, 2022a</ref><ref type="bibr">,b, Brand et al., 2022]</ref>. Parameter sharing has been employed in several transformer models to improve efficiency and reduce the number of parameters. Some notable examples include:</p><p>Universal Transformers <ref type="bibr" target="#b9">[Dehghani et al., 2018]</ref>: Universal Transformers extend the original transformer architecture by sharing parameters across layers. Instead of having separate parameters for each layer, the same set of weights is applied recursively for a fixed number of steps, updating the hidden states at each step. This approach substantially reduces the number of parameters while maintaining competitive performance on various NLP tasks.</p><p>ALBERT <ref type="bibr" target="#b23">[Lan et al., 2019]</ref>: ALBERT (A Lite BERT) shares all parameters across layers, significantly reducing model size compared to the original BERT. By doing so, ALBERT achieves comparable performance to larger models with fewer parameters, making it more efficient and easier to deploy.</p><p>LayerDrop [fan]: LayerDrop is a technique that randomly drops a subset of layers during training, effectively sharing parameters between different "depths" of the transformer. This approach can help improve efficiency and model performance without significantly increasing the number of parameters.</p><p>Tied Transformers <ref type="bibr" target="#b22">[Lample et al., 2017]</ref>: Tied Transformers use parameter sharing between the encoder and the decoder, sharing not only the layer parameters but also the token embeddings and positional embeddings. This approach reduces the number of parameters and improves the model's efficiency, particularly in unsupervised machine translation tasks.</p><p>Depthwise Separable Transformers <ref type="bibr" target="#b20">[Kitaev et al., 2020]</ref>: Depthwise Separable Transformers employ parameter sharing across different attention heads within the same layer. This method reduces the number of parameters while preserving the benefits of multi-head attention, leading to a more efficient model.</p><p>Longformer <ref type="bibr" target="#b2">[Beltagy et al., 2020]</ref>: Longformer utilizes a combination of local and global attention mechanisms, sharing parameters between local and global attention heads. This approach allows the model to process long documents efficiently without a significant increase in the number of parameters.</p><p>Big Bird <ref type="bibr" target="#b36">[Zaheer et al., 2020]</ref>: Big Bird employs a combination of sparse attention mechanisms and parameter sharing across different attention heads to create an efficient transformer capable of handling longer sequences. This approach reduces both the computational complexity and the number of parameters compared to the original transformer architecture.</p><p>Switch Transformers <ref type="bibr" target="#b11">[Fedus et al., 2022]</ref>: Switch Transformers introduce a novel parameter-sharing technique called "mixture of experts," in which different parts of the model specialize in different aspects of the data. By sharing parameters across experts and selectively activating only a subset of them during inference, Switch Transformers can scale to trillion-parameter models while maintaining high efficiency.</p><p>These examples demonstrate the effectiveness of parameter sharing in creating more efficient transformers while maintaining competitive performance on various NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Transformers were introduced by Vaswani et al. <ref type="bibr" target="#b33">[Vaswani et al., 2017]</ref> as a novel architecture for NLP tasks, relying on self-attention mechanisms to capture long-range dependencies in the input data. They consist of a stack of identical layers, each containing a multi-head self-attention mechanism and position-wise feed-forward networks. The self-attention mechanism allows the model to weigh the importance of different tokens in the input sequence relative to each other, enabling it to learn complex patterns and dependencies effectively.</p><p>Despite their impressive performance, transformers have high computational and memory requirements. These demands stem from the quadratic complexity of the self-attention mechanism, which requires calculating attention scores for each token pair in the input sequence. Additionally, transformers often have millions of parameters, making them resource-intensive to store and update during training and inference <ref type="bibr" target="#b35">[Yun et al., 2020]</ref>.</p><p>Parameter sharing is a technique used to reduce the number of model parameters by reusing them across different components of the model. This approach has been successfully applied in convolutional neural networks (CNNs) for image recognition tasks, where the same set of weights is used across different spatial locations, significantly reducing the number of parameters.</p><p>Parameter sharing offers several benefits in building efficient transformers:</p><p>• Reduced Model Size: By reusing parameters across different layers or components, parameter sharing substantially decreases the number of unique parameters required to represent the model. This reduction in model size makes it easier to deploy transformers on resourceconstrained devices, where memory is often limited. • Faster Training and Inference: With fewer parameters to update during training, the model can be trained more quickly, leading to faster convergence. This speed-up can be particularly beneficial when training large models or working with large-scale datasets. Moreover, during inference, a smaller model requires fewer computations, which translates to faster prediction times. • Regularization Effect: Parameter sharing can act as a form of implicit regularization, constraining the model's capacity and reducing overfitting. By reusing parameters, the model is forced to learn shared representations, which can lead to better generalization performance on unseen data.</p><p>Several parameter-sharing <ref type="bibr" target="#b27">[Radford et al., 2018]</ref> techniques have been proposed and explored to improve the efficiency of transformers. We discuss some of the most prominent approaches:</p><p>• Universal Transformers: Universal Transformers <ref type="bibr" target="#b9">[Dehghani et al., 2018]</ref> extend the original transformer architecture by sharing parameters across layers. Instead of having separate parameters for each layer, Universal Transformers apply the same set of weights recursively for a fixed number of steps, updating the hidden states at each step. This approach significantly reduces the number of parameters while maintaining competitive performance on various NLP tasks. • Layer-wise Parameter Sharing: Layer-wise parameter sharing involves reusing parameters across multiple layers in the transformer. For example, ALBERT <ref type="bibr" target="#b23">[Lan et al., 2019]</ref> shares all parameters across layers, leading to a substantial reduction in model size. By doing so, ALBERT achieves comparable performance to larger models with fewer parameters, making it more efficient and easier to deploy. • Head-wise Parameter Sharing: In transformers, the multi-head self-attention mechanism computes multiple attention scores for each token pair. Head-wise parameter sharing involves reusing the same set of weights across different attention heads within the same layer. This approach can help reduce the number of parameters while preserving the benefits of multi-head attention. • Parameter Sharing in Feed-Forward Networks: Transformers also include position-wise feed-forward networks in each layer, which can be another source of parameter reduction. Sharing parameters across the feed-forward networks of different layers can help achieve a more compact model while maintaining its capacity to learn complex patterns. • Pre-trained Models and Fine-tuning: Parameter sharing can also be employed through the use of pre-trained transformer models, such as BERT <ref type="bibr" target="#b10">[Devlin et al., 2018]</ref> and GPT <ref type="bibr" target="#b27">[Radford et al., 2018]</ref>. By fine-tuning these pre-trained models on specific tasks, it is possible to leverage the shared knowledge encoded in the model's parameters and achieve strong performance with fewer training samples and reduced training time.</p><p>Parameter sharing is not without its drawbacks and challenges. Some of the key issues include:</p><p>• Reduced Expressive Power: Sharing parameters can limit the model's expressive power, as it is forced to learn shared representations. In some cases, this reduction in capacity can lead to a degradation in performance, especially when the shared parameters are not suitable for the task at hand. • Hyperparameter Tuning: Parameter sharing introduces additional hyperparameters, such as the number of shared layers or the extent of sharing within a layer. Finding the optimal configuration of these hyperparameters can be challenging and may require extensive experimentation and search. • Task-Specific Considerations: Not all tasks may benefit from parameter sharing, and the optimal degree of sharing can vary depending on the nature of the task. For example, tasks that require modeling fine-grained dependencies or learning highly specialized representations may suffer from performance degradation with increased parameter sharing.</p><p>Parameter sharing has emerged as a promising approach to building efficient transformers, offering benefits such as reduced model size, faster training and inference, and implicit regularization. Various parameter sharing techniques have been proposed and explored, including layer-wise, head-wise, and feed-forward network sharing, as well as leveraging pre-trained models.</p><p>To enhance the Transformer model's efficiency and efficacy for self-supervised learning of language representations. The suggested technique does this by lowering the number of model parameters while maintaining or enhancing performance. The proposed technique uses two key strategies: parameter sharing and factorized embedding parameterization across layers, which results in a smaller model with fewer parameters. I call the transformer created by these techniques Generative Pretrained Transformer Efficio (GPT-Efficio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.1">Embedding layer factorization:</head><p>In this approach, the original embedding layer is factorized into two matrices, which reduces the total number of parameters in the model. This is particularly useful for large-scale models like BERT and GPT, where the embedding layer can consume a significant amount of memory due to the large vocabulary size. The factorization works by separating the original embedding layer into two smaller layers:</p><p>1. A word-piece embedding matrix that maps the input tokens into a lower-dimensional space (EmbeddingSize).</p><p>2. A projection matrix that maps the lower-dimensional embeddings into the model's hidden state dimension (HiddenSize).</p><p>The original embedding matrix of the decoder transformer has a total of V ocabSize × HiddenSize parameters. To get better performance, NLP models tend to use large vocabulary size and this layer can be billions of parameters <ref type="bibr" target="#b19">[Keskar et al., 2016]</ref>.</p><p>In this approach, I break up the embedding matrix into two smaller matrices, this will bring down the number of parameters to V ocabSize × EmbeddingSize + EmbeddingSize × HiddenSize. This technique is more effective when the EmbeddingSize is significantly smaller than the HiddenSize.</p><p>However, this kind of factorization can lead to a loss of information. Mapping the tokens to a smaller dimension might not capture all the nuances of the input data. In turn, this might affect the performance of the model, causing it to make more errors or have a lower overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.2">Parameter sharing across layers:</head><p>In my approach GPT-Efficio, the adoption of parameter sharing strategies is prevalent to mitigate the escalating complexity arising from an abundance of parameters. By sharing parameters, the model size can be substantially reduced, consequently enhancing generalization by introducing repetition in the underlying structure of the model.</p><p>In this context, I outline several methodologies for sharing parameters in GPT-Efficio:</p><p>1. Layer-wise Parameter Sharing: This technique involves sharing the same set of parameters across all transformer layers, which significantly diminishes the model size. This can be practically implemented, for instance, in PyTorch by defining a single transformer layer that is repeatedly employed in a loop for all layers. However, this method might potentially curtail the model's expressiveness, given that identical transformations are propagated at each layer.</p><p>2. Sub-layer Parameter Sharing: Here, the same parameters are shared between two principal components of each transformer layer, namely the self-attention and feed-forward sub-layers. This method further contributes to the reduction of model complexity.</p><p>3. Sub-layer Parameter Sharing in Groups: In this approach, the same parameters are shared between two principal components of each transformer layer, namely the self-attention and feed-forward sub-layers in groups. For example if the group number is 1, all the parameters are shared across all layers, if the group number is 2, the parameters will be shared between half of the layers and so on. The number of groups is a hyperparameter that should be optimized for the downstream tasks.</p><p>While the benefits of parameter sharing are evident in terms of model size reduction and potential generalization improvements, it is important to consider that these benefits may not uniformly translate across all problems. The risk of limiting the model's capacity and expressiveness is a noteworthy drawback that necessitates careful consideration of the specific use case when deciding on the adoption of parameter sharing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this chapter we present the results of each of our approaches in the context of language modeling (i.e. completion tasks) and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>The implementation of parameter sharing strategies in GPT-Efficio, yields an array of outcomes that significantly affect the performance and computational efficiency of these models.</p><p>Positive outcomes include:</p><p>1. Diminished Model Complexity: Parameter sharing can drastically reduce the count of unique trainable parameters, thereby decreasing the overall complexity of the model. This benefit is particularly advantageous in scenarios where computational resources or memory availability are constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Expedited</head><p>Training Process: Given fewer unique parameters, the training process can be substantially accelerated. This advantage becomes increasingly significant when dealing with large-scale datasets or in scenarios where time efficiency is a key consideration.</p><p>3. Enhanced Generalization: The adoption of parameter sharing could prompt the model to learn more generalized features that apply across various parts of the input. For instance, sharing weights across all positions in a sequence may force the model to learn positionindependent features. Similarly, sharing parameters across different layers or sub-layers can promote the extraction of features with broader applicability <ref type="bibr" target="#b4">[Bengio et al., 1994]</ref>.</p><p>4. Mitigation of Overfitting: By reducing the number of parameters, parameter sharing strategies can contribute to the prevention of overfitting, especially in situations where the training data available is relatively scarce.</p><p>Conversely, there exist drawbacks that merit consideration:</p><p>1. Limited Model Expressivity: Although parameter sharing can contribute to complexity reduction and improved generalization, it could concurrently constrain the model's capacity.</p><p>If identical parameters are applied across different layers or sub-layers, the model might be inhibited in its ability to learn distinct representations at different depths of the network, potentially compromising its performance on complex tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Risk of Sub-optimal</head><p>Solutions: There may be instances where the optimal solution necessitates different parameters in different components of the model. In such cases, parameter sharing could potentially culminate in sub-optimal solutions.</p><p>3. Dependence on Task and Dataset Specificity: The effectiveness of parameter sharing can be heavily influenced by the specific task and dataset in use. It is not guaranteed to invariably lead to performance improvements and could, in certain cases, negatively impact the model's performance.</p><p>Table <ref type="table" target="#tab_3">1</ref> demonstrates the GPT-Efficio performance in comparison with GPT-3.</p><p>Table 2 shows the GPT-Efficio performance in comparison with GPT-3. G P T -3 Z e r o -S h o t G P T -3 O n e -S h o t G P T -3 F e w -S h o t G P T -E f fi c io 0 20 40 60 80 100 76.2 72.5 86.4 67.1 83.2 84.7 87.7 80.5 78.9 78.1 79.3 72.6 Accuracy LAMBADA acc LAMBADA ppl StoryCloze acc Figure 1: Performance of parameter sharing approach on completion tasks G P T -3 Z e r o -S h o t G P T -3 O n e -S h o t G P T -3 F e w -S h o t G P T -E f fi c io 0 20 40 60 80 14.6 23 29.9 27.5 18.4 25.3 41.5 40.6 64.3 68 71.2 69.2 Accuracy NQ WebQ TriviaQA Figure 2: Performance of parameter sharing approach on QA tasks </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>This section investigates the effects of each hyperparameter in a transformer model and its influences on the performance:</p><p>1. Model Size (d model ): A larger model size typically enables the model to learn more complex representations, but it also increases the risk of overfitting and requires more computational resources.</p><p>2. Number of Layers (num layers ): More layers allow the model to learn more complex, hierarchical representations. However, adding too many layers can lead to difficulties in training due to problems like vanishing or exploding gradients.</p><p>3. Number of Heads (num heads ): More heads allow the model to focus on different parts of the input for each head. This can lead to better performance, but it also increases the computational cost and the risk of overfitting.</p><p>4. Feed Forward Network Dimension (d f f ): Increasing this value allows the feed-forward network to learn more complex mappings and can lead to better performance, but it also increases the model size and the risk of overfitting. 5. Learning Rate: If set too high, the model might converge too quickly to a suboptimal solution or might not converge at all. If set too low, the learning process can become too slow. There's usually a sweet spot that needs to be found via experimentation.</p><p>6. Batch Size: Larger batch sizes mean more stable and accurate gradient estimates, at the cost of increased memory usage. However, it's also been found that too large batch sizes can lead to poorer generalization <ref type="bibr" target="#b29">[Raffel et al., 2020]</ref>.</p><p>7. Epochs: Training for more epochs can lead to better performance on the training set, but also increases the risk of overfitting. Early stopping techniques can be used to mitigate this. 8. Warmup Steps: This hyperparameter is specific to the learning rate scheduler used in transformers. It helps in stabilizing the learning rate during the initial phase of training. 9. Dropout Rate: Dropout is a regularization technique. A higher dropout rate increases the amount of regularization, which can help prevent overfitting.</p><p>10. Max Sequence Length: This hyperparameter can affect both the computational cost and the kinds of sequences the model can handle. If set too small, important context might be lost.</p><p>11. Parameter Sharing Group Size: Grouped parameter sharing can help reduce overfitting by reducing the number of unique parameters. However, if set too high, the model might not be able to learn complex hierarchical representations effectively.</p><p>12. Optimizer: The choice of optimizer can significantly impact model performance and convergence speed. Adam is a common choice for transformer models due to its efficient handling of sparse gradients and adaptive learning rates.</p><p>13. Activation Function: This function adds non-linearity to the model, enabling it to learn more complex patterns. 'relu' and 'gelu' are common choices in transformer models.</p><p>14. Weight Initialization: Proper initialization can help prevent issues such as vanishing or exploding gradients, leading to faster and more stable convergence.</p><p>15. Gradient Clipping: This is used to prevent exploding gradients, which can cause numerical instability and poor performance <ref type="bibr" target="#b18">[Hendrycks and Gimpel, 2016]</ref>.</p><p>16. Learning Rate Decay: This involves reducing the learning rate as training progresses to enable fine-tuning of the model parameters in the later stages of training. 17. Regularization Techniques: Apart from Dropout, there are other regularization techniques like L1 and L2 regularization, and also techniques like layer normalization that help to stabilize learning and prevent overfitting.</p><p>The appropriate setting of these hyperparameters is task-dependent and usually requires a fair amount of trial and error, or more systematic approaches like grid search or Bayesian optimization, to find the most effective combination. They can significantly affect the time it takes to train your model, the quality of the model, and its ultimate performance.</p><p>Table <ref type="table" target="#tab_5">3</ref> shows the effect of changing the d model hyperparameter on the model's performance: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Increase Decrease</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Complexity</head><p>Increases the capacity of the model to learn more complex representations.</p><p>Reduces the capacity of the model, potentially making it unable to learn complex patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Risk of Overfitting</head><p>Increases, as a larger model with more parameters might fit the noise in the training data.</p><p>Reduces, as a smaller model has less capacity to fit the noise in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resources</head><p>Requires more computational resources, including memory and processing power.</p><p>Requires fewer computational resources. <ref type="bibr" target="#b1">[Ba and Caruana, 2014]</ref> Training Time</p><p>Likely increases due to the increased number of parameters.</p><p>Likely decreases due to fewer parameters to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Ability</head><p>Might decrease if the model is too large and starts to overfit.</p><p>Might increase up to a point, as a smaller model may generalize better, but if the model is too small, it might underfit the data.</p><p>It should be noted that these effects are not absolute and may vary depending on the other hyperparameters and the specific task the model is being trained for. The optimal d model value usually requires empirical tuning. Table 4 demonstrates the GPT-Efficio performance in comparison with GPT-3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Parameter sharing techniques in transformer-based models are used to mitigate issues related to the high computational cost and complexity that accompany these models. However, despite the apparent benefits, there are several limitations associated with the application of these techniques:</p><p>• Limited Model Expressivity: One of the most pronounced drawbacks of parameter sharing is the potential reduction in model expressivity. By using the same parameters across different layers or parts of the model, we inherently restrict the ability of the model to learn and represent diverse features and relationships. This could lead to a lack of depth in the learned representations, potentially impacting the model's performance on complex tasks that require the learning of diverse and intricate patterns.</p><p>• Sub-optimal Solutions: In certain cases, the optimal solution might require unique parameters in different parts of the model. By enforcing parameter sharing, we might be pushing the model towards sub-optimal solutions. This could lead to poor model performance, especially in tasks where the learning of unique representations at different parts or depths of the model is critical.</p><p>• Task and Data Dependence: The effectiveness of parameter sharing techniques is not universal and depends heavily on the specific task and data at hand. For instance, parameter sharing might work well for some tasks, such as text generation or sequence-to-sequence prediction, where recurrent patterns exist. However, for tasks where the requirement of unique representations across the model is high, parameter sharing could negatively impact performance.</p><p>• Risk of Over-simplification: While reducing complexity is one of the motivations behind parameter sharing, there's a risk of oversimplifying the model to the extent that it fails to capture the necessary complexity of the data. This can lead to underfitting, where the model is too simple to capture the underlying structure of the data, leading to poor performance.</p><p>• Difficulty in Optimization: Sharing parameters can make the optimization landscape more complex. When parameters are tied, a change in one location means a change in another, potentially leading to more difficult optimization and slower convergence.</p><p>While parameter sharing offers valuable advantages, it's not a one-size-fits-all solution. Consideration must be given to the specific nature of the task, the complexity of the data, and the capacity of the model to ensure that parameter sharing benefits outweigh the potential limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>Given the demonstrated effectiveness as well as the limitations of parameter sharing techniques in transformer-based models like GPT, future work in this domain could be directed towards several interesting and promising avenues:</p><p>• Adaptive Parameter Sharing: Developing methods for adaptive parameter sharing could be an exciting line of research. These methods would adjust the extent of parameter sharing based on the specific task or data. This could involve learning which parts of the model should share parameters and to what extent, potentially through the use of techniques like reinforcement learning or meta-learning.</p><p>• Hybrid Parameter Sharing Schemes: Exploring hybrid parameter sharing schemes that combine different types of parameter sharing could be beneficial. For instance, some layers could share parameters while others could have unique parameters, or different types of parameter sharing could be used for different parts of the model.</p><p>• Task-specific Parameter Sharing Strategies: Investigating task-specific parameter sharing strategies could lead to improvements in performance. For example, different parameter sharing techniques could be developed and evaluated for tasks like text generation, translation, summarization, or question-answering.</p><p>• Learning Shared and Unique Representations: Research could be directed towards models that can learn both shared and unique representations simultaneously. This could involve architectures that have a mix of shared and unique parameters, allowing them to benefit from the generalization of parameter sharing while also being able to learn task or dataspecific features.</p><p>• Theoretical Analysis of Parameter Sharing: Further theoretical analysis of parameter sharing could lead to a better understanding of its impacts on model capacity, generalization, optimization, and other aspects of model performance. This could involve both analysis of existing methods and the development of new theoretical models or frameworks.</p><p>• Improved Optimization Techniques: Given the potential difficulties in optimizing models with shared parameters, developing improved optimization techniques for such models could be beneficial. This could include new training algorithms or regularization techniques designed specifically for models with parameter sharing.</p><p>• Evaluation Across Diverse Tasks and Datasets: Lastly, systematic evaluation of parameter sharing techniques across a wider range of tasks and datasets could provide valuable insights into when and why these techniques are effective or ineffective. This could guide the development of more robust and adaptable parameter sharing methods.</p><p>By pursuing these directions, future work could overcome some of the limitations of current parameter sharing techniques and lead to more efficient and effective transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this approach, we introduced a new Large Language Model, GPT-Efficio, that implements two techniques we call "Embedding layer factorization" and "Parameter sharing across layers". The application of parameter sharing strategies in transformer-based models such as the Generative Pretrained Transformer (GPT) can produce a myriad of potential outcomes, each with significant implications for the model's performance, computational efficiency, and generalization capabilities.</p><p>In particular, these techniques offer a way to address several challenges endemic to machine learning, such as reducing model complexity, speeding up training processes, improving generalization, and mitigating overfitting risks. These potential benefits highlight the value of parameter sharing, especially in situations where computational resources, memory, or time are constraining factors, or where the available training data is limited.</p><p>While parameter sharing techniques provide promising avenues for optimizing transformer-based models, the decision to implement such strategies should be made judiciously, guided by the specific requirements of the task and empirical evidence. Careful considerations must be made to balance the trade-off between model complexity and expressivity, and extensive experimentation and validation are crucial to ascertain the most effective application of these methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance of parameter sharing approach on completion tasks</figDesc><table><row><cell>Model</cell><cell>nparams</cell><cell>LAMBADA</cell><cell>LAMBADA</cell><cell>StoryCloze</cell><cell>HellaSwag</cell></row><row><cell></cell><cell></cell><cell>(acc)</cell><cell>(ppl)</cell><cell>(acc)</cell><cell>(acc)</cell></row><row><cell>GPT-3 Zero-Shot</cell><cell>175B</cell><cell>76.2</cell><cell>3.00</cell><cell>83.2</cell><cell>78.9</cell></row><row><cell>GPT-3 One-Shot</cell><cell>175B</cell><cell>72.5</cell><cell>3.35</cell><cell>84.7</cell><cell>78.1</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>175B</cell><cell>86.4</cell><cell>1.92</cell><cell>87.7</cell><cell>79.3</cell></row><row><cell>GPT-Efficio</cell><cell>950M</cell><cell>67.1</cell><cell>9.2</cell><cell>80.5</cell><cell>72.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance of parameter sharing approach on QA tasks</figDesc><table><row><cell>Model</cell><cell>nparams</cell><cell>NQ</cell><cell>WebQ</cell><cell>TriviaQA</cell></row><row><cell>GPT-3 Zero-Shot</cell><cell>175B</cell><cell>14.6</cell><cell>14.4</cell><cell>64.3</cell></row><row><cell>GPT-3 One-Shot</cell><cell>175B</cell><cell>23.0</cell><cell>25.3</cell><cell>68.0</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>175B</cell><cell>29.9</cell><cell>41.5</cell><cell>71.2</cell></row><row><cell>GPT-Efficio</cell><cell>950M</cell><cell>27.5</cell><cell>40.6</cell><cell>69.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Effects of hyperparameter d model</figDesc><table><row><cell>Hyperparameter:</cell></row><row><cell>d model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Analysis of the effects of hyperparameter d model on completion tasks</figDesc><table><row><cell>Model</cell><cell cols="3">d model nparams LAMBADA</cell><cell>LAMBADA</cell><cell>StoryCloze</cell><cell>HellaSwag</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(acc)</cell><cell>(ppl)</cell><cell>(acc)</cell><cell>(acc)</cell></row><row><cell>GPT-3 Zero-Shot</cell><cell cols="2">12288 175B</cell><cell>76.2</cell><cell>3.00</cell><cell>83.2</cell><cell>78.9</cell></row><row><cell>GPT-3 One-Shot</cell><cell cols="2">12288 175B</cell><cell>72.5</cell><cell>3.35</cell><cell>84.7</cell><cell>78.1</cell></row><row><cell>GPT-3 Few-Shot</cell><cell cols="2">12288 175B</cell><cell>86.4</cell><cell>1.92</cell><cell>87.7</cell><cell>79.3</cell></row><row><cell>GPT-Efficio</cell><cell>2048</cell><cell>950M</cell><cell>67.1</cell><cell>9.2</cell><cell>80.5</cell><cell>72.6</cell></row><row><cell>GPT-Efficio</cell><cell>1536</cell><cell>660M</cell><cell>58.83</cell><cell>11.76</cell><cell>58.49</cell><cell>68.34</cell></row><row><cell>GPT-Efficio</cell><cell>1024</cell><cell>290M</cell><cell>47.79</cell><cell>14.43</cell><cell>50.21</cell><cell>59.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Analysis of the effects of hyperparameter d model on completion tasks</figDesc><table><row><cell>shows the GPT-Efficio performance in comparison with GPT-3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Analysis of the effects of hyperparameter d model on QA tasks Analysis of the effects of hyperparameter d model on QA tasks</figDesc><table><row><cell>Model</cell><cell>d model</cell><cell>nparams</cell><cell>NQ</cell><cell>WebQ</cell><cell>TriviaQA</cell></row><row><cell>GPT-3 Zero-Shot</cell><cell>12288</cell><cell>175B</cell><cell>14.6</cell><cell>14.4</cell><cell>64.3</cell></row><row><cell>GPT-3 One-Shot</cell><cell>12288</cell><cell>175B</cell><cell>23.0</cell><cell>25.3</cell><cell>68.0</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>12288</cell><cell>175B</cell><cell>29.9</cell><cell>41.5</cell><cell>71.2</cell></row><row><cell>GPT-Efficio</cell><cell>2048</cell><cell>950M</cell><cell>27.5</cell><cell>40.6</cell><cell>69.2</cell></row><row><cell>GPT-Efficio</cell><cell>1536</cell><cell>660M</cell><cell>19.32</cell><cell>31.86</cell><cell>58.49</cell></row><row><cell>GPT-Efficio</cell><cell>1024</cell><cell>290M</cell><cell>15.28</cell><cell>27.67</cell><cell>50.21</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a human-like opendomain chatbot</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</title>
		<meeting>the 2021 ACM conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text classification for online conversations with machine learning on aws</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liutong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourav</forename><surname>Bhabesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AWS Machine Learning Blog</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of the askmsr question-answering system</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">267</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5232" to="5270" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Khashe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.09921</idno>
		<title level="m">Alexa, predict my flight delay</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flight delay prediction using deep learning and conversational voicebased agents</title>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Khashe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Academic Scientific Research Journal for Engineering, Technology, and Sciences</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="72" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Zero-shot open-book question answering</title>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11520</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You don&apos;t need labeled data for open-book question answering</title>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Create, train, and deploy a billion-parameter language model on terabytes of data with tensorflow and amazon sagemaker</title>
		<author>
			<persName><forename type="first">Sia</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">Calderon</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Rauschmayr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AWS Machine Learning Blog</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04836</idno>
		<title level="m">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Troubling trends in machine learning scholarship</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08345</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The structure and performance of an open-domain question answering system</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="563" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">connections are expressive enough: Universal approximability of sparse transformers</title>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13783" to="13794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1253</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
