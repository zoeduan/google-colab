<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)</title>
				<funder ref="#_qYwPb9B">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-29">29 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
							<email>jianzhong.qi@unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuqing</forename><surname>Li</surname></persName>
							<email>zuqingl@student.unimelb.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Egemen</forename><surname>Tanin</surname></persName>
							<email>etanin@unimelb.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-29">29 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">AF4EB2A001C365D1B9B07C86E1B81769</idno>
					<idno type="DOI">10.1145/3589132.3625597</idno>
					<idno type="arXiv">arXiv:2309.17072v1[cs.DB]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatial Databases</term>
					<term>Large Language Models</term>
					<term>Model as a Database</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) are advancing rapidly. Such models have demonstrated strong capabilities in learning from large-scale (unstructured) text data and answering user queries. Users do not need to be experts in structured query languages to interact with systems built upon such models. This provides great opportunities to reduce the barrier of information retrieval for the general public. By introducing LLMs into spatial data management, we envisage an LLM-based spatial database system to learn from both structured and unstructured spatial data. Such a system will offer seamless access to spatial knowledge for the users, thus benefiting individuals, business, and government policy makers alike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>‚Ä¢ Information systems ‚Üí Spatial-temporal systems; Database query processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>spatial data representations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. These studies have focused on using ML to optimize the effectiveness or efficiency of modules of spatial database systems. The ML models developed help retrieve query results but do not change the general query processing paradigm: (1) Users submit queries in a special query language (e.g., SQL) <ref type="foot" target="#foot_0">1</ref> , which are parsed and translated into relational algebra expressions. (2) A query optimizer module analyzes the expressions and determines the order of execution (i.e., computes a query plan).</p><p>(3) The expressions are executed by a query execution engine, with the help of spatial indices to streamline the execution. (4) Relevant results are retrieved from the data tables and are returned to the users. This process may be repeated with altered queries until the intended results are found for the users.</p><p>The latest development of ML techniques, i.e., large language models (LLMs) such as ChatGPT, offer great opportunities to develop the next-generation spatial databases where, instead of using ML models for spatial database optimization, we envisage to use machine learning models as a spatial database (MaaSDB).</p><p>LLMs are neural network models with billions of parameters trained on a large text corpus. While such models are trained on simple tasks of predicting the next word in a sentence, they can capture the syntax and semantics of human language with a high accuracy. They can generate text and engage with human users in dialogues, to answer user questions and to perform text processing tasks. Most importantly, such models have been shown to be able to "memorize" the facts from the training data <ref type="bibr" target="#b6">[7]</ref>, effectively making themselves large data repositories with rich information.</p><p>In this paper, we present the vision of the next-generation spatial database systems exploiting the capability of LLMs to memorize facts from training data. We shift ML-based spatial database optimization from ML models for spatial databases to ML models as spatial databases. Such systems consist of ML models trained on structured and unstructured spatial data, which can generate query answers directly instead of retrieving data from tables.</p><p>There are important advantages that come with such systems:</p><p>(1) The ML models in such systems can learn from both structured and unstructured spatial data (e.g., tables and free text) and generate query results based on both types of data, unlike traditional systems where typically only either type of data are available, and it is difficult to link both types of data to answer complex queries.</p><p>(2) Since the ML models have full knowledge about the spatial data in the database, their inbuilt natural language-based user interface will be able to understand user intent better than existing text-to-SQL systems do, which have limited information about the underlying data. This leads to more relevant results returned from such systems and hence higher system usability. Such systems will significantly enhance the accessibility of spatial knowledge entailed in data stored in spatial databases. For example, a tourist may request such systems to generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23; an urban planner may request such systems to return the top-10 suburbs with the highest electric vehicle ownership-charging station ratios. Search engines like Google may retrieve partial answers to such queries through keyword matching, which however are limited by the availability of directly matched web documents, again due to the retrieval nature of the query processing procedure.</p><p>We make the following contributions: (1) We envisage a unified spatial database system that uses ML models as its core query execution engine to optimize user accessibility and system usability.</p><p>(2) We conduct a pilot experimental study to verify the feasibility of such a system. (3) We identify key research challenges and opportunities in realizing such a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review studies on ML-based (spatial) database optimization.</p><p>Existing works focus on using ML techniques to optimize the effectiveness and efficiency of different modules of spatial database systems, e.g., using ML models to replace (e.g., RSMI <ref type="bibr" target="#b12">[13]</ref>) or to optimize (e.g., RLR-tree <ref type="bibr" target="#b3">[4]</ref>) the structure of traditional spatial indices <ref type="bibr" target="#b25">[26]</ref>. A study <ref type="bibr" target="#b0">[1]</ref> trains autoencoder models to compute spatial embeddings, i.e., vectors encoding dataset characteristics such as distribution to help predict range query selectivity for spatial query optimization. Other studies compute embeddings for spatial objects (e.g., road segments <ref type="bibr" target="#b1">[2]</ref> or trajectories <ref type="bibr" target="#b23">[24]</ref>) to encode their spatial features for spatial query processing. In these studies, the ML models are second-class citizens -they help retrieve query results but do not change the classic retrieval-based query paradigm.</p><p>A few other studies use ML models to answer spatial queries directly. For example, Qi et al. <ref type="bibr" target="#b13">[14]</ref> train a feedforward neural network (FFN) to predict the shortest-path distance given two points on a road network. Zeighami et al. <ref type="bibr" target="#b24">[25]</ref> train FFNs to predict the answer for range count queries. While these studies show that ML models can memorize facts from spatial data, they focus on aggregate queries. Their models output scalar values and not data records. They do not have a natural language-based user interface.</p><p>In a broader context of database research, there are text-to-SQL studies <ref type="bibr" target="#b4">[5]</ref> that train ML models to translate textual queries into SQL queries, thus providing a natural language-based user interface. These models may exploit meta data such as column names of the data tables. However, they do not generate query results directly and typically do not access the actual data records at training.</p><p>Motivated by the strong performance of LLMs, several vision papers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> use pre-trained LLMs or transformer (the building block of LLMs)-based models trained on unstructured data to answer database queries. These papers share similar visions with ours in that they also envisage ML models to become first-class citizens in a database system. They differ from our vision in that they do not consider structured spatial data and the challenges. Tan <ref type="bibr" target="#b15">[16]</ref> presents several challenges on query processing over structured data with LLMs without envisaging a solution. A couple of other studies apply LLMs with structured data. Urban and Binnig <ref type="bibr" target="#b18">[19]</ref> extract tables from a document using LLMs, while Nobari and Rafiei <ref type="bibr" target="#b9">[10]</ref> transform tables into a desired representation for better joinability. They do not use the learned models to generate query answers directly. Overall, none of these studies consider the specific challenges and opportunities brought by LLMs to spatial databases. Our paper fills this gap. Musleh et al. <ref type="bibr" target="#b8">[9]</ref> envisage a BERT-based system for trajectory analysis, while Xue et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> use language models for time series forecasting, exploiting the analogy between trajectories/time series and sentences. Our study complements the studies by considering spatial data and queries beyond trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PILOT STUDY 3.1 The Vision of the Future System</head><p>We envisage a next-generation spatial database system as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. This system consists of a query analyzer and query plan generator, a set of query result generators, and a result synthesizer, which are all formed by ML models and are connected together to generate answers for user queries. The system provides a natural language-based interface for users to query the spatial knowledge learned by the ML models from spatial data stored in the system.</p><p>Query analyser and query planner Unstructured spatial data Spatial data tables Result synthesizer MaaSDB User submit queries return results Query result generators model training Users can interact with the system (e.g., via a computer or a smartphone) to submit queries in natural language. Upon receiving a query (e.g., generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23), the query analyzer and query plan generator (which may be an LLM) will analyze the query intent, generate sub-queries, and assign the subqueries to the relevant subset of the query result generators (e.g., a sub-query to find the conference venue of SIGSPATIAL'23 and a sub-query to find POIs within walking distance from the conference venue). The invoked query result generators will generate an answer for each sub-query. Different types of query result generators will be built by training on different types of data. For example, a (transformer-based) query result generator trained on unstructured conference web pages will be able to answer the sub-query on the conference venue, while a (GAN-based, detailed in Section 3.2) query result generator trained on a table of POIs in Hamburg will be able to answer the sub-query about the POIs. When the results of all sub-queries have been generated, the result synthesizer (which may be another LLM) will combine them based on the user query and generate the final query answer to be returned to the user.</p><p>Multiple challenges and research opportunities arise from the envisaged system, which will be discussed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminary Experimental Study</head><p>We verify the feasibility of the envisaged spatial database system through a preliminary experimental study. Settings. We focus on structured data, since the aforementioned recent vision papers have shown the feasibility of using pre-trained LLMs or transformer-based models to answer certain types of database queries. We train an ML model on a data table and study how well the model can remember the (key characteristics of the) data.</p><p>We use a multi-dimensional dataset (instead of a table of just spatial coordinates, for generality) named CensusIncome. <ref type="foot" target="#foot_1">2</ref> The dataset has 48,842 records, each with 8 categorical (e.g., occupation and marital status) and 6 numeric (e.g., age and capital gain) attributes. Since the dataset does not come with a query workload, we follow a previous study <ref type="bibr" target="#b10">[11]</ref> and generate 20,000 range queries with randomly selected numeric attributes and ranges.</p><p>We use a generative adversarial network (GAN)-based model which has been shown to be able to generate tabular data <ref type="bibr" target="#b11">[12]</ref>. We train a GAN model with the dataset and test how well it can generate data records that preserve the data distribution and answer range count queries, i.e., given a query range, we return the number of records in the range. A GAN model has a generator module ùê∫ and a discriminator module ùê∑. The generator generates a record given a random noise vector, z, as its input, while the discriminator classifies whether the generated record (i.e., ùê∫ (z)) resembles a real record from the training dataset. The model is trained with a loss function that aims to generate records that cannot be distinguished from the real records. We adapt the loss function of the generator to add a Q-Error <ref type="bibr" target="#b7">[8]</ref> loss term, which measures how well a generated table preserves the selectivity of given range queries, as follows:</p><formula xml:id="formula_0">min ùê∫ L (ùê∫ ) = E z‚àºùëùz (z) [log(1-ùê∑ (ùê∫ (z) ) ) ]+ 1 ùëÅ ‚àëÔ∏Å ùëñ max 1, ùë†ùëíùëô (ùëû ùëñ ) ≈ù ùëíùëô (ùëû ùëñ ) , ≈ù ùëíùëô (ùëû ùëñ ) ùë†ùëíùëô (ùëû ùëñ )</formula><p>The second term here is the Q-Error loss, where ùëÅ denotes the number of range queries, ùëû ùëñ denotes the ùëñth range query, ùë†ùëíùëô (ùëû ùëñ ) denotes the ground truth selectivity of ùëû ùëñ on the training dataset, and ≈ù ùëíùëô (ùëû ùëñ ) denotes the selectivity of ùëû ùëñ on the generated table. We name the adapted GAN model RC-GAN. We omit the detailed model structure and hyperparameter values due to space limit.</p><p>The experiments are run on a desktop computer with a 16-core CPU, 32 GB memory, and 24 GB GPU memory.</p><p>Results. We train RC-GAN (implemented with PyTorch 1.13.1) on the CensusIncome dataset in 10 epochs (which take about an hour) and use the trained model to generate a table of the same size of the dataset. We report the Q-Error of the generated data in Table <ref type="table" target="#tab_0">1</ref>, where "Training queries" refers to computing the Q-Error with the 20,000 range queries as described above, which have been used in model training, while "Testing queries" refers to computing the Q-Error with another set of 5,000 range queries that are generated separately (with the same procedure) and have not been seen at training. We can see that the median Q-Errors are very close to 1 under both settings, i.e., the generated table has almost the same query selectivity as the original dataset for half of the queries. The Q-Errors at the 75th percentile are still within 2, while they only deteriorate to larger values at the 90th percentile. Importantly, the Q-Errors for the testing queries are close to those for the training queries. These results demonstrate the potential of ML models to "memorize" the key characteristics of structured data records. We further train two Gradient Boosting classifiers with 15% of the CensusIncome dataset and with 5% of the CensusIncome dataset plus 10% of data generated by RC-GAN, respectively. The classifiers predict if the income attribute of a record is greater than 50,000 given the other attributes. We test the classifiers on 1,000 randomly selected records of CensusIncome not seen at training. Table <ref type="table" target="#tab_1">2</ref> reports the results. We see that the classifiers trained under both settings have very close performance, confirming the capability of RC-GAN to "memorize" the data distribution characteristics.</p><p>Our results above are obtained with an ML model where the number of parameters is at the thousand scale. When larger models with more parameters are available, even better results are expected.</p><p>Learning spatial knowledge with LLMs. To provide further evidence on LLMs' potential to learn spatial knowledge, we query ChatGPT with prompts: the geo-coordinates of the top 50 cities in Australia are and can you give me more cities, until 50 cities were returned. The returned geo-coordinates were correct for 49 cities, with only the geo-coordinates of Hervey Bay (a small city in Queensland) being off by 10 km. We further randomly pair up the cities to form 50 pairs. For every pair of cities ùê¥ and ùêµ, we query Chat-GPT with prompt: ùê¥ is to which side of ùêµ. The returned position results were correct for 44 pairs, with another 5 pairs obtained correct results after the geo-coordinates are further included in the prompt. Only one pair (Canberra and Orange) retained a wrong result (southwest was returned while the answer should be south).</p><p>These demonstrate the potential of LLMs to learn spatial knowledge and answer queries, and the research opportunities to train such models to answer more complex queries faithfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS AND CHALLENGES</head><p>We presented a next-generation spatial database system. This system treats ML models as first-class citizens and trains such models to "memorize" data stored in a spatial database and to generate query answers. It enables a new generation-based query paradigm that replaces the traditional retrieval-based paradigm.</p><p>The system will significantly enhance the accessibility of spatial database systems, as the ML models can offer an inbuilt natural language-based user interface and well understand users' query needs. It will bring huge benefits in spatial analytics and query processing, encouraging a new generation of location-based services and allowing better-informed location-based decision making.</p><p>To realize such a system, there are various challenges, a subset of which are summarized below. Simply fine-tuning an open-sourced LLM such as Llama 2 directly cannot realize the system.</p><p>(1) Faithful query result generation. Being able to generate query results directly without an extra data retrieval process offers great opportunities to answer complex spatial (analytical) queries. This, however, also brings significant challenges to ensure the faithfulness of the results generated. ML models are known to return inaccurate results. In terms of LLMs, hallucination is a known problem that impinges LLMs' wider applicability. When LLMs are applied to form the query engine of spatial databases, it is important to address the hallucination problem, e.g., to build a system that returns faithfulness scores together with the generated answers. A unique opportunity arises when building such a system for spatial databases, as traditional retrieval-based query procedures can be applied in parallel to compute query answers that serve as the ground truth for training the faithfulness scoring module.</p><p>(2) Large model training with structured spatial data. There are two major issues that prevent training LLMs on structured data records directly (which are probably the reason why the other vision papers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> did not take this approach): (i) There is limited availability of structured spatial data. Comparing with the volume of free texts (e.g., web documents), the number of spatial data tables available is much smaller. The number of data tables in a spatial database is even smaller. How to train an LLM generalizable to different queries with data in such smaller scale is challenging. (ii) There is incompatibility between structured spatial data and the training procedure of LLMs. LLMs are trained via the task of predicting the next word in a sentence. Simply treating every spatial data record as a sentence and every data field as a word to train an LLM is ineffective. This is because words in a sentence have a strong correlation, and the context of a word implies the semantics of the word. In contrast, different fields of a data record may be much less relevant, and the nearby fields of a value do not necessarily imply the semantics of the value. Further, values in a data record may be numeric and continuous, and the same value may have completely different meanings in different fields, while words are discrete and each word has much fewer different meanings. Novel model design and training procedures are needed for structured spatial data.</p><p>(3) Versatile query processing. A problem related to the difficulty in model training given limited structured spatial data is how to answer different types of spatial queries using a model trained with limited data. While data of limited scale may be easier to be "memorized" by an ML model, it does not help train a model that is generalizable to different types of queries. Also due to the limited scale of data, the trained models may not have seen too many different prompts that imply different types of queries. The generalizability of the trained models would most likely need to come from unstructured spatial data, e.g., the Wikipedia article of a POI. Algorithms to fine-tune such models and incorporate knowledge from structured spatial data await exploration.</p><p>Further, the models need to have multi-step reasoning capabilities to answer complex spatial queries. For example, to "generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23" would require (i) producing the location of the conference venue, (ii) producing POIs within walking distance around it, and (iii) selecting and ordering the POIs to form a trip. While prompting, training, and fine-tuning strategies have been proposed for this issue, achieving such advanced reasoning capabilities remains an open challenge <ref type="bibr" target="#b2">[3]</ref>.</p><p>(4) Challenges in managing ML models for data management.</p><p>There are inherent problems in data management with ML models, such as how to update the trained ML models when the underlying data have changed (e.g., moving objects <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>). Such challenges have been discussed in the literature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> and are not reiterated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the future spatial database system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Q-Error of RC-GAN</figDesc><table><row><cell>Query set</cell><cell cols="2">Median 75th 90th</cell></row><row><cell>Training queries</cell><cell>1.23</cell><cell>1.71 3.24</cell></row><row><cell>Testing queries</cell><cell>1.25</cell><cell>1.86 4.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification Accuracy with Generated Data</figDesc><table><row><cell>Training data</cell><cell cols="2">Precision Recall F1</cell></row><row><cell>5% of CensusIncome + 10% of RC-GAN</cell><cell>0.79</cell><cell>0.97 0.87</cell></row><row><cell>15% of CensusIncome</cell><cell>0.82</cell><cell>0.96 0.88</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>There are studies on using ML models to translate queries in natural language into SQL queries (text-to-SQL)<ref type="bibr" target="#b4">[5]</ref>, but not yet for spatial queries to the best of our knowledge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://archive.ics.uci.edu/dataset/20/census+income</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by <rs type="funder">Australian Research Council (ARC)</rs> <rs type="projectName">Discovery</rs> Project <rs type="grantNumber">DP230101534</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_qYwPb9B">
					<idno type="grant-number">DP230101534</idno>
					<orgName type="project" subtype="full">Discovery</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial Embedding: A Generic Machine Learning Model for Spatial Query Optimization</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Belussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Migliorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Eldawy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGSPATIAL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatial Structure-Aware Road Network Embedding via Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Yanchuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egemen</forename><surname>Tanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faith and Fate: Limits of Transformers on Compositionality</title>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Sclar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Management of Data</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning Approaches for Text-to-SQL</title>
		<author>
			<persName><forename type="first">George</forename><surname>Katsogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Meimarakis</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Koutrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cao</surname></persName>
		</author>
		<title level="m">AI Meets Database: AI4DB and DB4AI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In SIGMOD</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human Language Understanding &amp; Reasoning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Preventing Bad Plans by Bounding the Impact of Cardinality Estimation Errors</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Moerkotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Let&apos;s Speak Trajectories</title>
		<author>
			<persName><forename type="first">Mashaal</forename><surname>Musleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">F</forename><surname>Mokbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofiane</forename><surname>Abbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGSPATIAL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Dargahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobari</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Davood</forename><surname>Rafiei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.06748</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards an Analysis of Range Query Performance in Spatial Data Structures</title>
		<author>
			<persName><forename type="first">Bernd-Uwe</forename><surname>Pagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Werner</forename><surname>Six</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Toben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Widmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Noseong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gorde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushil</forename><surname>Jajodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Synthesis Based on Generative Adversarial Networks. PVLDB</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Kulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Effectively Learning Spatial Indices. PVLDB</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Learning Based Approach to Predict Shortest-Path Distances</title>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowei</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.00472</idno>
		<title level="m">Querying Large Language Models with SQL</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models?</title>
		<author>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13010</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Natural Language Processing to Neural Databases</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Trappolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodol√†</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01447</idno>
		<title level="m">Multimodal Neural Databases</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Binnig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.13559</idno>
		<title level="m">Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous Visible K Nearest Neighbor Query on Moving Objects</title>
		<author>
			<persName><forename type="first">Yanqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time Continuous Intersection Joins over Large Sets of Moving Objects Using Graphic Processing Units</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translating Human Mobility Forecasting through Natural Language Generation</title>
		<author>
			<persName><forename type="first">Flora</forename><forename type="middle">D</forename><surname>Hao Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongli</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging Language Foundation Models for Human Mobility Forecasting</title>
		<author>
			<persName><forename type="first">Bhanu</forename><surname>Hao Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flora</forename><forename type="middle">D</forename><surname>Prakash Voutharoja</surname></persName>
		</author>
		<author>
			<persName><surname>Salim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGSPATIAL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">T3S: Effective Representation Learning for Trajectory Similarity Computation</title>
		<author>
			<persName><forename type="first">Peilun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Neural Database for Differentially Private Spatial Range Queries</title>
		<author>
			<persName><forename type="first">Sepanta</forename><surname>Zeighami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ghinita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards a Painless Index for Spatial Objects</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Stradling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
