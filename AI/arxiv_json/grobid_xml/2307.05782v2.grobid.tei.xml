<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-06">6 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Douglas</surname></persName>
							<email>mdouglas@cmsa.fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMSA</orgName>
								<orgName type="institution" key="instit2">Harvard University Dept. of Physics</orgName>
								<orgName type="institution" key="instit3">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-06">6 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">960539365E04C904AFBC25C831976B44</idno>
					<idno type="arXiv">arXiv:2307.05782v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial intelligence is making spectacular progress, and one of the best examples is the development of large language models (LLMs) such as OpenAI's GPT series. In these lectures, written for readers with a background in mathematics or physics, we give a brief history and survey of the state of the art, and describe the underlying transformer architecture in detail. We then explore some current ideas on how LLMs work and how models trained to predict the next word in a text are able to perform other tasks displaying intelligence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At the end of November 2022, OpenAI released a system called ChatGPT which interacts with its users in natural language. It can answer questions, engage in dialogs, translate between languages and write computer code with a fluency and ability far exceeding all previous publicly available systems. Although it falls well short of human abilities in many ways, still the large language model technology of which it is an example is widely considered to be a major advance in artificial intelligence. <ref type="foot" target="#foot_0">1</ref>Few developments in science and technology entered the popular consciousness as quickly as ChatGPT. There is no mystery about why. The ability to use language is a defining property of humanity, and for the first time a computer is doing this well enough to make a comparison with humans interesting. All of the hopes and fears which have developed around AI, robots and technology more generally are being brought into the discussion. In my opinion this is justified; the speed of recent progress makes it urgent to better understand AI, to forecast its capabilities and limitations, and to make wise decisions about its development and use. With great opportunities will come great challenges, which will concern all of us.</p><p>In these lecture notes we give an introduction to this subject for mathematicians, physicists, and other scientists and readers who are mathematically knowledgeable but not necessarily expert in machine learning or artificial intelligence. We begin with a very brief overview of AI in §2 to explain some ideas we consider to be essential context, the basic principles of the symbolic and connectionist approaches. In §3 we define statistical language models and relate the history of transformer-based LLMs up through GPT-4. In §4 we discuss measures of what LLMs do and how well they do it. We then give a precise explanation of simpler language models in §5 and the transformer architecture in §6.</p><p>It is amazing that a model defined by a few short equations, trained to go through a text and simply predict each next word as it appears -a task which seems only loosely related to any definition of intelligence -can do tasks which "obviously" require intelligence, such as solving word problems like the one in Figure <ref type="figure">1</ref> below. At present nobody really understands how this works. Even the interpretation of what LLMs are doing is controversial, ranging from the belief that they are "simply" rearranging the sentences they were trained on, all the way to the belief that the LLMs are learning sophisticated models of the world and that "simply" scaling up the computations will produce artificial general intelligence. Any forecast for progress must take into account the current models' shortcomings -lack of long term memory and ability to plan, tendency to make up facts and "hallucinate," unreliability in logical reasoning, etc.. Do these problems have technical solutions which will also look simple once we have them? Or are they more significant barriers?</p><p>Most current work on LLMs takes an engineering and problem solving per-and σ 2 = m 2 -1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>. We solve the system of equations:</p><formula xml:id="formula_0">n 2 -1 12 =<label>10</label></formula><formula xml:id="formula_1">m 2 -1 3 = 16.</formula><p>This translates into the system n 2 = 121</p><formula xml:id="formula_2">m 2 = 48 + 1 = 49.</formula><p>Therefore, n = 11 and m = 7 , so n + m = 18 .</p><p>Figure <ref type="figure">1:</ref> A question-answer pair solved by Minerva. From Lewkowycz et al <ref type="bibr" target="#b75">[77,</ref><ref type="bibr">1]</ref>, "Solving quantitative reasoning problems with language models," 2022.</p><p>spective, but there are many interesting works which focus more on understanding how LLMs work. One would think this should be far easier than understanding how human brains work, as we have full knowledge of an LLM's microscopic workings and can do a wide variety of experiments on it. 2 These efforts are in their early days, but in §7 we survey current approaches to understanding how LLMs do what they do. We conclude in §8 with more general discussion, some questions and potentially important developments to watch for. 3  Before we start, let me say a little about my own background. I was trained as a theoretical physicist and most of my contributions to science are in string theory and its interface with mathematics, but I have followed AI fairly closely since the 80's and in detail since 2016. In addition I spent eight years in quantitative finance where I gained a good deal of "hands-on" experience with machine learning. I have given many lectures telling computer scientists about physics and physicists about computational topics, and benefited from conversations with many people -more than I can name here, but let me thank Gerald Jay Sussman, David McAllester, Yann LeCun, Sanjeev Arora, Surya Ganguli, Jeremy Avigad, Vijay Balasubramanian, Dmitri Chklovskii, David Donoho, Steve Skiena, Christian Szegedy, Misha Tsodyks, Tony Wu and the many speakers in the CMSA New Technologies seminar series, 4 and Josef Urban and the AITP community. 5 Thanks to David McAllester and Sergiy Verstyuk for comments on the first draft. These notes would not have been possible without their input and advice, and I hope their signal to noise ratio approaches that of what they shared with me.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Symbolic and connectionist AI</head><p>The goal of artificial intelligence is to build computational systems which can perform tasks which require intelligence. Although intelligence is hard to define precisely, an operational definition suitable for LLMs is ability at tasks requiring language, reasoning, and planning, as judged by humans who interact with the system. Some famous and difficult "challenge tasks" include playing chess <ref type="bibr" target="#b120">[122]</ref>, proving mathematical theorems <ref type="bibr" target="#b102">[104]</ref>, and answering natural language questions using generally known facts and common sense. 6  These problems have been the subject of intense investigation since the mid-50's, and a few textbooks and histories are <ref type="bibr" target="#b91">[93,</ref><ref type="bibr" target="#b103">105,</ref><ref type="bibr" target="#b117">119,</ref><ref type="bibr" target="#b119">121]</ref>. Essentially from the start, two broad approaches were set out, which would later be called symbolic and connectionist AI. 7 The symbolic approach originated in mathematical logic and generative linguistic theory, and tracked the development of computer technology (both hardware and software) as a tool for solving practical and scientific problems. Central topics in this approach are formal logic and language theory, search and heuristics, and engineering techniques for designing and building large and complex systems.</p><p>Symbolic AI systems are designed, meaning that their creators develop a detailed understanding of the task, and encode this understanding into the system by programming, by hardware design and otherwise. As an example, consider the task of parsing: given an input string of words, determine its grammatical structure. Most of us learned to diagram sentences in elementary school, and although linguists have developed far more sophisticated notions of grammar, this simple notion gives the right idea. The grammar of a language is encoded in rules, which belong to a formal framework -see the appendix for the example of context-free grammars. Given such a framework, one can design a parsing algorithm which takes as input a rule set and an input string, and produces an output which states whether the string is a grammatical sentence and if so makes its structure explicit. This is a symbolic AI approach, not because the words "symbolize" anything (after all grammar does not have to come with mean-4 <ref type="url" target="https://live-hu-cmsa-222.pantheonsite.io/">https://live-hu-cmsa-222.pantheonsite.io/</ref> 5 <ref type="url" target="http://aitp-conference.org/2023/">http://aitp-conference.org/2023/</ref>  6 This challenge originates with Turing's famous test, but the restriction to question answering makes it better defined and testable using benchmarks, standardized question-answer sets. Discussion of the original test can be found at <ref type="url" target="https://en.wikipedia.org/wiki/Turing">https://en.wikipedia.org/wiki/Turing</ref> test 7 Symbolic AI is sometimes called "GOFAI" for "good old-fashioned AI." Related terms include "rule based," "logic based," "expert system" and "feature engineering." The connectionist approach has many other names, reflecting its mixed ancestry: "neural," "deep learning," "parallel distributed processing," "differentiable," and "representation learning." ing), but because the grammatical rules and the parsing algorithm (including its internal data structures) have a clear meaning to their designers.</p><p>Symbolic methods have had considerable success at many tasks requiring intelligence, famously including chess playing <ref type="bibr" target="#b58">[60]</ref> and symbolic algebra<ref type="foot" target="#foot_1">foot_1</ref> as well as more prosaic but very central tasks such as translating high level computer languages to machine code (compiling). And a great deal of work has been done to broaden their scope, for example to build question answering systems such as the well known IBM Watson. Many valuable techniques came out of this; ways to systematize rules into "knowledge bases" or "knowledge graphs," methods for automated logical reasoning, and so on. But it was long ago realized that once one goes beyond "formal worlds" such as chess and algebra to the complex and messy situations of real life, although one can postulate rules which capture many truths and can be used for reasoning, rules which are valid in all cases are very rare. Furthermore, the sheer number of rules required to cover even the likely possibilities is very large. These difficulties were addressed by implementing probabilistic reasoning and by getting teams of humans to develop the requisite enormous rule sets, leading to the "expert system" approach which was applied (for example) to medical question answering. Cyc, <ref type="foot" target="#foot_2">9</ref> an early and well known expert system, is commercially available and has a database of commonly known facts with over 25 million assertions; however this is dwarfed by knowledge bases such as Wikidata (over one billion "facts") but which do not have a systematic reasoning engine. It is clear that any approach which depends on careful human analysis of such large knowledge bases is impractical.</p><p>Meanwhile, a very different "connectionist" approach to AI was being championed by other researchers. They drew their inspiration from hypotheses about how the brain works, from information theory and statistics, from physics and other natural sciences, and from applied mathematics and particularly optimization theory. These diverse points of view came together in the 1990's and led to a great deal of interdisciplinary work,<ref type="foot" target="#foot_3">foot_3</ref> of which the part most related to AI and which lies behind LLMs is called machine learning (ML).</p><p>The usual starting point in modern treatments of ML is to rephrase a task as a statistical inference problem based on a large dataset. A canonical example is image recognition -say, given an array of pixels (light intensity values), estimate the probability that the image depicts a cat. Rather than design a system to do this, one starts with a large dataset of images with labels (cat and non-cat). One then designs a very general statistical model and "trains" it on this dataset to predict the label given the image. This is supervised learning, one can also do "self-supervised" learning in which the system predicts some part of the data given other parts (say, filling in part of an image). A third standard ML paradigm is reinforcement learning, which applies to tasks which involve choosing actions to fulfill a longer term goal. The classic example is game playing, as in AlphaGo and AlphaZero.</p><p>In any case, since the problem is formulated statistically, it is possible to consider the training dataset one item at a time, and use it to incrementally improve the model. This is almost always done by formulating the task in terms of an "objective function" which measures the quality with which it is performed, for example the accuracy with which correct labels are assigned to images. One then takes a parameterized model and trains it by optimizing this function, evaluated on the training dataset, with respect to the model parameters. For the classic models of statistics this can be done analytically, as in a least squares fit. For more general models one uses numerical methods, such as gradient descent. Either way, a central question of statistics and machine learning is generalization, meaning the extent to which the model well describes data not in the training set but sampled from the same probability distribution. A well known principle which speaks to this question is "Occam's razor," that simpler models will generalize better. This is often simplified to the rule that a model should have the minimal number of parameters needed to fit the dataset. Not all machine learning systems are "deep learning" <ref type="bibr" target="#b74">[76]</ref> or "connectionist" <ref type="bibr" target="#b116">[118]</ref>. These terms generally refer to the use of neural networks with large numbers of parameters which provide effective universal function approximators. While the idea is very old <ref type="bibr" target="#b115">[117]</ref>, before 2012 it was widely believed to be impractical. One argument for this was the "dull side" of Occam's razor -models with so many parameters were destined to overfit and would not generalize. Evidently this is not the case, leading to concepts such as "benign overfitting." <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b13">15]</ref> Another argument was that the objective functions for these models are highly nonconvex and optimization would get stuck at poor quality local minima. This can be a problem, but turns out to be solvable for reasons that are partially understood <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b47">49]</ref>. Finally, despite the effectiveness of the trained model in performing a task, the large number of parameters often makes it very hard to understand how such a model works, and why a given input produces a particular output. This "interpretability problem" remains a key issue with deep learning models, and is the subject of much research <ref type="bibr" target="#b50">[52]</ref>.</p><p>There are many other variations and hybrid approaches in the story. Another important one is the "pattern recognition" approach <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b100">102]</ref>. This is also based on statistical inference but -like the symbolic approach -it emphasizes the value of detailed understanding of the problem domain in designing the system. For example, one could hand-code the initial layers of an image recognition network to detect lines or other significant "features" of the image. But unlike a purely symbolic approach, these features would be used as input to a general statistical or neural model.</p><p>Another concept which illustrates the relation between the two approaches is probabilistic reasoning, the use of rules such as "the chance of rain when it is cloudy is 50%". One can state and use such rules in a symbolic approach (see for example <ref type="bibr" target="#b67">[69]</ref>), the essential distinction with connectionism is not the use of probabilities but rather the representation of knowledge in terms of explicit and meaningful rules.</p><p>As we suspect every reader has already heard, the symbolic approach was dominant from the early days until 2012, and (along with many other successes) led to a superhuman chess player, but seemed inadequate for our other two challenge tasks (theorem proving and question answering). In 2012 the connectionist approach surpassed other approaches to computer vision <ref type="bibr" target="#b68">[70]</ref>, and ever since neural systems have gone from triumph to triumph. In 2017 the deep learning system AlphaZero surpassed the symbolic AI chess players (and of course humans). Over the last few years, transformer models trained on a large corpus of natural language to predict each next word as it appears, have revolutionized the field of natural language processing. As we write this the state of the art GPT-4 demonstrates truly remarkable performance at question answering, code generation and many other tasks <ref type="bibr" target="#b22">[24]</ref>.</p><p>The simplest and arguably deepest explanation for this history is that it is a consequence of the exponential growth of computational power and training datasets, which continues to the present day. Given limited computing power and data, the ability of the symbolic and pattern recognition approaches to directly incorporate human understanding into a system is a significant advantage. On the other hand, given sufficiently large computing power and data, this advantage is nullified and may even become disadvantageous, as the human effort required to code the system becomes the limiting resource. This point, that the most significant advances in AI (and computation more generally) have come from hardware improvements and replacing human engineering with datadriven methods, is forcefully made by Sutton in his "bitter lesson" essay <ref type="bibr" target="#b126">[128]</ref>. In §3, §4 and §8 we will discuss scaling laws and evidence for and against the idea that by continuing along the current path, training ever-larger models on ever-larger datasets, we will achieve AGI (artificial general intelligence, whatever that means) and the realms beyond.</p><p>Up to now the symbolic and connectionist approaches have generally been considered to be in tension. 11 There is another point of view which considers them complementary, with a symbolic approach better suited for certain problems (for example logical reasoning) and connectionist for others (for example image recognition). Given this point of view one can seek a synthesis or "neurosymbolic" approach, advocated in many works <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b70">72]</ref>.</p><p>But are they in conflict at all? Another reconciliation is the hypothesis that problems which in the symbolic approach are solved using rules and algorithms, are also being solved that way by neural systems and in particular by LLMs. However, rather than the algorithms and rules being coded by humans, as the result of its training procedure the LLM has somehow learned them, encoded in its networks in some as yet mysterious way. This vague hypothesis can be sharpened in many ways, in part by proposing specific mechanisms by which algorithms and rules are encoded, in part by making general claims about the algorithms which are being learned. We discuss these ideas in §7 and §8. 11 To better discuss this point one should refine the symbolic-connectionist dichotomy into multiple axes: system design versus learning from data; meaningful rules versus uninterpreted models; combinatorial versus differentiable optimization; deterministic versus probabilistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language models</head><p>Throughout the history of linguistics, languages have been described in terms of rules: rules of grammar, phonology, morphology, and so on, along with logical and other frameworks for describing meaning. This remains the case in Chomskyan linguistics and in much of theoretical linguistics.</p><p>By contrast, LLMs are statistical language models, meaning that they encode a probability distribution on strings of words, call this P (w 1 . . . w L ), which approximates the distribution realized by a large body (or "corpus") of text in the language. The simplest example is the frequency or "1-gram" model defined by taking the words to be independently distributed, so</p><formula xml:id="formula_3">P (w 1 . . . w L ) = L i=1 P (w i ); P (w) =</formula><p>number of occurrences of w in the corpus total number of words in the corpus .</p><p>(1) Of course, this model captures very little of the structure of language, which involves dependencies between the word choices.</p><p>LLMs are generative models, <ref type="foot" target="#foot_4">12</ref> by which we will mean that there is a practical method for sampling from the distribution. To explain this, consider a word prediction task in which some words in a string are given (the "input") and others left blank (the "output"). Given a probability distribution P (w 1 . . . w L ), there is a corresponding conditional probability distribution for the output given the input. As an example, suppose we are given the string "The cat [BLANK] outside," where "[BLANK]" is a "token" which marks the position of the missing word. The relevant conditional probabilities might be P (the cat went outside | the cat [BLANK] outside) = 0.5 P (the cat sat outside | the cat [BLANK] outside) = 0.2 and so on, summing to total probability 1. In the masked word prediction task, the model must determine (or sample from) this distribution.</p><p>A particularly convenient case is to give the conditional probability of the word which follows a given string, which we denote as</p><formula xml:id="formula_4">P (w n+1 | w 1 w 2 . . . w n-1 w n ).</formula><p>(</p><formula xml:id="formula_5">)<label>2</label></formula><p>By sampling this distribution to get a new word w n+1 and appending it to the end, the string can be extended one word at a time. Repeating this process gives an arbitrarily long string, which by the laws of probability is a sample from the original probability distribution P (w 1 . . . w L ), for example P (the cat went outside) = P (the)P (cat | the)P (went | the cat)P (outside | the cat went).</p><p>This factorization of the probability into successive conditional probabilities defines the class of autoregressive models. One could furthermore require that the conditional probability Eq. 2 depends only on the k most recent words, in which case one would have a Markov model whose state is a string of k words.</p><p>To evaluate how good a language model is, we want to quantify how well its probability distribution approximates that of the corpus (the empirical distribution). The standard measure of this is the cross entropy. For an autoregressive model this is a sum of terms, one for each word in the corpus,<ref type="foot" target="#foot_5">foot_5</ref> </p><formula xml:id="formula_6">L = - 1 N N -n i=1 log P (w i+n | w i w i+1 . . . w i+n-1 )<label>(3)</label></formula><p>One also refers to exp -L as "perplexity." In a machine learning approach, we can use Eq. 3 as an objective function and minimize it as a function of the network parameters to train the network. We can then apply the many tools of ML: backpropagation, splitting the sum into batches, varying the learning rate and so on, to get an efficient and effective model. While the details are an art which depends on the particular domain and model architecture, <ref type="foot" target="#foot_6">14</ref> conceptually these are much the same for LLMs as for other machine learning models. This statistical approach to modeling language has been pursued since the late 80's <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b85">87]</ref> and many models were developed, such as the recurrent neural network (RNN) which we will describe in §5. Following the general machine learning experience that supervised tasks (learning from input-output pairs) are easier than unsupervised tasks, many of these works addressed machine translation and parsing, for which there are good labeled datasets (documents with their translations; sentences with their grammatical structure). However unlabeled datasets are much larger and by 2015 or so there was a sense that selfsupervised learning was the next frontier <ref type="bibr" target="#b72">[74]</ref>, leading to more focus on masked word prediction.</p><p>The history of transformer models starts with the 2017 proposal of Vaswani et al <ref type="bibr" target="#b130">[132]</ref>. Their model was designed for a translation task and was more complicated than what we will explain in §6, but the essential idea to use attention and positional encoding to represent all the relations between the words in a text originated here and is fully present.</p><p>The transformer architecture was taken up by many groups, and particularly influential 2018 works include BERT <ref type="bibr" target="#b37">[39]</ref> and GPT <ref type="bibr" target="#b110">[112]</ref>. BERT was trained by masking arbitrary words in a sentence (not just the next word), which allows the model to look backward and forward for context and leads to better results. However it is not straightforward to sample from such a model, and eventually the simpler next word prediction approach followed by GPT won out.</p><p>Both of these models, and most work of this period, followed the paradigm of pretraining followed by fine tuning. The idea was to first train for word prediction on a very large corpus, to get a general purpose model. This would then be adapted to specific tasks such as question answering by fine tuning. This means doing a second pass of supervised learning on a much smaller labeled dataset, replacing next word prediction by the objective function for the specific task. Say we are doing question answering, this could be the accuracy of the answers. This two step procedure was justified by the notion of transfer learning, meaning that the capabilities of the general purpose model "transfer" to related but different tasks. This approach led to SOTA<ref type="foot" target="#foot_7">foot_7</ref> results on many benchmarks and motivated much further work.</p><p>Most importantly, a great deal of ingenuity and hard work was put into solving the engineering problems of training larger and larger models on larger and larger datasets. As for the data, a lot of text is available on the web, with one much used archive of this data provided by Common Crawl. <ref type="foot" target="#foot_8">16</ref> Training can largely be done in parallel by dividing up this data, and the availability of large clusters of GPU-enabled servers at industrial labs and through cloud computing meant that sufficient computing resources were available in principle. However, the overall cost of training scales as (at least) the product of model size and dataset size, and this was becoming expensive. While the precise cost figures for the GPT series are not public, it is estimated that a single training run of the largest GPT-3 models cost tens of millions of dollars. To motivate and efficiently carry out such costly experiments, one needs some ability to predict in advance how changes in model and dataset size will affect the training methods (for example the optimal choice of learning rate) and performance.</p><p>An important advance in this direction was the observation of power law scaling in language model performance <ref type="bibr" target="#b65">[67]</ref>. Figure <ref type="figure" target="#fig_0">2</ref> plots the test loss <ref type="foot" target="#foot_9">17</ref> against the logarithms of the sizes and compute resources used, and these straight lines correspond to a power law relation between size and perplexity. This scaling holds over many decades in model size and, while the exponents α ∼ -0.076 to -0.095 are rather small, this is a strong argument that larger models will have better performance. These ideas were also used to determine optimal modeldataset size tradeoff <ref type="bibr" target="#b56">[58]</ref> and the scaling of hyperparameters <ref type="bibr" target="#b139">[141]</ref>. These results were a significant input into the decision to do this very expensive research.  Now it should be realized that, while the measure being improved here is fairly objective, still there was no strong reason to think that improving it would lead to models with qualitatively new "emergent" capabilities. But it appears that this is what happened: GPT-3 and its fine-tuned cousins (such as Codex) were able to do tasks, such as write computer code from a natural language description, for which smaller models were almost worthless. <ref type="foot" target="#foot_10">18</ref> We will discuss more of this progress shortly, and speculate a bit in the conclusions.</p><p>One of the most interesting LLM phenomena is in-context learning, first discussed in the original GPT-3 paper <ref type="bibr" target="#b20">[22]</ref>. This refers to the ability of an LLM to carry out tasks different from its original objective without modifying its parameters, indeed without any need for additional training on the new task (fine tuning). Rather, after being given (as input text) a few examples of input-output pairs, the LLM can be given another input and will generate a suitable output. Say the new task is question answering, then after a few question-answer examples the LLM will answer the next question it is given. While intuition based on human abilities might find this unremarkable, it is actually quite unusual for an ML model and this is why the pretraining-fine tuning paradigm was the usual approach in previous work. Of course the training set already contains many examples of QA pairs. More striking are tasks which are not much represented in the training set, such as finding anagrams or rearranging letters in words. One can even do in-context "meta-learning" of machine learning tasks such as linear regression (see §4).</p><p>Once it is established that the model can generalize from a few examples, a further step towards human capabilities is to try zero examples, instead simply explaining the task in natural language. At this point it becomes difficult to classify the tasks -should we consider the task of writing code from a natural language specification to be a form of translation, or an example of explaining the task, or something else? The relation between the input text or "prompt" and the output has many surprising features. For example, a standard technique in LLM question answering which measurably improves performance is to precede the question with a prompt such as "I will answer this question helpfully and truthfully." Is this somehow biasing the network towards certain texts and away from others (after all the internet corpus is hardly a reliable source of truth) ? Suppose we have a theory of how this works, how can we test it? Does the model "know" anything about the truth of statements? <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b77">79]</ref> As has been much reported, one of the major difficulties in using LLMs for practical tasks is their propensity to invent facts (especially citations) and their limited ability to do logical reasoning, algebra and other symbolic tasks. A device for improving this, called "chain of thought prompting," is to give examples (say of question answer task for definiteness) with some intermediate reasoning steps spelled out. This was used in the Minerva QA system <ref type="bibr" target="#b75">[77]</ref> which produced the example in Figure <ref type="figure">1</ref>. Still the fraction of problems it solved correctly is around 50% (the later GPT-4 is similar). Even for simpler questions, the reliability of GPT-4 is more like 90%. Much current research is devoted to this problem of reliable reasoning, as we discuss in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Phenomenology of language models</head><p>In this section we discuss general claims, "non-invasive" experiments, and theoretical arguments which do not depend on "microscopic details" of the models such as the trained weights. <ref type="foot" target="#foot_11">19</ref> This includes evaluation of model capabiliities, qualitative observations and scaling laws.</p><p>What can LLMs do? There is a huge body of work on this question, and any attempt to review it would rapidly go out of date, but let us review the primary method for studying it. This is benchmarking, the development of standardized sets of test items for which model accuracy can be evaluated in a reproducible way. This is in principle straightforward if the input corresponds to a single correct output, as in multiple choice question answering. <ref type="foot" target="#foot_12">20</ref> If the answer is freeform text, one can use text comparison metrics such as the ROUGE score. One current standard for evaluating LLMs, BIG-bench <ref type="bibr" target="#b125">[127]</ref>, combines 204 language tasks (at first publication; they accept new tasks) including translation, QA, puzzle solving, text classification and summarization, and tests of common sense reasoning. A leaderboard listing the current best LLMs is at <ref type="foot" target="#foot_13">21</ref> . Another is the EleutherAI "Language Model Evaluation Harness" <ref type="foot" target="#foot_14">22</ref> and leaderboard. <ref type="foot" target="#foot_15">23</ref> The benchmark suite HELM <ref type="bibr" target="#b78">[80]</ref> measures additional metrics such as tendency to repeat copyrighted material, bias, toxicity and the like.</p><p>Reasoning ability is of particular interest for mathematical and scientific applications -of course we all look forward to the day when computers will help us grade assignments, referee papers and do our research. There are many benchmarks for solving logical problems expressed in natural language. Benchmarks for mathematical theorem proving include NaturalProofs <ref type="bibr" target="#b133">[135]</ref>, MiniF2F <ref type="bibr" target="#b144">[146]</ref> and ProofNet <ref type="bibr" target="#b4">[6]</ref>; as of mid-2023 LLMs (and the best other systems) can find many proofs (20-80%) but still fail on some seemingly easy cases. Simpler aspects of reasoning which have benchmarks are the ability to deal with negation <ref type="bibr" target="#b141">[143]</ref>, consistency (between different phrasings of the same question) <ref type="bibr" target="#b59">[61]</ref>, and compositionality (the ability to analyze statements and problems into simpler parts, solve these and combine the results) <ref type="bibr" target="#b109">[111]</ref>.</p><p>Natural language tasks are very complex, and benchmarks constructed from real world data cannot be used directly in theoretical considerations. For this purpose one generally defines "toy worlds" and generates synthetic data. The possibilities are endless, but some which have been used are arithmetic problems (decimal arithmetic; modular arithmetic), game play, solving systems of equations, and parsing formal languages. A particularly interesting task is linear regression <ref type="bibr" target="#b46">[48]</ref>; since this is the prototypical case of statistical inference, a system which learns to do it can be said to be "learning how to learn."</p><p>Coming to scaling laws, denote the model size (number of parameters) as P and the dataset size (number of tokens in the corpus) as D, then there are two general regimes. If we hold one of these (say P ) fixed and take the other (say D) to infinity, then a law of large numbers applies and L ∼ 1/D. On the other hand, if we take one parameter very large and study the dependence on the other, nontrivial power law scaling can emerge. In principle one can get different exponents for D and P , suggesting the ansatz</p><formula xml:id="formula_7">L(P, D) = P c P α P /α D + D c D α D . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where L is test loss Eq. 3 computed in an optimally regularized model. <ref type="foot" target="#foot_16">24</ref> This is a good fit to Figure <ref type="figure" target="#fig_0">2</ref>. While in Figure <ref type="figure" target="#fig_0">2</ref> the two exponents appear to differ, there is not really convincing evidence that this is significant. Before working hard on this, one should ask if there is any way to control the many choices involved, so as to define universal exponents. One context in which this can be studied systematically is transfer learning, by distinguishing the dependence on the pretraining and fine tuning datasets <ref type="bibr" target="#b53">[55]</ref>. Another relevant and practical question is whether one can prune the dataset to improve the scaling. It is intuitively plausible and can be shown in examples that sets of data items are worth more if they are diverse than if they are similar. The challenge is to find simple ways to quantify this similarity; in <ref type="bibr" target="#b124">[126]</ref> many proposals are studied.</p><p>Scaling laws can arise in many ways, not specific to language models. One hypothesis is that the data lies on a low dimensional submanifold in a higher dimensional space. 25 Both the number of parameters and the number of points required to fit this manifold go as the dimension d of the manifold, and this leads to α P = α D = 4/d (the precise coefficient 4 depends on assumptions about smoothness) <ref type="bibr" target="#b6">[8]</ref>.</p><p>A related hypothesis is that the spectral density of the data covariance falls off as a power law, and in <ref type="bibr" target="#b83">[85]</ref> Eq. 4 is derived for a random feature model with this covariance. This hypothesis follows from the low dimensional hypothesis but it is more general, for example these authors argue that additional features derived from the data (as in nonlinear models such as FFN's) generally have the same spectrum as the original data. One can also try to relate Eq. 4 and corrections to it to hypotheses about how tasks are learned <ref type="bibr" target="#b96">[98]</ref>.</p><p>What does the scaling of the information theoretic quantity Eq. 3 have to do with performance on tasks requiring intelligence? A priori, not much, but one way to motivate a focus on it is to draw an analogy with particle physics. In the 30's cosmic ray observations gave strong hints of new physics at higher energies, but the interesting events were too rare and uncontrolled to draw solid conclusions. Thus physicists were motivated to build accelerators. These are not that expensive when they fit on a tabletop, but rapidly grow in size and cost. How large does an accelerator need to be? The right measure is not its size per se but rather the energy of the particles it can produce. The physics relating size and energy is not trivial (due to effects such as synchrotron radiation) but can be worked out, so one can make a good prediction of energy reach. Still, as one increases energy, will one find a smooth extrapolation of what came before, or will one discover qualitatively new phenomena? In the golden age of accelerator physics (the 50's-70's) much new physics was discovered, mostly associated with new particles which are produced only above sharp energy thresholds. Currently the highest energy accelerator is the Large Hadron Collider at Cern, where the Higgs particle was discovered in 2012. While we are still waiting for further important discoveries, the potential for discovery is determined by measurable properties of the accelerator -by energy and secondarily by intensity or "luminosity" -which we can judge even in the absence of qualitative discoveries. In the analogy, perplexity is playing a similar role as an objective measure of language model performance defined independently of the more interesting qualitative behaviors which reflect "intelligence."</p><p>How far can one push this analogy? Could perplexity be as central to language as energy is to physics? Eq. 3 has a fairly objective definition, so the idea is not completely crazy. But, not only was its relation to performance on actual tasks not predictable in advance, even after the fact clear "thresholds" or other signals for emergence of tasks have not yet been identified <ref type="bibr" target="#b131">[133]</ref>. Perhaps if there are universal thresholds, evidence for them could be seen in humans. 26  More likely, additional variables (the quality and nature of the training corpus, 25 In §5 we explain how text can be thought of embedded in a high dimensional space. 26 Thanks to Misha Tsodyks for this suggestion. details of the tasks, etc.) would need to be controlled to see them. This is another question probably better studied in simpler tasks using synthetic data.</p><p>The final topic we discuss is the behavior of the objective function (Eq. 3) as a function of training time. <ref type="foot" target="#foot_17">27</ref> In almost all ML runs, such a plot shows long plateaus interspersed with steep drops. This has been interpreted in many ways, ranging from evidence about the nature of learning, to a simple consequence of randomness of eigenvalues of the Hessian of the loss function. A more recent observation is to compare training and testing accuracy on the same plot. In <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b108">110]</ref> it was argued that these two metrics improve at two distinct stages of training. First, the model memorizes training examples. Later, it generalizes to the testing examples. This "grokking" phenomenon has been suggested as evidence for learning of circuits <ref type="bibr" target="#b101">[103]</ref>, an idea we discuss in §7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Simpler language models</head><p>Here we describe a few generative language models in detail to fix the concepts. As points of notation, let W be the set of words (or, if the reader prefers, numbers which index a position in a list of words). We denote the cardinality of a set S as |S|, so |W| is the number of distinct words. The space of N -component real vectors is denoted R N .</p><p>The simplest model is the N-gram model defined in terms of the conditional probabilities P (w N |w 1 w 2 . . . w N -1 ), <ref type="bibr" target="#b3">(5)</ref> which are all taken to be independent. Given this minimalist assumption, a plausible way to estimate them from the corpus is (6) This simple model with N = 3 or 4 works better than one might think (see examples in <ref type="bibr" target="#b62">[64]</ref>) and can be improved a bit by simple statistical tricks ("smoothing"). But the exponential growth of the number of strings in N means that there is no hope of taking N large enough to model even a single paragraph. The entire internet contains (in order of magnitude) 10 12 words, and such a corpus will contain only a vanishingly small fraction of the likely twenty word strings. <ref type="foot" target="#foot_18">28</ref>A more general principle which we can take from the N-gram model is the distributional hypothesis, which has been pithily summarized as "you shall know a word by the company it keeps." <ref type="bibr" target="#b42">[44]</ref> In other words, by proper use of the statistics of neighboring words, one can define quantities which capture properties and even the meanings of words. The simplest expression of this idea is the co-occurrence matrix. Before explaining this, let us mention a detail of practical systems, which in place of words use "tokens," meaningful components of words. A physics illustration is the word "supersymmetrization." Even for a non-physicist reader encountering it for the first time, this word naturally breaks up into "super," "symmetry" and "ization," pieces which appear in many words and which are called tokens. And not only does this decomposition apply to many words, it helps to understand their meaning. This process of replacing single words by strings of tokens ("tokenization") is a first step in LLM processing, and henceforth when we say "word" we will mean word or token in this sense.</p><formula xml:id="formula_9">P (w N |w 1 w 2 . . . w N -1 )</formula><p>Given a corpus, we define its N -gram co-occurrence matrix M N to be the |W| × |W| matrix whose (w, w ′ ) entry counts the number of N -grams in the corpus containing both words. This matrix defines a map from words to vectors</p><formula xml:id="formula_10">ι : W → R p<label>(7)</label></formula><p>(where the dimension p = |W|), by taking a word to the corresponding column of M N . Such a map is called a word embedding.</p><p>Applying this map to each word independently, we can map a string of k words (in W k ) to a string of vectors, and this is the next step (after tokenization) of LLM processing. One might worry that these are very high dimensional vectors with many zero entries, which seems wasteful. A standard statistical cure for this problem is to do principal component analysis (PCA). In words, instead of columns of M N we use the columns of a p × |W| matrix Z chosen such that Z t Z is the best rank p approximation to M N in the sense that it minimizes tr (Z t Z -M N ) 2 . One can do better, but this gives the right idea.</p><p>Next, we feed this string of vectors into some machine learning model to get an output which we use to predict the next word. If we just want the most likely next word, a good way is to output a vector v ∈ R p , and choose the word w which maximizes the inner product v • ι(w). We denote this relationship as v ∼ ι(w). More generally, the standard inverse map from a vector to a probability distribution on words is the Boltzmann distribution on the inner products. Explicitly, we postulate an inverse temperature β = 1/T and take<ref type="foot" target="#foot_19">foot_19</ref> </p><formula xml:id="formula_11">v → P (w) = e βv•ι(w) w ′ e βv•ι(w ′ )<label>(8)</label></formula><p>Here is an observation <ref type="bibr" target="#b97">[99]</ref> which supports the idea that word embeddings contain information about meaning. Since the embeddings are vectors, they can be added. Consider the following equation:</p><formula xml:id="formula_12">ι(king) -ι(man) + ι(woman) ∼ ι(?)<label>(9)</label></formula><p>One might hope that the word which maximizes this inner product is "queen," and indeed it is so. There are many more such examples; empirically one needs the dimension p ≳ 100 for this to work. One can argue <ref type="bibr" target="#b106">[108,</ref><ref type="bibr" target="#b3">5]</ref> that it follows from relations between co-occurence statistics: <ref type="foot" target="#foot_20">30</ref>∀w, M N (w, king)/#(king) M N (w, queen)/#(queen) ≈ M N (w, man)/#(man) M N (w, woman)/#(woman) <ref type="bibr" target="#b8">(10)</ref> Given these ideas and a map F from a list of vectors to a vector, we can now propose a very general class of L-gram autoregressive language models as the combination of the following steps:</p><p>1. Map the L input words w i to L vectors ι(w i ).</p><p>2. Apply F to the list of these vectors to get a prediction vector v.</p><p>3. Use the inverse map Eq. 8 to get a probability distribution over words.</p><p>Furthermore, if the map F has parameters, given a corpus we can determine them by optimizing the function Eq. 3 with respect to the parameters. And once we bring in optimization, we can also optimize with respect to the coefficients of the embedding map Eq. 7, so that we can dispense with co-occurence statistics. This is the general prescription followed by the LLMs, and to complete it we just need to specify a family of maps F . One possibility is to use a general (fully connected) feed forward neural network (FFN, also called MLP for multilayer perceptron). We recall that an FFN is a composition of two general types of functions, linear maps W i and nonlinear maps θ, so that</p><formula xml:id="formula_13">F (v) = W d • θ • W d-1 • θ • . . . • W 1 • θ • W 0 .<label>(11)</label></formula><p>In more concrete terms, the maps W i are multiplication by rectangular matrices of parameters (usually called "weights" in this context), while the maps θ act independently on each component of their input vector by a fixed nonlinear function such as tanh or (more typically) ReLU (identity for x ≥ 0 and zero for x &lt; 0). The main fact we recall about FFN's is that, in the limit that the number of parameters becomes large, they can approximate any given function arbitrarily well <ref type="bibr" target="#b36">[38]</ref>. We refer the reader interested in learning more to <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b114">116]</ref>.</p><p>We can get a very natural deep learning version of the L-gram models by using an FFN for the map F in the prescription above <ref type="bibr" target="#b16">[18]</ref>. Since this asked for a map from a list of vectors to a vector, we need to convert the input list into a single vector. This is easy: we can take the direct sum of the input vectors, i.e. the dimension L × p vector whose components are the concatenated lists of their components. Using today's FFNs, one could implement this with L ∼ 100 or so. There does not seem to be much work on large fully connected FFN language models, because by the time the technology advanced to this point the far more efficient transformer models had taken over. Still, they illustrate the general idea and also one of its most obvious limitations. Even with L ∼ 100, often predicting the next word requires remembering words which appeared farther back. To solve this problem we need to incorporate some sort of memory into the model.</p><p>The simplest memory is an additional state variable which is updated with each word and used like the other inputs. To do this, we should take the state to be a vector in some R q . This brings us to the recurrent neural network or RNN. Its definition is hardly any more complex than what we saw before. With each word position (say with index i) we will associate a state vector s i which can depend on words up to w i and on the immediately previous state. Then, we let the map F determine both the next word and the next state as</p><formula xml:id="formula_14">(v i+1 , s i+1 ) = F (s i , v i , v i-1 , v i-2 , . . . , v i-k+1 ),<label>(12)</label></formula><p>where the parenthesis notation on the left hand side means that the output vector of F is the concatenation of two direct summand output vectors. Mathematically, Eq. 12 is a discrete dynamical system. If we grant that F can be an arbitrary map, this is a very general class of systems. One way of characterizing its generality is through computational complexity theory, by asking what classes of computation it can perform. In <ref type="bibr" target="#b121">[123]</ref> it was argued that the RNN is a universal computer, but this granted that the computation of F in Eq. 11 could use infinite precision numbers. Under realistic assumptions the right complexity class is a finite state machine, which can recognize regular languages <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b132">134]</ref>. We will say more from this point of view in §7.</p><p>There are many variations on the RNN such as LSTM's <ref type="bibr" target="#b55">[57]</ref>, each with their own advantages, but we must move on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Recipe for an LLM</head><p>We are now ready to define the transformer model. <ref type="foot" target="#foot_21">31</ref> It is simply another class of maps F from lists of vectors to a vector to be used in the prescription above. Indeed, it is a natural generalization of the FFN which is associated to permutational symmetry. This is in direct analogy to the use of convolutional neural networks (CNNs) for image recognition, which are FFNs which are equivariant under the symmetry of translations in two dimensions which is natural for the set of images.</p><p>A transformer is a composition of two types of functions (layers) taken in alternation, each mapping an input list of L vectors {u i } to an output list of L vectors {v i }. One of these is an FFN as previously discussed, but now applied to each embedding vector independently, so</p><formula xml:id="formula_15">v i = F F F N (u i ).</formula><p>The other layer type is called attention, and it is defined as follows:</p><formula xml:id="formula_16">{u i } → {v i = W i j=1 c i,j u j }<label>(13)</label></formula><formula xml:id="formula_17">c i,j ≡ exp u i • B • u j i j=1 exp u i • B • u j (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>where B is a learnable matrix whose elements are model parameters (equivalently, u • B • v is a bilinear form) and W is a linear map (also learnable). <ref type="foot" target="#foot_22">32</ref>In words, an item v i in the output vector is (a linear transformation of) a weighted sum of the inputs u j with j ≤ i and can depend on any of them. <ref type="foot" target="#foot_23">33</ref>The weights c i,j are given by a "softmax" or Boltzmann weight just as in Eq. 8. Thus there is a very general learnable way for each output to choose which of the input vectors are most useful as inputs. Suppose the product u • B • v is the dot product, then attention selects the input components u j most similar to the current unit's input, u j ∼ u i in the notation of §5. The matrix B allows for comparing different parts of the embeddings and ignoring other parts, in a way determined by optimizing the objective function Eq. 3.</p><p>Composing these two types of functions (or layers) produces a map from R p×L to R p×L . Often one takes, instead of the pure FFN and attention functions, sums of these with the identity function (residual connections). The FFNs generally have a single hidden layer which can be of a different dimension, call this p h . <ref type="foot" target="#foot_24">34</ref> Finally, while the language model prescription asked for a map to R p , this is easily obtained by just taking the last vector in the final output list.</p><p>There are two more essential details to cover (and many minor details we will skip). The first is the concept of "attention head." The definition Eq. 13 allowed for a general linear transformation W whose range is the output vector. We are free to choose its dimension, call it q, and typically one takes this to be much less than the embedding dimension p. In return one can use many copies of Eqs. 13,14 in parallel with different choices for B and W , to produce many outputs. One can then concatenate these outputs to get a final output of dimension p. These copies are called attention heads and we will denote their number by H, so p = Hq.</p><p>The second essential detail is that, so far, there is nothing in the definition that keeps track of the order of the list of input vectors; the output of Eq. 13 will be invariant under a general permutation of the input vectors. While this is an elegant property, it is not what we want for processing language, for which the order of the words matters. The cure for this is very simple: one takes as inputs not the word embeddings Eq. 7, but the direct sum (concatenation) of these with positional embedding vectors, i.e. vectors which encode the position (index) of the word in the string. These can be a combination of sines and cosines of various frequencies, such as <ref type="bibr" target="#b130">[132]</ref> (e 2i-1 , e 2i ) = ( cos position 10000 2i/dpos , sin position 10000 2i/dpos ); i ∈ {1, . . . ,</p><formula xml:id="formula_19">d pos 2 }<label>(15)</label></formula><p>One could instead treat these vectors as learnable parameters. Still, the trig function basis for positions may be significant. It has been generalized to represent other graph structures by using eigenfunctions of the graph Laplacian as positional embeddings.</p><p>The invariance of the transformer model under permutation symmetry is reminiscent of the point we mentioned earlier, that translation symmetry motivates the CNN. However permutation symmetry is badly broken in language, even in the simplest formal languages, 35 and it is not obvious why this should be a useful property for the model to have. One might argue that although any particular language breaks permutation symmetry, it acts naturally on the ensemble of languages and thus should have a simple representation. For example, besides the usual infix arithmetic notation "a + b", one could instead use prefix "+ a b" or postfix "a b +". Translating between these notations is arguably easier for permutation invariant maps using position embeddings. An opposing view would be that permutation symmetry is just a secondary property of the simplest model using attention, and that the main point is to explain the value of attention. In addition to its ability to select similar items, it provides a simple way to take products of embedding vectors. In computational complexity terms, attention enlarges the class of circuits which can be simulated by a constant depth transformer <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b93">95,</ref><ref type="bibr" target="#b94">96]</ref>. Physics analogies of Eqs. 13,14, especially to the Hopfield model, may be important <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b113">115]</ref>.</p><p>A major practical advantage of the transformer over the RNN and other previous architectures is that the computations in the attention mechanism can be done in parallel, so (given sufficiently many processors) the time required does not increase with the window length L. This is by contrast with the RNN in which information propagates from one word to the next, so a window of length L requires time L to process. On the other hand the ability of each unit to pay attention to every previous unit means that the total computation required by the transformer scales as L 2 . This is the limiting factor for increasing L and this is widely seen as a problem. There has been a lot of work to improve this scaling, by removing some of the connections (as in sparse attention <ref type="bibr" target="#b28">[30]</ref>), by introducing multiscale structure, or in other ways.</p><p>Let us summarize by listing the hyperparameters 36 and their values for the largest (175B) GPT-3 <ref type="bibr" target="#b20">[22]</ref>. They are • Embedding dimension p = 12288 and hidden layer dimension p h = 4p.</p><p>• Window length L = 4096 or 8192. 35 Compare the logical implications A → B and B → A. 36 This term refers to model choices which are not learned through gradient descent.</p><p>• Depth D = 96, counting both FFN (Eq. 11) and attention (Eq. <ref type="bibr" target="#b11">13)</ref> layers. <ref type="foot" target="#foot_25">37</ref>• Number of heads H = 96 (the equality with D is a coincidence as far as I know).</p><p>The total number of parameters is roughly 12Dp 2 . As mentioned earlier, all of these parameters, and the parameters of the embedding map Eq. 7, are determined as follows. One generally starts with "random" initial conditions, usually meaning that each parameter is drawn from a normal distribution with mean zero and variance chosen so that the linear maps have expected norm independent of the hyperparameters. As in random matrix theory, this typically means var(W i,j ) ∼ 1/p, though there are refinements <ref type="bibr" target="#b139">[141]</ref>. One then sequences through the training corpus and performs a step of gradient descent of Eq. 3 for each "batch" of words (here a group of ∼ 10 6 words). In each step, the parameters ⃗ θ are modified as</p><formula xml:id="formula_20">⃗ θ → ⃗ θ -η ∂L b ∂ ⃗ θ (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where L b is Eq. 3 restricted to the batch, the conditional probability P comes out of Eq. 8 applied to the output of the transformer, and η is a positive real number (the learning rate hyperparameter, here around 10 -4 ).</p><p>The result of following this procedure on a dataset of natural language text,<ref type="foot" target="#foot_26">foot_26</ref> supplemented by many enhancements which are described in the literature and in the model source codes but which may be less important for conceptual understanding, is an LLM with the capabiliities we described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Studying the internal workings</head><p>The success of this procedure raises many questions. Some can be asked about more or less any ML model -for example, questions about when and how optimization of the objective function Eq. 3 achieves "good" local minima (value near the global minimum and models which generalize well), and the origin of scaling laws like Eq. 4. These are the subject of the general theory of machine learning, for which we refer to <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b95">97,</ref><ref type="bibr" target="#b114">116]</ref> and much other work.</p><p>Other questions, and understanding the many striking abilities discussed earlier, sound more specific to LLMs. What would it mean to understand how ChatGPT writes poetry based on prompts, or solves physics word problems? At present this is by no means clear and it may be that entirely new concepts are needed to do this. Still, I share the belief that we can go very far towards understanding LLMs by building on previous work in computer science, machine learning and AI, and many other fields. There is a well established field of statistical physics and ML <ref type="bibr" target="#b95">[97]</ref> which will surely contribute. Physics ideas are also very relevant for tasks with spatial symmetry, such as image generation <ref type="bibr" target="#b123">[125]</ref> and recognition <ref type="bibr" target="#b33">[35]</ref>. The unexpected mathematical simplicity of the transformer model means that mathematical insights could be valuable. We can also follow approaches used in neuroscience, psychology, and cognitive science.</p><p>An evident observation is that the paradigm of neuroscience -careful study of the microscopic workings of the system, following a reductionist philosophy -is far more practical for ML models than it is for human brains, as the microscopic workings are fully explicit. This is not to say that it is easy, as we still face the difficulty of extracting meaning from a system with billions of components and parameters. How could we do this for LLMs?</p><p>One familiar starting point in neuroscience is to measure the activity of neurons and try to correlate it with properties of the system inputs or outputs. The "grandmother cell" which fires when a subject sees his or her grandmother is an extreme (and controversial) example. Better established are the "place cells" in the hippocampus which fire when an animal passes through a specific part of its environment.</p><p>Generally there is no reason why the representation should be so direct; there might be some "neural code" which maps stimuli onto specific combinations or patterns of activity. The details of the neural code could even be different between one individual and the next. Analogous concepts in LLMs are the maps from input strings to intermediate results or "activations." The first of these is the embedding map Eq. 7. Considering each layer in succession, its outputs (sometimes called "contextualized embeddings") also define such a map. The details of these maps depend on details of the model, the training dataset and the choices made in the training procedure. Besides the hyperparameters, these include the random initializations of the parameters, the order in which data items are considered in training and their grouping into batches. Even small differences can be amplified by the nonlinear nature of the loss landscape.</p><p>One way to deal with this indeterminacy is to look for structure in the maps which does not depend on these choices. The linear relations Eq. 9 between word embeddings are a very elegant example, telling us (and presumably the model) something about the meanings of the words they represent. Moving on to the later layers, one can ask whether contextualized embeddings carry information about the grammatical role of a word, about other words it is associated to (such as the referent of a pronoun), etc.. One can go on to ask whether any of the many structures which -one would think -need to be represented to understand the real world, are visible in these embeddings.</p><p>Many structures are too intricate to show up in linear relations. A more general approach is to postulate a "target" for each training data item and train a "probe" model (usually an FFN) to predict it from the embeddings. If this works, one can go on to modify the internal representation in a minimal way which changes the probe prediction, and check if this leads to the corresponding effects on the output (see <ref type="bibr" target="#b10">[12]</ref> and references there). This procedure is simpler to explain in an example. A pretty example of probing for a world model is the recent work of Li et al <ref type="bibr" target="#b76">[78]</ref> (see also <ref type="bibr" target="#b128">[130]</ref>) on representations in a transformer model trained to play the board game Othello. <ref type="foot" target="#foot_27">39</ref>They train a model "Othello-GPT" <ref type="foot" target="#foot_28">40</ref> to take as input a sequence of 60 legal moves, for example "E3 D3 ..." in the standard algebraic notation, and at each step to predict the next move. The trained model outputs only legal moves with very high accuracy, and the question is whether this is done using internal representations which reflect the state of the game board, say the presence of a given color tile in a given position. Following the probe paradigm, they obtain FFNs which, given intermediate activations, can predict whether a board position is occupied and by which color tile. Furthermore, after modifying the activations so that the FFN's output has flipped a tile color, the model predicts legal moves for the modified board state, confirming the identification. Neuroscientists can only dream of doing such targeted experiments.</p><p>Numerous probe studies have been done on LLMs. One very basic question is how they understand grammatical roles and relations such as subject, object and the like. This question can be sharpened to probing their internal representations for parse trees, a concept we review in the appendix. To get the targets for the probe, one can use a large dataset of sentences labeled with parse trees, the Penn Treebank <ref type="bibr" target="#b88">[90]</ref>. This was done for BERT in <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b86">88]</ref> by the following procedure: denote the embedding (in a fixed layer) of word i as u i , then the model learns a projection P on this space, such that the distances d(i, j) ≡ ||P (u i -u j )|| in this inner product well approximate the distance between words i and j defined as the length of the shortest path connecting them in the parse tree. For BERT (with d ∼ 1000) this worked well with a projection P of rank ∼ 50.</p><p>Once one knows something about how information is represented by the models, one can go on to try to understand how the computations are done. One approach, also analogous to neuroscience, is to look for specific "circuits" which perform specific computations. An example of a circuit which appears in trained transformer models is the induction head <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b105">107]</ref>. This performs the following task: given a sequence such as "A B . . . A" it predicts a repetition, in this example "B." The matching between the tokens (the two A's in the example) is done by attention. A number of works have proposed and studied such circuits, with various motivations and using various theoretical lenses: interpretability and LLMs <ref type="bibr" target="#b104">[106]</ref>, in-context learning <ref type="bibr" target="#b105">[107,</ref><ref type="bibr" target="#b0">2]</ref>, formal language theory <ref type="bibr" target="#b92">[94,</ref><ref type="bibr" target="#b26">28]</ref>, computational complexity theory <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b80">82]</ref>, etc..</p><p>Reverse engineering a large network ab initio, i.e. with minimal assumptions about what it is doing, seems challenging, but maybe automated methods will be developed <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b44">46]</ref>. Another approach is to first develop a detailed computational model (CM) to perform a task without looking too much at the system under study, and then look for evidence for or against the hypothesis that the system under study uses it. This approach also has a long history in neuroscience <ref type="bibr" target="#b89">[91]</ref> and ways to test such hypotheses have been much discussed. As an example of a research tactic which does not require opening the black box, one can consider illusions which fool the system in some way. The response to these will often depend on contingent and non-optimal aspects of the model, so one can distinguish different models which solve the same task. A new class of predictions which becomes testable for LLMs is to look at performance as a function of model size (depth; number of parameters). A particular CM might require a certain model size or dataset properties in order to perform well. And of course, one can open the black box: by assuming a particular CM, one can make predictions for what probe experiments should work.</p><p>Simple tasks studied in this approach include modular addition <ref type="bibr" target="#b101">[103]</ref> and linear regression <ref type="bibr" target="#b0">[2]</ref>, where several CM's (gradient descent, ridge regression and exact least squares) were compared. Turning to language processing, a CM for parsing by transformer LLMs was developed in Zhou et al <ref type="bibr" target="#b142">[144]</ref>. While this is too lengthy to explain in detail here, let us give the basic idea, starting from the PCFG framework discussed in the appendix. Rather than try to represent a parse tree in terms of nodes and edges, it is represented by giving each position i in the list of words a set of variables α i,t,j , where t indexes a nonterminal (a left hand side of a rule) and j is another position. If α i,t,j is turned on, this means that a rule with t on the l.h.s. was used to generate that part of the tree stretching from position i to position j. This can be generalized to let α i,t,j be the probability that a rule is used. These variables (and additional variables β describing the rules used higher in the tree) satisfy simple recursion relations (the Inside-Outside parsing algorithm <ref type="bibr" target="#b85">[87]</ref>). If the rules have at most two symbols on the r.h.s., <ref type="foot" target="#foot_29">41</ref> these recursion relations are quadratic in the variables. By encoding the α variables as components of the embedding, they can be implemented using attention.</p><p>Naively, this model predicts that embedding dimension p must be very large, of order the number of nonterminals times the length of a sentence. Since realistic grammars for English have many hundreds of nonterminals, this seems to contradict the good performance of transformers with p ∼ 1000. This problem is resolved by two observations, of which the first is that one can get fairly good parsing with many fewer (∼ 20) nonterminals. The second is compression, that embeddings and circuits which are simple and interpretable can be mapped into more "random-looking" lower dimensional forms. This is a well understood concept for metric spaces <ref type="bibr" target="#b90">[92]</ref>, which was implicit in the discussion of word embeddings in §5. There the simplest construction (the co-occurence matrix) produced vectors with one component for each word, but by projecting on a subspace one could greatly reduce this dimension with little loss in accuracy. The generalization of these ideas to neural networks seems important.</p><p>Once one believes an LLM is carrying out a task using a particular circuit or CM, one can go on to ask how it learned this implementation from the data. One can get theoretical results in the limit of infinite training data and/or for simple tasks in which the dataset is constructed by a random process. Learning in transformer models trained on realistic amounts of data is mostly studied empirically and using synthetic data. A few recent interesting works are <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b101">103]</ref>. Intuitively one expects that simpler instances of a task are learned first, allowing the model to learn features which are needed to analyze more complex instances, and there is a lot of evidence for this. The idea that many submodels can be learned simultaneously, including straight memorization and submodels which rely on structure, also seems important. Ultimately learnability is crucial but we should keep in mind that in analogous questions in physics, evolution, and so on, it is much easier to understand optimal and critical points in the landscape than to understand dynamics.</p><p>This brings us to in-context learning, the ability of an LLM to perform diverse tasks given only a few examples of input-output pairs. The simplest hypothesis is that the model has learned the individual tasks, and the examples are selecting a particular task from this repertoire. It has been argued that this is guaranteed to happen (in the infinite data limit) for a model trained on a mixture of tasks <ref type="bibr" target="#b138">[140,</ref><ref type="bibr" target="#b134">136]</ref>. If the many tasks have common aspects (for example parsing might be used in any linguistic task), one can ask how the model takes advantage of this, a question discussed in <ref type="bibr" target="#b52">[54]</ref>.</p><p>Understanding LLMs is a very active research area and there is much more we could say, but let us finish by summarizing the two main approaches we described. One can postulate a representation and a computation designed to perform a task, and look for evidence that the LLM actually uses the postulated structure. Alternatively, one can look for a function in some simpler class (such as digital circuits) which well approximates the function computed by the transformer model, and then "reverse engineer" the simpler function to find out what it is doing. Either or both of these procedures could lead to interpretable systems and if so, are answers to the question "what has the LLM learned." There is no guarantee that they will work and it might turn out that one cannot understand LLMs without new ideas, but they deserve to be tried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Questions and discussion</head><p>Large language models have revolutionized computational linguistics and opened up many new applications of AI. Understanding how they work is both straightforward (we explained it in §6) and at the same time an outstanding scientific challenge. This is because the question "how do they work" has multiple meanings. On the one hand, LLMs are a relatively simple solution to the task of predicting the likely next word in a text. On the other hand, they also seem to perform many other tasks which require intelligence, such as solving the physics word problem in Figure <ref type="figure">1</ref>. While we do not have a strong understanding of what a system which can perform these tasks must do, a vast body of work in cognitive science and AI supports one's first naive intuition that such a system must be doing sophisticated analyses of language, must contain models of the real world, and must be able to do fairly general logical reasoning. Before it was demonstrated, the idea that all this could be learned as a byproduct of word prediction would have seemed hopelessly optimistic, had anyone dared to suggest it.</p><p>Extraordinary claims should be greeted with skepticism. One must guard against the possibility that a successful ML system is actually picking up on superficial aspects or statistical regularities of the inputs, the "clever Hans" effect. Addressing this is an important function of the benchmark evaluations discussed in §4. Of course as LLMs get good at performing tasks of practical value, the skeptical position becomes hard to maintain.</p><p>Intelligence and language are incredibly complex and diverse. According to Minsky,<ref type="foot" target="#foot_30">foot_30</ref> this diversity is a defining feature of intelligence. The goal of understanding LLMs (or any general AI) will not be accomplished by understanding all of the content in their training data, the "entire internet." Rather, the trick we need to understand is how a single system can learn from this diverse corpus to perform a wide range of tasks. Theories of "what is learnable" are a central part of computer science <ref type="bibr" target="#b66">[68]</ref>. Although theoretical understanding has a long way to go to catch up with LLM capabilities, for simpler and better understood tasks much is known.</p><p>In these notes we mostly looked at this question through the lens of computer science, and took as the gold standard for explaining how an LLM learns and performs a task, a computational model expressed as an algorithm or a circuit together with arguments that the trained LLM realizes this model. This point of view has many more insights to offer, but before we discuss them let us consider some other points of view. In §7 we drew the analogy between detailed study of transformer circuits and neuroscience -what others can we consider?</p><p>Another analogy is with cognitive psychology. LLMs are sufficiently humanlike to make this interesting, and there is a growing literature which applies tests and experimental protocols from psychology to LLMs, see for example <ref type="bibr" target="#b51">[53]</ref> and the many references there. When discussing this, we should keep in mind the vast differences between how humans and LLMs function. Human brains are not believed to use the backpropagation learning algorithm, indeed it has been argued that biological neural systems cannot use it <ref type="bibr" target="#b35">[37]</ref>. Perhaps related to this, brains are not feed-forward networks but have many bidirectional connections. Whatever brains are doing, it works very well: LLMs (like other current deep learning systems) need far more training data than humans. Furthermore, the LLMs we discussed do not interact with the world. Some argue that on philosophical grounds, a model trained only on language prediction can never learn meaning <ref type="bibr" target="#b14">[16]</ref>. While I do not find this particular claim convincing, I agree that we should not assume that LLMs perform tasks the same way humans do. Still both similarities and differences are interesting; can we make the analogies with cognitive psychology more precise?</p><p>One analogy <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b48">50]</ref>, is with the well known concept of "fast and slow thinking" in behavioral psychology <ref type="bibr" target="#b64">[66]</ref>. To summarize, humans are postulated to have two modes of thought, "system 1" which makes fast, intuitive judgments, and "system 2" which can focus attention and do calculations, logic, and planning. While system 2 is more general and less error-prone, using it requires conscious attention and effort. According to the analogy, LLMs implement system 1 thinking, and are weak at system 2 thinking.</p><p>In <ref type="bibr" target="#b82">[84]</ref> it is argued that LLMs have "formal linguistic competence" but not "functional competence." In plainer terms, they are solving problems by manipulating language using rules, but they lack other mechanisms of human thought. While it may be surprising that a purely rule-based system could do all that LLMs can do, we do not have a good intuition about what rule-based systems with billions of rules can do.</p><p>What are the other mechanisms? There is a long-standing hypothesis in cognitive science, modularity of mind <ref type="bibr" target="#b43">[45]</ref>, according to which the human brain has many "mental modules" with different capabilities. These include a language module of the sort that Chomsky famously advocated and many others, including one for geometric and physical reasoning, another for social reasoning and theory of mind, and perhaps others. Notably, formal logic and mathematical reasoning seem to call upon different brain regions from those which specialize in language <ref type="bibr" target="#b1">[3]</ref>, suggesting that these functions are performed by different mental modules. One can thus hypothesize that LLMs have commonalities with the human language module and might be useful scientific models for it, <ref type="foot" target="#foot_31">43</ref> but that progress towards human level capability will eventually stall without analogs of the other modules. <ref type="bibr" target="#b71">[73]</ref> A related claim is that current LLMs, even when they perform well on benchmarks, do not construct models of the world. Consider reasoning about spatial relations -for example if A is in front of B is in front of C, then A is in front of C. Such reasoning is greatly facilitated by representing the locations of objects in space, perhaps in terms of coordinates, perhaps using "place cells" or in some other non-linguistic way. If distance from the observer is explicitly represented and used in reasoning, then it becomes hard to get this type of question wrong. Conversely, to the extent that LLMs do get it wrong, this might be evidence that they lack this type of world model or cannot effectively use it.</p><p>There are many papers exhibiting LLM errors and suggesting such interpretations, but often one finds that next years' model does not make the same errors. At the present rate of progress it seems premature to draw any strong conclusions. My own opinion is that there is no barrier in principle to LLMs constructing internal non-linguistic models of the world, and the work <ref type="bibr" target="#b76">[78]</ref> on Othello-GPT discussed in §7 is a nice demonstration of what is possible. This is not to say that any and all models can be learned, but rather that it might be better for now to focus on other significant differences between LLM and human reasoning, of which there are many. I will come back to this below.</p><p>If LLMs and other connectionist systems do not work in the same way as brains, what other guidance do we have? In §7 we discussed one answer, the hypothesis that they work much like the algorithms and circuits studied in computer science. Perhaps trained LLMs implement algorithms like those designed by computational linguists, or perhaps new algorithms which were not previously thought up but which can be understood in similar terms. In either version this is still a hypothesis, but if we grant it we can draw on insights from theoretical computer science which apply to all such algorithms.</p><p>Computational complexity theory <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b135">137]</ref> makes many statements and conjectures about how the time and space required by a particular computation depends on the size of the problem (usually meaning the length of the input). The most famous of these, the P ̸ = NP conjecture, states (very loosely) that for problems which involve satisfying general logical statements, finding a solution can be much harder than checking that the solution is correct.</p><p>From this point of view, a central question is the complexity class of circuits which can be realized by constant depth transformers, meaning that the number of layers does not grow with the window size. Roughly, this is the complexity class TC 0 of constant depth circuits with threshold gates <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b93">95,</ref><ref type="bibr" target="#b94">96]</ref>. Of course in an autoregressive LLM one can repeat this operation to compute a sequence of words: thus the circuit defines the transition function of a finite state machine (FSM) where the state is the window, and the LLM has learned to simulate this FSM. If a natural algorithm to perform a task is in a more difficult complexity class than the FSM can handle, this is a reason to think the task cannot be learned by this type of LLM. Conversely, one might conjecture that any task for which there is an algorithm in this class can be learned, at least in the limit of an infinite amount of training data.</p><p>What about the lenses of pure mathematics, theoretical physics and allied fields? Besides my own personal interest in them, these fields have made substantial contributions to statistics and machine learning, especially the interface between statistical physics and machine learning is a vibrant field of research <ref type="bibr" target="#b69">[71,</ref><ref type="bibr" target="#b95">97]</ref>. Spin glass theory made a very deep impact, starting with the Hopfield model and developing into a far-reaching theory of optimization landscapes and complexity. Random matrix theory is central to high dimensional statistics <ref type="bibr" target="#b61">[63]</ref> and in many approaches to understanding deep learning <ref type="bibr" target="#b114">[116]</ref>. Mathematical approaches to language such as <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b84">86,</ref><ref type="bibr" target="#b87">89]</ref> can reveal new structure and provide deeper understanding.</p><p>Another reason to think pure mathematics and theoretical physics have more to contribute is that neural networks, transformers, and many of the models of neuroscience, are formulated in terms of real variables and continuous mathematics. By contrast, computer science is largely based on discrete mathematics, appropriate for some but not all questions. Perhaps word embeddings have important geometric properties, or perhaps the dynamics of gradient descent are best understood through the intuitions of continuous mathematics and physics. Arguments such as those in §7 which reduce neural networks to digital circuits, even if they do explain their functioning, may not be adequate to explain how they are learned.</p><p>Having at least mentioned some of the many points of view, let me combine these insights and speculate a bit on where this is going. Let me focus on three capabilities which seem lacking in current LLMs: planning, confidence judgments, and reflection.</p><p>Planning, solving problems whose solution requires choosing a series of actions and/or the consideration of future actions by other agents, is one of the core problems of AI. Making plans generally requires search, and in general search is hard (assuming P ̸ = NP). A familiar example is a chess program, which searches through a game tree to judge the longer term value of a candidate move by hypothesizing possible future moves. While much of the success of AlphaGo and AlphaZero is attributed to reinforcement learning by self-play, they also search through game trees; indeed the Monte Carlo tree search algorithm on which they built <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b21">23]</ref> was considered a key enabling breakthrough.</p><p>By contrast, LLMs have no component dedicated to search. While it does not seem impossible that search trees or other structures could be learned internally (like world models), it seems intuitively clear that an autoregressive model which predicts one word at a time and cannot go back to revise its predictions in light of what comes later will be seriously handicapped in planning. This observation is motivating a fair amount of current work on ways to incorporate search. LeCun has suggested adding a dynamic programming component to search through multiword predictions, as part of his "path towards autonomous machine intelligence" <ref type="bibr" target="#b73">[75]</ref>. Another proposal, the "tree of thoughts" model <ref type="bibr" target="#b140">[142]</ref>, works with a search tree of LLM responses. A system which uses hierarchical planning for mathematical theorem proving was developed in <ref type="bibr" target="#b60">[62]</ref>.</p><p>The next capability on my list, making and working with confidence judgments, has to do with the well known "hallucination" problem, that LLMs often simply invent statements, including untrue facts and imaginary citations. While advantageous for a poetry generator, and bearable for a system which makes suggestions which an expert human user will verify, this is a huge obstacle to many practical applications. Thus it is the subject of a great deal of research -a few of this month's papers are <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b77">79,</ref><ref type="bibr" target="#b79">81]</ref>. Perhaps by the time you read these words there will have already been major progress.</p><p>Why are LLMs producing these hallucinations? One intuition is that they are doing some sort of compression, analogous to JPEG image compression, which introduces errors <ref type="bibr" target="#b27">[29]</ref>. This point of view suggests that the problem will eventually be solved with larger models and perhaps better training protocols which focus on the more informative data items <ref type="bibr" target="#b124">[126]</ref>.</p><p>A related intuition is that the problems follow from inability to properly generalize. This comes back to the point about "world models" -a correct model, for example an internal encoding of place information, by definition correctly treats the properties being modeled. Now suppose we grant that the LLM is solving some class of problems, not by constructing such a model, but by rule-based reasoning. In other words, the LLM somehow learns rules from the corpus which it uses to make particular inferences which agree with the model. While such rules can cover any number of cases, there is no clear reason for such a rule set to ever cover all cases.</p><p>Another intuition is that the training data contains errors and this is reflected in the results. Certainly the internet is not known for being a completely reliable source of truth. This intuition also fits with the observation that adding code (computer programs) to the training set improves natural language reasoning. Code is a good source of rules because almost all of it has been debugged, leading to rules which are correct in their original context (of course they might not be correctly applied). It is a longstanding question whether internal representations (both in AI and in humans) are shared between different natural languages; it would be truly fascinating to know how much they are also shared with code. If this intuition is right, then LLMs reasoning capability might be improved by training on far more code and other content which is guaranteed to be correct. Such content could be generated synthetically as tautologies, or even better as formal verified mathematics (as proposed in <ref type="bibr" target="#b127">[129]</ref>).</p><p>Here is a different point of view: the problem is not that the systems make things up, after all creativity has value. Rather, it is that they do not provide much indication about the confidence to place in a particular output, and do not have ways to adapt their reasoning to statements known at different levels of confidence. Much of our reasoning involves uncertain claims and claims which turn out to be false, the point is to distinguish these from justified claims and keep track of our confidence in each belief. While it is possible to extract confidence scores from LLMs <ref type="bibr" target="#b63">[65]</ref>, there is also a philosophical point to make here: not all facts have the same epistemological status. Some facts are grounded in evidence; others are true by definition.</p><p>LLMs are of course statistical models. Even for a completely deterministic task, say doing arithmetic, a statistical approach to learning is very powerful. This is because learning based on inputs which consist of finitely many training examples, given in a random order, is naturally formulated in statistical terms. But without making additional non-statistical assumptions, one can never go from almost 100% confidence to 100% confidence.</p><p>This difference is crucial in many aspects of human thought. Of course, logical reasoning and mathematics stand out as prime examples. Long chains of reasoning are only possible if the individual links are reliable. But it is also crucial in social reasoning. There is an essential difference between statistical and evidence-based statements, say "Michael is a popular name," and tautological, definitional and descriptive statements such as "My name is Michael." While the first statement might be a subject of discussion, a model which can get confused about the second statement is clearly missing a defining aspect of human thought, and will lose the confidence of its interlocutor. Perhaps epistemological status and tautological correctness need to be somehow represented in the model. It need not be designed in, but the model needs to be given additional signals beyond next word prediction to learn it.</p><p>The third point on my list, reflection, does not seem to be much discussed, but to me seems just as important. In computer science, reflection is the capability of a system to work with its programs as a form of data <ref type="bibr" target="#b122">[124,</ref><ref type="bibr" target="#b136">138]</ref>. This is naturally possible for a computer programmed in assembly language, in which instructions are encoded in integers. To some extent it is also possible in Lisp, in which programs are encoded in a universal list data structure. As type systems and other programming language refinements are introduced, reflection becomes more difficult to provide, but it is necessary for systems-level programming and makes various standard tasks easier to implement.</p><p>Since an LLM operates on language, reflection for an LLM is the ability to work with its internal model in linguistic terms. This is related to ML interpretability, the ability to translate a model into understandable terms. In §7 we discussed interpretability of LLMs in terms of circuits and computational models, implicitly leaving these for a human to interpret and understand. One can imagine an "interpretation engine" which given a model, automatically produces a more interpretable description, in terms of circuits, rules, or even a description of the model's functioning in natural language. Given such an interpretation engine, by applying it to an LLM and sending its output as an input to the LLM, we can implement a form of reflection.</p><p>A basic human capability which corresponds to this process is the translation from procedural or other implicit forms of memory to linguistic, explicit memory. Very often, we learn by doing -riding a bicycle, solving math problems, interacting socially. We then reflect on what we have learned -in some unconscious way -and occasionally come up with verbal observations, summaries, in a word reflections. It is fascinating that combining the ideas we discussed brings us into contact with such topics.</p><p>To conclude, and for what it is worth, out of the forty years I have followed AI, this is by far the most exciting period. I agree with those who think LLMs are a major milestone and believe the ideas behind them -including the transformer architecture -will remain important even in the light of future progress. The questions they raise are interesting and important enough that -even as the specialists make remarkable progress -we need not leave the field to them, but as scientists and thinkers we should engage and try to contribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Grammars and parsing</head><p>Most readers will have encountered the idea of "sentence diagram," which graphically represents the decomposition of a sentence into clauses with a subject, verb and object, the assignment of adjectives and prepositional phrases to the nouns and verbs they modify, and so on. Formal versions of this concept are foundational in linguistics and computer science, and a short introduction (or review) is a good way to bring the general ideas we are discussing to life.</p><p>A formal grammar can be given by a set of "production rules" which can be used to generate grammatical strings. A simple example is in Figure <ref type="figure" target="#fig_2">3</ref>. Each line is a rule, which is made up of two strings of symbols separated by →, the left hand side or lhs and right hand side or rhs. These symbols can be "terminals" which appear in the language (such as +, * , x, 0 and so on in our example) or "nonterminals" which do not (such as TERM).</p><p>These rules are used as follows: We start with a string S containing a dis- tinguished "start" symbol (here EXPR). We then iterate the following process: choose a rule whose lhs occurs in S, and apply it by substituting one occurrence of this lhs in S with the rhs. Every string S which can be obtained by a finite sequence of these operations is considered grammatical, and by keeping track of the rule applications we get a parse tree. This is a graph whose nodes are symbols and whose edges connect a symbol which appears on the lhs of a rule application with the nodes for each of the symbols which appear on the rhs. <ref type="foot" target="#foot_32">44</ref>A good exercise is to work out the parse tree for the expression y + 1 * x and check that multiplication takes precedence over addition. Our example of a grammar is a context-free grammar, meaning that the left hand side of each rule consists of a single symbol. If we do not put this restriction, the resulting class of languages are universal computers (and thus suffer from potential undecidability). There is also a more restricted class of grammars called regular grammars (this hierarchy was found by Chomsky), but these cannot describe nested structures such as the parentheses of Eq. 17. The context-free grammars are the right degree of complexity for many purposes. In particular, programming languages and the formal languages of mathematical logic can be described using CFG's and thus the algorithms for working with them and associated theory are well developed.</p><p>Besides recognizing and parsing languages, one can describe other linguistic tasks in similar terms. A trivial example would be word replacement, with rules such as OLD i → NEW i . Realistic tasks benefit from frameworks with more structure. For example, to use the grammar in Eq. 17 to do arithmetic, we would be much better off with a framework in which the token VALUE carries an associated numerical or symbolic value. This can be done with the framework of attribute grammars. When we suggest in §8 that LLMs perform natural language tasks using systems of large numbers of rules, we have this sort of extended grammatical framework in mind.</p><p>CFG's are not really adequate for natural languages, with their inherent ambiguity and their many special cases and exceptions. A more general formalism is the probabilistic CFG. This is obtained by associating a probability distribution to each symbol which appears on the left hand side of a rule (the nonterminals). For example, we might stipulate that a VALUE has a 75% chance to be a number and a 25% chance to be a variable. With this information, a PCFG defines a probability distribution on strings, which gives zero probability to nongrammatical strings.</p><p>A symbolic approach to parsing would propose two primary algorithms. One is a parser, which given a grammar and an input produces the parse tree. Another would be an algorithm for learning a grammar from a corpus. Since any finite corpus can be described by many grammars, PCFG's are better suited than CFG's to this problem. In any case the learning and parsing algorithms are not necessarily related.</p><p>In the connectionist approach followed by LLMs, these two algorithms are subsumed into the definition of a model which can parse any PCFG whose rules are encoded in its weights. By training this on a corpus, the model learns a particular PCFG which generates the corpus. Interpretability as discussed in §7 then means reversing this relation, by extracting a parser and PCFG from the trained model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Language modeling performance as a function of model size, dataset size, and amount of compute used for training. From Kaplan et al, "Scaling Laws for Neural Language Models," 2020 [67].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>= Number of occurrences of w 1 w 2 . . . w N -1 w N w Number of occurrences of w 1 w 2 . . . w N -1 w .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A grammar for arithmetic expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Large Language Models (M/B/T = million/billion/trillion). In many cases several model sizes were considered; we quote the largest.</figDesc><table><row><cell cols="4">Year Model Number of Parameters Dataset size (tokens)</cell></row><row><cell>2018</cell><cell>GPT</cell><cell>110M</cell><cell>1B</cell></row><row><cell cols="3">2018 BERT 340M</cell><cell>3B</cell></row><row><cell cols="3">2019 GPT-2 1.5B</cell><cell>10B</cell></row><row><cell cols="3">2020 GPT-3 175B</cell><cell>500B</cell></row><row><cell cols="3">2022 PaLM 540B</cell><cell>780B</cell></row><row><cell cols="3">2023 GPT-4 1.4T (?)</cell><cell>?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A few of the many other milestones in LLM development are<ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b111">113,</ref><ref type="bibr" target="#b112">114]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_1"><p>https://www.sigsam.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_2"><p>https://cyc.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_3"><p>I first learned about this from<ref type="bibr" target="#b81">[83,</ref><ref type="bibr" target="#b99">101]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_4"><p>Many different definitions of this term can be found in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_5"><p>With the sign, L ≥ 0 and better models have smaller L. The term "loss function" is often used for an objective function with these properties.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_6"><p>In CS this term generally refers to the large scale arrangement of components of a system.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_7"><p>State of the art, in other words an improvement over all previously evaluated models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_8"><p>https://commoncrawl.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_9"><p>This is Eq. 3 (minus log perplexity) evaluated on texts which were removed or "held out" of the training set, to get a measure of generalization ability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_10"><p>A quantitative version of this claim is that performance for the "emergent" capability improves rapidly at some threshold value of the word prediction loss. This claim is disputed, see<ref type="bibr" target="#b118">[120,</ref><ref type="bibr" target="#b131">133]</ref> for discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_11"><p>We are calling this "phenomenology" following the physics use of the term, not its use in psychology and philosophy to describe the study of subjective experience.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_12"><p>A potential pitfall is that after a benchmark is published, the test items can find their way into future training data and then be solved by memorization. Methods to detect and prevent this are discussed in the references.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_13"><p>https://paperswithcode.com/dataset/big-bench</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_14"><p>https://github.com/EleutherAI/lm-evaluation-harness</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_15"><p>https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_16"><p>Regularization is a standard technique in statistics and ML used to control overfitting by models with too many parameters. If one does not regularize one sees other phenomena such as double descent<ref type="bibr" target="#b12">[14]</ref>. For further discussion see<ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_17"><p>This is roughly the time in which the gradient descent operates, see Eq. 16. In LLMs one often considers each data item only once in a training run, so it is related to (but different from) dataset size.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_18"><p>Statistical estimates of perplexity are in the 100's, and the best current LLMs have perplexity ∼ 20.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_19"><p>T is the temperature parameter which can be set in (say) the GPT user interface. Also, this ratio of exponentials is usually called "softmax" in machine learning as its β → ∞ limit is the "argmax" function producing a vector whose nonzero components have the same index values as the largest of the input(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_20"><p>Here #(w) denotes the number of occurences of "w" in the corpus. These ratios can also be expressed in terms of the pairwise mutual information, P N (w, u)/P (w)P (u).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_21"><p>Other reviews explaining these definitions include<ref type="bibr" target="#b107">[109,</ref><ref type="bibr" target="#b129">131]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_22"><p>One usually writes B as the product of two "key" and "query" matrices, and this can be used to restrict its rank.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_23"><p>The restriction j ≤ i to previous or current inputs is done to get an autoregressive model; one can relax this for other purposes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34" xml:id="foot_24"><p>Explicitly, v i = W 1 • max(0, W 0 • u i + b 0 ) + b 1, where b 0,1 are more learnable parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="37" xml:id="foot_25"><p>Some of the attention layers in GPT-3 are sparse.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="38" xml:id="foot_26"><p>As always in ML it is important that the dataset be "clean" -consistently tokenized, not having too much garbage text or repetitions, etc.. Many later LLMs also use programming language code in the dataset. Besides making code generation possible, it has been reported that this improves performance on natural language reasoning tasks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="39" xml:id="foot_27"><p>For readers not familiar with this game, two players alternate in placing black and white tiles on an 8 × 8 board, and each move results in "flipping" some opponent pieces to the player's color. The main point for us is that the function from moves to board state is easily computable yet very nonlocal and nonlinear.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="40" xml:id="foot_28"><p>While this model shares the GPT architecture, it is not trained on any language data, just on Othello games.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="41" xml:id="foot_29"><p>One can rewrite any grammar to have this property (Chomsky normal form) by introducing more nonterminals.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="42" xml:id="foot_30"><p>What magical trick makes us intelligent? The trick is that there is no trick. The power of intelligence stems from our vast diversity, not from any single, perfect principle.<ref type="bibr" target="#b98">[100]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="43" xml:id="foot_31"><p>Chomsky rejects this idea, saying that "The child's operating system is completely different from that of a machine learning program." (New York Times, March 8, 2023).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="44" xml:id="foot_32"><p>One can see examples for English sentences in the Wikipedia article "Parse tree."</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What learning algorithm is in-context learning? Investigations with linear models</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.15661</idno>
		<idno type="arXiv">arXiv:2211.15661</idno>
		<ptr target="http://arxiv.org/abs/2211.15661" />
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A distinct cortical network for mathematical knowledge in the human brain</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Amalric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2019.01.001</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1053811919300011" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computational complexity: a modern approach</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03520</idno>
		<idno>arXiv: 1502.03520</idno>
		<ptr target="http://arxiv.org/abs/1502.03520" />
		<title level="m">A Latent Variable Model Approach to PMI-based Word Embeddings</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics</title>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Piotrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Avigad</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.12433</idno>
		<idno type="arXiv">arXiv:2302.12433</idno>
		<ptr target="http://arxiv.org/abs/2302.12433" />
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dimensions of Neural-symbolic Integration -A Structured Survey</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Hitzler</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.cs/0511042</idno>
		<idno type="arXiv">arXiv:cs/0511042</idno>
		<ptr target="http://arxiv.org/abs/cs/0511042" />
		<imprint>
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explaining Neural Scaling Laws</title>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sharma</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.06701v1" />
		<imprint>
			<date type="published" when="2021-02">February 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">L</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2207.08799</idno>
		<idno type="arXiv">arXiv:2207.08799</idno>
		<ptr target="http://arxiv.org/abs/2207.08799" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benign overfitting in linear regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><surname>Tsigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="30063" to="30070" />
			<date type="published" when="2020">2020</date>
			<publisher>National Acad Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning: a statistical viewpoint</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><surname>Rakhlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09177</idno>
		<idno>arXiv: 2103.09177</idno>
		<ptr target="http://arxiv.org/abs/2103.09177" />
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probing classifiers: Promises, shortcomings, and advances</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2102.12452" />
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14368</idno>
		<idno>arXiv: 2105.14368</idno>
		<ptr target="http://arxiv.org/abs/2105.14368" />
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconciling modern machine-learning practice and the classical bias-variance trade-off</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1903070116</idno>
		<ptr target="https://www.pnas.org/content/116/32/15849" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15849" to="15854" />
		</imprint>
	</monogr>
	<note>Publisher: National Academy of Sciences eprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">To understand deep learning we need to understand kernel learning</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.01396v3" />
		<imprint>
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.463</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.463" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">From system 1 deep learning to system 2 deep learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://slideslive.com/38922304/from-system-1-deep-learning-to-system-2-deep-learning" />
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName><surname>Nasrabadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An enriched category theory of language: from syntax to semantics</title>
		<author>
			<persName><forename type="first">Tai-Danae</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Terilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Vlassopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07890</idno>
		<idno>arXiv: 2106.07890</idno>
		<ptr target="http://arxiv.org/abs/2106.07890" />
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<idno>arXiv: 2005.14165</idno>
		<ptr target="http://arxiv.org/abs/2005.14165" />
		<title level="m">Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods</title>
		<author>
			<persName><forename type="first">Cameron</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sparks of Artificial General Intelligence: Early experiments with GPT-4</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.12712v1" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discovering Latent Knowledge in Language Models Without Supervision</title>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.03827</idno>
		<ptr target="http://arxiv.org/abs/2212.03827" />
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent Neural Networks as Weighted Language Recognizers</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Maletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1711.05408</idno>
		<idno type="arXiv">arXiv:1711.05408</idno>
		<ptr target="http://arxiv.org/abs/1711.05408" />
		<imprint>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Finding Universal Grammatical Relations in Multilingual BERT</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04511</idno>
		<idno>arXiv: 2005.04511</idno>
		<ptr target="http://arxiv.org/abs/2005.04511" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tighter Bounds on the Expressivity of Transformer Encoders</title>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Cholak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Pillay</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.10743</idno>
		<idno type="arXiv">arXiv:2301.10743</idno>
		<ptr target="http://arxiv.org/abs/2301.10743" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chatgpt is a blurry jpeg of the web</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The New Yorker</title>
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generating Long Sequences with Sparse Transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.10509v1" />
		<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Loss Surfaces of Multilayer Networks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1412.0233</idno>
		<idno type="arXiv">arXiv:1412.0233</idno>
		<ptr target="http://arxiv.org/abs/1412.0233" />
		<imprint>
			<date type="published" when="2015-01">January 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<idno>arXiv: 2204.02311</idno>
		<ptr target="http://arxiv.org/abs/2204.02311" />
		<title level="m">Scaling Language Modeling with Pathways</title>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations</title>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Chughtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.03025</idno>
		<idno type="arXiv">arXiv:2302.03025</idno>
		<ptr target="http://arxiv.org/abs/2302.03025" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mathematical Foundations for a Compositional Distributional Model of Meaning</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1003.4394</idno>
		<idno type="arXiv">arXiv:1003.4394</idno>
		<ptr target="http://arxiv.org/abs/1003.4394" />
		<imprint>
			<date type="published" when="2010-03">March 2010</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07576</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in monte-carlo tree search</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on computers and games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The recent excitement about neural networks</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Crick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">337</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">V</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805v1" />
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guergana</forename><surname>Petrova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14501</idno>
		<idno>arXiv: 2012.14501</idno>
		<ptr target="http://arxiv.org/abs/2012.14501" />
		<title level="m">Neural Network Approximation</title>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Inductive Biases and Variable Creation in Self-Attention Mechanisms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10090</idno>
		<idno>arXiv: 2110.10090</idno>
		<ptr target="http://arxiv.org/abs/2110.10090" />
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Toy Models of Superposition</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.10652</idno>
		<idno type="arXiv">arXiv:2209.10652</idno>
		<ptr target="http://arxiv.org/abs/2209.10652" />
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Trapping LLM Hallucinations Using Tagged Context Prompts</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.06085</idno>
		<idno type="arXiv">arXiv:2306.06085</idno>
		<ptr target="http://arxiv.org/abs/2306.06085" />
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Studies in linguistic analysis</title>
		<author>
			<persName><forename type="first">John</forename><surname>Rupert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firth</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Wiley-Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The modularity of mind</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning Transformer Programs</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.01128</idno>
		<idno type="arXiv">arXiv:2306.01128</idno>
		<ptr target="http://arxiv.org/abs/2306.01128" />
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Neurosymbolic AI: The 3rd Wave</title>
		<author>
			<persName><forename type="first">Artur D'avila</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">C</forename><surname>Lamb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05876</idno>
		<ptr target="http://arxiv.org/abs/2012.05876" />
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</title>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.01066</idno>
		<idno type="arXiv">arXiv:2208.01066</idno>
		<ptr target="http://arxiv.org/abs/2208.01066" />
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Inductive Biases for Deep Learning of Higher-Level Cognition</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2011.15091</idno>
		<idno type="arXiv">arXiv:2011.15091</idno>
		<ptr target="http://arxiv.org/abs/2011.15091" />
		<imprint>
			<date type="published" when="2022-08">August 2022</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Grokking modular arithmetic</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gromov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.02679</idno>
		<idno type="arXiv">arXiv:2301.02679</idno>
		<ptr target="http://arxiv.org/abs/2301.02679" />
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
	<note>cond-mat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
		<idno>doi:10.1145/ 3236009</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3236009" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Thilo</forename><surname>Hagendorff</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.13988</idno>
		<idno type="arXiv">arXiv:2303.13988</idno>
		<ptr target="http://arxiv.org/abs/2303.13988" />
		<title level="m">Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods</title>
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A Theory of Emergent In-Context Learning as Implicit Structure Induction</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.07971</idno>
		<idno type="arXiv">arXiv:2303.07971</idno>
		<ptr target="http://arxiv.org/abs/2303.07971" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scaling Laws for Transfer</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2102.01293</idno>
		<idno type="arXiv">arXiv:2102.01293</idno>
		<ptr target="http://arxiv.org/abs/2102.01293" />
		<imprint>
			<date type="published" when="2021-02">February 2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A Structural Probe for Finding Syntax in Word Representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Training Compute-Optimal Large Language Models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.15556</idno>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<ptr target="http://arxiv.org/abs/2203.15556" />
		<imprint>
			<date type="published" when="2022-03">March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><surname>Krotov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.07253</idno>
		<idno type="arXiv">arXiv:2302.07253</idno>
		<ptr target="http://arxiv.org/abs/2302.07253" />
	</analytic>
	<monogr>
		<title level="j">Energy Transformer</title>
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
	<note>cond-mat, q-bio, stat</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Behind Deep Blue: Building the computer that defeated the world chess champion</title>
		<author>
			<persName><forename type="first">Feng-Hsiung</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Consistency Analysis of Chat-GPT</title>
		<author>
			<persName><forename type="first">Myeongjun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.06273</idno>
		<idno type="arXiv">arXiv:2303.06273</idno>
		<ptr target="http://arxiv.org/abs/2303.06273" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs</title>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.12283</idno>
		<idno type="arXiv">arXiv:2210.12283</idno>
		<ptr target="http://arxiv.org/abs/2210.12283" />
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">High Dimensional Statistical Inference and Random Matrices</title>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/math/0611589v1" />
		<imprint>
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Speech and language processing</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Language Models (Mostly) Know What They Know</title>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Das-Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Tran-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauna</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05221</idno>
		<ptr target="http://arxiv.org/abs/2207.05221" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fast and slow thinking. Allen Lane and Penguin Books</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Scaling Laws for Neural Language Models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2001.08361</idno>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<ptr target="http://arxiv.org/abs/2001.08361" />
		<imprint>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">An introduction to computational learning theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umesh</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Ricci-Tersenghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Zdeborova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Tramel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leticia</forename><forename type="middle">F</forename><surname>Cugliandolo</surname></persName>
		</author>
		<title level="m">Statistical Physics, Optimization, Inference, and Message-Passing Algorithms: Lecture Notes of the Les Houches School of Physics: Special Issue</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013-10">October 2013. 2013. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aab3050</idno>
		<idno>doi:10.1126/science. aab3050</idno>
		<ptr target="https://www.sciencemag.org/lookup/doi/10.1126/science.aab3050" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Building Machines That Learn and Think Like People</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1604.00289" />
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Popular talks and private discussion</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BZ5a1r-kVsf" />
		<title level="m">A path towards autonomous machine intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Solving Quantitative Reasoning Problems with Language Models</title>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Gutman-Solo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.14858</idno>
		<idno type="arXiv">arXiv:2206.14858arXiv:2206.14858</idno>
		<ptr target="http://arxiv.org/abs/2206.14858" />
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aspen</forename><forename type="middle">K</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.13382</idno>
		<idno type="arXiv">arXiv:2210.13382</idno>
		<ptr target="http://arxiv.org/abs/2210.13382" />
		<imprint>
			<date type="published" when="2023-02">February 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oam</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2306.03341</idno>
		<idno type="arXiv">arXiv:2306.03341</idno>
		<ptr target="http://arxiv.org/abs/2306.03341" />
		<title level="m">Hanspeter Pfister, and Martin Wattenberg. Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</title>
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Holistic Evaluation of Language Models</title>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilara</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Cosgrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Acosta-Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Koreeda</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.09110</idno>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<ptr target="http://arxiv.org/abs/2211.09110" />
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Let&apos;s Verify Step by Step</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Hunter Lightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yura</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teddy</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Cobbe</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.20050</idno>
		<idno type="arXiv">arXiv:2305.20050</idno>
		<ptr target="http://arxiv.org/abs/2305.20050" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Transformers Learn Shortcuts to Automata</title>
		<author>
			<persName><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10749</idno>
		<ptr target="http://arxiv.org/abs/2210.10749" />
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Dissociating language and thought in large language models: a cognitive perspective</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">A</forename><surname>Ivanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><forename type="middle">A</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Kanwisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Fedorenko</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.06627</idno>
		<idno type="arXiv">arXiv:2301.06627</idno>
		<ptr target="http://arxiv.org/abs/2301.06627" />
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">A Solvable Model of Neural Scaling Laws</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Sully</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.16859</idno>
		<idno type="arXiv">arXiv:2210.16859</idno>
		<ptr target="http://arxiv.org/abs/2210.16859" />
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
	<note>hep-th, stat</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Semantic Spaces. arXiv.org</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Manin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Marcolli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04238v1</idno>
		<ptr target="http://arxiv.org/abs/1605.04238v1" />
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Emergent linguistic structure in artificial neural networks trained by self-supervision</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="30046" to="30054" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Mathematical Structure of Syntactic Merge</title>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Marcolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Berwick</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.18278</idno>
		<idno type="arXiv">arXiv:2305.18278</idno>
		<ptr target="http://arxiv.org/abs/2305.18278" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Lecture notes on metric embeddings</title>
		<author>
			<persName><surname>Jir Ǐ Matoušek</surname></persName>
		</author>
		<ptr target="https://kam.mff.cuni.cz/~matousek/ba-a4.pdf" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Machines who think: A personal inquiry into the history and prospects of artificial intelligence</title>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mccorduck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cli</forename><surname>Cfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06866</idno>
		<idno>arXiv: 2004.06866</idno>
		<ptr target="http://arxiv.org/abs/2004.06866" />
		<title level="m">On the Linguistic Capacity of Real-Time Counter Automata</title>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">The Parallelism Tradeoff: Limitations of Log-Precision Transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2207.00729</idno>
		<idno type="arXiv">arXiv:2207.00729</idno>
		<ptr target="http://arxiv.org/abs/2207.00729" />
		<imprint>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.16213</idno>
		<idno>arXiv: 2106.16213</idno>
		<ptr target="http://arxiv.org/abs/2106.16213" />
		<title level="m">Saturated Transformers are Constant-Depth Threshold Circuits</title>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Information, physics, and computation</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mezard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">The Quantization Model of Neural Scaling</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uzay</forename><surname>Girit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.13506</idno>
		<idno type="arXiv">arXiv:2303.13506</idno>
		<ptr target="http://arxiv.org/abs/2303.13506" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
	<note>condmat</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Minsky</surname></persName>
		</author>
		<title level="m">Society of mind. Simon and Schuster</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
		<idno>arXiv preprint math/0212400</idno>
		<title level="m">Pattern theory: the mathematics of perception</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Pattern theory: the stochastic analysis of real-world signals</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnès</forename><surname>Desolneux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Progress measures for grokking via mechanistic interpretability</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.05217</idno>
		<idno type="arXiv">arXiv:2301.05217</idno>
		<ptr target="http://arxiv.org/abs/2301.05217" />
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Empirical explorations of the logic theory machine: a case study in heuristic</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Clifford</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability</title>
		<imprint>
			<date type="published" when="1957">1957</date>
			<biblScope unit="page" from="218" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">The quest for artificial intelligence</title>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2022/mech-interp-essay/index.html" />
		<title level="m">Mechanistic interpretability, variables, and the importance of interpretable bases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Das-Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.11895</idno>
		<idno type="arXiv">arXiv:2209.11895</idno>
		<ptr target="http://arxiv.org/abs/2209.11895" />
	</analytic>
	<monogr>
		<title level="j">In-context Learning and Induction Heads</title>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Formal Algorithms for Transformers</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.09238</idno>
		<ptr target="http://arxiv.org/abs/2207.09238" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Grokking</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02177</idno>
		<idno>arXiv: 2201.02177</idno>
		<ptr target="http://arxiv.org/abs/2201.02177" />
		<title level="m">Generalization Beyond Overfitting on Small Algorithmic Datasets</title>
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Measuring and Narrowing the Compositionality Gap in Language Models</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.03350</idno>
		<idno type="arXiv">arXiv:2210.03350</idno>
		<ptr target="http://arxiv.org/abs/2210.03350" />
		<imprint>
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
	<note>Publisher</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners. undefined</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://www.semanticscholar.org/paper/" />
	</analytic>
	<monogr>
		<title level="m">Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/ 9405cc0</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>d6169988371b2755e573cc28650d14dfe</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<idno>arXiv: 1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Text Transformer</note>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Hopfield Networks is All You Need</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milena</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Kjetil Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2008.02217</idno>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<ptr target="http://arxiv.org/abs/2008.02217" />
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sho</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Yaida</surname></persName>
		</author>
		<author>
			<persName><surname>Hanin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10165</idno>
		<idno>arXiv: 2106.10165</idno>
		<ptr target="http://arxiv.org/abs/2106.10165" />
		<title level="m">The Principles of Deep Learning Theory</title>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
		</imprint>
	</monogr>
	<note>hep-th, stat</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">A general framework for parallel distributed processing</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Artificial intelligence a modern approach</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Stuart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Pearson Education, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Are emergent abilities of large language models a mirage?</title>
		<author>
			<persName><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brando</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<idno>ArXiv, abs/2304.15004</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">The deep learning revolution</title>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Xxii. programming a computer for playing chess</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">314</biblScope>
			<biblScope unit="page" from="256" to="275" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory</title>
		<meeting>the fifth annual workshop on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Procedural reflection in programming languages volume i</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cantwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Beyond neural scaling laws: beating power law scaling via data pruning</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sorscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.14486</idno>
		<idno type="arXiv">arXiv:2206.14486</idno>
		<ptr target="http://arxiv.org/abs/2206.14486" />
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<idno>arXiv:2206.04615</idno>
		<ptr target="http://arxiv.org/abs/2206.04615" />
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs, stat] type: article</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">The bitter lesson</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A promising path towards autoformalization and general artificial intelligence</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computer Mathematics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">Chess as a Testbed for Language Model State Tracking</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2102.13249</idno>
		<idno type="arXiv">arXiv:2102.13249</idno>
		<ptr target="http://arxiv.org/abs/2102.13249" />
		<imprint>
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">An Introduction to Transformers</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.10557</idno>
		<idno type="arXiv">arXiv:2304.10557</idno>
		<ptr target="http://arxiv.org/abs/2304.10557" />
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="https://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2206.07682</idno>
		<ptr target="https://arxiv.org/abs/2206.07682" />
		<title level="m">Emergent Abilities of Large Language Models. 2022. Publisher: arXiv Version Number: 2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">On the Practical Computational Power of Finite Precision RNNs for Language Recognition</title>
		<author>
			<persName><forename type="first">Gail</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1805.04908</idno>
		<idno type="arXiv">arXiv:1805.04908</idno>
		<ptr target="http://arxiv.org/abs/1805.04908" />
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">NaturalProofs: Mathematical Theorem Proving in Natural Language</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">The Learnability of In-Context Learning</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.07895</idno>
		<idno type="arXiv">arXiv:2303.07895</idno>
		<ptr target="http://arxiv.org/abs/2303.07895" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Mathematics and computation: A theory revolutionizing technology and science</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Wigderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Reflective_programming" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">What Is ChatGPT Doing</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wolfram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and Why Does It Work? Stephen Wolfram</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">An Explanation of In-context Learning as Implicit Bayesian Inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2111.02080</idno>
		<idno type="arXiv">arXiv:2111.02080</idno>
		<ptr target="http://arxiv.org/abs/2111.02080" />
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.03466</idno>
		<idno type="arXiv">arXiv:2203.03466</idno>
		<ptr target="http://arxiv.org/abs/2203.03466" />
		<title level="m">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</title>
		<imprint>
			<date type="published" when="2022-03">March 2022</date>
		</imprint>
	</monogr>
	<note>condmat</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.10601</idno>
		<idno type="arXiv">arXiv:2305.10601</idno>
		<ptr target="http://arxiv.org/abs/2305.10601" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.17311</idno>
		<idno type="arXiv">arXiv:2305.17311</idno>
		<ptr target="http://arxiv.org/abs/2305.17311" />
		<imprint>
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Do Transformers Parse while Predicting the Masked Word?</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08117</idno>
		<idno type="arXiv">arXiv:2303.08117</idno>
		<ptr target="http://arxiv.org/abs/2303.08117" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.18223</idno>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<ptr target="http://arxiv.org/abs/2303.18223" />
		<title level="m">A Survey of Large Language Models</title>
		<imprint>
			<date type="published" when="2023-09">September 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">MiniF2F: a crosssystem benchmark for formal Olympiad-level mathematics</title>
		<author>
			<persName><forename type="first">Kunhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Polu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2109.00110</idno>
		<idno type="arXiv">arXiv:2109.00110</idno>
		<ptr target="http://arxiv.org/abs/2109.00110" />
		<imprint>
			<date type="published" when="2022-02">February 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
