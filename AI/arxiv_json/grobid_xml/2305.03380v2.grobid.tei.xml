<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualization in the Era of Artificial Intelligence: Experiments for Creating Structural Visualizations by Prompting Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-11">11 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hans-Georg</forename><surname>Fill</surname></persName>
							<idno type="ORCID">0000-1111-2222-3333</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Group Digitalization and Information Systems</orgName>
								<orgName type="institution">University of Fribourg</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Muff</surname></persName>
							<idno type="ORCID">1111-2222-3333-4444</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Group Digitalization and Information Systems</orgName>
								<orgName type="institution">University of Fribourg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visualization in the Era of Artificial Intelligence: Experiments for Creating Structural Visualizations by Prompting Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-11">11 May 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">ED0AF5E291798A122ADD4B7B9FFBFAAE</idno>
					<idno type="arXiv">arXiv:2305.03380v2[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text and images from textual input. However, their potential to generate complex 2D/3D visualizations has been largely unexplored. We report initial experiments showing that LLMs can generate 2D/3D visualizations that may be used for legal visualization. Further research is needed for complex 2D visualizations and 3D scenes. LLMs can become a powerful tool for many industries and applications, generating complex visualizations with minimal training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The creation and use of visualizations are common in many areas of science and practice. In particular, the field of legal visualization has a long historical tradition of using visualizations to explain complex legal relationships and scenarios <ref type="bibr" target="#b0">[1]</ref>. The core advantage of visualization is that it provides a graphical representation of something that would otherwise be difficult for humans to understand. It is thus a form of complexity reduction, aiming at human comprehension and understanding. This can include large data sets that are easier for humans to understand in visual formats, and where certain properties of the data can only be discovered through visual means. Examples range from visual representations of business data, as commonly used in statistics and data analytics, to visualizations of medical data, e.g., to discover features in the human body, or visualizations of sensor data and their extrapolations to create animated weather maps <ref type="bibr" target="#b5">[6]</ref>. Visualization further includes visual representations of knowledge for facilitating human communication, cf. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In contrast to visualizations of data that are typically generated through algorithms, such visualizations are created and curated by humans manually <ref type="bibr" target="#b4">[5]</ref>. Based on the intended purpose of the visualization, they may be based on a formal or semi-formal language -as typically used in conceptual modeling <ref type="bibr" target="#b8">[9]</ref> -or may consist of ad-hoc compositions of shapes and drawings, e.g., as typically found in the area of infographics, cf. <ref type="bibr" target="#b1">[2]</ref> or in structural legal visualization <ref type="bibr" target="#b13">[14]</ref>.</p><p>The creation of this latter type of visualization typically requires not only a deep understanding of the domain and scenario to be represented but also skills for using appropriate tools and procedures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. These tools can range from drawing tools that allow the creation of arbitrary shapes or the reuse and adaptation of existing shapes, over modeling tools that require the use of predefined shapes and concepts, to sophisticated dedicated visualization tools, e.g., for the creation of 3D representations cf. <ref type="bibr" target="#b6">[7]</ref>. While standard drawing tools are fairly easy to operate, modeling and specialized visualization tools often require significant training to use them effectively.</p><p>With the recent emergence of highly capable large language models, this last aspect can be addressed in previously unimagined ways, especially through creating computer code in various languages <ref type="bibr" target="#b10">[11]</ref>. At their core, large language models are trained on very large amounts of text so that they represent a probabilistic model of word sequences <ref type="bibr" target="#b9">[10]</ref>. Each word in the vocabulary of the language model is thus assigned a probability that can follow a given input sequence or an already generated sequence of words. Recent scientific and technological advances have made it possible to increase the training sets for such language models to hundreds of billions of parameters <ref type="bibr" target="#b12">[13]</ref>. In November 2022, the company OpenAI made one large language model publicly accessible in the form of a dialogue system called ChatGPT and a corresponding API. The underlying GPT language models have been trained on more than 175 billion parameters and are not only able to generate text sequences in different styles based on input sequences in the form of prompts but can also output code of programming languages and may soon be able to process multimodal data as well, e.g. images.</p><p>Recently, the application of these large language models has been explored in a variety of domains to see how they can support the creation of text and code in various ways -e.g., <ref type="bibr" target="#b7">[8]</ref>. Therefore, in the following, we report on some experiments we have conducted to create visualizations using prompts to a large language model in the form of GPT-4 . The goal of these experiments was to explore which types of visualizations may be generated in this way, with the particular goal of supporting at some point structural legal visualizations as used in legal informatics. Thereby we aim to contribute to the discussion on how novel types of artificial intelligence can support visualizations from a metaperspective <ref type="bibr" target="#b11">[12]</ref>. The approach we use in the following targets the generation of visualizations based on graphical primitives, i.e., using vector graphics or programming code. This is in contrast to other approaches of using large language models, for generating pixel graphics as done for example by Dall-E.</p><p>We structure the experiments according to the following categories: a) twodimensional visualizations, b) simple three-dimensional visualizations for a webbased environment, and c) complex three-dimensional visualizations for dedicated 3D modeling editors. Thereby, the first category aims at structural legal visualizations as typically generated today manually, e.g., in PowerPoint, whereas the second and the third category target real three-dimensional visualizations, which are today being generated for example using programming libraries such as three.js or the open-source 3D modeling toolkit Blender . The latter requires advanced knowledge of either programming or specialized 3D modeling software. Generating visualizations for these environments without this knowledge seemed to us the most interesting use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prompts for Generating Two-Dimensional Visualizations</head><p>The prompts we will show in the following were preceded by several trials to find out which type of instructions would lead to the most promising results. Further, it was required to assess which type of code for describing visualizations could be generated by GPT-4. Thereby we found that SVG (Scalable Vector Graphics) code, which is an open standard for representing vector graphics, could be successfully generated by GPT-4. Thus, the first prompt we show was formulated as follows:</p><p>Prompt 1: Create the visualization of two trees that are connected by a black line in SVG. Only show the code surrounded by triple backticks, do not add any explanation.</p><p>The result of this prompt is shown in Figure <ref type="figure">1a</ref>. We decided not to show the actual code generated by GPT4, but rather the result in the form of the resulting graphical representation when the code is executed in an SVG viewer application. The last sentence of the prompt refers to the display of the code result in the ChatGPT application, which can be forced to display the code in a separate area with this command. The example shows that the LLM can interpret the command and come up with a valid solution. The type of prompt we used here is a so-called zero-shot prompt, which means that we rely on information already known to the large language model, i.e., it already knows about SVG code and how to create it. Nevertheless, it's fascinating that it can easily produce an abstract representation of a tree in this format. In the next experiment we stayed in the same session of the prompting and asked to modify the previous result with the following prompt:</p><p>Prompt 2: Change one tree to represent a fur with blue branches.</p><p>The result can be seen in Figure <ref type="figure">1b</ref>. It correctly modified one tree and now shows a fur with blue branches. The next alteration we wanted to conduct was to change the surrounding of the trees. Thus we again stayed in the same session and executed this prompt: Prompt 3: Put the two trees in the middle of a yellow lake.</p><p>As shown in Figure <ref type="figure">1c</ref>, the result was not quite what we intended. The entire background of the image was now colored yellow. Of course, one could regard this as a valid result, but we would have imagined something more like a yellow ellipse surrounding the trees. Finally, as common for many structural legal visualizations, we tried to add text to the previously generated image by the following prompt: Prompt 4: Add the text "Paragraph 1" below the left tree. This worked equally well, as can be seen from Figure <ref type="figure">1d</ref>. The results we received via these prompts for generating two-dimensional visualizations are already quite promising.</p><p>(a) Prompt 1 (b) Prompt 2 (c) Prompt 3 (d) Prompt 4</p><p>Fig. <ref type="figure">1</ref>: Results of Prompts one to four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prompts for Generating Three-dimensional Visualizations</head><p>Thus, we advanced by trying to generate three-dimensional representations as well. We found that GPT-4 is capable of generating valid code in the JavaScript programming language. This language is today very common in web-based environments and can be executed by all major web browsers. It also contains commands to access the WebGL API, which allows the creation of three-dimensional visualizations whose rendering can be accelerated directly by graphic processing units. Therefore, we have provided GPT-4 with the following prompt:</p><p>Prompt 5: Generate JavaScript code for displaying a simple house with yellow walls and a red roof using WebGL. Surround the code by triple backticks and do not add any explanation.</p><p>We then executed the resulting code in a standard Firefox web browser and took a screenshot, shown in Figure <ref type="figure" target="#fig_1">2a</ref>. Despite the three-dimensional representation in the code, the visualization shown only resembles a 2D image. We found that the mechanisms for interacting with the three-dimensional representation were missing. This led to the next experiment as shown in Prompt 6. Here, we reverted to the three.js API, an open-source library for abstracting from the core WebGL code and making it easier to describe three-dimensional scenes. GPT-4 is also capable of generating code for this API. Prompt 6: Create the code for displaying a three-dimensional house on a website using Three.js. The house shall have yellow walls and a red roof. It stands on a blue lake, which is made of a material that reflects objects. Add several lights to the scene. Only display the code using triple backticks without any explanations.</p><p>The result of Prompt 6 is shown in Figure <ref type="figure" target="#fig_1">2b</ref>. Now, we can already better grasp the three-dimensional nature of the scene. Although three.js does have mechanisms for interacting with scenes, this has not been automatically added. So in Prompt 7, we specifically asked to include an animation loop so that the scene would be animated. Further, we asked for adding several lights to the scene. Although this prompt still does not require programming knowledge, it requires familiarity with some concepts used by three.js for displaying three-dimensional scenes.</p><p>Prompt 7: Create the code for displaying a three-dimensional house on a website using Three.js. The house shall have yellow walls and a red roof. It stands on a blue lake, which is made of a shiny material that reflects objects. In front of the house there is a fur tree. Add several lights to the scene. Add an animation loop to the scene so that the camera flies in a circle around the house. Only display the code using triple backticks without any explanations.</p><p>The result of Prompt 7 is shown in Figure <ref type="figure" target="#fig_1">2c</ref>. Now, the user sees an animated version of the scene where the camera rotates around the house. Unfortunately, the requested blue lake, which had been correctly included in previous iterations, was now missing from this result as well as a correct tree representation. Although three.js is a very advanced API for displaying 3D scenes, it always has to be embedded in the context of a website, which requires additional code to be generated. Due to current limitations of the accessible versions of ChatGPT, the amount of code that can be generated is rather limited. Therefore, we were interested in trying a more efficient approach. The open-source tool Blender is a full-fledged three-dimensional modeling and animation tool. It can be used to generate any kind of three-dimensional representation, including videos of animations. It is freely available, well-documented, and greatly supported by a large open-source community. In addition, Blender provides a scripting interface based on the Python programming language. This permits to access almost any functionality of the tool programmatically, i.e., by writing programming code and executing it, instead of accessing the user interface. One typical use case for this scripting interface is the creation of multiple 3D objects in particular arrangements, which would require a lot of manual For example, this approach would allow a script to create hundreds of random three-dimensional objects without a single click. For our purposes, we discovered that GPT-4 can generate valid Python code based on the Blender libraries, which permits the generation of 3D scenes from natural language descriptions. As shown in Prompt 8 we executed the following instruction: Prompt 8: Create the Python code for Blender for the following scene: There is a house with a red roof and white walls with blue windows. The camera looks at the house slightly from above and a point light points to the top of the roof. The house is placed on a large white plane where shadows are casted by the point light. Only show the code surrounded by triple backticks and do not add any explanation. This prompt generated Python code that could be directly executed using the scripting interface of Blender. The result of the Blender user interface is shown in Figure <ref type="figure">3</ref>. It correctly displays a three-dimensional scene including a Fig. <ref type="figure">3</ref>: Scene generated by the Python scripting interface in Blender using the result of Prompt 8 camera and a light, all with the correct positioning. From there, this scene could be directly further extended or modified, or exported in a large variety of file formats, e.g., to integrate it a PowerPoint presentation. Although a user would still need to have the skills to do this in Blender, the main step of creating the 3D objects did not require any knowledge of 3D modeling at all, but was based on natural language statements.</p><p>Finally, we issued a more complex prompt for generating Python code for Blender. As shown in Prompt 9, we added further objects to the scene and wanted to see how GPT-4 would represent a car with yellow windows. Prompt 9: Create the Python code for Blender for adding the following items to the scene above: A tree with a brown trunk and green leaves stands in front of the house. A blue car with yellow windows stands next to the house. An arrow points from the car to the tree. Only show the code surrounded by triple backticks and do not add any explanations.</p><p>The result of this prompt is shown in Figure <ref type="figure" target="#fig_2">4</ref>. The tree is nicely modeled using a green sphere and a brown cylinder representing the trunk of the tree. However, the car is very abstract and is not recognizable, as is the arrow that we tried to add. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The experiments permit several interesting insights into the current and future possibilities of using large language models for generating structural visualizations. First, none of the experiments required us to write a single line of code.</p><p>Rather, the code was only generated and then pasted into a corresponding execution environment (i.e., an SVG viewer, the Browser, and the Blender Python scripting editor). It can be expected that the further advancement of large language models will offer direct integration into such execution environments, e.g., via plugins for Blender that directly access the GPT API, which would already be technically feasible today. Then, there would be a seamless transition from specifying scenes in natural language and viewing them in the corresponding tool. In terms of the quality of the generated visualizations, we only showed prompts for comparatively simple scenarios. We have also tried to describe more complex structural legal visualizations, such as those available on Prof. Friedrich Lachmayer's website at legalvisualization.com, but have not yet been able to assemble complex scenes based on a textual description. It turned out that textual descriptions of visualizations that are precise enough for code generation become quite extensive, especially for positioning objects to each other in three-dimensional space. In such cases, it is still much easier to arrange objects using the mouse. However, for creating 3D objects that can then be arranged, the use of large language models seems an interesting option for the future as it permits also users not experienced in the details of 3D modeling to create objects which can then be positioned or scaled as needed for example. Unlike existing databases of 3D objects, users can create their 3D objects and even textures in their own style without knowing the technical details of the creation environment. For the two-dimensional models, the use of large language models worked rather well. Although we still need to explore further, which types of objects can be created in this way, the created visualizations already showed a high degree of exactness and matched mostly very well the issued prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Outlook</head><p>In this short paper, we reported on first experiments for creating structural visualizations via large language models. The insights we gained may serve as inspiration for further experiments by the legal visualization community. For example, in the future one could imagine trying to create comics of legal scenarios in this way, or maybe even animations for use in legal design. From our side, we plan to further investigate how large language models can be trained or instructed in a way to better understand the different visualization formats and integrate them into conceptual visual modeling platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Results of Prompts five to seven.</figDesc><graphic coords="5,155.51,443.29,62.25,87.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Result of the Blender scene created by Prompt 9</figDesc><graphic coords="7,134.77,369.56,345.82,177.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="6,134.77,424.19,345.82,204.86" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual law and legal design: Questions and tentative answers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Brunschwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Legal Informatics Symposium IRIS 2021</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Schweighofer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Kummer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Saarenpää</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Eder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Hanke</surname></persName>
		</editor>
		<meeting>the 24th International Legal Informatics Symposium IRIS 2021<address><addrLine>Bern</addrLine></address></meeting>
		<imprint>
			<publisher>Editions Weblaw</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="179" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Getting graphic about infographics: design lessons learned from popular infographics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Lowenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Literacy</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge visualization: towards a new discipline and its fields of application</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eppler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Burkhard</surname></persName>
		</author>
		<ptr target="https://doc.rero.ch/record/5196/files/1wpca0402.pdf" />
	</analytic>
	<monogr>
		<title level="m">Università della Svizzera italiana</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic visualization of heterogenous knowledge sources</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Fill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modellierung für Wissensmanagement -Workshop im Rahmen der Modellierung</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Hinkelmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Reimer</surname></persName>
		</editor>
		<imprint>
			<publisher>Sonderdrucke der Fachhochschule Nordwestschweiz</publisher>
			<date type="published" when="2006">2006. 2006. 2006</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the technical realization of legal visualizations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Fill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10 Jahre IRIS: Bilanz und Ausblick</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Schweighofer</surname></persName>
		</editor>
		<meeting><address><addrLine>Boorberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="463" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visualisation for Semantic Information Systems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Fill</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-8349-9514-8</idno>
		<ptr target="https://doi.org/10.1007/978-3-8349-9514-8" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Gabler</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bridging pictorial and model-based creation of legal visualizations: The pict-mod method</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Fill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kooperation/Co-operation-Digitale Ausgabe zum Tagungsband des 18. Internationalen Rechtsinformatik Symposions IRIS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conceptual modeling and large language models: Impressions from first experiments with chatgpt. Enterprise Modelling and Information Systems Architectures (EMISAJ)</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Fill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fettke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Köpke</surname></persName>
		</author>
		<idno type="DOI">10.18417/emisa.18.3</idno>
		<ptr target="https://doi.org/https://doi.org/10.18417/emisa.18.3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Conceptual Modeling</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fundamental view on the process of conceptual modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hoppenbrouwers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Proper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Van Der Weide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M L</forename><surname>Delcambre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mylopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1007/11568322_9</idno>
		<ptr target="https://doi.org/10.1007/115683229" />
	</analytic>
	<monogr>
		<title level="m">Conceptual Modeling -ER 2005, 24th International Conference on Conceptual Modeling</title>
		<title level="s">Proceedings. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Pastor</surname></persName>
		</editor>
		<meeting><address><addrLine>Klagenfurt, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">October 24-28, 2005. 2005</date>
			<biblScope unit="volume">3716</biblScope>
			<biblScope unit="page" from="128" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/~jurafsky/slp3/" />
		<title level="m">Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</title>
		<imprint/>
	</monogr>
	<note>Third edition draft edn. (2023</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Evolution through large models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.08896</idno>
		<idno>CoRR abs/2206.08896</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.08896" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On metavisualization and properties of visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sreevalsan-Nair</surname></persName>
		</author>
		<idno type="DOI">10.5220/0011794300003417</idno>
		<ptr target="https://doi.org/10.5220/0011794300003417" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -IVAPP</title>
		<meeting>the 18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -IVAPP</meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="230" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1706.03762</idno>
		<ptr target="https://doi.org/https://doi.org/10.48550/arXiv.1706.03762" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural legal visualization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Čyras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lachmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lapin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="219" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
