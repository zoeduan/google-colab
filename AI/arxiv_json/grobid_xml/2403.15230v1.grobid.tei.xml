<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-22">22 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Katzy</surname></persName>
							<email>j.b.katzy@tudelft.nl</email>
						</author>
						<author>
							<persName><forename type="first">Răzvan-Mihai</forename><surname>Popescu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arie</forename><surname>Van Deursen</surname></persName>
							<email>arie.vandeursen@tudelft.nl</email>
						</author>
						<author>
							<persName><forename type="first">Maliheh</forename><forename type="middle">2024</forename><surname>Izadi</surname></persName>
							<email>m.izadi@tudelft.nl</email>
						</author>
						<author>
							<persName><surname>An</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<settlement>Netherlands Arie van Deursen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<country>Netherlands Maliheh Izadi</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-22">22 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">BF4E9C3B1F9A00A77A83236703F372FD</idno>
					<idno type="DOI">10.1145/3650105.3652298</idno>
					<idno type="arXiv">arXiv:2403.15230v1[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large Language Models</term>
					<term>Foundation Models</term>
					<term>Code Licensing</term>
					<term>Software Engineering</term>
					<term>ML4SE</term>
					<term>Machine Learning</term>
					<term>Datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.</p><p>Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The datasets for training Large Language Models (LLMs) have expanded rapidly, mirroring the increase in the number of parameters in cutting-edge models. This surge has necessitated the quick creation of numerous large datasets for training purposes. Alongside this growth in model size, there has been a notable shift in adapting Programming Language Models (PLMs) for end-user applications. This shift has piqued the interest of businesses looking to utilize these models commercially, leading to rising concerns about the legal implications of using copyrighted data in such large-scale training datasets. The significance of adopting permissive licenses in training LLMs has been recognized by entities like Together Computer <ref type="bibr" target="#b9">[11]</ref> and The BigCode Project <ref type="bibr" target="#b25">[27]</ref>. They have released models <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b26">28]</ref> and datasets <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b25">27]</ref>, claiming that they consist exclusively of permissively licensed code. Permissive licenses, like the MIT and Apache licenses, allow for minimal restrictions on software use, modification, and distribution, even allowing incorporation into proprietary software. In contrast, strong copyleft licenses, such as the GNU General Public License (GPL), require that any derivative works also be open source, maintaining the same user freedoms as the original software.</p><p>In similar domains, there have been legal cases centered on copyright holders objecting to the use of their data for training LLMs <ref type="bibr">[1]</ref><ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref>. The majority of these disputes involve claims of lost profits due to unlicensed data used in model training. Additionally, there are complaints about potential damage to a company's reputation when its name is linked to low-quality or inaccurate information. Some companies have demanded the deletion of LLM weights trained using their data, a move that could cost the model creators millions of dollars <ref type="bibr" target="#b1">[3]</ref>. The common thread in these legal cases, and a looming concern for future LLM development, revolves around the extensive scraping of online data without regard for associated licenses or ownership rights. This practice mirrors the prevalent approach to code dataset compilation, where most data is scraped from platforms like GitHub without considering licensing.</p><p>One key challenge in the wider adoption of LLMs stems from their classification as Foundation Models (FMs), which are trained on extensive datasets and then fine-tuned for specific tasks. This practice of reusing weights, however, introduces risks for end-users. Concerns such as data memorization and membership inference attacks, as highlighted in recent studies <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b36">38]</ref>, enable the detection of copyrighted or licensed content in both the original and fine-tuned model versions. Additionally, distributing these models could be interpreted as redistributing copyrighted material. Therefore, it is essential to mitigate the risk of future legal challenges <ref type="bibr" target="#b2">[4]</ref>. To determine the potential for future legal challenges associated with LLMs, we adopt a bottom-up approach. The primary source of such legal issues is likely to be the data used for training these models. While some studies have focused on compiling 'safe' datasets, they often overlook the origin of their data. To confirm this concern, we conducted a comprehensive survey of the LLM field, collecting data on the models currently in use and their training datasets. We then analyzed this information to identify potential future licensing problems. More concisely, we will be answering the following research questions:</p><p>(1) How has the interest in including source code in the training of both generic and specific language models evolved over time? <ref type="bibr" target="#b0">(2)</ref> What is the minimum level of existing strong copyleft-licensed code in the training data of PLMs? (3) What types of sensitive information might be present in the datasets of PLMs?</p><p>Our contributions are as follows.</p><p>• We provide a detailed overview of how source code is utilized as a data source in contemporary FMs, • We compile a comprehensive summary of the datasets currently employed in training, • We assess the exposure of FMs to potential issues with copyright and license holders, focusing on publicly available datasets, • We introduce a dataset comprising the opening comments from 171 million code files, designed to aid in identifying copyright and licensing concerns in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We introduce this work with an overview of the literature. When it comes to potential legal challenges, we focus on the possibility of extracting verbatim copies of code from the training set. We show what works have focused on membership inference attacks (showing that a given code file is included in the training set of a model) and memorization (verbatim copying of training data in LLMs). Finally, we cover the bridge between the theoretical limitations of LLMs and the legal field, including possible safeguards that have been suggested in the literature when working with code in LLMs.</p><p>Membership Inference on Code. Being able to confirm if a code file has been used in the training of a model is important information to determine whether a license could have been infringed. Determining whether data has been seen during the training of a model is known in the literature as a membership inference attack.</p><p>Beyond the scope of Large Language Models for Software Engineering, membership inference attacks have been widely studied in the machine learning literature <ref type="bibr" target="#b23">[25]</ref>. Traditionally, topics related to the privacy of people that are contained in datasets were addressed, although the main focus was on tasks such as classification <ref type="bibr" target="#b23">[25]</ref>. Recently, researchers have started using membership inference attacks to extract training data from code models. As mentioned above, there are two distinct settings for membership inference for code models, representation-generating models, and output-generating models.</p><p>For the representation-generating code models, the BUZZER <ref type="bibr" target="#b38">[40]</ref> approach was proposed. In this approach, the authors attempt to identify if a code fragment had been present in the pretraining data for the models; CodeBERT <ref type="bibr" target="#b13">[15]</ref>, GraphCodeBert <ref type="bibr" target="#b20">[22]</ref>, Unixcoder <ref type="bibr" target="#b19">[21]</ref>, and CodeT5+ <ref type="bibr" target="#b33">[35]</ref>. The authors used a white box approach (BUZZER had access to the internal states of the model), a gray box approach (BUZZER had access to the internal states of a shadow model), and a black box approach (BUZZER had no access to any internal states) approach, and showed that they achieved 90% accuracy when determining whether code was used during training in the white box setting, around 80% accuracy in the gray box setting and around 60% accuracy in the black box setting <ref type="bibr" target="#b23">[25]</ref>.</p><p>In the output-generating approach, the Gotcha <ref type="bibr" target="#b35">[37]</ref> approach was proposed. In the Gotcha approach, the open-source CodeGPT <ref type="bibr" target="#b27">[29]</ref> model was analyzed to determine its exposure to data leakage. In this paper, the membership inference attack trains surrogate models, to mimic the behavior of CodeGPT, and later a classifier to determine whether data was or was not in the training data of CodeGPT. For the surrogate models, different architectures were used to determine the effectiveness of the attack, based on how much knowledge the attacker had of the target model. The evaluated models were: CodeGPT (when the architecture and training data are known), GPT-2 (only the architecture is known), Transformer (only the type of architecture is known), and LSTM (no architecture or training data is known). In the best-performing setting, the model is known and 20% of the training data is used. Gotcha achieved an error rate of only 10% and an AUC of 0.98%. This high performance shows that code models are vulnerable to membership inference attacks when generating code, and together with the high accuracy reported by BUZZER they validate each other's results <ref type="bibr" target="#b35">[37]</ref>.</p><p>Memorization. While the design of membership inference attacks is still very new when applied to Foundational Models for code, measuring and preventing a model from returning memorized code has had more attention in the literature. When looking at memorization in the output of Large Language Models, there are many similar definitions to what a memorized output is. The one thing they have in common is that they look for overlap between outputs from the model and substrings of the dataset. Some papers only look at exact duplicates of outputs <ref type="bibr" target="#b7">[9]</ref>, while others look for close matches, and yet others will add constraints to how long a substring must be before it can count as memorization when output by a model <ref type="bibr" target="#b36">[38]</ref>.</p><p>When dealing with the memorization of code, it has been shown that for models, the number of parameters correlates to the amount of memorization <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9]</ref>. As models have been increasing in size rapidly over the last years, memorization of training data (including code) will become an ever-increasing issue. While the largest models are often not open-sourced by the creators, some works look at the rate at which code is memorized by models when the train set is available.</p><p>It has been shown the 81% of the top-100 outputs generated by StarCoder <ref type="bibr" target="#b26">[28]</ref> are copied directly from GitHub, and 75% of the outputs generated by InCoder <ref type="bibr" target="#b15">[17]</ref> are copied from GitHub. Furthermore, when evaluating the open-source model CodeParrot, they found that the repetition of code in the dataset, as well as querying a model multiple times, and for longer output sequences, increased the chance that a memorized code snippet was returned. Finally, the authors also manually identified a taxonomy of memorizations that code models output, where the most common type of memorization was returning the license information for a file <ref type="bibr" target="#b36">[38]</ref>.</p><p>Although the detection of exact memorization seems like an easy task that can be prevented by a filter, Copilot, an implementation of the Codex <ref type="bibr" target="#b8">[10]</ref> model, can evade its filters <ref type="bibr" target="#b24">[26]</ref>. This also adds an extra layer of confidence in the model's ability to generate original content, while not addressing the underlying issues with LLMs.</p><p>Relations to the Legal Field. A final question is to what extent foundation models can claim that their outputs are protected under fair use <ref type="bibr" target="#b21">[23]</ref>: a substantial part of an output must be an exact copy of the original, for fair use to no longer be applicable. A brief analysis has been done on extracting examples of strong copyleft licensed code using the ChatGPT model (GPT-4), while also evaluating the average match percentage of the code-cushman-001, code-davinci-001, and code-davinci-002 models to be around 50% when prompting them with function signatures of the Linux kernel. The authors of this paper continue to discuss the problems with language models that remove copyright information that must be copied when using code from a file <ref type="bibr" target="#b21">[23]</ref>.</p><p>As a solution to the issue of copyright infringements, the authors suggest a number of technical fixes. First, they suggest that training data are selected based on the license that is assigned to a file, similar to how some datasets are created <ref type="bibr" target="#b25">[27]</ref>. They also suggest to focus on the quality of the data being used. Suggesting to remove duplicate data, something that has been shown to increase memorization of models <ref type="bibr" target="#b36">[38]</ref>. Furthermore, they suggest adding filters to the output of a model, which may be beneficial, however, it has been shown to give false confidence in the model <ref type="bibr" target="#b24">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEGAL ASPECTS</head><p>Having covered the limitations of LLMs, with respect to memorization and vulnerability to membership inference attacks, we next see how these limitations manifest themselves in the real world. We first address three lawsuits that are currently being litigated in courts concerning the data contained in models and datasets. Then we give an overview of common licenses that are applied to source code and what implications their requirements have for LLMs that may have been trained on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lawsuits</head><p>To understand the qualms data owners have with their data being included in training data, we give a brief overview of the legal troubles surrounding the Books3 corpus, the lawsuits surrounding the stable diffusion models and how they can produce copyrighted imagery, and finally the lawsuit between the New York Times and OpenAI regarding the inclusion of New York Times articles in the training of their models.</p><p>Books3. The Books3 corpus is a dataset of books scraped from an online site that distributed materials protected by copyright. Its original aim was to level the playing field between big AI companies and individuals working with Large Language models. After its creation, it was included in a dataset that combined existing datasets to create an extensive corpus and released under the name, The Pile <ref type="bibr" target="#b16">[18]</ref>.</p><p>This dataset was eventually used by several companies such as EleutherAI, Bloomberg, and Microsoft to train for-profit large language models. In the United States of America, a lawsuit was launched by many authors (who have authored works included in Books3) who demand that the mentioned companies stop using their books permanently (it is unclear if they want current models to be removed) along with compensation for the use of their works <ref type="bibr" target="#b0">[2]</ref>.</p><p>The main arguments brought forth in the lawsuit allege that Microsoft, Meta and Bloomberg:</p><p>• All developed LLMs while knowing that the training data was copyrighted. • Did not attempt to obtain a license for the copyrighted works while knowing that the original works were obtained illegally. • All chose to use stolen work to train models with the goal of generating a profit.</p><p>Furthermore, a Danish interest group, Rights Alliance, has issued a DMCA takedown request for the dataset The Pile for containing the Books3 corpus, making the entire dataset unavailable for download <ref type="bibr" target="#b29">[31]</ref>.</p><p>Stable Diffusion. In a related court case also running in the United States of America at the moment, Getty Images, a provider of digital images, claims that their copyright has been infringed by StableAIs' Diffusion model, claiming that StableAI used 12 million images curated by Getty Images in the training of their model <ref type="bibr">[1]</ref>. The lawsuit launched by Getty Images is based on three main arguments:</p><p>• The StableAI models sometimes produce images that contain the recognizable Getty Images banner, which would be an infringement of the trademark. • The images used for training were scraped from the Getty Images site, which is against the license of their website. • Getty Images was not contacted for a license of their images included in the dataset, which they spend a large amount of money curating, including captions, titles, keywords, and other metadata.</p><p>OpenAI. The most recent and relevant lawsuit currently in progress is between The New York Times, Microsoft, and OpenAI. The lawsuit is based on the usage of articles written for The New York Times and copyrighted by The New York Times in the training of LLMs distributed by OpenAI and Microsoft <ref type="bibr" target="#b1">[3]</ref>. Similarly to the previous two lawsuits, the issue is centered on the inclusion of copyrighted material in the dataset used to train AI models. The issues that The New York Times has raised in this lawsuit can be summarized as follows: Although the first two points are similar to the issues raised by Getty Images and the Books3 corpus, the New York Times lawsuit goes more in-depth into the technical limitations of current LLMs and how they affect suppliers of training data.</p><p>The first point, of AI models hallucinating 'facts', is backed up by screenshots of prompts in the lawsuit. The lawsuit focuses on Bing Chat, which when prompted for specific parts of an article produces text that is not from a New York Times article. Furthermore, the lawsuit shows evidence of Bing Chat creating citations of people that are supposedly in a New York Times article, while this was not the case. Finally, the last example of hallucinations is Bing Chat claiming facts are published in New York Times articles when the articles it references were never published.</p><p>The second point, where AI models copy data from the training set, is backed up in the lawsuit filings by showing examples of outputs from the GPT-4 model and comparing it to the text published in the New York Times, which was largely identical. Furthermore, they showed examples of GPT-4 returning exact copies of New York Times articles when they prompted it by saying they were blocked by a paywall.</p><p>In contrast to the previous lawsuits, there had been rounds of negotiations between The New York Times and OpenAI about licensing the data from their articles; however, they did not come to an agreement. The defense of OpenAI to the allegations of copyright infringement is that the use of articles in the training of models can be seen as transformative and should be allowed under fair use. Furthermore, they argue that the methods used by The New York Times to extract their articles from the GPT-4 model are against OpenAI terms of service and are not permitted to be used.</p><p>Finally, The New York Times not only sues for damages they perceive to have been inflicted on them by Microsoft and OpenAI, but they also sue for the destruction of all GPT or LLMs weights that were trained on datasets containing New York Times articles, as well as the destruction of all datasets containing their articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Existing Licenses</head><p>The code used in the training and fine-tuning of LLMs is governed by an extensive range of licenses that impose various restrictions on the use and distribution of the software. While describing any potential limitations and restrictions, each license category gives users a specific set of rights. Licenses can be classified into three main groups: strong copyleft, weak copyleft, and permissive.</p><p>Permissive. This category of licenses enables users to utilize, alter, and redistribute the software with great freedom and without being subject to strict limitations. The non-restrictive nature of these licenses allows for flexibility in integrating code into both proprietary and open-source projects. This characteristic stems from their ability to support changes and the production of derivative works without the requirement of sharing under an identical license <ref type="bibr" target="#b14">[16]</ref>. Permissive licenses enable the incorporation of code with permissive licenses into projects that have varying licensing needs. Users have the privilege of using the altered code under a separate license or retain it as proprietary. In contrast to copyleft licenses, permissive licenses only require users to provide attribution and exempt the author from liability.</p><p>Weak Copyleft. As their name suggests, weak copyleft licenses allow the integration of code into proprietary projects without requiring the entire derived work to be open-source <ref type="bibr" target="#b31">[33]</ref>. In other words, only modified parts of the original code must be released under the same weak copyleft license. This category of licenses achieves a middle ground between proprietary software and collaborative open-source development. Aside from sharing modified code, weak copyleft licenses also mandate attribution to the original authors, and potentially a disclaimer of liability. CuBERT 1 WebGPT Strong Copyleft. What sets strong copyleft licenses apart from weak ones is their enforcement of a reciprocal condition that any derivative work that incorporates or alters the original code must be distributed under the same strong copyleft license. This share-alike condition ensures that the same rights are preserved in subsequent versions and derivatives of the software <ref type="bibr" target="#b31">[33]</ref>. The central aspect of our work revolves around this type of license, due to their strength in maintaining the open-source nature of the codebase across all iterations and contributions. In light of this, we can examine and validate the condition of the data utilized for training and finetuning various LLMs, and compare it to the information presented in the works of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>The overall approach of the paper consists of two main aspects. First, we gather a comprehensive list of LLMs. We conduct a tertiary study by searching databases for recent surveys on LLMs. We then analyze the papers, repositories, and blog posts released about the identified LLMs to gain information about which datasets are used in their pretraining. Once we have collected this data from the papers, we analyze the datasets that we were able to find by seeing if they contain any copies of code that are also released under a strong copyleft license, as well as analyzing the first comment in each file to see if there are any other sources of confidential information being embedded into the weights of the models. We present a graphical summary of our approach in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Study Collection</head><p>To understand the extent of the issue of licensed code in datasets, we must first identify which models are being trained on codebases.</p><p>To understand this, we collect surveys of LLMs from online paper databases. This will give an overview of the datasets that are most commonly used in the literature and which models are trained on which datasets. We limit our search for surveys to only those that were published in 2023.</p><p>After compiling a list of LLMs from the surveys, we collect all papers, where available, and blog posts/repositories where not, that relate to the model. We further filter through these papers to identify the models that aim to only include permissively licensed code in their training procedure. Initially, we apply a rough filter on all papers, removing all models that were not trained on code. Subsequently, we exclude all models that were not trained on file-level code. File-level code refers to code that has been extracted from repositories and not altered after collecting the data. We prioritize datasets containing file-level code as they offer a more effective means of identifying duplicated code. Extracting methods or classes may result in false positives due to common elements such as getters, setters, and common algorithms We also discard models trained exclusively on websites such as stack overflow or competition data. After filtering the models, we first assess the availability of the datasets they were trained on. Then, we proceed to collect all publicly accessible datasets. Moreover, we extract the section from the paper/documentation that details the source of the training dataset and whether it has a specified name. Finally, we add a class of datasets, named custom datasets. These are datasets curated by the authors of papers but not named or released. Many of these datasets are scraped from online repositories; however, not enough information is given to fully reproduce them accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Processing Collected Datasets</head><p>After analyzing the data collected in the tertiary study, we extract a collection of datasets that are publicly available and contain at least a subset of code that has been extracted from a code base (without being modified in any way). These datasets will be the subject of the following investigations on licensed code.</p><p>We analyze two aspects of the datasets when determining whether a code file could be licensed. First, we hash all code files using the SHA-256 hash. This is a function commonly used to detect exact duplicates of code in a dataset <ref type="bibr" target="#b25">[27]</ref>. We limit ourselves to exact duplicates, for two reasons; first, adding near deduplication adds a layer of discussion between if a piece of code is a duplicate or a slightly different, yet similar implementation of a common code structure or algorithm <ref type="bibr" target="#b6">[8]</ref>. The main goal of this paper is to analyze whether there is a reason to be concerned about licensed code in datasets, we answer this question by giving a lower bound of duplicated files, by only looking at exact copies. Second, we extract the first comment if there is any present. With the first comment, we refer to any block of comments that start in the first 20 characters of the file but may extend beyond the first 20 characters. We can search this set of comments to extract possible licenses, as well as other copyright information and disclaimers regarding the ownership and distribution of the content of the file. We release all our data in the replication package to enable future research into detecting the exact extent of license inconsistencies in datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Collect Strong Copyleft Licensed Code</head><p>While datasets are often scraped from GitHub, some take care to only scrape permissively licensed code. In a perfect world, the overlap between code in a permissively licensed repository and a repository made available under a strong copyleft license is zero.</p><p>To test how big the overlap is in the real world, and if there is a difference between datasets that check for code licenses and those that do not, we scrape our own dataset from GitHub<ref type="foot" target="#foot_0">foot_0</ref> . For generating the dataset, we query the GitHub API to generate a list of all repositories that are released under either a GPL 2.0, GPL 3.0, or AGPL license. We limit the search to 10,000 repositories and select repositories where the majority language is one of the languages we include in the investigation. We do not include files of a language when they are the minority language in a repository. In case there are less than 10,000 repositories, we use as many as available. An overview of the languages and the number of available repositories is given in Table <ref type="table" target="#tab_3">4</ref>. Our primary focus on these languages stemmed from their prevalence within the filelevel code datasets accessible to us. When extracting the code files from the repository, we selected files using their file extension and included all files with that extension. We did not do any secondary filtering on the length or variety of the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Research Questions</head><p>Finally, we will give an overview of how we use the previously gathered information to answer each of our research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1 -Interest in Licensed Code.</head><p>To depict to what extent there is an understanding of the issue of licensed code in datasets used to train foundation models, we look at the information we gathered during the tertiary study, we use the publication date, whether the models are trained on code, and if they claim to account for permissively licensed code to extract the trends on an annual basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2 -Strong</head><p>Copyleft Violations. To examine the prevalence of strong copyleft-licensed code files in datasets, we conducted two experiments. First, we calculate the SHA-256 hash of all code files in the gathered datasets. We also calculated the SHA-256 hash of all the code files we scraped from GitHub, which only contained code from repositories with a strong copyleft license. This gives us two sets of code that we can compare for each dataset, our licensed set of hashes, and the hashes of all the files of the dataset. We report the overlap in the hashes as the number of files that appear in both the licensed repositories and the collected datasets. Second, we extract the first comment from each file in the collected datasets. We then search these comments for license names in order to determine whether it is referencing a GPL 2.0, GPL 3.0 or AGPL license.</p><p>RQ3 -Distribution disclaimers. Finally, to evaluate whether the authors of a code file may have issues with the further distribution of their code, we apply the same procedure as for the detection of strong copyleft licenses in the first comment. However, for RQ3 we change the strings we search for to exclude all GPL 2.0, GPL 3.0, and AGPL boilerplate license declarations, and look for other language that refers to ownership and copying of code. This includes phrases as 'confidential', 'please do not share' and 'following conditions are met'. For an exhaustive list of all search strings we refer the reader to the reproduction package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>To answer the research questions, we first need the results of the tertiary study. To present our results, we first give an overview of the surveys that we have collected, what papers we were able to extract from the surveys, the datasets that they resulted in, and the availability of the datasets. After collecting all the information, we proceed with the aforementioned approach to answer the research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tertiary Study</head><p>For the tertiary study, we collected 6 Surveys conducted on LLMs in the year 2023, and we give an overview of these surveys in Table 1. From these surveys, we extracted 106 LLMs, which we further refined to 75 LLMs trained on code, 53 trained on file-level code, 23 trained on permissively licensed file-level code, and 30 trained on non-permissively licensed file-level code. This distribution can be observed in Table <ref type="table" target="#tab_1">2</ref>. The superscripts for file-level code models denote the datasets they were trained on, based on the reference numbers assigned in Table <ref type="table" target="#tab_2">3</ref>. Due to page limitations for the references, the table is replicated with references in the reproduction package.</p><p>There were a total of 14 datasets that we identified. Of these datasets, 7 were not released by the authors, 5 were fully open, 1 was released selectively to practitioners that applied, 1 was open to all but required payment to generate and download, and 1 was removed due to a DMCA takedown request, but re-uploaded by a third party without the offending material.</p><p>Aside from these datasets, some models were trained on scrapes of GitHub that the authors conducted themselves. Often, there was not enough information presented to fully replicate the dataset the authors claimed to have created. These cases were not considered in this work.</p><p>The final set of datasets used for further investigations are The Pile<ref type="foot" target="#foot_1">foot_1</ref> , The Stack v1<ref type="foot" target="#foot_2">foot_2</ref> , RedPajama<ref type="foot" target="#foot_3">foot_3</ref> , CodeParrot<ref type="foot" target="#foot_4">foot_4</ref> , Github-Code<ref type="foot" target="#foot_5">foot_5</ref> , Code-Clippy <ref type="foot" target="#foot_6">7</ref> .</p><p>Among these datasets, The Stack v1, CodeParrot, GitHub-Code, and RedPajama incorporate a license field within their data structure. Additionally, permissive code is mentioned in reference to The Stack v1 and RedPajama, whereas CodeParrot and GitHub-Code incorporate both permissive and copyleft licenses in their code. In the absence of a license field or a specific mention of permissive code for CodeClippy and The Pile, an examination of their data reveals a mixture of both permissive and copyleft licenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RQ1 -Interest in Licensed Code</head><p>To judge the interest of the field on the presence of licensed code in training data, we analyse both the presence of code in training setups, as well as the references to code licenses in papers. We show the results of this in Figure <ref type="figure" target="#fig_1">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>We see that there has been a gradual increase in the use of code in training setups since 2020. We attribute this rise in interest to the popularity of products such as copilot <ref type="foot" target="#foot_7">8</ref> and ChatGPT<ref type="foot" target="#foot_8">foot_8</ref> , as well as the emerging benefit of using code data when training models for natural language reasoning <ref type="bibr" target="#b28">[30]</ref>. We further evaluate the attention being paid to permissively licensed code when training models. We see that there has been a large increase in interest in using datasets that only contain permissively licensed code. Looking at Figure <ref type="figure" target="#fig_2">3</ref> we see that there is a large jump in papers referencing permissively licensed code. We attribute this to the recent increase in news coverage of other generative models, such as stable diffusion <ref type="bibr" target="#b30">[32]</ref> and datasets such as The Pile getting targeted by legal action <ref type="bibr" target="#b16">[18]</ref>. Furthermore, this increase in interest also coincides with the release of The Stack v1 <ref type="bibr" target="#b25">[27]</ref>, one of the datasets under investigation, which puts a large emphasis on containing only permissively licensed code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RQ2 -Strong Copyleft Inconsistencies</head><p>To answer RQ2 we will approach the problem from two directions. First, we gather a dataset that we scraped from GitHub containing only repositories with strong copyleft licenses. We then check to see if there are exact copies of these files in the datasets used for training. Additionally, we retrieve the first comment from all files and search for substrings that match the boilerplate license disclaimer commonly used by strong copyleft licenses.</p><p>Table <ref type="table">features</ref> data on both the number of exact duplicates (using the SHA-256 hash) and the number of comments denoting the use of strong copyleft licenses across all datasets. The dotted line separates the datasets that claim to exclusively utilize permissive licenses, positioned at the top, from the remaining ones. After analyzing the obtained results, it is evident that there is a considerable overlap of exact duplicates between the dataset containing only strong copyleft licensed repositories and other datasets.</p><p>Although we see that the percentage of files that overlap is lower for The Stack v1 and RedPajama than it is for other datasets, such as The Pile, CodeParrot, and CodeClippy which do not filter on licenses, the datasets that checked for licenses still have a larger overlap than Github-Code, which did not check for licenses.    Furthermore, we see that when looking at comments that contain the license of the file, we see that the datasets that check the repositories for licenses have a significantly smaller match percentage with strong copyleft licenses than those without.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">RQ3 -Distribution Disclaimers</head><p>Finally, we analyze the prevalence of any language in the opening comment of all files. The goal of this is to identify if any comments show the authors/owners of a file did not want the contents to be shared or distributed. Table <ref type="table" target="#tab_5">6</ref> presents the results of the searches, detailing the number of copyright disclaimers found in the first comments and the number of first comments across datasets. Additionally, we provide such an example in Figure <ref type="figure" target="#fig_5">4</ref> to illustrate the nature of the identified comments.</p><p>We see that when looking a the percentages of comments containing distribution disclaimers to total amount of first comments, the prevalence of distribution disclaimers is between 5% and 7.5% for all datasets except for RedPajama that had 1.3%. This shows that there are a large number of code files that also have some restrictions on redistribution; however, do not use the standard disclaimer or a specific license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In the results, we found interesting trends regarding the popularity of permissive code, the prevalence of incorrectly classified licensed code, and the presence of other language that places limitations on the distribution and use of code files. We begin by demonstrating how the findings relate to the wider field of LLMs for code; we then give some recommendations to other practitioners about how to deal with code licenses. We complete the section with an overview of avenues for future work and any limitations we found with our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implications</head><p>The real-world implications of our study affect a number different stakeholders. For maintainers and curators of datasets, we have shown that there is more information present in code comments in relation to the distribution of code than only the license. We have also shown that there is a significant overlap in repositories that have different licenses, making it hard to judge where code originates from and what license should be applied to it. This also affects practitioners who train LLMs, as they carry the risk of their models being targeted if they are trained on licensed code. Finally, end users of LLMs need to be wary of accidentally inserting licensed code into their code bases as the final outcome of doing so with strong copyleft code is not yet known but could in theory lead to the open-sourcing of their code bases. The main question that these implications raise for the wider field of LLMs for code is who is responsible for the training data and output of the final models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Work</head><p>From the results gathered, we have identified a number of venues for future work. The main areas for future work relate to the analysis of comments and extraction of licenses/intent from comments, as well as determining if a piece of code is licensed and under which license it is licensed.</p><p>Code Comment Intent. We have seen that aside from the legal aspects of potential license violations, there is also an issue with the consent of the authors to include code in training data. Although some curators provide an opt-out system to be removed from the dataset <ref type="bibr" target="#b25">[27]</ref>, it should be possible to detect an authors intent from the comment, if present.</p><p>Duplicate Code. Similar to using basic strings to search for comment intent, we also used a basic exact duplicate finding technique using the SHA-256 hash function. Although it showed the presence of duplicates between our two sets of data, it takes minimal effort to avoid detection. Small code changes, which could be automatic, such as changing from 4 space indent to 2 space indent, would give a false negative. Furthermore, removing licensing information would also lead to an undetected duplicate. In future work, we believe that it would be beneficial to the community to create a taxonomy of code changes that take place when code is copied from one repository to another. In addition, it gives a better overview of how much code of each category is copied. This would give a good understanding of potential issues with copied code and is a good way to relate it to the legal implications of fair use.</p><p>Identifying a License Violation. One of the issues with only detecting duplicates is that we do not know for certain if a license was violated by including it in a non-permissive dataset vs. a permissive dataset as we only know that the file is duplicated. An interesting area of research would be to analyze networks of code repositories, such as GitHub, to automatically detect whether a file or snippet of code was copied in a way that violates the license. This can be a quick analysis of license changes when looking at forks, or a more thorough investigation of tracking code changes and different versions of code through time and different repositories.</p><p>It must also be said that this problem is not unique to LLM datasets. Previous work has focused on the provenance of code <ref type="bibr" target="#b10">[12]</ref> and the identification of license violations in large software projects <ref type="bibr" target="#b17">[19]</ref>, as well as on the automatic identification of licenses for code files in software projects <ref type="bibr" target="#b18">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Recommendations</head><p>To address the issues uncovered during our investigation, we propose several recommendations for developers who are involved in training LLMs.</p><p>First and foremost, our investigation has revealed that no LLM to date has been trained on a dataset entirely devoid of licensing issues. Given that fine-tuning a model retains the information embedded in its weights, it is crucial to be mindful of this when choosing a model for the fine-tuning process.</p><p>Moreover, our investigation has revealed that even in cases where providers of permissively licensed source code datasets have good intentions, straightforward searches often result in numerous inconsistencies. Therefore, we recommend that when fine-tuning or training a model from scratch, it is advisable to conduct thorough searches of the datasets to identify and rectify any potential licensing issues before starting training. This precaution is particularly important, as removing information from the model's weights has not been proven to be a foolproof solution <ref type="bibr" target="#b34">[36]</ref>.</p><p>Finally, we recommend adapting and scaling up existing methods for license detection to be able to work for large datasets of code in order to reduce the number of license inconsistencies that could be present in datasets scraped from online repositories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Limitations</head><p>Our approach faces two primary limitations. Firstly, the dynamic nature of code means that copied snippets are subject to change over time. Secondly, there is a notable lack of transparency from many entities regarding their training methodologies. We elaborate on these limitations in the subsequent sections of this paper.</p><p>Collected code. As codebases are in a constant state of evolution, we have established a fixed point in time for gathering code. However, obtaining copies of our strongly copyleft-licensed dataset from the exact moment each individual dataset was curated proves challenging due to resource limitations. In some cases, it is simply impossible as the specific scraping date of these datasets remains undisclosed. Furthermore, since the creation of our dataset, changes may have occurred. Some repositories may have been deleted or converted to private status, while new repositories have emerged. Consequently, our datasets may contain slightly different sets of code. Nevertheless, it is worth noting that our primary focus is on identifying the presence of strong copyright-protected code within these datasets. The discrepancies in dataset composition do not impact the conclusions drawn in this paper.</p><p>Non-reproducible Papers. Finally, one of the limitations we experienced when collecting information about the training of proprietary models, and analysis of their datasets is that some companies were not transparent about how the models were trained. In many cases when a custom dataset was scraped, information that would be needed for an exact replication of the dataset was missing. This was usually information such as the date it was scraped, which exact repositories were scraped, or if there were any additional filtering criteria for the code files. Furthermore, we noticed that, especially when companies described their models in blog posts or white papers, they were very ambiguous about what exact data the models were trained on. Oftentimes, the exact nature of the data could not be inferred from the paper. Unfortunately, due to the competitive nature of LLMs and the price associated with curating data and training models, this trend will probably continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We evaluated the presence of code licensed under a strong copyleft license in datasets used to train foundation models. To analyze the datasets used and the attention paid to permissive code, we collected 6 literature surveys, from which we extracted 106 foundation models. Of these models, 53 were trained on file-level code and 23 specifically referenced training exclusively on permissively licensed code. We further analyzed the papers and extracted 30 datasets that worked with file-level code. Of these datasets, 17 were custom GitHub scrapes. Furthermore, of these datasets, we were able to access 6 that were distributed online without restrictions. Of these 6 datasets, 4 did not filter the collected code on licenses while the other 2 did. In total, we collected 514 million files of code across all 6 datasets that we used to evaluate the presence of strong copyleft licenses.</p><p>To evaluate the presence of strong copyleft-licensed code we scraped GitHub, we selected up to 10,000 repositories released under either a GPL 2.0, GPL 3.0, or AGLP license, covering 32 languages. This resulted in a dataset of 35 million code files. We calculated the SHA-256 hash of all collected datasets and the strong copyleft dataset we collected ourselves. We then use the hashes to look for an overlap of exact copies. We found that all datasets had a substantial overlap with the dataset of strong copyleft licensed code. Although datasets that checked for permissively licensed code generally had less of an overlap, there was still an overlap of at least 5%.</p><p>Furthermore, we extracted the first comments from the datasets to detect licenses that are used on a file level. We saw that when detecting license comments, datasets that claim to contain permissively licensed code perform better, with 0.05% and 0.8% of files having a license comment; however, this still amounts to more than 2 million files with a strong copyleft license in the case of The Stack v1. Finally, we also analyzed the comments gathered from the datasets, to evaluate the presence of any other disclaimers concerning copying the contents, which is not directly related to any license. We find that there is a higher prevalence of these nonlicense disclaimers than there are comments containing a strong copyleft license. These disclaimers are also prevalent in all datasets.</p><p>To enable further investigations into detecting and removing licensed code from public datasets, we release a dataset containing all comments that appear at the start of a file of code that we collected during this investigation. The 450 million files resulted in a dataset of 171 million code comments, and we removed all PII before releasing the data. Overall we have shown that while the interest in creating datasets that contain only permissive code has grown rapidly in the last year. However, there is evidence of license inconsistencies that need to be addressed in order to fully avoid future problems with regards to licensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DATA AVAILABILITY</head><p>To make our experiments reproducible, we released a replication package at <ref type="url" target="http://www.github.com/AISE-TUDelft/CodeLicensingExploration">www.github.com/AISE-TUDelft/CodeLicensingExploration</ref>. We share the repositories we collected, the raw results of the tertiary study, and the code we used. We also upload the dataset containing the leading comments to huggingface at <ref type="url" target="http://www.huggingface.co/datasets/AISE-TUDelft/leading-comments">www.huggingface.co/  datasets/AISE-TUDelft/leading-comments</ref>, we used the StarPII<ref type="foot" target="#foot_9">foot_9</ref> model to remove any Personal Identifiable Information (PII) from the dataset prior to uploading.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical overview of the presented approach</figDesc><graphic coords="4,53.80,83.68,505.59,148.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of LLMs trained on code per year over the total number of LLMs</figDesc><graphic coords="8,53.80,83.69,253.44,190.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Percentage of LLMs trained on permissive file code per year over the total number of LLMs trained on file level code</figDesc><graphic coords="8,317.96,83.69,253.44,190.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 this software contains proprietary and confidential 3 information</head><label>23</label><figDesc>of &lt;Company&gt; and its contributors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4</head><figDesc>use, disclosure and reproduction is prohibited without 5 prior consent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Restrictions on sharing and distributing code contained in a file, extracted from the RedPajama dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Literature Surveys used to identify Large Language Models</figDesc><table><row><cell>Title</cell><cell>Reference</cell></row><row><cell>Software Testing with Large Language Model: Survey, Landscape, and Vision</cell><cell>[34]</cell></row><row><cell>A bibliometric review of large language models research from 2017 to 2023</cell><cell>[14]</cell></row><row><cell>A survey of large language models</cell><cell>[41]</cell></row><row><cell>Large Language Models for Software Engineering: A Systematic Literature Review</cell><cell>[24]</cell></row><row><cell>Large Language Models Meet NL2Code: A Survey</cell><cell>[39]</cell></row><row><cell>A Survey on Large Language Models for Software Engineering</cell><cell>[13]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Division of Large Language Models based on training data</figDesc><table><row><cell>Only Natural</cell><cell>Method-Level</cell><cell>Permissive</cell><cell>Non-permissive</cell></row><row><cell>Language</cell><cell>Code</cell><cell>File-Level Code</cell><cell>File-Level Code</cell></row><row><cell>GPT-3</cell><cell>PLBART</cell><cell>CodeGen 1,2,9</cell><cell>Codex 15</cell></row><row><cell>T5</cell><cell>Tk-Instruct</cell><cell>InCoder 15</cell><cell>CodeT5 1</cell></row><row><cell>BART</cell><cell>ERNIE-Code</cell><cell>FLAN-T5 3</cell><cell>BLOOM 1,7</cell></row><row><cell>mT5</cell><cell>PyMT5</cell><cell>LLaMa 1</cell><cell>Galactica 15</cell></row><row><cell>CPM-2</cell><cell>LaMDA</cell><cell>CodeGen 2 2,3,9</cell><cell>Baichuan 2 15</cell></row><row><cell>PanGu-𝛼</cell><cell>InstructGPT</cell><cell>StarCoder 3</cell><cell>QWEN 15</cell></row><row><cell>T0</cell><cell>CodeBERT</cell><cell>Gopher 10</cell><cell>Skywork 8</cell></row><row><cell>UL2</cell><cell>CodeRetriever</cell><cell>CodeT5Mix 11</cell><cell>Pythia 2</cell></row><row><cell>OPT</cell><cell>TraceBERT</cell><cell>CodeRL 11</cell><cell>Jurassic-1 2</cell></row><row><cell>NLLB</cell><cell>GraphCodeBERT</cell><cell>AlphaCode 15</cell><cell>JuPyT5 15</cell></row><row><cell>GLM</cell><cell>BERT Overflow</cell><cell>PaLM 6</cell><cell>MT-NLG 2</cell></row><row><cell>FLM</cell><cell>CoText</cell><cell>LLaMa 2 1</cell><cell>PyCodeGPT 15</cell></row><row><cell>GShard</cell><cell>PanGu-Coder</cell><cell>WizardLM 1</cell><cell>U-PaLM 6</cell></row><row><cell>HyperClova</cell><cell>CodeGPT</cell><cell>CodeT5+ 11</cell><cell>PanGu-Σ 15</cell></row><row><cell>Yuan 1.0</cell><cell>CodeGPT-adapted</cell><cell>WizardCoder 3</cell><cell>PaLM 2 15</cell></row><row><cell>GLaM</cell><cell>CoditT5</cell><cell>SantaCoder 3</cell><cell>Mistral 15</cell></row><row><cell>AlexaTM</cell><cell>SPT-Code</cell><cell>PaLM-Coder 6,13</cell><cell>GPT-C 15</cell></row><row><cell>WeLM</cell><cell>FLAN</cell><cell>Vicuna 1</cell><cell>PolyCoder 15</cell></row><row><cell>BERT</cell><cell>UnixCoder</cell><cell>Stable Code 3</cell><cell>GPT-3.5 15</cell></row><row><cell>mBART</cell><cell>PanGu-Coder-FT</cell><cell>StableLM 2,3,4</cell><cell>Code LLaMa 1,14</cell></row><row><cell>GPT-1</cell><cell>PanGu-Coder 2</cell><cell>StableLM Zephyr 2,3,4</cell><cell>GPT-NeoX 2</cell></row><row><cell>XLNet</cell><cell>T5-Learning</cell><cell>Japanese StableLM 4</cell><cell>CodeGeeX 2,5,15</cell></row><row><cell>Sparrow</cell><cell></cell><cell>Stable Beluga 1</cell><cell>CodeParrot 5</cell></row><row><cell>PRCBERT</cell><cell></cell><cell></cell><cell>GPT-CC 12</cell></row><row><cell>seBERT</cell><cell></cell><cell></cell><cell>Chinchilla 15</cell></row><row><cell>ALBERT</cell><cell></cell><cell></cell><cell>GPT-J 2</cell></row><row><cell>RoBERTa</cell><cell></cell><cell></cell><cell>FIM 15</cell></row><row><cell>OPT-IML</cell><cell></cell><cell></cell><cell>GPT-Neo 2</cell></row><row><cell>ERNIE 3.0</cell><cell></cell><cell></cell><cell>Falcon 2</cell></row><row><cell>GPT-2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>File-Level Code datasets used for training foundational models</figDesc><table><row><cell>Ref</cell><cell>Dataset</cell><cell>Available</cell><cell>Count</cell></row><row><cell>1</cell><cell>Big Query</cell><cell>Pay-wall</cell><cell>10</cell></row><row><cell>2</cell><cell>The Pile</cell><cell>DMCA-takedown</cell><cell>12</cell></row><row><cell>3</cell><cell>The Stack v1</cell><cell>Open</cell><cell>8</cell></row><row><cell>4</cell><cell>RedPajama</cell><cell>Open</cell><cell>3</cell></row><row><cell>5</cell><cell>CodeParrot</cell><cell>Open</cell><cell>2</cell></row><row><cell>6</cell><cell>PaLM Dataset</cell><cell>Not Released</cell><cell>3</cell></row><row><cell>7</cell><cell>Roots</cell><cell>Not Open to All</cell><cell>1</cell></row><row><cell>8</cell><cell>SkyPile</cell><cell>Not Released</cell><cell>1</cell></row><row><cell>9</cell><cell>BigPython</cell><cell>Not Released</cell><cell>2</cell></row><row><cell>10</cell><cell>MassiveText</cell><cell>Not Released</cell><cell>1</cell></row><row><cell>11</cell><cell cols="2">GitHub-Code Dataset Open</cell><cell>3</cell></row><row><cell>12</cell><cell>CodeClippy Dataset</cell><cell>Open</cell><cell>1</cell></row><row><cell>13</cell><cell>ExtraPythonData</cell><cell>Not Released</cell><cell>1</cell></row><row><cell>14</cell><cell cols="2">Code LLaMa Dataset Not Released</cell><cell>1</cell></row><row><cell>15</cell><cell>Custom Dataset</cell><cell>Not Released</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Programming language distribution and repository counts</figDesc><table><row><cell>Programming Languages</cell><cell>Repositories</cell></row><row><cell>C, C#, C++, Go, JavaScript, Java, Kotlin, Lua, Mat-</cell><cell>10000</cell></row><row><cell>lab, Perl, PHP, Python, R, Ruby, Rust, Shell, Swift,</cell><cell></cell></row><row><cell>TypeScript</cell><cell></cell></row><row><cell>Assembly, Dart, Haskell</cell><cell>5000 -9999</cell></row><row><cell>DM, Elixir, Fortran, Julia, Lisp, OCaml, Pascal, Scala</cell><cell>1000 -4999</cell></row><row><cell>Agda, Erlang, SQL</cell><cell>&lt; 1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Amount of code files found to be associated with a strong copyleft license</figDesc><table><row><cell>Dataset</cell><cell>Files</cell><cell cols="2">Exact Duplicates Count Percentage</cell><cell cols="2">License Comments Count Percentage</cell></row><row><cell cols="3">The Stack v1 262,678,972 16,122,976</cell><cell cols="2">6.14% 2,067,830</cell><cell>0.78%</cell></row><row><cell>RedPajama</cell><cell>28,793,312</cell><cell>1,579,521</cell><cell>5.49%</cell><cell>15,544</cell><cell>0.05%</cell></row><row><cell>The Pile</cell><cell>18,044,000</cell><cell>4,113,263</cell><cell>22.80%</cell><cell>823,546</cell><cell>4.56%</cell></row><row><cell>CodeParrot</cell><cell>18,695,559</cell><cell>2,681,590</cell><cell cols="2">14.34% 2,844,150</cell><cell>15.21%</cell></row><row><cell cols="2">GitHub-Code 115,086,922</cell><cell>5,537,734</cell><cell cols="2">4.81% 7,548,615</cell><cell>6.56%</cell></row><row><cell>CodeClippy</cell><cell>71,140,482</cell><cell>7,993,768</cell><cell cols="2">11.24% 2,823,923</cell><cell>3.97%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Amount of code files found to be associated with some form of ownership/copyright disclaimer</figDesc><table><row><cell>Dataset</cell><cell cols="2">Copyright Count Percentage</cell><cell>First Comments</cell></row><row><cell cols="2">The Stack v1 5,073,823</cell><cell>6.54%</cell><cell>77,595,559</cell></row><row><cell>RedPajama</cell><cell>30,500</cell><cell>1.34%</cell><cell>2,281,378</cell></row><row><cell>ThePile</cell><cell>501,877</cell><cell>7.39%</cell><cell>6,794,995</cell></row><row><cell>CodeParrot</cell><cell>773,062</cell><cell>5.38%</cell><cell>14,372,397</cell></row><row><cell cols="2">GitHub-Code 2,669,845</cell><cell>5.89%</cell><cell>45,301,797</cell></row><row><cell>CodeClippy</cell><cell>1,695,556</cell><cell>6.72%</cell><cell>25,223,157</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/monology/pile-uncopyrighted</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/datasets/bigcode/the-stack</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/datasets/codeparrot/codeparrot-clean-valid</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://huggingface.co/datasets/codeparrot/github-code</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://huggingface.co/datasets/CodedotAI/code_clippy_github</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/features/copilot</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://openai.com/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://huggingface.co/bigcode/starpii</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">Mike</forename><surname>Huckabee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Relevate</forename><surname>Group</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kinnaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsh</forename><surname>Oxenreider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysa</forename><surname>Terkeurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Plaintiffs</surname></persName>
		</author>
		<author>
			<persName><surname>Meta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename><surname>Platforms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Bloomberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bloomberg Finance, L.P</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23" to="09152" />
		</imprint>
		<respStmt>
			<orgName>Microsoft Corporation, and The Eleutherai Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The New York Times Company v</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llc</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llc</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Openai</forename><surname>Opco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Global LLC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="11195" />
			<date type="published" when="2023">2023</date>
			<publisher>LLC. United States District Court Southern District of New York. Case No</publisher>
		</imprint>
		<respStmt>
			<orgName>OAI Corporation, LLC, and OpenAI Holdings</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The (ab)use of Open Source Code to Train Large Language Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Kaswan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/NLBSE59153.2023.00008</idno>
		<ptr target="https://doi.org/10.1109/NLBSE59153.2023.00008" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering</title>
		<imprint>
			<biblScope unit="page" from="9" to="10" />
			<date type="published" when="2023">2023. 2023</date>
			<publisher>NLBSE). IEEE Computer Society</publisher>
			<pubPlace>Los Alamitos, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Ali</forename><surname>Al-Kaswan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maliheh</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Van Deursen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07735</idno>
		<title level="m">Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Traces of Memorisation in Large Language Models for Code</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Al-Kaswan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maliheh</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Van Deursen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3597503.3639133</idno>
		<ptr target="https://doi.org/10.1145/3597503.3639133" />
	</analytic>
	<monogr>
		<title level="m">46th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Munoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Jane</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Lamy</forename><surname>Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Troshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Abulkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>García Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Zocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Guha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.03988[cs.SE]</idno>
		<imprint/>
	</monogr>
	<note>Harm de Vries, and Leandro von Werra. 2023. Santa-Coder: don&apos;t reach for the stars</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Some Misconceptions about Software in the Copyright Literature</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Samuelson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511265.3550449</idno>
		<ptr target="https://doi.org/10.1145/3511265.3550449" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Symposium on Computer Science and Law</title>
		<meeting>the 2022 Symposium on Computer Science and Law<address><addrLine>Washington DC, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="131" to="141" />
		</imprint>
	</monogr>
	<note>) (CSLAW &apos;22). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantifying Memorization Across Neural Language Models</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TatRHT_1cK" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<title level="m">RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software Bertillonage: Finding the Provenance of an Entity</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<idno type="DOI">10.1145/1985441.1985468</idno>
		<ptr target="https://doi.org/10.1145/1985441.1985468" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Working Conference on Mining Software Repositories</title>
		<meeting>the 8th Working Conference on Mining Software Repositories<address><addrLine>Waikiki, Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>) (MSR &apos;11)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gokkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitya</forename><surname>Lyubarskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03533</idno>
		<title level="m">Large language models for software engineering: Survey and open problems</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Lizhou</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanggyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libby</forename><surname>Hemphill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02020</idno>
		<title level="m">A bibliometric review of large language models research from 2017 to 2023</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Apache Software Foundation</title>
		<ptr target="https://www.apache.org/licenses/LICENSE-2.0" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Apache License, Version 2</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">InCoder: A Generative Model for Code Infilling and Synthesis</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hQwb-lbM6EL" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027[cs.CL]</idno>
		<title level="m">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">License integration patterns: Addressing license mismatches in component-based development</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">E</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><surname>Hassan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSE.2009.5070520</idno>
		<ptr target="https://doi.org/10.1109/ICSE.2009.5070520" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 31st International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="188" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sentence-matching method for automatic license identification of source code files</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuro</forename><surname>Manabe</surname></persName>
		</author>
		<author>
			<persName><surname>Inoue</surname></persName>
		</author>
		<idno type="DOI">10.1145/1858996.1859088</idno>
		<ptr target="https://doi.org/10.1145/1858996.1859088" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering</title>
		<meeting>the 25th IEEE/ACM International Conference on Automated Software Engineering<address><addrLine>Antwerp, Belgium; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UniXcoder: Unified Cross-Modal Pre-training for Code Representation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7212" to="7225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GraphCodeBERT: Pre-training Code Representations with Data Flow</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Daya Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Liu Shujie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15715</idno>
		<title level="m">Foundation models and fair use</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large language models for software engineering: A systematic literature review</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10620</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Salcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gillian</forename><surname>Dobbie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3523273</idno>
		<ptr target="https://doi.org/10.1145/3523273" />
	</analytic>
	<monogr>
		<title level="j">Membership Inference Attacks on Machine Learning: A Survey. ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2022-09">2022. sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">Choquette</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.inlg-main.3</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.inlg-main.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Natural Language Generation Conference</title>
		<editor>
			<persName><forename type="first">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria</forename><surname>Keet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</editor>
		<meeting>the 16th International Natural Language Generation Conference<address><addrLine>Prague, Czechia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="28" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leandro von Werra, and Harm de Vries</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Muñoz Ferrandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15533[cs.CL]</idno>
	</analytic>
	<monogr>
		<title level="m">The Stack: 3 TB of permissively licensed source code</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06161</idno>
		<title level="m">StarCoder: may the source be with you! arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=6lE4dQXaUcb" />
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16298</idno>
		<title level="m">At Which Training Stage Does Code Data Help LLMs Reasoning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rights Alliance Removes the Illegal Books3 Dataset Used to Train Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Rettigheds</forename><surname>Alliancen</surname></persName>
		</author>
		<ptr target="https://rettighedsalliancen.com/rights-alliance-removes-the-illegal-books3-dataset-used-to-train-artificial-intelligence/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">What is copyleft?</title>
		<ptr target="https://www.gnu.org/licenses/licenses.html#WhatIsCopyleft" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>GNU Operating System</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07221</idno>
		<title level="m">Software testing with large language model: Survey, landscape, and vision</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghi</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.68</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.emnlp-main.68" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1069" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.174</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.emnlp-main.174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Juan</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalika</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2875" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieke</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01166</idno>
		<title level="m">Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in Code Models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieke</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.09932</idno>
		<title level="m">What do code models memorize? an empirical study on large language models of code</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large Language Models Meet NL2Code: A Survey</title>
		<author>
			<persName><forename type="first">Daoguang</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianjie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yongji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.411</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.acl-long.411" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7443" to="7464" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.07200</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
