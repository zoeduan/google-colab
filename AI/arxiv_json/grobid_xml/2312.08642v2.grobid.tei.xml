<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT</title>
				<funder ref="#_eV385G6">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
				<funder ref="#_RFGJt7c">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality, China</orgName>
				</funder>
				<funder ref="#_rDycJBa">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_zsU3SCw #_8bTXKWQ">
					<orgName type="full">Research Project of Changning District Science and Technology Committee</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of AI for Education</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention</orgName>
								<orgName type="department" key="dep2">School of Psychology and Cognitive Science</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Mental Health and Psychological Crisis Intervention</orgName>
								<orgName type="department" key="dep2">School of Psychology and Cognitive Science</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of AI for Education</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Changning Mental Health Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">METACOGNITION-ENHANCED FEW-SHOT PROMPTING WITH POSITIVE REINFORCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE8A0AD7E9C81F2460D5A2FB6E9126D1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-Shot Prompting</term>
					<term>Metacognition</term>
					<term>Positive Reinforcement</term>
					<term>Large Language Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot prompting elicits the remarkable abilities of large language models by equipping them with a few demonstration examples in the input. However, the traditional method of providing large language models with all demonstration input-output pairs at once may not effectively guide large language models to learn the specific input-output mapping relationship. In this paper, inspired by the regulatory and supportive role of metacognition in students' learning, we propose a novel metacognition-enhanced few-shot prompting, which guides large language models to reflect on their thought processes to comprehensively learn the given demonstration examples. Furthermore, considering that positive reinforcement can improve students' learning motivation, we introduce positive reinforcement into our metacognition-enhanced few-shot prompting to promote the few-shot learning of large language models by providing response-based positive feedback. The experimental results on two real-world datasets show that our metacognition-enhanced few-shot prompting with positive reinforcement surpasses traditional few-shot prompting in classification accuracy and macro F1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, Large Language Models (LLMs) (e.g., ChatGPT, GPT-4, LLaMA 2, and Claude 2) have exhibited impressive capabilities in various downstream tasks (e.g., sentiment analysis <ref type="bibr" target="#b1">[1]</ref> and personality prediction <ref type="bibr" target="#b2">[2]</ref>) with the assistance of different prompting strategies (e.g, few-shot prompting <ref type="bibr" target="#b3">[3]</ref>). As one of the commonly employed prompting strategies, fewshot prompting provides a few demonstration examples of desired input-output pairs in the input of LLMs, which enables LLMs to quickly learn the specific input-output map-Sentence: It might have been a little too spicy for my friend , which you can couteract with eat more rice and keeping water on hand . What is the sentiment polarity of the aspect water in this sentence? Label: neutral Sentence: My friend had a burger and I had these wonderful blueberry pancakes . What is the sentiment polarity of the aspect burger in this sentence? Label: positive Ground Truth: neutral ping relationship corresponding to the downstream task <ref type="bibr" target="#b4">[4]</ref>. However, this passive learning of the given demonstration examples (similar to spoon-feeding in education <ref type="bibr" target="#b5">[5]</ref>) makes LLMs lack the autonomous reflection of their thought processes, which may limit their cognitive development and consequently affect their performance in downstream tasks. Take Apsect-Based Sentiment Classification (ABSC) task <ref type="bibr">[6]</ref> as an example, we select two similar samples from 14-Restaurant dataset, with one serving as a demonstration example and the other as a test sample. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, even if fewshot prompting provides the demonstration input-output pair for ChatGPT to learn, ChatGPT still makes wrong prediction for the test sample which is similar to the given demonstration example. This phenomenon indicates that only providing demonstration input-output pairs directly to LLMs may not necessarily help LLMs learn the specific mapping relationship behind the given demonstration examples.</p><p>In this paper, we are motivated to propose a novel MetaCognition-enhanced Few-Shot (MCeFS) prompting to improve the performance of traditional few-shot prompting, where the idea is inspired by the regulatory and supportive role of learners' metacognition in their learning <ref type="bibr" target="#b7">[7]</ref>. Concretely, metacognition refers to thinking about thinking, which is the individual's ability to plan, monitor, evaluate, and reflect on her/his own learning <ref type="bibr" target="#b8">[8]</ref>. Recently, some educational psychologists have attempted to improve students' learning performance in classroom by enhancing their metacognition <ref type="bibr" target="#b9">[9]</ref>. Similarly, we design MCeFS prompting to enhance the metacognition of LLMs to better accomplish downstream tasks. Concretely, MCeFS prompting requires LLMs to analyze the given demonstration examples one by one and reflect on their thought processes about the analysis of demonstration examples, thus enabling LLMs to better understand the specific mapping relationship behind the given demonstration examples. Furthermore, considering that teachers normally use positive reinforcement (i.e., providing rewards to increase the frequency of good learning behaviors) to enhance students' learning motivation <ref type="bibr" target="#b10">[10]</ref>, we introduce positive reinforcement into our MCeFS prompting. To be specific, we offer appropriate positive feedback to LLMs based on their analysis results of the given demonstration examples, which could guide LLMs to develop their thinking towards accurately completing specific downstream task.</p><p>The main contributions of our work are as follows:</p><p>(1) We have proposed a novel MCeFS prompting to better elicit the abilities of LLMs with a few demonstration examples. Compared with traditional few-shot prompting, our MCeFS prompting could guide LLMs to learn the given demonstration examples more comprehensively.</p><p>(2) We have introduced positive reinforcement into the few-shot learning of LLMs. By providing positive feedback corresponding to the responses of LLMs, LLMs are promoted to accomplish downstream tasks more precisely.</p><p>(3) We have conducted experiments on two real-world datasets to verify the performance of our MCeFS prompting with positive reinforcement. The experimental results illustrate that our MCeFS prompting with positive reinforcement outperforms traditional few-shot prompting in terms of classification accuracy and macro F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Few-shot Prompting. With the rapid development of LLMs, more and more researchers attempted to develop and optimize different prompting strategies to effectively employ LLMs in various downstream tasks (e.g., sentiment analysis <ref type="bibr" target="#b1">[1]</ref> and personality prediction <ref type="bibr" target="#b2">[2]</ref>) <ref type="bibr" target="#b11">[11]</ref>. Among the various prompting strategies, few-shot prompting is one of the simple and effective prompt strategies, which allows LLMs to capture the specific mapping relationship corresponding to downstream task by providing a few demonstration examples in the input of LLMs <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>. Therefore, some researchers attempted to optimize few-shot prompting for better eliciting the abilities of LLMs <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. For example, Lu et al. <ref type="bibr" target="#b12">[12]</ref> adopted the generative nature of language models to construct demonstration examples and utilized entropy-based statistics to sort the demonstration examples. Liu et al. <ref type="bibr" target="#b13">[13]</ref> proposed to retrieve demonstration examples that are semantically-similar to a test sample to construct its corresponding prompting. Ma et al. <ref type="bibr" target="#b14">[14]</ref> introduced a metric to evaluate the predictive bias of a fixed prompting against labels and designed greedy search based strategy to select prompting. However, most of them normally provide all demonstration input-output pairs for LLMs to learn. In fact, in this passive learning approach, LLMs may only learn the specific output format rather than the deeper input-output mapping relationship.</p><p>Metacognition. Metacognition refers to the ability to think about and reflect on one's own cognitive process <ref type="bibr" target="#b8">[8]</ref>. Previous studies have shown that metacognition plays a critical role in successful learning <ref type="bibr" target="#b15">[15]</ref>. Therefore, an increasing number of researchers tried to enhance individuals' metacognition to assist them in learning more effectively. For example, Meteier et al. <ref type="bibr" target="#b16">[16]</ref> improved the learning of nursing students by enhancing their metacognition with eye-tracking glasses. Ward et al. <ref type="bibr" target="#b17">[17]</ref> designed a search-assistance tool to encourage users to engage in metacognitive activities during their search processes (e.g., reflect on the information they found). However, most researchers focus on improving individuals' metacognition while overlooking the impact of metacognition on the learning of LLMs.</p><p>Positive Reinforcement. Positive reinforcement involves the usage of desirable or pleasant stimuli after the performance of certain behaviors to increase the likelihood that the behaviors will occur <ref type="bibr" target="#b18">[18]</ref>. The field of education is one of the common fields where positive reinforcement is widely employed. Concretely, teachers often use positive reinforcement (e.g., praise or other verbal reinforcement, tangible rewards, and token rewards <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>) to help boost students' learning motivation, thus guiding them to learn more effectively <ref type="bibr" target="#b10">[10]</ref>.</p><p>Hence, we would like to enhance the metacognition of LLMs to guide them to better learn the given demonstration examples. Besides, we are interested in introducing positive reinforcement into the few-shot learning of LLMs to promote them to better complete downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>In this section, we will introduce the details of our proposed MCeFS prompting with positive reinforcement. Fig. <ref type="figure" target="#fig_2">2</ref> shows the learning process of our MCeFS prompting with positive reinforcement. To be specific, for each demonstration example, we no longer provide the corresponding input-output pair in the input of LLMs. Instead, we ask LLMs to complete the specific downstream task according to the given demonstration example, while the corresponding ground truth is not provided. We then assess the prediction result of LLMs. If the prediction result of LLMs is consistent with the corresponding ground truth, we will praise them (i.e., one type of positive reinforcement) and ask them to reflect on their thought processes. Otherwise, we will require them to reflect on their thought processes as well and urge them to avoid making comparable errors again. Note that, for the employment of positive reinforcement, we request LLMs to provide several common praises (e.g., you're really good) and use them to simulate the learning motivation of LLMs. Finally, we select the best one according to the results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we conduct experiments on two real-world datasets to validate the effect of MCeFS prompting and positive reinforcement <ref type="foot" target="#foot_0">1</ref> . Specifically, we aim to answer the following Research Questions (RQs):</p><p>RQ1: Whether our MCeFS prompting could guide LLMs to better learn the given demonstration examples than traditional few-shot prompting? (see Section 4.3) RQ2: Whether positive reinforcement is effective for promoting the few-shot learning of LLMs? (see Section 4.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>For a fair comparison, we choose the same downstream task (i.e., ABSC task) as Wang et al. <ref type="bibr" target="#b1">[1]</ref>. Similarly, we conduct experiments on SemEval-2014 Datasets<ref type="foot" target="#foot_1">foot_1</ref> (i.e., 14-Laptop and 14-Restaurant). The statistics of the two datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. Besides, we use Accuracy and Macro F1 as evaluation metrics <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>For the usage of LLMs, we adopt the representative version of ChatGPT (i.e., gpt-3.5-turbo). In addition, we set the temperature to 0 to produce more deterministic and focused responses. To minimize the variance resulting from the sampling of demonstration examples, we adopt three random seeds (i.e., 13, 42, 550) for sampling to conduct experiments and report the average performance. Considering the limitations of ChatGPT on the input length, the shot number is selected from <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b9">9]</ref>. Moreover, we utilize the zer-shot prompting <ref type="foot" target="#foot_2">3</ref> designed by Wang et al. <ref type="bibr" target="#b1">[1]</ref> to request LLMs to  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness of MCeFS Prompting (RQ1)</head><p>The related experimental results are shown in Table <ref type="table" target="#tab_1">2</ref>, where the performance of the State-Of-The-Art (SOTA) models on 14-Laptop and 14-Restaurant datasets are quoted from <ref type="bibr" target="#b22">[22]</ref> and <ref type="bibr" target="#b23">[23]</ref>. In addition, Few-Shot (k) and MCeFS (k) represent ChatGPT with traditional few-shot prompting and our MCeFS prompting respectively, while k stands for the shot number. It can be seen from Table <ref type="table" target="#tab_1">2</ref> that compared with few-shot prompting, our MCeFS prompting better elicits the sentiment analysis ability of ChatGPT and largely reduces the performance gap between ChatGPT and SOTA. Concretely, under the same shot number, our MCeFS prompting has better performance than traditional few-shot prompting w.r.t. classification accuracy and macro F1 on the two datasets. For example, relative to Few-Shot (3), MCeFS (3) increases its macro F1 from 0.671 to 0.775 on 14-Restaurant dataset. Furthermore, even if our MCeFS prompting uses fewer demonstration examples than traditional few-shot prompting, the performance of our MCeFS prompting still surpasses that of traditional few-shot prompting. For instance, the classification accuracy of MCeFS (3) has been improved from 74.4% to 80.3% on 14-Laptop when compared to Few-Shot <ref type="bibr" target="#b9">(9)</ref>. As for the reason, traditional few-shot prompting allows LLMs to learn the given demonstration input-output pairs by the way of passively receiving information, which may limit the autonomous reflection and thinking development of LLMs. In contrast, by guiding LLMs to reflect on their own thought processes regarding the completion of the downstream task,</p><p>1 3 9 Shot 78.9% 79.2% 79.5% 79.8% 80.1% 80.4% Accuracy 79.5% 80.3% 79.0% 79.8% 80.2% 79.5% MCeFS MCeFS+PR (a) 14-Laptop &amp; Accuracy 1 3 9 Shot 0.720 0.725 0.730 0.735 0.740 0.745 0.750 0.755 Macro F1 0.746 0.745 0.722 0.751 0.744 0.729 MCeFS MCeFS+PR (b) 14-Laptop &amp; Macro F1 1 3 9 Shot 84.0% 84.5% 85.0% 85.5% 86.0% 86.5% Accuracy 84.7% 84.2% 86.0% 85.0% 84.0% 86.2% MCeFS MCeFS+PR (c) 14-Restaurant &amp; Accuracy our MCeFS prompting enhances the metacognition of LLMs, which enables LLMs to understand the mapping relationship corresponding to the downstream task more precisely. Hence, LLMs could possess a more targeted ability to solve the downstream task with a few demonstration examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effectiveness of Positive Reinforcement (RQ2)</head><p>The related experimental results are shown in Fig. <ref type="figure" target="#fig_3">3</ref>, where MCeFS+PR denotes MCeFS prompting with Positive Reinforcement. Concretely, we observe that under the same shot number, the performance of our MCeFS prompting has been further enhanced after incorporating positive reinforcement in most cases. For instance, compared with MCeFS (1), MCeFS+PR (1) increases its macro F1 from 0.746 to 0.751 on 14-Laptop dataset. The possible reason is that positive reinforcement offers response-based positive feedback (i.e., praise) to simulate the learning motivation of LLMs, enabling them to learn in the direction of accurately analyzing given demonstration examples, thereby gaining a deeper understanding of the specific mapping relationship behind the given demonstration examples and ultimately completing the downstream task precisely. This discovery offers new insights into guiding LLMs learning from a psychological perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Case Study</head><p>To illustrate the effectiveness of our MCeFS prompting, we take the samples used in Fig. <ref type="figure" target="#fig_0">1</ref> to display the learning process of ChatGPT with our MCeFS prompting. It can be seen from Fig. <ref type="figure" target="#fig_4">4</ref>, ChatGPT does not analyze the given demonstration example correctly. Subsequently, with the guidance of our MCeFS prompting, ChatGPT reflects on its thought process and recognizes its mistake. Finally, when encountering the Sentence: It might have been a little too spicy for my friend , which you can couteract with eat more rice and keeping water on hand . What is the sentiment polarity of the aspect water in this sentence?</p><p>The sentiment polarity of the aspect water in this sentence is positive.</p><p>Your answer is wrong. The right answer is neutral. Please reflect on your thought process and avoid making similar mistakes again.</p><p>Apologies for the incorrect response. Upon reviewing the sentence again, I can see that no positive or negative sentiment is expressed towards water specifically. Therefore, the sentiment polarity of the aspect water in this sentence is indeed neutral. Thank you for pointing out the mistake, and I'll strive to avoid similar errors in the future. Sentence: My friend had a burger and I had these wonderful blueberry pancakes . What is the sentiment polarity of the aspect burger in this sentence?</p><p>Label: neutral Ground Truth: neutral test sample that is similar to the demonstration example, Chat-GPT successfully makes correct prediction for the test sample. This case study shows that our MCeFS prompting could enhance the performance of LLMs on downstream tasks by guiding LLMs to autonomous reflection. In addition, the reflective process presented by LLMs also benefits researchers in uncovering cognitive errors in LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>To address the problem of insufficient learning from demonstration examples in traditional few-shot prompting, we propose a novel metacognition-enhanced few-shot prompting to guide LLMs in better learning the given demonstration examples. Furthermore, we introduce positive reinforcement into our MCeFS prompting to promote LLMs learning towards accurate completion of downstream tasks. The experimental results on two real-world datasets show that our MCeFS prompting with positive reinforcement could better elicit the abilities of LLMs than traditional few-shot prompting. As for future work, we are interested in designing a Human-In-The-Loop (HTIL) learning framework and introducing expert feedback corresponding to the reflective process of LLMs to further develop their thinking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of the weakness of few-shot prompting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Ask</head><figDesc>LLMs to complete specific downstream task accoridng to demonstration example (without the corresponding ground truth provided) Response of LLMs Your answer is right. Please reflect on your thought process. Keep it up! If the predicted result of LLMs is consistent with the corresponding ground truth Your answer is wrong. The right answer is [corresponding ground truth]. Please reflect on your thought process and avoid making similar mistakes again. Keep going! If the predicted result of LLMs is inconsistent with the corresponding ground truth Response of LLMs Ask LLMs to complete specific downstream task accoridng to next demonstration example (without the corresponding ground truth provided) ......</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The learning process of our MCeFS prompting with positive reinforcement. The blue and orange parts are core of MCeFS prompting and positive reinforcement respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The experiment results about the effectiveness of positive reinforcement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Example of our MCeFS prompting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of 14-Laptop and 14-Restaurant datasets</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Train #Test</cell></row><row><cell>14-Laptop</cell><cell>2,282</cell><cell>632</cell></row><row><cell>14-Restaurant</cell><cell>3,608</cell><cell>1,119</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The experiment results about the effectiveness of MCeFS prompting. The boldface indicates the best model results of the dataset, and the underline indicates the second best model result of the dataset</figDesc><table><row><cell>Model</cell><cell cols="4">14-Laptop Accuracy Macro F1 Accuracy Macro F1 14-Restaurant</cell></row><row><cell cols="2">Fully-supervised models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SOTA</cell><cell>83.7%</cell><cell>0.801</cell><cell>89.5%</cell><cell>0.849</cell></row><row><cell cols="3">ChatGPT with few-shot prompting</cell><cell></cell><cell></cell></row><row><cell>Few-Shot (1)</cell><cell>74.5%</cell><cell>0.622</cell><cell>82.3%</cell><cell>0.640</cell></row><row><cell>Few-Shot (3)</cell><cell>75.5%</cell><cell>0.641</cell><cell>83.2%</cell><cell>0.671</cell></row><row><cell>Few-Shot (9)</cell><cell>74.4%</cell><cell>0.613</cell><cell>83.2%</cell><cell>0.667</cell></row><row><cell cols="3">ChatGPT with our MCeFS prompting</cell><cell></cell><cell></cell></row><row><cell>MCeFS (1)</cell><cell>79.5%</cell><cell>0.746</cell><cell>84.7%</cell><cell>0.743</cell></row><row><cell>MCeFS (3)</cell><cell>80.3%</cell><cell>0.745</cell><cell>84.2%</cell><cell>0.775</cell></row><row><cell>MCeFS (9)</cell><cell>79.0%</cell><cell>0.722</cell><cell>86.0%</cell><cell>0.773</cell></row><row><cell cols="2">complete ABSC task.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code is available at https://github.com/jiyu0201/MCeFSL.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://alt.qcri.org/semeval2014/task4/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Zer-shot prompting: Sentence: {sentence} What is the sentiment polarity of the aspect {aspect} in this sentence?</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work is funded by <rs type="funder">National Natural Science Foundation of China</rs> (under project No. <rs type="grantNumber">62377013</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality, China</rs> (under project No. <rs type="grantNumber">21511100302</rs>), <rs type="funder">Natural Science Foundation of Shanghai</rs> (under project No. <rs type="grantNumber">22ZR1419000</rs>), the <rs type="funder">Research Project of Changning District Science and Technology Committee</rs> (under project No. <rs type="grantNumber">CNKW2022Y37</rs>), and the <rs type="projectName">Medical Master's and Doctoral Innovation Talent Base Project of Changning District</rs> (under project No.<rs type="grantNumber">RCJD2022S07</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rDycJBa">
					<idno type="grant-number">62377013</idno>
				</org>
				<org type="funding" xml:id="_RFGJt7c">
					<idno type="grant-number">21511100302</idno>
				</org>
				<org type="funding" xml:id="_eV385G6">
					<idno type="grant-number">22ZR1419000</idno>
				</org>
				<org type="funded-project" xml:id="_zsU3SCw">
					<idno type="grant-number">CNKW2022Y37</idno>
					<orgName type="project" subtype="full">Medical Master&apos;s and Doctoral Innovation Talent Base Project of Changning District</orgName>
				</org>
				<org type="funding" xml:id="_8bTXKWQ">
					<idno type="grant-number">RCJD2022S07</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Is chatgpt a good sentiment analyzer? a preliminary study</title>
		<author>
			<persName><forename type="first">Zengzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04339</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Is chatgpt a good personality recognizer? a preliminary study</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11048" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Against spoonfeeding. for learning. reflections on students&apos; claims to knowledge</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M Ann</forename><surname>Dehler</surname></persName>
		</author>
		<author>
			<persName><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JME</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="875" to="893" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tell model where to attend: Improving interpretability of aspect-based sentiment classification via small explanation annotations</title>
		<author>
			<persName><forename type="first">Zhenxiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fostering metacognition to support student learning and performance</title>
		<author>
			<persName><forename type="first">Julie Dangremond</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">J</forename><surname>Sebesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dunlosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBE-LSE</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using metacognitive prompts to enhance self-regulated learning and learning outcomes: A metaanalysis of experimental studies in computer-based learning environments</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCAL</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="811" to="832" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simas eric learning model (selm): Enhance student metacognitive skill based on the academic level</title>
		<author>
			<persName><forename type="first">Ericka</forename><surname>Darmawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siti</forename><surname>Zubaidah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="642" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Rizhal Hendi Ristanto, Muhammad Rizal Akbar Zamzami, and Bevo Wahono</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building children&apos;s learning motivation through positive reinforcement in science and math classroom</title>
		<author>
			<persName><surname>Sumiati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Septiani</surname></persName>
		</author>
		<author>
			<persName><surname>Widodo</surname></persName>
		</author>
		<author>
			<persName><surname>Caturiasari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JPCS</title>
		<imprint>
			<biblScope unit="volume">1318</biblScope>
			<biblScope unit="page">12023</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large language models are human-level prompt engineers</title>
		<author>
			<persName><forename type="first">Yongchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Ioan</forename><surname>Muresanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8086" to="8098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What makes good in-context examples for gpt-3?</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
	<note>in Dee-LIO</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fairness-guided fewshot prompting for large language models</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13217</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An overview: Metacognition in education</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJMCER</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="529" to="535" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing the metacognition of nursing students using eye tracking glasses</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Meteier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mugellini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Verdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Senn-Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Vasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETRA</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orgbox: Supporting cognitive and metacognitive activities during exploratory search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><surname>Capra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2570" to="2574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using positive reinforcement with young children</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jessica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragan H</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><surname>Mcleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Beyond Behavior</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding positive reinforcement and replacement behaviors within the classroom</title>
		<author>
			<persName><forename type="first">K</forename><surname>Reesha M Adamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Kilpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Depaepe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CLD</publisher>
			<pubPlace>Overland Park</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Teachers&apos; classroom instruction reinforcement strategies in english language class</title>
		<author>
			<persName><forename type="first">Wuli</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Fitriati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agung</forename><surname>Fatmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjaniputra</forename><surname>Ginanjar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EduLearn</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="599" to="608" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metrics for multi-class classification: An overview</title>
		<author>
			<persName><forename type="first">Margherita</forename><surname>Grandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Spa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Bagli</surname></persName>
		</author>
		<author>
			<persName><surname>Visani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling aspect correlation for aspect-based sentiment analysis via recurrent inverse learning guidance</title>
		<author>
			<persName><forename type="first">Longfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6887" to="6896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards unifying the label space for aspect-and sentence-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
