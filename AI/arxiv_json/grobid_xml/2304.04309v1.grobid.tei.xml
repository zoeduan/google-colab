<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models for Business Process Management: Opportunities and Challenges ⋆</title>
				<funder ref="#_tpBsw25">
					<orgName type="full">Einstein Foundation Berlin</orgName>
				</funder>
				<funder ref="#_qbrXXJX">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_7RF54hE">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
				<funder ref="#_5J3vmG4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-04-09">9 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Vienna University of Economics and Business</orgName>
								<address>
									<addrLine>Welthandelsplatz 1</addrLine>
									<postCode>1020</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<addrLine>Unter den Linden 6</addrLine>
									<postCode>10099</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Weizenbaum Institute</orgName>
								<address>
									<addrLine>Hardenbergstraße 32</addrLine>
									<postCode>10623</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Models for Business Process Management: Opportunities and Challenges ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-09">9 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">DF820F3BEEDA5D7402389CD4B27D63C1</idno>
					<idno type="arXiv">arXiv:2304.04309v1[cs.SE]</idno>
					<note type="submission">⋆⋆ Equal contribution Preprint submitted to arXiv</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural language processing</term>
					<term>Large language models</term>
					<term>Generative Pre-Trained Transformer</term>
					<term>Deep learning</term>
					<term>Research Challenges</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models are deep learning models with a large number of parameters. The models made noticeable progress on a large number of tasks, and as a consequence allowing them to serve as valuable and versatile tools for a diverse range of applications. Their capabilities also offer opportunities for business process management, however, these opportunities have not yet been systematically investigated. In this paper, we address this research problem by foregrounding various management tasks of the BPM lifecycle. We investigate six research directions highlighting problems that need to be addressed when using large language models, including usage guidelines for practitioners.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent releases of applications building on Large Language Model (LLM) have been quickly adopted by large circle of users. ChatGPT stands out with reaching 100 million users in 2 months <ref type="bibr" target="#b30">[31]</ref>. The key factor explaining this fast uptake is their general applicability making them a general-purpose technology. Also many tasks in research can be approached with LLM applications, include finding peer reviewers, evaluating manuscripts and grants, improving prose in manuscripts, and summarizing texts <ref type="bibr" target="#b31">[32]</ref>. For this reason, some argue that LLM -especially conversational LLM -are a "game-changer for science" <ref type="bibr" target="#b5">[6]</ref>.</p><p>Much of the current discussion of applications like ChatGPT is concerned with the question how good it works now and in the future. We believe that this question needs to be approached with a clearly defined task in mind. Starting with a task focus will move the discussion away from funny or disturbing errors and biases <ref type="bibr" target="#b30">[31]</ref> towards how the collaboration between human experts and LLM applications can be organized. Furthermore, this bears the chance to learn about specific categories of failures, which eventually will help to refine the technology in a systematic way.</p><p>In this paper, we address the research challenge of how LLM applications can be integrated at different stages of business process management. To this end, we refer to the BPM lifecycle <ref type="bibr" target="#b7">[8]</ref> and its various management tasks <ref type="bibr" target="#b12">[13]</ref>. Our research approach is exploratory in a sense that we developed strategies of how LLM applications can be integrated in specific BPM tasks. We observe various promising usage scenarios and identify challenges for future research.</p><p>The paper is structured as follows. Section 2 discusses the essential concepts of Deep Learning (DL) and LLM in relation to Business Process Management (BPM) practises. In Section 3 we identify and discuss LLM applications within BPM and along the different BPM lifecycle phases. Based on these applications, Section 4 describes six core research directions ranging from how LLM change the dynamics and execution BPM projects, to data sets, and benchmarks specific to BPM. Section 5 identifies challenges when using LLM. Furthermore, we provide an outlook on how LLM might evolve in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The advent of LLM applications paves the way towards a plethora of new BPMrelated applications. So far, BPM has adopted natural language processing <ref type="bibr" target="#b0">[1]</ref>, artificial intelligence <ref type="bibr" target="#b6">[7]</ref>, and knowledge graphs <ref type="bibr" target="#b13">[14]</ref> to support various application scenarios. In this section, we discuss the foundations of DL (Section 2.1) and LLMs (Section 2.2). In this way, we aim to clarify their specific capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep learning</head><p>Recent LLM applications build on machine learning and deep learning models, such as recurrent neural networks (RNNs) and transformer networks. Machine Learning (ML) studies algorithms that are "capable of learning to improve their performance of a task on the basis of their own previous experience" <ref type="bibr" target="#b14">[15]</ref>. In essence, ML techniques use either supervised learning, unsupervised learning, and reinforcement learning as a paradigm. Several of them are relevant for LLM.</p><p>In supervised learning, the ML algorithm receives as an input a collection of pairs, where one pair consists of features representing a concept, along with a label. Importantly, this label is task specific and encodes what the algorithm should learn about the concepts. Such labels can be, for instance, spam and no spam for a spam classifier, or bounding boxes with annotations for an image. There are two cases of supervised learning that are relevant for LLM: few-shot and zero-shot learning. Few-shot learning is when a ML algorithm adapts to a new situation with little amount of labelled data, and zero-shot learning is when the algorithm can do this with no labelled data at all. For example, a language model can be provided with a few input-output pairs, and the model can inverse the mapping function without any parameter changes. In unsuper-vised learning, the algorithm only receives a feature tensor of a concept as an input and the desired output is unknown. The algorithm then finds structural properties of the concepts present in the feature tensor. A typical application is dimensionality reduction, for instance using auto-encoders. In reinforcement learning, the algorithm receives a feature tensor of a concept as an input for which an output is produced, which is then evaluated through rewards. The algorithm then uses this feedback to improve its parameters. ChatGPT uses a form of reinforcement learning known as deep reinforcement learning to improve its language generation capabilities, in particular, "Learning to summarize from human feedback" <ref type="bibr" target="#b28">[29]</ref>. ChatGPT is fine-tuned using a reward signal that assesses the quality of its generated responses, with the goal of maximizing the reward signal over time. The model's ability to learn from the reward signal allows it to generate increasingly relevant and coherent responses.</p><p>Deep learning (DL) is a ML method based on Neural Networks (NN). In general, they are NNs with many layers stacked on top each other, which enables them to learn multiple layers of representations <ref type="bibr" target="#b9">[10]</ref>. Importantly, these representations can be learned without supervision.Networks with only one hidden layer are called shallow.Deep networks are able to handle more complex problems compared to shallow networks. Combined with the availability of large amounts of data, improvements on how to speed up the optimization, and powerful computing resources, enables them to be trained effectively. In the context of natural language processing, deep learning has been particularly effective in tasks such as machine translation, sentiment analysis, and named entity recognition. The ability of deep learning to learn multiple layers of representations from input data has proven to be particularly powerful for these tasks. This is because natural language processing involves dealing with sequences of words and characters, and the relationships between these sequences are often complex and multi-layered. The use of large amounts of labeled training data and powerful computational resources has enabled deep learning models to achieve state-of-the-art results in many Natural Language Processing (NLP) tasks. For example, the transformer architecture, introduced in the paper "Attention is All You Need" by Vaswani et al. <ref type="bibr" target="#b32">[33]</ref> has become the standard architecture for many NLP tasks, including language translation and language modeling.</p><p>In recent years, BPM research has integrated the capabilities of deep learning to a large extent for process prediction. For an overview, see <ref type="bibr" target="#b15">[16]</ref>. There are also recent applications for automatic process discovery <ref type="bibr" target="#b27">[28]</ref>, for generating process models from hand-written sketches <ref type="bibr" target="#b26">[27]</ref>, and for anomaly detection <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large language models</head><p>LLM are DL models trained on vast amounts of text data to perform various natural language processing tasks. These models, which typically range from hundreds of millions to billions of parameters, are designed to capture the complexities and nuances of human language. The largest models, such as GPT-1 and GPT-3, are capable of generating human-like text, answering questions, translating languages, and computer code. The training process of these models involves processing massive amounts of text data, which is used to learn patterns and relationships between words and phrases. These models then use this information to predict the likelihood of a given token, or sequence of tokens, in a specific context. This allows them to generate coherent and contextually relevant text or perform other language-related tasks. The rise of large language models has resulted in significant advancements in the field of NLP, and they are widely used in various applications, including chatbots, virtual assistants, and text generation systems. One of their strengths is their ability to perform few-shot and zero-shot learning with prompt-based learning <ref type="bibr" target="#b10">[11]</ref>.</p><p>In 2018, Radford et al. introduced GPT-1 (also sometimes called simply GPT) in their paper on "Improving language understanding by generative pretraining" <ref type="bibr" target="#b22">[23]</ref>. Generative Pre-trained Transformer (GPT)-1 refers to the largest model the authors have trained (110 million parameters). In the paper, the authors studied the ability of transformer networks trained in two phases for language understanding. In the first phase, they trained a transformer network to predict the next token given a set of tokens that appeared before (also called unsupervised pre-training, generative pre-training, or in statistics auto-regressive). In the second phase, the transformer networks was fine tuned on tasks with supervised learning (also called discriminative fine-tuning). In summary, their major finding is that combining task agnostic unsupervised learning in the first phase, then using this model in a second phase with supervised learning for fine tuning on tasks can lead to performance gains -from 1.5% on textual entailment to 8.9% on commonsense reasoning.</p><p>In 2019, Radford et al. introduced GPT-2 in their paper "Language Models are Unsupervised Multitask Learners" <ref type="bibr" target="#b23">[24]</ref>. Again, GPT-2 refers to the largest model they have trained. GPT-2 is hence a scaled up version of GPT-1 in model size (1.5 billion parameters), and also in training data size. In particular, GPT-2 has roughly more than ten times the number of parameters than GPT-1, and is trained on roughly more than ten times the amount of training data. They report two major findings. First, the unsupervised GPT-2 can outperform language models that are trained on task specific data set, without these data sets being in the training data set of GPT-2. Second, GPT-2 seems to learn tasks (for example question answering) from unlabeled text data. In both cases, however, the performance did not reach the state-of-the-art. In summary, their major finding is that LLMs can learn tasks without the need to train them on these tasks, given that they have sufficient unlabeled training data.</p><p>In 2020, Brown et al. introduced GPT-3 with the paper "Language Models are Few-Shot Learners" <ref type="bibr" target="#b4">[5]</ref>. Unlike the above two cases, GPT-3 refers to all the models the authors have trained, i.e. it refers to a family of models. The largest model the authors have trained is GPT-3 175B, a model with 175 billion parameters. In their paper, the authors showed that language models like GPT-3 can learn tasks with only a few examples, hence the title includes "few-shot learners". The authors demonstrated this ability by fine-tuning GPT-3 on various tasks, including question answering and language translation, using only a small number of examples.</p><p>In 2023, OpenAI introduced GPT-4 <ref type="bibr" target="#b20">[21]</ref>. Contrary to previous versions of GPT, this version is a multimodal model as it can process text and images as an input to produce text. This model is a major step forward as it improves on numerous benchmarks; however, it suffers from reliability issues, a limited context window, and inability to learn from experience like previous GPT models. This release, however, diverges from previous GPT models as OpenAI is secretive about "the architecture (including model size), hardware, training compute, dataset construction, training method, or similar". We only know about the model that it is a transformer style-model, pre-trained on predicting the next token on publicly available and not disclosed licensed data, and then fine-tuned with Reinforcement Learning from Human Feedback (RLHF). Notwithstanding this departure, the authors include in their report findings on predicting model scalability. They in particular report on predicting the loss as a function of compute, and the mean log pass rate (a measure on how many code sample pass a unit test) as a function of compute given a training methodology. In both cases, they find that they could predict the respective measure with high accuracy based on data generated with significantly less compute (1.000 to 10.000 less). They also find the inverse scaling price for a task, meaning that the performance on a task first decreases as a function of model size and then increases after a particular model size.</p><p>In 2022, OpenAI introduced a conversational LLM -called ChatGPT <ref type="bibr" target="#b18">[19]</ref>. As a model, the first version of ChatGPT was based on GPT-3.5 and is an InstructGPT sibling. GPT-3.5 is a GPT-3.0 model trained on a training data set that contains text and software code up to the fourth quarter of 2021 <ref type="bibr" target="#b17">[18]</ref>. InstructGPT was introduced in "Training language models to follow instructions with human feedback " <ref type="bibr" target="#b21">[22]</ref>, and is a GPT-3 model fine tuned with supervised learning in the first step, and in the second step with reinforcement learning from human feedback <ref type="bibr" target="#b28">[29]</ref>. ChatGPT is hence a GPT-3.5 model fine tuned for conversational interaction with the user. In other words, the user interacts with the model via sequence of text (the conversation) to accomplish a task. For example, we can copy and paste a text into ChatGPT's input field and ask it to summarize it. We can even be more specific, we can say that the summary should be 10 sentences long and be written in a preferred style. Importantly, if we are unhappy with the result we can ask ChatGPT to refine its own summary without copying and pasting the text it should summarize. At the moment of this writing, ChatGPT can be used with GPT-4 as the backend LLM.</p><p>There are also other large language models. In 2022, Zhang et al. introduced Open Pre-trained Transformer (OPT) with the paper "OPT: Open Pre-trained Transformer Language Models" <ref type="bibr" target="#b33">[34]</ref>. The main contribution of that paper is that it makes all artifacts including the nine models available for interested researchers.These models are GPT-3 class models in parameter size and performance. Another open LLM is BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) (176 billion parameters), which was developed in the BigScience Workshop <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uptake of large language models</head><p>Above, we briefly discuss LLMs, where we focus particularly on the GPT model family as these are the most popular LLMs, we hypothesise. It is important to recognize the transition from GPT-3 to GPT-4, as it brought a massive increase on a variety of benchmarks, particular on academic and professional exams <ref type="bibr" target="#b20">[21]</ref>. These performance increases in NLP tasks are a result of natural language understanding and have, as we argue, massive implications for what can be automated -the automation frontier. This frontier is arguably shifted further when natural language understanding is combined with plugin software components. In fact, at the time of writing, the company behind GPT is experimenting with Chat-GPT plugins. Among the currently offered plugins are Klarna, Wolfram, the integration with vector data bases for information retrieval, and an embedded code interpreter for Python <ref type="bibr" target="#b19">[20]</ref>. This has an impact on Robotics Process Automation (RPA), and more broadly on business process automation including Business Process Management Systems (BPMSs), and more generally on how work is carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large language models and the BPM lifecycle</head><p>In this section we identify applications of LLM within BPM. We systematically explore these applications along the phases of the BPM lifecycle, namely identification, discovery, analysis, redesign, implementation, and monitoring <ref type="bibr" target="#b7">[8]</ref>. In this way, we complement recent efforts to build an overarching inventory of LLM applications, such as in other fields like data mining<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identification</head><p>The BPM lifecycle starts from Identification. Normally, at this stage there is not much structured process knowledge available in the company, and relevant information has to be extracted from heterogeneous internal documentation. This is exactly where LLM shine as they can quickly scan and summarize large volumes of text, highlighting important documents or directly outputting required information.</p><p>Identifying processes from documentation The idea is to give LLM all relevant documentation existing in the organization as input. This can include legal documents, job descriptions, advertisements, internal knowledge bases and handbooks. The LLM is then tasked to identify which processes are taking place in the organization. It can be further instructed to classify the input documents according to processes they describe. Multimodal LLMs can improve the results even further as charts, presentations and photos can also directly be used as information sources.</p><p>Process selection LLM can be further asked to assess strategic importance of processes based on, e.g. number and types of documents that refer to them as well as extract this information from process descriptions. If given access to information systems supporting the process or other KPIs, LLM can also assess process health. Finally, assessing feasibility is also theoretically achievable as long as necessary information, e.g. recent technology reports, is given as input as well. Based on these criteria, LLM can prioritize the processes for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discovery</head><p>The second stage of BPM lifecycle is Process Discovery. At this stage one or a combination of process discovery methods is selected to produce process models. When one speaks of automated process discovery, one usually means process mining -a technique of extracting process models and other relevant data out of event logs left by information systems supporting the execution of a process. However, with LLM also other discovery techniques can benefit from (at least partial) automation.</p><p>Process discovery from documentation Apart from process mining, documentation analysis is an established process discovery method. In this method, process analyst uses the information found in heterogeneous sources such as internal documentation, job advertisements, handbooks, etc. Searching in these documents might require a lot of time and effort. LLM are extremely suitable for this task as they can summarize high volumes of text in a concise and structured way. More precisely, they can output process descriptions in desired format (plain text, numbered lists, etc.). One can also specify the level of detail, as to whether the output should include only the activities and events or also resources and additional information. Finally, as some LLM are also capable of working with structured document formats such as XML, in fact even BPMN models can be produced automatically.</p><p>Process discovery from communication logs Another information source that can be used in evidence-based discovery is communicaiton logs, i.e. e-mails and chats between process participants: internal employees but also external partners and customers. LLM can extract patterns from these communication logs, which can be seen as various steps in a process. Then, they can similarly produce process descriptions or models.</p><p>Interview chat bot Possible applications of LLM in process discovery can also go beyond evidence-based discovery. Another common discovery method are interviews with domain experts. In these interviews, process analyst asks questions about the process and produces a process model based on several interviews. Typically, several separate interviews with different domain experts are required to produce the first version of process model. Afterwards, additional rounds of interviews are conducted in order to get and incorporate feedback and to perform validation. In the worst case, domain experts might have conflicting perceptions of the process, then resolving such conflicts becomes a very difficult and timeconsuming task for both process analyst and domain experts.</p><p>LLM can solve parts of this problem by providing a chat bot interface for domain experts. In this way, the domain experts answer questions in the chat. This can bring a lot of advantages. First, the domain experts do not have to allocate lengthy time slots for interviews but instead talk with the chat bot at desired pace. Second, the feedback loop gets shorter as LLM can produce process models directly after or even during the conversation with the domain expert and also do updates to the model, thus validation can happen simultaneously with model creation. Finally, the benefits will only grow if multiple domain experts interact with the chat bot simultaneously (and independently) but the chat bot can use all of this input in the conversations. The latter option is, however, more difficult to implement.</p><p>Combined process discovery All process discovery methods have their advantages and drawbacks. Often, a combination of these methods is used to achieve best results. However, this combination is limited by the resources that are allocated for process discovery task. Discovery methods presented above give valuable output yet requiring much less resources. Thus, it is possible to apply more of them simultaneously for even better result. The combination of these methods can be used in addition to traditional process mining or "manual" process discovery, which will provide the richest insights. While it could happen that the results of different methods have some inconsistencies that will have to be fixed, also fixing them can be done in (semi-)automated manner.</p><p>Process model querying As LLM seem to "understand" process models serialized as XML, they can be used to answer some questions about the model. This can be very useful for quality assurance. First of all, it can be used for checking syntactic quality. While there are tools out there that can do it already, and with much less overhead, it is still convenient to have this feature in LLM because LLM, in contrast to other methods, may be able to check other quality aspects as well. For instance, it can also check semantic quality. Indeed, process analyst can give LLM both interview transcript and a process model as input and LLM can check both validity and completeness based on this interview. It must be noted, of course, that this will only work under the assumption that the interview transcript has these features of validity and completeness. Another way of checking semantic quality of the model would be via process simulation, e.g. to explicitly ask LLM whether the given process model could have produced a given execution sequence or to ask LLM to give possible execution sequences that can be generated by the model. LLM are known to be able to simulate Linux shell, for instance, thus they might be also able to simutale a BPMS execution engine as long as enough input is provided. Finally, LLM can also (at least so some extent) check pragmatic quality of the models as long as some definition of guidelines, e.g. 7PMG is provided as input as well. It must be also noted that LLM can not only spot these quality issues but also suggest fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>The next stage is Process Analysis. At this stage, the discovered processes are analyzed to find problems and bottlenecks. While this is a cognitively loaded task, LLM can be used to help human analysts in some regard.</p><p>Issue discovery If an issue exists in a process, chances are high somebody has already complained about it. Depending on the company, product, and process it can be the customer, partner or an employee and in can happen on different platforms, including social media, support service or internal communication tools. LLM are good at summarizing large volumes of unstructured text as well as finding patterns, and this capability can be used for this task. It is as easy as just scraping the text from these platforms and giving it as input to the LLM with a simple prompt like "find all things customers have complained about".</p><p>Issue spotting After an issue in the process is found, the next step is to spot the part of the process that creates this issue. In some cases, it can be a difficult task, especially in a complex process. The idea here is to give LLM all process models (or models of the relevant process in case it is known that only one process causes the issue and it is known exactly which process) and the spotted problems. The task of LLM is, by analyzing task names and descriptions to make suggestions which tasks may be responsible for the issue. In advanced cases, LLM might be even capable of suggesting some fixes. It might be something as simple as suggesting to automate some manual task that takes too long but it also might be some more complex process redesign suggestion as long as LLM is given redesign methods as additional input or is trained on redesign methods as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Redesign</head><p>The fourth phase of the BPM lifecycle is Process Redesign. In this stage, process improvement suggestions are developed based on discovered issues and general process improvement methods. These suggestions are evaluated, and a to-be process model is developed at the end of this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Business process improvement</head><p>An obvious yet very promising use case is t just ask LLM to redesign the process. As already mentioned, simple issues arising from just one activity can be fixed by the LLM. However, it does not stop there and is theoretically only depending on the quality of the input given to the LLM. Indeed, if it is given exhaustive information about the process (detailed process model as well as description of the process or tasks) as well as detailed description of some redesign method (or it is trained on some redesign methods), redesigning the business process is as simple as just telling the LLM to apply the method on the process. This can, however, be improved even further. First, the description of the issues discovered in the previous phase can be given as additional input to guide process redesign to fix those first. Second, LLM can be instructed to apply different redesign methods and to give separate lists of suggestions given by each of them so the analyst can then select the best options. Moreover, LLM itself can be asked to choose the best suggestions and motivate its choice. It must be noted, of course, that this will only work if sufficient input is given. For instance, for inward-looking redesign methods, the methods themselves as long as detailed process information is required. For outward-looking methods, in addition to that, there should be enough outside information and/or a way for LLM to properly communicate with the outside world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation</head><p>The next phase of the BPM lifecycle is Process Implementation. It covers organizational and technical changes required to change the way of working of process participants as well as IT support for the to-be process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BPMN model explanations with plain text</head><p>As mentioned, LLM can work with BPMN models serialized in XML. We have already discussed how LLM can manipulate process models in order to increase quality as well as suggest or incorporate redesign ideas. To close the circle, LLM can produce textual explanations of BPMN models. What is more interesting, one can control the level of detail as well. So, depending on the target audience, LLM can produce textual overview but also detailed descriptions of the models. It can transform it into requirements for software developers if enough details are contained in the BPMN model itself.</p><p>BPMN model chatbot Building on top of the previous use case, model description can be also tailored to every specific user. This way, given a model or -better -model repository with additional documentation, LLM can prepare specific descriptions for, e.g. process owner but also for individual participants for which all specific tasks they are responsible for are also described and explained in detail. Furthermore, in this use case one can add interaction between the user and LLM. This way, user may ask clarifications for parts he did not understand or generally ask for more details as long as some guidance is required.</p><p>Process orchestrator LLMs can be accessed via APIs and at the same time can access APIs themselves, opening a huge variety of opportunities. While the former means it can be used for automated tasks and be called by the orchestrator, the latter means that it could theoretically be an orchestrator itself: given executable process model and additional constraints as context as well as the required instance data as input, it can theoretically execute a process by calling other APIs and assigning tasks in a more flexible way than a traditional orchestrator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Monitoring</head><p>The last phase of the BPM lifecycle is Process Monitoring. At this stage, already implemented processes are executed, and their performance is monitored. The observations collected in this phase are used for operational management as well as serve as input for further iterations of the lifecycle.</p><p>Process dashboard chatbot Dashboards are a powerful tool that provides overview of the most important KPIs of a process on a single screen. However, the ultimate goal of them is to tell the viewer whether the status of the process is good or not, and the numbers and colors are mostly used as an intermediary medium. LLM can take away this intermediate step and allow the user to directly know the status of the processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Research directions</head><p>In this section we propose the research directions. We categorize the research directions into three groups. The first group studies the use of LLM, and their applications, in practice. This includes the use within BPM projects in companies or as part of an Information System (IS) (Section 4.1), the development of usage guidelines for practitioners and researchers (Section 4.2), and also the derivation of BPM tasks (Section 4.3) and their corresponding data sets (Section 4.4). The second group studies how LLM can be combined with existing BPM tools, and more generally BPM technologies, to increase user experience (Section 4.5). Crucially, this group draws from findings in the first group. The third and final group develops large language models specifically for business process management, so these models can understand the context and language of business processes and support various tasks, such as process discovery, monitoring, analysis, and optimization (Section 4.6). Again, this group builds upon the findings of the first group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The use of large language models in BPM practice</head><p>The first research direction studies the use of LLM in practice. One major question to answer is for which tasks LLM can be used. In Section 3, we present a list of tasks for which LLM can be used. However, this list might not be complete, in addition some of the tasks might turn out to be of little use. Tied to this is the question what tasks will bring, and ultimately bring the most value for an organization. The next big question is the relation between a task and the model properties needed to achieve a pre-defined value. One question here is which tasks can be achieved with already existing models. Another question to study is whether we always need the largest, and hence most accurate model, for each task. We hypothesize that this might not be the case. Finally, and most importantly, the next big question to answer is how LLM will change how work is carried out within BPM projects, and within processes that are actively managed. We for example hypothesize that conversational LLMs might take the spot of the duck in the famous duck approach<ref type="foot" target="#foot_1">foot_1</ref> . This question is a socio-technical systems question, and we hence strongly believe that the BPM community, and the information systems community more broadly, is especially well equipped to contribute to this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Usage guidelines for researchers and practitioners</head><p>The second research direction builds usage guidelines for BPM researchers and practitioners. One question such guidelines have to answer is given an organizational context, the lifecycle phase, and the process context of a task, suggest a LLM to achieve an expected value. In addition, such guidelines systematically collect best practices for creating prompts. For example, for the BPM lifecycle phase process implementation, and monitoring and controlling, a company might consider using a LLM within a managed process. Let us assume this company is a bank and wants to automate the task of replying to customer inquiries with LLM. Then this guideline proposes for the process implementation a specific LLM, with the number of parameters it has, gives examples on how to create a prompt template, fill the template with customer background information, and finally on how to integrate the customer inquiry within the prompt template. For process monitoring and controlling, the guidelines might propose a different model for analyzing different inquiry clusters as the lifecycle phase context is different. As an example, consider here that the LLM first categorizes each inquiry into a positive and negative sentiment, and then lists for both the top five inquiry reasons. This research direction builds upon the first research direction, as first research direction, among others, determines the tasks for which LLM can be used in principle.</p><p>4.3 Creation, release, and maintenance of task variants specific to BPM This research direction builds and maintains two different task lists. The first list maps general NLP tasks to tasks within BPM. As an example, consider the general NLP task of text summarizing. Within BPM, text summarizing can relate to summarizing a set of process descriptions or task descriptions. We can think of this list as a one to many mapping between NLP tasks on the one hand, and BPM tasks on the other. The second list enumerates tasks that are unique to BPM. This research direction uses the findings from the directions presented in Section 4.1 and Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Creation, release, and maintenance of data sets and benchmarks</head><p>Public data sets and benchmarks are crucial for the progress of LLM in research as they allow researchers to measure progress. In addition, they are also important for practitioners as they define data set properties (such as metainformation) they are likely to need themselves when they fine tune a model. As a result, data sets and benchmarks need to be properly aligned with the automation needs of BPM. Blagec et al. argue similarly as we, but for the clinical profession <ref type="bibr" target="#b2">[3]</ref>. In their study, they analyzed 450 NLP data sets and found that "AI benchmarks of direct clinical relevance are scarce and fail to cover most work activities that clinicians want to see addressed". A research direction for the BPM community is hence to do the same for BPM. One question worth studying is whether existing NLP data sets and benchmarks are of relevance to BPM, for example, if they cover the activities of BPM researchers and practitioners. This research direction builds upon the research direction in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">LLM and BPM artifacts</head><p>This research direction studies the interplay of LLM, BPM artifacts, and BPM tasks. The goal is to understand which artifacts are necessary for LLM, and their multimodal successors, to create useful outputs. It can hence be understood as a special case of prompt engineering, which we might call multimodal prompt engineering for BPM. This is an important research direction as the output quality of a LLM depends heavily on the context quality and quantity it is given. In other words, the more context, and the higher the quality of each context, the higher the output quality of the LLM. For this reason, we believe that it should be considered its own research direction. As an example, consider again the customer inquiry process from above. In this case, we can imagine that the context of the LLM depends on the inquiry. In one case, the customer might include an image in the inquiry. Or think of the redesign phase of the inquiry process. During this phase, artifacts are created, for example drawings of processes on a board, comments to these processes in a word processor, and remarks on data availability and access in an audio file. This information might be useful when we ask -a possibly different -LLM why a customer inquiry on current special offers cannot yet be answered. The reason here might be that a central system which stores special offers does not yet exist. This research direction builds upon the directions presented in Section 4.1 and Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Development and release of Large Language Models for</head><p>Business Process Management This research directions studies how LLM are build for BPM tasks, all previously discussed research direction are the foundation for this direction. The goal of the research direction is to build LLMs that are attuned to the specific challenges and requirements of BPM, compared to general-purpose language models. This includes specialized models in the sense of exclusive for, and also general-purpose language models that are fine-tuned on the BPM domain. An important aspect of this direction is to open source the created LLM, as is done for OPT <ref type="bibr" target="#b33">[34]</ref>. This is important for researchers can use this model in their studies, and practice as companies can use these models free of charge for their use cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section we discuss the challenges of LLM, the power of combination and inflated expectations, and end with an outlook and future work.</p><p>Challenges The use of LLM entails opportunities and challenges. For example, they can help to understand difficult research, but they also carry over deficiencies (including factual errors) in the training data set to the texts they generate <ref type="bibr" target="#b31">[32]</ref>. In a systematic study of these errors, Borji analyzes errors of ChatGPT and categorizes them -the author further outlines and discusses the risks, limitations and societal implication of such models<ref type="foot" target="#foot_2">foot_2</ref>  <ref type="bibr" target="#b3">[4]</ref>. The failure categories identified by the author include reasoning, factual, math, and coding. A similar deficiencies study was done in <ref type="bibr" target="#b1">[2]</ref>, but these authors focus on LLM in general. A news feature in Nature discusses these and the risks of using LLM <ref type="bibr" target="#b8">[9]</ref>. One consequence for education might be that essays as an assignment should be re-considered <ref type="bibr" target="#b29">[30]</ref>.</p><p>The power of combination and managing expectations The major innovation of ChatGPT was not the introduction of a new technology, but the combination of already existing ones and an easy to use user-interface <ref type="bibr" target="#b11">[12]</ref>. This effect of combination extends beyond LLM, NLP, or ML innovations. For example, OpenAI is currently experimenting with integrating ChatGPT with software plugins, which might even in the short run lead to a software marketplace for their platform 7 . For this reason, we suggest and advocate in our research directions above to study and build these combinations with existing BPM technologies, instead of solely focusing on developing new ones. In this paper, we have so far made the case for the opportunities LLM realize, shortly discussed their shortcomings, and pointed out how important it is to combine technologies within a field, and across field boundaries. However, we also stress here how important it is to manage, maybe even overshooting, expectations driven by this very recent developments. For example, the speculation about the possible capabilities on the successor of GPT-3 were driven up by the hype to a point where "people are begging to be dissapointed" <ref type="bibr" target="#b11">[12]</ref>.</p><p>Outlook and future work LLM are used, and will be used in commercial products with huge amounts of users. We speculate that this will have an effect on research, as funding agencies might increase the amount of grants for this research field. An ever increasing user base that interacts with LLM (directly or indirectly) is therefore, in our view, inevitable. For future work, we plan to work on developing research directions that are beyond the scope of this paper. We expect that LLM will have an effect on how work is carried out (see Section 2.3 and Section 4.1). But this may have far greater impacts than what we cover here, for example on the BPM capabilities, which are strategy, governance, information technology, people, and culture <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we present six research directions for studying and building LLMs for BPM. We use the BPM lifecycle to propose applications of LLM to showcase the impact of these models.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>See for example the OpenAI Cookbook GitHub repository, which provides code examples for the OpenAI API</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Rubber duck debugging</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>See the ChatGPT failure archive (GitHub) for an up-to-date list</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This research received funding from the <rs type="projectName">Teaming</rs>.<rs type="projectName">AI</rs> project, which is part of the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> under grant agreement No <rs type="grantNumber">957402</rs>. The research by <rs type="person">Jan Mendling</rs> was supported by the <rs type="funder">Einstein Foundation Berlin</rs> under grant <rs type="grantNumber">EPP-2019-524</rs> and by the <rs type="funder">German Federal Ministry of Education and Research</rs> under grant <rs type="grantNumber">16DII133</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_5J3vmG4">
					<orgName type="project" subtype="full">Teaming</orgName>
				</org>
				<org type="funded-project" xml:id="_qbrXXJX">
					<idno type="grant-number">957402</idno>
					<orgName type="project" subtype="full">AI</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_tpBsw25">
					<idno type="grant-number">EPP-2019-524</idno>
				</org>
				<org type="funding" xml:id="_7RF54hE">
					<idno type="grant-number">16DII133</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Challenges and opportunities of applying natural language processing in business process management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Aa</surname></persName>
		</author>
		<author>
			<persName><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leopold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2018: The 27th International Conference on Computational Linguistics: Proceedings of the Conference</title>
		<meeting><address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">August 20-26. 2018. 2018</date>
			<biblScope unit="page" from="2791" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
	<note>FAccT &apos;21, Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Blagec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kraiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Frühwirt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="page">104274</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2302.03494</idno>
		<ptr target="https://arxiv.org/abs/2302.03494" />
		<title level="m">A categorical archive of ChatGPT failures</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020. 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ChatGPT: five priorities for research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A M</forename><surname>Van Dis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bockting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="224" to="226" />
			<date type="published" when="2023-02">Feb 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AI-augmented business process management systems: a research manifesto</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Limonad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Montali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Rehse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Accorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fahland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Management Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Reijers</surname></persName>
		</author>
		<title level="m">Fundamentals of Business Process Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robo-writers: the rise and risks of language-generating AI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hutson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">591</biblScope>
			<biblScope unit="page" from="22" to="25" />
			<date type="published" when="2021-03">Mar 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Loizos</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=ebjkD1Om4uw" />
		<title level="m">StrictlyVC in conversation with Sam Altman, part two (OpenAI)</title>
		<imprint>
			<date type="published" when="2023">Jannuary 2023</date>
		</imprint>
	</monogr>
	<note>YouTube channel of Connie Loizos</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying do&apos;s and don&apos;ts using the integrated business process management framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business Process Management Journal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Research directions in process modeling and mining using knowledge graphs and machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahmud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Services Computing -SCC 2022</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Qingyang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="86" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learning for science: state of the art and future prospects</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mjolsness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="2051" to="2055" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A systematic literature review on state-of-theart deep learning methods for process prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lahann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fettke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepalign: alignment-based process anomaly correction using recurrent neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seeliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mühlhäuser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Information Systems Engineering: 32nd International Conference, CAiSE 2020</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">June 8-12, 2020. 2020</date>
			<biblScope unit="page" from="319" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="https://platform.openai.com/docs/model-index-for-researchers/model-index-for-researchers" />
		<title level="m">OpenAI: Model index for researchers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Chatgpt: Optimizing language models for dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/" />
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://openai.com/blog/chatgpt-plugins" />
		<title level="m">OpenAI: ChatGPT plugins</title>
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://cdn.openai.com/papers/gpt-4.pdf" />
		<title level="m">OpenAI: GPT-4 technical report</title>
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/language-unsupervised/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The six core elements of business process management</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vom Brocke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on business process management 1</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="105" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176b-parameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sketch2process: Endto-end bpmn sketch recognition based on neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der Aa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leopold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Supervised learning of process discovery techniques using graph neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fahland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="page">102209</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
		</imprint>
	</monogr>
	<note>f89885d556929e98d3ef9b86448f951-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AI bot ChatGPT writes smart essays-should academics worry?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stokel-Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Welcome to the era of chatgpt et al. the prospects of large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Flath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business &amp; Information Systems Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How language-generation AIs could transform science</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Noorden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">605</biblScope>
			<biblScope unit="issue">7908</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">OPT: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
