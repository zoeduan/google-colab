<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The (ab)use of Open Source Code to Train Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-28">28 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Al-Kaswan</surname></persName>
							<email>a.al-kaswan@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maliheh</forename><surname>Izadi</surname></persName>
							<email>m.izadi@tudelft.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The (ab)use of Open Source Code to Train Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-28">28 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">FB4DD8FDF99E14FEF80A476A185E51F9</idno>
					<idno type="arXiv">arXiv:2302.13681v2[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering. LLMs for Code are commonly trained on large unsanitized corpora of source code scraped from the Internet. The content of these datasets is memorized and emitted by the models, often in a verbatim manner. In this work, we will discuss the security, privacy, and licensing implications of memorization. We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide four actionable recommendations to address this issue.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. LANGUAGE MODELS FOR CODE</head><p>Large Language Models (LLMs) have gained significant attention in the field of Natural Language Processing (NLP) in recent years due to their ability to perform a wide range of NLP tasks with impressive accuracy. These models, trained on massive amounts of data, improve in accuracy as they grow from millions to billions of parameters. LLMs for code are trained on massive amounts of data and can learn the structure and syntax of programming languages, making them well-suited for tasks such as code summarization, generation, and completion [1, 2]. LLMs are even making their way into commercial products like GitHub's Copilot, Replits's GhostWriter and Tabnine. Meanwhile, some have identified that LLMs can memorize large swaths of training data [3]. Memorization enables the extraction of the data using Data Extraction Attacks. Some attacks have even been able to extract addresses and other personal information from public models [3]. Memorization also impacts LLMs for code, with all its associated consequences. We will discuss these consequences in three categories: security, privacy and licensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SECURITY IMPLICATIONS</head><p>Text memorization has strong security implications. Firstly, massively mined code datasets are not sanitized or manually curated, the datasets could therefore contain many biases, <ref type="bibr" target="#b0">1</ref> and instances of badly written or buggy and insecure code. A recent study found that around 40% of GitHub Copilot's code generations for MITRE's top 25 Common Weakness Enumerations, a list of the most dangerous software weaknesses were found to be vulnerable [4]. If these models become more prevalent and trusted, they can introduce more vulnerable code into software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRIVACY IMPLICATIONS</head><p>Memorization enables adversaries to access training data, and everything contained within, simply by accessing the model. This has major privacy implications since code can contain private information. Think of credentials, API keys, directory structures, logged info, or in-code discussions by developers. Code can also contain personal information like emails or contact information. If personal data is published on the Internet, the data could be retracted and deleted from the source. But once it is mined and used to train an LLM, the information is forever embedded in a compact representation, which is queryable at scale. With query access to these models, an adversary can potentially extract this data [3] and threaten Internet users' privacy. There are many reasons why one could publicly share private information; (1) simply by accident, or (2) a malicious actor could share this information in a doxing campaign [3]. Even if the data is published willingly, the owner has a certain use and audience in mind and might not wish to share this information with the entire world. This is referred to as the re-purposed data problem. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. LICENSING</head><p>Publicly available source code is also subject to licences, some of which heavily regulate the use of the material. Initially, developers raised concerns about licensed code on social media. GitHub Copilot could be prompted to produce verbatim copies of copyrighted code, without providing the required attribution or licence terms. <ref type="bibr" target="#b1">2</ref> Similarly, Copilot was producing copyrighted code while attributing the wrong author and providing the wrong license. <ref type="bibr" target="#b2">3</ref> Later, a lawsuit was filed against GitHub, Microsoft and OpenAI, claiming that Copilot is violating the licence of open-source code. <ref type="bibr" target="#b3">4</ref> Broadly, open-source code is licensed under two types of licences. Permissive licenses, allow users to use, modify, and distribute the software for any purpose, without requiring that the user share their work. Non-permissive licenses, also known as "copyleft" licenses, require that users freely share their own software under the same licence if they distribute the software or any derivatives of it. Creating closed or commercial software based on non-permissively licensed code is unethical and possibly even illegal [5]. But this does raise the following question: Does training LLMs on copyleft code infringe on their license?</p><p>Firstly, we must determine how many LLMs for code are trained on copyleft code. Looking at some of the most popular code models, we can observe that the vast majority are trained on open-source code. CodeBERT and CodeT5 are trained on CodeSearchNet, which contains copyleft code. We also found that CodeBERT, CodeGen, and CodeClippy make use of The Pile, a collection of 22 datasets, one of which is a GitHub dataset containing copyleft data.</p><p>We found that only InCoder makes an effort to prevent training on copyleft code. InCoder does however make use of a dataset of StackOverflow answers, which are licensed under varying CC-BY-SA licences, all of which require attribution. <ref type="bibr" target="#b4">5</ref> Despite the public attention, it is not completely clear whether Codex, the model behind Copilot, is trained on nonpermissive code. Many imply that it does,<ref type="foot" target="#foot_5">foot_5</ref> citing the copying of copyleft code and the fact that the system has encountered a copy of the GPL licence many times during training. The training data for Codex is not publicly available, and neither OpenAI nor Github have provided any clarification. <ref type="bibr">7</ref> LLMs for code can be seen as derivatives of their training data. So unless the model is published under the same licence as the training data and includes the copyright notice, this would be a clear violation. Moreover, many licences are not inter-compatible, i.e., the inclusion of code licensed under them automatically warrants an infringement as the combined licence agreements contain irreconcilable conditions. Some opponents,<ref type="foot" target="#foot_7">foot_7</ref> including OpenAI, argue that the use of public code is an instance of transformative fair use, which is a defence that allows the use of copyrighted works in new and unexpected ways and exists in many jurisdictions including the US. 8 Yet, it is still unclear whether the fair use defence applies to ML-systems, <ref type="bibr" target="#b3">4</ref> as it has not yet been tested in court. Furthermore, the fair use argument is sometimes based on the assumption that models do not memorize and emit training data, which is false. Even if the fair use argument protects the use of the data, the verbatim outputting might not be protected.</p><p>A moral argument can also be made on this issue. Training LLMs on copyleft code goes against the will of some opensource developers, who share their code for the betterment of society and who believe in the principle of free and open software so profoundly, they're willing to add a full legal clause to their work to perpetuate this ideal. The use of their work, without attribution, especially by commercial parties, is not what they had in mind.</p><p>Finally, some researchers have also proposed a different approach to this issue by letting the authors of open-source code take matters into their own hands. Using data poisoning techniques, the authors can reduce the performance and embed watermarks into the models [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND RECOMMENDATIONS</head><p>To conclude we recommend the following:</p><p>• The ML community should carefully consider the licence of their training material, from both a legal and an ethical point of view. The authors of published LLMs should be transparent about the licences of their training material.</p><p>• More research should be conducted on the nature and proportionality of text memorization in LLMs for code and LLMs in general. Other topics include memorized text extraction and prevention.</p><p>• Lawmakers should clarify whether the use of copyleft code (and copyrighted materials in general) and text to train LLMs constitutes fair use and under which conditions this clause applies.</p><p>• Finally, the software engineering community should clarify their stance on this issue. Developers could make informed decisions and clearly denote if their source code can be used to train AI models. LLMs for code are likely to stay and bring new tools that change the way software is engineered. So the community needs to answer important questions on this matter. For instance, should open-source code be allowed for training these models? If so, should the developers be credited and compensated, and under which license should the models be released? Alternatively, do we need to revise current code licenses to clarify the community's stance?<ref type="foot" target="#foot_8">foot_8</ref> </p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Does GPT-2 Know Your Phone Number?: http://archive.is/LxsyA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Matrix Transpose: http://archive.is/YU5Bl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Fast Inverse Square: http://archive.is/HNiyg</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>GitHub Complaint (p.26): http://archive.is/3PFAs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>StackOverflow license: http://archive.is/obaoy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Comment on Copilot and OSS: http://archive.is/6gEOU</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>If Software is My Copilot, Who Programmed My Software?: Link</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>GitHub Copilot is not infringing your copyright: http://archive.is/PYlm5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>Additional Reading Material: Link to our GitHub Repository</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extending source code pre-trained language models to summarise decompiled binaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Kaswan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)</title>
		<meeting>the 30th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Codefill: Multitoken code completion by jointly learning from structure and naming sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gismondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gousios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International Conference on Software Engineering (ICSE)</title>
		<meeting>the 44th International Conference on Software Engineering (ICSE)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting training data from large language models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th USENIX Security Symposium (USENIX Security 21)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2633" to="2650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Asleep at the keyboard? assessing the security of github copilot&apos;s code contributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coprotector: Protect open-source code against unauthorized training usage with data poisoning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
