<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
							<email>yangkang@webank.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoqiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijing</forename><surname>Chen</surname></persName>
							<email>weijingchen@webank.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lixin</forename><surname>Fan</surname></persName>
							<email>lixinfan@webank.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<email>qyang@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">WeBank</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14BEB47BAE379AE0F39B2516599E911A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs), such as Chat-GPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient finetuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at <ref type="url" target="https://github.com/FederatedAI/FATE-LLM">https://github.com/FederatedAI/FATE-LLM</ref> to facilitate the research of FedLLM and enable a broad range of industrial applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent few years, the advent of large language models (LLMs) <ref type="bibr">[Yang et al., 2023b;</ref><ref type="bibr" target="#b16">Zhou et al., 2023]</ref> has been reshaping the field of artificial intelligence. In particular, the most advanced LLMs, such as ChatGPT <ref type="bibr" target="#b8">[OpenAI, 2022]</ref>, <ref type="bibr">GPT-4 [OpenAI, 2023]</ref>, and PaLM <ref type="bibr" target="#b2">[Chowdhery et al., 2022]</ref> that boast billions of parameters have gained considerable attention due to their remarkable performance in a variety of natural language generation tasks. Many open-sourced LLMs with high performance have been released, and the public's enthusiasm for research and application of LLMs has been stimulated.</p><p>However, grounding LLMs in real-world applications faces many challenges. The two main challenges are (i) training LLMs consumes vast computing resources, which prevents LLMs from being adopted by small and medium-sized companies with limited computing resources; (ii) training LLMs requires a large amount of public data, which may run out soon <ref type="bibr" target="#b11">[Villalobos et al., 2022]</ref>.</p><p>Federated learning (FL) <ref type="bibr" target="#b7">[McMahan et al., 2017</ref><ref type="bibr" target="#b14">] [Yang et al., 2019]</ref>, a privacy-preserving collaborative machine learning paradigm, is a promising approach to deal with these two challenges. For one thing, FL enables many companies with different computing resources to collaboratively train powerful machine learning models such that the computational burden of training large models can be alleviated. For another, massive high-quality data are scattered among companies that are typically isolated from each other, and FL can exploit these data silos in a privacy-preserving way.</p><p>In this work, we propose FATE-LLM, built upon FATE (Federated AI Technology Enabler) <ref type="bibr">[Liu et al., 2021b]</ref>, to facilitate federated learning for large language models. More specifically, FATE-LLM (1) enables federated learning for both homogeneous and heterogeneous large language models (FedLLM); (2) promotes efficient training of FedLLM through parameter-efficient fine-tuning methods, such as LoRA <ref type="bibr" target="#b3">[Hu et al., 2021]</ref> and P-Tuning-v2 <ref type="bibr">[Liu et al., 2021a]</ref>;</p><p>(3) protects the intellectual property of LLMs using federated intellectual property protection approach <ref type="bibr">[Li et al., 2022]</ref>; (4) protects data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at <ref type="url" target="https://github.com/FederatedAI/FATE-LLM">https://github.com/FederatedAI/  FATE-LLM</ref> to promote the research of FedLLM and enable a broad range of industrial applications.</p><p>arXiv:2310.10049v1 [cs.LG] 16 Oct 2023 2 Related Work</p><p>In this section, we briefly review related work regarding large language models and federated learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Language Models</head><p>The advancements in large language models(LLMs) have led to significant advances in a variety of NLP tasks. A great example of LLMs application is ChatGPT <ref type="bibr" target="#b8">[OpenAI, 2022]</ref>. ChatGPT is fine-tuned from the generative pretrained transformer GPT-3.5, which was trained on a blend of text and code. ChatGPT applies reinforcement learning from human feedback (RLHF), which has become a promising way to align LLMs with a human's intent. LLMs are generally divided into two categories: encoder-decoder or encoder-only large language models and decoder-only large language models <ref type="bibr">[Yang et al., 2023b]</ref>. <ref type="bibr">Bert [Devlin et al., 2018]</ref> is the representative of encoder-only large language models. GPTs <ref type="bibr" target="#b9">[Radford et al., 2018]</ref> is the representative of decoder-only large language models. At the early stage of LLMs development, decoder-only LLMs were not as popular as encoderonly and encoder-decoder LLMs. However, after 2021, with the introduction of GPT-3 <ref type="bibr" target="#b0">[Brown et al., 2020]</ref>, decoder-only LLMs experienced a significant boom. At the same time, after the initial explosion brought about by <ref type="bibr">BERT [Devlin et al., 2018]</ref>, encoder-only LLMs gradually began to fade away. Recently, many decoder-only LLMs have been released, such as LLaMA <ref type="bibr" target="#b11">[Touvron et al., 2023]</ref>, OPT <ref type="bibr">[Zhang et al., 2022a]</ref>, <ref type="bibr" target="#b2">PaLM [Chowdhery et al., 2022], and</ref><ref type="bibr" target="#b9">BLOOM [Scao et al., 2022]</ref>. These LLMs demonstrated reasonable few-/zero-shot performance via prompting and in-context learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Federated Learning</head><p>Federated learning (FL) <ref type="bibr" target="#b7">[McMahan et al., 2017</ref><ref type="bibr" target="#b14">] [Yang et al., 2019;</ref><ref type="bibr" target="#b7">Liu et al., 2022]</ref> is a distributed machine learning paradigm that enables clients (devices or organizations) to train a machine learning model collaboratively without exposing clients' data. Unlike traditional centralized machine learning techniques, data are fixed locally rather than being gathered in a central server, which exists many of the systemic privacy risks and costs <ref type="bibr" target="#b3">[Kairouz et al., 2021]</ref>. Hence, FL is a promising approach to deal with this data isolation challenge. To enhance data privacy, federated learning uses a variety of secure computing protocols. The most popular protocols are Homomorphic Encryption (HE) <ref type="bibr" target="#b9">[Paillier, 1999]</ref>, Multi-Party Computation(MPC) <ref type="bibr" target="#b10">[Shamir, 1979</ref><ref type="bibr" target="#b2">] [Damgård et al., 2012]</ref>, and Differential Privacy (DP) <ref type="bibr" target="#b2">[Dwork et al., 2014]</ref>. In recent years, the literature has presented various algorithms in the FL setting. <ref type="bibr">[Hardy et al., 2017]</ref> proposed vertical logistic regression (VLR) using homomorphic encryption (HE) to protect data privacy. <ref type="bibr" target="#b2">[Chen et al., 2021]</ref> further enhanced the privacy-preserving capability of VLR by employing a hybrid strategy combining HE and secret sharing (SS). <ref type="bibr" target="#b2">[Cheng et al., 2021]</ref> proposed the SecureBoost, a VFL version of XGBoost, that leverages HE to protect the parameters exchanged among parties. <ref type="bibr">[Kang et al., 2022]</ref> applied a semi-supervised learning method to estimate missing features and labels for further training. <ref type="bibr" target="#b7">[McMahan et al., 2017]</ref> proposed Secure Aggregation to enhance data protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FATE-LLM System Design</head><p>We introduce the FATE-LLM system, including its components, architecture, and roadmap.  In FL, clients may have sufficient computing resources to train LLMs of the same size. However, in many heterogeneous scenarios, clients are likely to have quite different computing or data resources so that they can afford to train LLMs of quite different sizes. FATE-LLM offers Federated Homogeneous LLMs (FedHomoLLM) and Federated  Initializing clients with an LLM distilled from a larger one hosted by the server enables federated LLMs to obtain a better global model more efficiently than starting clients' models from random initialization <ref type="bibr" target="#b12">[Wang et al., 2023]</ref>. On the other hand, the domain knowledge captured by clients' local LLMs allows the server's larger LLM to continue to evolve. FATE offers the FedCoLLM (Federated Co-tuning LLM) framework to co-evolve the LLMs of the server and clients. Figure <ref type="figure" target="#fig_4">3</ref>(c) illustrates the FedCoLLM. Specifically, in FedCoLLM, each client having a LLaMa-7B model conducts federated learning applying PEFT techniques. On the server side, the server distills the knowledge between its LLaMa-65B model and the aggregated LLaMa-7B mode to co-evolve models on both sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of FATE-LLM system</head><p>[ <ref type="bibr" target="#b14">Xiao et al., 2023]</ref> proposed Offsite-Tuning, a privacypreserving and efficient transfer learning framework that can adapt an LLM to downstream tasks without access to the LLM's full weights. More specifically, in Offsite-Tuning, the server sends two adaptors and an emulator of its LLM to a client, which in turn finetunes adaptors with the help of the frozen emulator using its domain-specific data. Next, the client sends adaptors back to the server, which then plugs them into its LLM to form an adapted LLM for the client. The Offsite-Tuning has the potential to protect the client's data privacy and the server's model property.</p><p>FATE-LLM offers the FedOST (Federated OffSite-Tuning) that extends the Offsite-Tuning framework to the federated learning setting (see Figure <ref type="figure" target="#fig_4">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture of FATE-LLM</head><p>FATE-LLM is running on the infrastructure of FATE, which consists of FATE-Flow, Eggroll, and OSX as the main components. FATE-Flow is a task scheduling engine for the multi-party federated learning end-to-end pipeline, Eggroll is the distributed computing engine, and OSX (open site exchange) is the multi-party federated communication engine. FATE-LLM Algorithm Hub and LLM Optim Lib Hub are tailored to perform FedLLM. FATE-LLM Algorithm Hub includes Communication-Efficient Hub, FedLLM Model Hub, and FedLLM Privacy Hub (see Figure <ref type="figure" target="#fig_2">2</ref>). LLM Optim Lib Hub includes DeepSpeed and Megatron-LM. As of June 2023, FATE has integrated DeepSpeed into Eggroll, which can manage the GPUs cluster well and dispatch DeepSpeed LLMs tasks. Figure <ref type="figure" target="#fig_7">5</ref> shows the architecture of FATE-LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RoadMap of FATE-LLM</head><p>We present the roadmap of FATE-LLM in Figure <ref type="figure" target="#fig_8">6</ref>. As of June 2023, three versions of FTE-LLM have been released: FATE-LLM 1.0, FATE-LLM 1.1, and FATE-LLM 1.2. The three versions integrate Bert, GPT-2, ChatGLM-6B, and LLaMA, consecutively, and adopt FedIPR and privacypreserving techniques to protect data privacy and model ownership.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the scenario in which each client owns a ChatGLM-6B <ref type="bibr">[Du et al., 2022]</ref> model, and all clients want to fine-tune their models collaboratively through federated learning. Since fine-tuning all parameters of ChatGLM-6B involves huge computational and communication costs, all clients leverage a PETuning method to only fine-tune a small portion of the ChatGLM-6B parameters through federated learning.</p><p>We leverage our FedLLM modules to conduct these experiments using both LoRA <ref type="bibr" target="#b3">[Hu et al., 2021]</ref> and P-Tuning-v2 <ref type="bibr">[Liu et al., 2021a]</ref>. Figure <ref type="figure" target="#fig_9">7</ref> illustrates this scenario we conduct our experiments on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We detail the experimental setup, including the dataset, FL setting, and baselines.</p><p>Dataset and setting. We conduct experiments on Adver-tiseGen <ref type="bibr" target="#b10">[Shao et al., 2019]</ref>, a dataset for advertising text generation. We simulate the FL setting with 2 clients and randomly split the AdvertiseGen dataset such that each client has 57K samples. Each client is assigned 8 NVIDIA V100 and trained on DeepSpeed. We set the FL training epoch to 5 and run the experiments in the LAN network environment.</p><p>Baselines. We adopt two types of baselines. One is centralized, in which data of all clients are centralized to conduct fine-tuning (either LoRA or P-Tuning-v2) on a ChatGLM-6B model. The another is that each client uses local data to finetune its local ChatGLM-6B model.</p><p>Evaluation metrics. We adopt Rouge-1, Rouge-2, Rougel <ref type="bibr" target="#b4">[Lin, 2004]</ref> and <ref type="bibr">BLEU-4 [Papineni et al., 2002]</ref> to evaluate the performance of fine-tined LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Performance</head><p>The experimental results for FedLLM using LoRA and P-Tuning-v2 are reported in Table <ref type="table" target="#tab_1">1</ref> and <ref type="table">Table 2</ref>, respectively, which show that LoRA Federated and P-Tuning-v2 Federated generally outperform their individual client counterparts across all performance metrics, demonstrating that federated learning help enhance the fine-tuning performance for each client. From Table <ref type="table" target="#tab_1">1</ref> and <ref type="table">Table 2</ref>, we also observe that the performance of LoRA and P-Tuning-v2 federated fine-tuning are generally worse than their centralized counterparts across all performance metrics, indicating that there has room to improve federated fine-tuning methods.  Table 2: FedLLM fine-tuning ChatGLM-6B using P-Tuning-v2.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication Cost</head><p>We investigate the communication cost for FedLLM using LoRA and P-Tuning-v2 in terms of the size of parameters to be fine-tuned. Table <ref type="table">3</ref> reports the results, and it shows that FedLLM using LoRA consumes 0.058% communication cost of FedLLM fine-tuning all parameters, while FedLLM using P-Tuning-v2 accounts for 0.475% communication cost of FedLLM fine-tuning all parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Model Size (MB) Param Percent (%) LoRA 3.6 0.058 P-Tuning-v2 29.3 0.475 Fine-tune All 6173 100</p><p>Table <ref type="table">3</ref>: Comparison of communication cost for FedLLM finetuning all parameters of ChatGLM-6B, fine-tuning ChatGLM-6B using LoRA and P-Tuning-v2. Model Size denotes the size of parameters to be fine-tuned. Param Percent denotes the ratio of parameters to be fine-tuned to all parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We proposed FATE-LLM, an industrial-grade federated learning framework for large language models(FedLLM). As an open-sourced software, FATE-LLM encourages collaboration among the research and industry communities and expects to receive increasing feedback on its use.</p><p>In the future, we may consider research directions: (1) reconcile LLMs of different model architectures during FL finetuning; (2) fine-tune private LLMs of one party using private data of another party without compromising the data privacy and model ownership; (3) protect the privacy of user prompts efficiently in the inference stage; (4) apply FedLLM to vertical federated learning <ref type="bibr" target="#b7">[Liu et al., 2022]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Large Language Models are federated on FATE.</figDesc><graphic coords="1,348.30,217.72,176.40,121.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>FATE-LLM 1 was open-sourced as a submodule of FATE, and it contains three components: Communication-Efficient Hub, FedLLM Model Hub, and FedLLM Privacy Hub. Figure 2 overviews the FATE-LLM system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Components of the FATE-LLM system.</figDesc><graphic coords="2,315.00,177.95,246.95,86.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>(a) FedHomoLLM (Federated homogeneous LLMs): Clients have LLMs with the same architecture leverage PEFT to train their LLMs. (b) FedHeteroLLM (Federated Heterogeneous LLMs): Clients have LLMs with different architecture leverage knowledge distillation and PEFT to train their LLMs. (c) FedCoLLM (Federated Co-tuning LLMs): Not only clients but also the server owns LLMs. They leverage PEFT and knowledge distillation to fine-tune their LLMs. (d) FedOST (Federated OffSite-Tuning): Clients transfer their knowledge to the LLM hosted by the server through offsite-tuning in a federated way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FATE-LLM Trainers. FATE-LLM offers four trainers for four different federated LLM learning scenarios. Heterogeneous LLMs (FedHeteroLLM) to support both scenarios. FedHomoLLM leverages PEFT techniques to train clients' LLMs with the same architecture and size (illustrated in Figure 3(a)). FedHeteroLLM leverages knowledge distillation (KD) [Shen et al., 2020] and PEFT techniques to deal with the FL scenario where FL clients own LLMs of different sizes (illustrated in Figure 3(b)). Specifically, each client in FedHeteroLLM leverages KD to learn a mentee model from its local pre-trained LLM. Then, all clients send adaptor or prompt parameters to the server for secure aggregation. Next, the server dispatches the aggregated model to all clients for the next round of training.</figDesc><graphic coords="3,56.49,236.94,241.92,140.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>(d)). In FedOST, multiple clients collaboratively train two global adaptors that adapt the LLM to all clients. FedOST brings two additional benefits than Offsite-Tuning: (1) FedOST enhances data privacy by adopting secure aggregation, and (2) it adapts an LLM to clients that did not even participate in the FL because of the generalization of the FL global model. The FedLLM Privacy Hub integrates various privacy and security protection technologies, including federated intellectual property protection (FedIPR) [Li et al., 2022], secure aggregation (SecureAgg) [McMahan et al., 2017], Differential Privacy (DP) and Multi-Party Computation (MPC) to protect data privacy and model security. Specifically, FedIPR [Li et al., 2022] proposed a federated deep neural network ownership verification scheme that enables private watermarks to be embedded into private DNN models during FL training (see Figure 4) such that each client can independently verify the existence of embedded watermarks and claim its ownership of the federated model without disclosing private training data and watermark information. FedIPR can be applied to FedLLM to verify the IP ownership of the federated LLMs. SecureAgg, DP, and MPC can be applied to FedLLM during training and fine-tuning to protect clients' data privacy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: FedIPR![Li et al., 2022]. Private watermarks are generated and embedded into the trainable parameters (i.e., adaptors or prompts) of local large language models. Then, trainable parameters are aggregated through FedAvg.</figDesc><graphic coords="4,54.54,241.30,241.92,129.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Architecture of the FATE-LLM system.</figDesc><graphic coords="4,315.54,589.90,241.92,92.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: RoadMap of FATE-LLM.</figDesc><graphic coords="5,54.54,242.10,241.91,51.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Multiple clients leverage LoRA or P-Tuning-v2 to finetine their local ChatGLM-6B models through federated learning.</figDesc><graphic coords="5,54.54,333.47,241.92,71.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FedLLM fune-tuning ChatGLM-6B using LoRA.</figDesc><table><row><cell></cell><cell cols="5">Metrics LoRA Federated LoRA Centralized LoRA Client-1 LoRA Client-2</cell></row><row><cell></cell><cell>Rouge-1</cell><cell>32.331</cell><cell>32.384</cell><cell>31.824</cell><cell>31.764</cell></row><row><cell></cell><cell>Rouge-2</cell><cell>7.740</cell><cell>8.150</cell><cell>7.849</cell><cell>7.765</cell></row><row><cell></cell><cell>Rouge-l</cell><cell>25.600</cell><cell>25.830</cell><cell>25.408</cell><cell>25.404</cell></row><row><cell></cell><cell>BLEU-4</cell><cell>8.344</cell><cell>8.730</cell><cell>8.340</cell><cell>8.366</cell></row><row><cell cols="6">Metrics P-Tuning-v2 Federated P-Tuning-v2 Centralized P-Tuning-v2 Client-1 P-Tuning-v2 Client-2</cell></row><row><cell>Rouge-1</cell><cell>32.227</cell><cell></cell><cell>32.184</cell><cell>31.362</cell><cell>31.18</cell></row><row><cell>Rouge-2</cell><cell>7.644</cell><cell></cell><cell>8.048</cell><cell>7.472</cell><cell>7.478</cell></row><row><cell>Rouge-l</cell><cell>25.853</cell><cell></cell><cell>26.010</cell><cell>25.454</cell><cell>25.227</cell></row><row><cell>BLEU-4</cell><cell>8.490</cell><cell></cell><cell>8.851</cell><cell>8.329</cell><cell>8.221</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>FATE-LLM was open-sourced in April</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2023" xml:id="foot_1"><p>in the FATE Community and is running on the infrastructure of FATE.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10162</idno>
		<title level="m">Autofednlp: An efficient fednlp framework</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When homomorphic encryption marries secret sharing: Secure large-scale sparse logistic regression and applications in risk control</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<idno>arXiv:1711.10677</idno>
	</analytic>
	<monogr>
		<title level="m">Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<editor>
			<persName><surname>Cheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cynthia Dwork, Aaron Roth</addrLine></address></meeting>
		<imprint>
			<publisher>Stephen Hardy, Wilko Henecka, Hamish Ivey-Law</publisher>
			<date type="published" when="2012">2021. 2021. 2021. 2022. 2022. 2012. 2018. 2018. 2022. 2014. 2014. 2017. 2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="211" to="407" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Secureboost: A lossless federated learning framework Scaling language modeling with pathways Damgård et al., 2012 Multiparty computation from somewhat homomorphic encryption Annual Cryptology Conference Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining Du et al., 2022 Glm: General language model pretraining with autoregressive blank infilling Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Long Papers) The algorithmic foundations of differential privacy Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
	</analytic>
	<monogr>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021. 2021. 2022. 2022. 2022. 2022</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="210" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Kairouz et al., 2021 Advances and open problems in federated learning IEEE Transactions on Big Data Fedipr: Ownership verification for federated deep neural network models Foundations and Trends® in Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">P-tuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07602</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fate: An industrial grade platform for collaborative learning with data protection</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">226</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12814</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2022. 2022. 2017. 2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
	<note type="report_type">Vertical federated learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2023</date>
		</imprint>
	</monogr>
	<note>OpenAI, 2023] OpenAI. Gpt-4</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Public-key cryptosystems based on composite degree residuosity classes</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Paillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Paillier</surname></persName>
		</author>
		<author>
			<persName><surname>Papineni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999. 1999. 2002. 2002. 2018. 2022</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bleu: a method for automatic evaluation of machine translation International conference on the theory and applications of cryptographic techniques Radford et al., 2018 Improving language understanding by generative pre-training Scao et al., 2022 A 176b-parameter open-access multilingual language model</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long and diverse text generation with planning-based hierarchical variational model</title>
		<author>
			<persName><forename type="first">Adi</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06605</idno>
		<idno>arXiv:2006.16765</idno>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1979">1979. 1979. 2019. 2019. 2020</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="612" to="613" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>How to share a secret Shen et al., 2020 Federated mutual learning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><surname>Touvron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<idno>arXiv:2211.04325</idno>
	</analytic>
	<monogr>
		<title level="m">Will we run out of data? an analysis of the limits of scaling datasets in machine learning</title>
		<imprint>
			<date type="published" when="2022">2023. 2023. 2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>Sewoong Oh, Zheng</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Yongfeng Huang, and Xing Xie. Communication-efficient federated learning via knowledge distillation</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Deployable Generative AI at International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2023. 2022. 2022</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2032</biblScope>
		</imprint>
	</monogr>
	<note>Can public large language models help private cross-device federated learning?</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Offsite-tuning: Transfer learning without full model</title>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04870</idno>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2019">2023. 2023. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Federated learning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10305</idno>
		<idno>arXiv:2304.13712</idno>
		<title level="m">Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond</title>
		<imprint>
			<date type="published" when="2023">2023. 2023. 2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Yang et al., 2023b Open largescale language models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">When federated learning meets pre-trained language models&apos; parameter-efficient tuning methods</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<idno>arXiv:2302.09419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<editor>
			<persName><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lizhen</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zenglin</forename><surname>Qu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Xu</surname></persName>
		</editor>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<publisher>Lifang He</publisher>
			<date type="published" when="2018">2018. 2018. 2022. 2022. 2022. 2022. 2022. 2023. 2023</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Opt: Open pre-trained transformer language models Lq-nets: Learned quantization for highly accurate and compact deep neural networks Reduce communication costs and preserve privacy: Prompt tuning method in federated learning et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
