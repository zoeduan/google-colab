<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OLaLa: Ontology Matching with Large Language Models</title>
				<funder ref="#_qAXbxdC">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
							<idno type="ORCID">0000-0003-4386-8195</idno>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
							<idno type="ORCID">0000-0003-4386-8195</idno>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OLaLa: Ontology Matching with Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">94AC2CCF714B1E4D1DE87BEDC91B4475</idno>
					<idno type="DOI">10.1145/3587259.3627571</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Knowledge representation and reasoning</term>
					<term>• Information systems → Semantic web description languages</term>
					<term>Entity resolution Large Language Model, Ontology Matching, Entity Resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>From the first days of the Semantic Web and Linked Open Data, data integration has played a crucial role. Due to the open Web nature, everybody is able to create their own datasets and concept URIs without relying on a central instance. Thus everyone can create their own URI for the same real-world concept (a.k.a. non-unique name assumption). As a consequence, it is necessary to specify that two different URIs actually represent the same concept. In Ontology or more general Knowledge Graph Matching, the task is to automatically find a set of correspondences between classes, properties, and instances of two different KGs such that the links are only generated if the corresponding concepts are equal.</p><p>In ontologies, the semantics are described with 1) natural language texts (e.g. rdfs:label or rdfs:comment) and 2) relations to other concepts and formal axioms (e.g. taxonomies, domain and range definitions for properties). For a long time, the first was deemed to be only human interpretable, while the second was interpretable by humans and machines alike. Now with the arrival of large language models, this assumption is questionable, since computers are also able to process and interpret textual descriptions.</p><p>Thus, with the rise of transformer-based models <ref type="bibr" target="#b28">[28]</ref>, textual descriptions play an increasingly important role in Ontology Matching systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. However, there are still a lot of disadvantages in using those models. The first one is the need for large training data. Most of the used language models are pre-trained and need a so-called head (usually a simple dense layer at the very end) to be used in a classification setting. This neural network layer is initialized randomly and needs training to differentiate between matches and non-matches. This approach is usually referred to as fine tuning. Another disadvantage is the restricted amount of tokens (words/ pieces of text) that can be processed in such models. Thus the descriptions of concepts need to be short and precise.</p><p>With the development of Large Language Models (LLMs), it is possible to better capture the meaning of a text and also allow to reason about it. One of the most famous models, ChatGPT <ref type="foot" target="#foot_0">1</ref> , was developed by OpenAI and launched on November 30, 2022 to the public. The interface (input and output) is purely based on texts which allows humans to have a chat with the bot. Due to its capabilities, it is applied in closely related fields, such as product matching <ref type="bibr" target="#b19">[20]</ref>.</p><p>There are also disadvantages for ChatGPT when applied to tasks such as KG matching. The most important drawback is that it is not open source, but hidden behind an API. Thus, all achieved results are not reproducible (because OpenAI might change the model behind the API or even shut down a model that is afterwards not available anymore). Furthermore, it is not possible to have full access to the model, and thus no intermediate scores can be retrieved. Moreover, the company providing the closed-source models can charge the user with some cost per query. If the number of queries increases (e.g. with larger ontologies), it is questionable whether the use of ChatGPT is still economically sensible. For those reasons, we will apply only open-source large language models to the task of Ontology Matching.</p><p>Applying LLMs for ontology matching requires a number of design decisions, including (1) the selection of models that actually perform best for this task, (2) how to present the matching task to the system, (3) how to generate candidates, (4) how to translate concepts into natural language text, (5) which prompts to use, and (6) detection of the final answer and extraction of confidences. In this work, we provide a system that allows for systematic experimentation on all those questions. We show how to apply open-source LLMs to the task of ontology matching.</p><p>The contributions of this paper is as follows:</p><p>(1) implementation of different LLM-based matching components in MELT <ref type="bibr" target="#b8">[9]</ref> (2) evaluation of an LLM-based system against in OAEI tracks (3) analysis of the main driving factors for good results We show that with only a handful of examples for few-shot prompting and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems using a much larger portion of the ground truth.</p><p>The paper is structured as follows: We briefly review related work in section 2. We present our approach coined OLaLa in section 3, followed by an evaluation, including an extensive ablation study, in section 4. We conclude the paper with an outlook on future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section is divided into two parts. We first show approaches based on pre-trained language model which are related to the ontology matching task and afterwards we list related work based on large language models (both ChatGPT and open-source LLMs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pretrained Language Models for Ontology Matching</head><p>One of the first systems which applied transformer-based models to ontology matching was DITTO <ref type="bibr" target="#b11">[12]</ref> in 2020. They used BERT <ref type="bibr" target="#b2">[3]</ref>, DistilBERT <ref type="bibr" target="#b23">[23]</ref>, and RoBERTa <ref type="bibr" target="#b14">[15]</ref> to detect if two entities are similar. One difference is that the schema is fixed (meaning that each entity has the same attributes). They overcome the issue of small input sizes by reducing the amount of text with tf-idf weighting. Neutel et al. <ref type="bibr" target="#b16">[17]</ref> provides a system based on BERT but mainly for the automatic alignment of two occupation ontologies. The BERTMap <ref type="bibr" target="#b3">[4]</ref> system evaluates on datasets from the Ontology Alignment Evaluation Initiative (OAEI) <ref type="bibr" target="#b20">[21]</ref>. It includes a fine-tuning of the LMs and finally repairs the mapping in case of inconsistencies. The corresponding candidates are generated by sub-word inverted indices (which only include entity pairs that share many (sub-)words. Our previous approach KERMIT <ref type="bibr" target="#b9">[10]</ref> is also fine-tuned either supervised (based on a fraction of the reference alignment) or unsupervised (based on a high precision matcher). One difference to BERTMap is that the candidates are generated with Sentence-BERT <ref type="bibr" target="#b22">[22]</ref>. This embedding-based retrieval system can also include matching candidates that do not share any tokens (such as synonyms).</p><p>For ontology and KG integration, it is not only important to find equivalence relations between concepts and especially between classes but also other types of relations such as subsumption or meronymy relations. He et al. <ref type="bibr" target="#b5">[6]</ref> thus applied a language model to detect also the type of relation whereas <ref type="bibr" target="#b7">[8]</ref> provides an already fine-tuned model based on various KGs such as DBpedia <ref type="bibr" target="#b0">[1]</ref> and Wikidata. <ref type="bibr" target="#b24">[24]</ref> used BERT models to predict subsumption relations in the e-commerce setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large Language Models for Ontology Matching</head><p>Due to the fact that large language models (LLMs) are relatively new, only a few papers already exist. We first present papers using ChatGPT: Peeters et al. <ref type="bibr" target="#b19">[20]</ref> use the chatbot to check if two product descriptions refer to the same product. <ref type="bibr" target="#b17">[18]</ref> use ChatGPT for ontology alignment by providing the whole source and target ontology to the bot and asking for the final alignment between them. They applied their approach to the conference track of OAEI (the ontologies are rather small) and achieved a high recall but the final F1 score is below the baseline (string equivalence) because of a low precision. For ontology engineering, Mateiu et al. <ref type="bibr" target="#b15">[16]</ref> tuned a GPT-3 model to translate between natural language text and OWL Functional Syntax. Thus it is used mainly to add axioms to an ontology and enrich it. The closest related work is from Wang et al. <ref type="bibr" target="#b29">[29]</ref>. They apply LLaMa 65B <ref type="bibr" target="#b26">[26]</ref>, GPT3.5, and GPT4 to the Biomedical Datasets for Equivalence and Subsumption Matching <ref type="bibr" target="#b4">[5]</ref>. The candidate generation is done by computing top k neighbors in an embedding space generated out of SapBERT <ref type="bibr" target="#b12">[13]</ref> (a pre-trained BERT model designed for the biomedical domain). It is shown that especially GPT4 can outperform the state-of-the-art by a large margin. Pan et al. <ref type="bibr" target="#b18">[19]</ref> provide an overview of how LLMs can be used for Knowledge Graphs in general. Section 4.1.1 discusses the application of entity resolution and matching and section 4.3.3 ontology alignment.</p><p>Most of the presented approaches use closed-source LLMs. This means that the results might not be reproducible after OpenAI discontinues some models or changes the models behind the API. Thus we focus in this work on open-source models and present the system OLaLa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of the architecture of the OLaLa system. All components are implemented in MELT <ref type="bibr" target="#b8">[9]</ref>, a framework for matcher development and evaluation. MELT is also used by the OAEI to package, submit, and evaluate the systems. Thus, it is possible for the ontology matching community to reuse and customize each component in their own matching pipeline. The implementation of OLaLa is publicly available, and we provide a command line application <ref type="foot" target="#foot_1">2</ref> which allows to run the system and modify the most important parameters.</p><p>At the beginning, matching candidates need to be extracted from the two given input ontologies O1 and O2. Afterwards, those candidates are included in the user-defined prompt and presented to the LLM. Two options are possible: 1) each correspondence is analyzed independently of each other 2) given a source entity, all possible target entities are presented and the LLM needs to decide which one is correct (or none of them). The output of the high-precision matcher is added to ensure that the simple matches are included as well. Finally, some filters are applied to fulfill the usual requirements for an alignment such as a 1:1 mapping (cardinality filter). The confidence filter at the end ensures that only correspondences with reasonably high confidence are returned. In the following sections, we will describe each step in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Generation</head><p>Due to the fact that the LLMs can usually not analyze the input ontologies as a whole (except small ontologies like those in the OAEI conference track, see <ref type="bibr" target="#b17">[18]</ref>), some correspondence candidates need to be generated. In this stage, only the recall is relevant and the higher the recall the better. Some of the related approaches apply an inverted index to find possible similar entities. This requires some textual overlap of those concepts. In OLaLa, the well-known Sentence BERT models (SBERT) are used to generate those candidates. This allows a higher recall because it can also find similar entities without any textual overlap. The trained SBERT models are finetuned siamese BERT models on a huge set of paraphrases <ref type="bibr" target="#b22">[22]</ref>. SBERT as well as all LLMs only process text, but the input is an ontology. Thus it is necessary to verbalize the concepts into some natural language text. In MELT they are called TextExtractors (see section 3.3).</p><p>For the candidate generation step, we use the so-called Text-ExtractorSet. It extracts all texts of a resource which are either labels (e.g. rdfs:label, skos:prefLabel, schema:name) or descriptions (e.g. rdfs:comment, dc:description, schema:comment). In addition to that, the URI fragment is extracted in case it does not contain more than 50% digits. As a last step, all annotation properties are followed recursively and all labels of those resources are added as well.</p><p>All those extracted texts for each resource are embedded, and a semantic search is executed. It computes the cosine similarity between a list of query embeddings and a list of corpus embeddings and returns the top-k neighbors for each text. From those, we select the top-k best neighbors per resource. This procedure is repeated twice so that each of the input ontologies serves once as a query and one as a corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LLM Application</head><p>There are two principal approaches how the candidates are presented to the LLM. The first one is binary decisions, i.e., deciding whether one candidate is correct or not; the second is multiple choice decisions, i.e., selecting the most likely correspondence for one concept from a set of possible targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Binary Decisions.</head><p>Binary decisions are implemented in the class LLMBinaryFilter. For each candidate correspondence, the source and target entity are verbalized as text and replaced in the prompt given by the user. The output of generative models, such as the ones applied in this work, is always natural language text. To convert this into a binary decision, the following technique is applied: We search for target tokens/words that indicate the result (e.g. true/yes or false/no). If such a token is found, the generation process is directly stopped. Due to the high computation cost, such an early stopping approach is useful to process a large number of candidates. Up to now, only the decision is extracted and in case the model generates other texts like "This is a correct match", we fail at detection.</p><p>To overcome this issue and also extract a specific confidence, we do the following. If any of the target tokens is detected, then we retrieve the scores of the complete vocabulary and apply the softmax function to it. This corresponds to the probability that the word is generated at this position. We check the probability for all words in the positive class (e.g. yes, true) and take the maximum value which is normalized by the maximum value of the negative class (e.g. 0.4 0.4+0.1 = 0.8 where 0.4 corresponds to the probability of one token in the positive class like yes and 0.1 corresponds to the maximum negative class tokens probability). Thereby, we get a confidence between zero and one, and every confidence above 0.5 is a predicted positive token.</p><p>In case no positive or negative token is generated, the probabilities at the first generated token are used. All those computations would not be possible with a model accessed by an API such as ChatGPT. <ref type="foot" target="#foot_2">3</ref>The default generation strategy <ref type="foot" target="#foot_3">4</ref> is greedy such that each token with the highest probability is chosen and the generation process is continued with this text. The implementation also allows to switch to e.g. contrastive search <ref type="bibr" target="#b25">[25]</ref> but due to the usual short answers, it is neither necessary nor helpful.</p><p>3.2.2 Multiple Choice Decisions. Multiple choice decisions are implemented in the class LLMChooseGivenEntityFilter. It provides the LLM with more context such that for a given source entity all possible target entities with identifying letters are also shown. The task is to pick the one that represents the same entity or to generate a default answer such as "none". Confidences are extracted in the same ways as before. The normalization is applied to all possible outcomes including "none". There is also the possibility to use it directly for filtering such that the one with the highest confidence is kept and all others are removed. In case of a "none" prediction, all correspondences are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TextExtractors / Verbalizers</head><p>In all the above cases, the extracted/verbalized texts for a given resource should be only one text and not multiple texts as for the candidate generation step. Thus some of the possible extractors are now explained.</p><p>In addition to combining all texts from the TextExtractorSet explained before, an even simpler extractor called TextExtractor-OnlyLabels is implemented. It extracts only one textual label which can originate from the following properties(in decreasing importance): skos:prefLabel, rdfs:label, URI fragment, skos:alt-Label, skos:hiddenLabel. This means if a skos:prefLabel is detected, only this label is used.</p><p>Including more context in those examples is achieved by the TextExtractorVerbalizedRDF. It selects all RDF triples from the corresponding KG where the resource is in the subject position. Those triples are verbalized -meaning that each subject, predicate, and object is replaced by the text of OnlyLabels extractor. All triples with a label-like property are skipped because the information is already included. As an example, the statement":MA_0000002 rdfs:subClassOf :MA_0001112" is converted to "spinal cord grey matter sub class of grey matter".</p><p>As a variation of the previous extractor, it is also tried out to provide the triples directly as serialized RDF. The default of the ResourceDescriptionInRDF extractor is to serialize to turtle format where the prefixes are used but the prefix definition is excluded from the generated text to make it shorter (other serializations can also be configured). If there are resources in the object position of the triples, they will be also replaced by a literal containing the corresponding label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">High-Precision Matcher</head><p>The high-precision matcher is a simple matcher in MELT that efficiently searches for concepts with the exact same normalized label (or URI fragment if a label is not available). <ref type="foot" target="#foot_4">5</ref> The normalization includes lowercasing, camel case, and deletion of non alpha-numeric characters. If there is only one such candidate for a concept, then it is matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Postprocessing</head><p>After the application of the LLM, the resulting alignment is further post-processed by filters. To keep the matching pipeline simple, only two additional filters are applied. The cardinality filter ensures a one-to-one mapping which is usually required. To solve the assignment problem, it is reduced to the maximum weight matching in a bipartite graph <ref type="bibr" target="#b1">[2]</ref> (class MaxWeightBipartiteExtractor in MELT).</p><p>To further improve the alignment and remove correspondences that are likely to be incorrect, the confidence filter is applied. All correspondences that do not have a higher or the same confidence as a predefined threshold value are excluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We evaluate our approach on the anatomy, biodiv, and commonkg tracks of OAEI <ref type="foot" target="#foot_5">6</ref> . Moreover, we show results on the Knowledge Graph track <ref type="bibr" target="#b6">[7]</ref>, where only class correspondences are considered. For all tracks, we compare OLaLa against the three best-performing systems in the different OEAI tracks in the 2022 edition of the OAEI <ref type="bibr" target="#b20">[21]</ref>. The evaluation was performed using the MELT framework on a server running RedHat with 256 GB of RAM, 2x64 CPU cores (2.6 GHz), and 4 Nvidia A100 (40GB) graphics cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Final Configuration</head><p>For the final configuration, a lot of parameters need to be fixed. The SBERT model for the candidate generation step is set to multi-qampnet-base-dot-v1, <ref type="foot" target="#foot_6">7</ref> and the value k during the top-k neighbors search is set to five. This gives a balance between the number of generated correspondences as well as the achieved recall. The Text-ExtractorSet is used to generate multiple text representations of the resource to run the search in the embedding space.</p><p>The LLM model is set to upstage/Llama-2-70b-instruct-v2<ref type="foot" target="#foot_7">foot_7</ref> and to generate the text in prompt 7 (see table <ref type="table">6</ref>), i.e., a few-shot prompt with three positive and negative examples each <ref type="foot" target="#foot_8">9</ref> , Text-ExtractorOnlyLabels is used. With this prompt, the binary decision approach is automatically selected. For the text generation, the maximum number of tokens (max_new_tokens <ref type="foot" target="#foot_9">10</ref> ) is set to 10 but this number of tokens is usually not reached because a positive or negative word is detected before. The next parameter which is fixed is the temperature. The lower the value, the more deterministic the results are (the token with the highest probability is chosen as the predicted token). With increased temperature, the outputs are more randomized (resulting in more creative texts). We set the temperature to zero such that the results are reproducible. Other generation parameters are set to their default values. The cardinality filter does not require any parameters, and the value of the confidence filter is set to 0.5. With this setting, we filter out all correspondences where the LLM predicts a negative word (such as "no" or "false"). Thus we do not need to tune the confidence value and do not require any training alignment for it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the overall results of OLaLa across the different tracks in the configuration above. Although it might be possible to tweak the parameters per track to achieve better results, we use only one configuration across all tracks in order to show a fair comparison. We can see that in many test cases, OLaLa scores among the top 3 systems, delivering good results with an out-of-the-box setup. It is worth mentioning that the other approaches often use domainspecific knowledge (especially in the biomedical domain) and/or extensively utilize the structure of the ontologies, while OLaLa solely relies on the textual descriptions of entities. <ref type="foot" target="#foot_10">11</ref>At the same time, it can be observed that the runtimes utilizing LLMs are very often much higher than those for other models. This can be observed in particular in the Biodiv track, where the runtime of OLaLa is often a few hours, compared to other systems which can solve the respective tasks in under a minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we investigate the impact of the different parts and parameters of the system on the final result. Due to the fact that all combinations on all tracks would drastically increase the number of experiments, we restrict ourselves to the anatomy track and only modify one component while keeping the rest of the system stable to the final configuration introduced in section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Candidate generation.</head><p>In this stage, the SBERT model and corresponding k value for neighbor search need to be selected. The available pretrained models are already evaluated on 14 datasets which checks the performance of the sentence embeddings as well as on six datasets for the performance of semantic search <ref type="foot" target="#foot_11">12</ref> . The best three models of each evaluation are selected to be tested on the anatomy track. All models are publicly available via the huggingface model hub. Table <ref type="table" target="#tab_1">2</ref> shows the results grouped by the value k. On the one hand, with increasing k, the number of generated candidates gets also much higher and results in a large runtime of the following LLM model. On the other hand, all correspondences which are not found in this stage cannot be part of the final result. Thus, only the recall value and alignment size are important at this step. The results correlate with the performance on the semantic search datasets which is why the multi-qa-mpnet-base-dot-v1 is selected (the top performing system on those 6 datasets). The parameter k is set to five because recall could be increased by 1.2 (from k=3 to k=5), whereas changing from k=5 to k=10 only increases the recall marginally, but nearly doubles the amount of marginally candidates.</p><p>4.3.2 LLM Model. Table <ref type="table" target="#tab_2">3</ref> shows the performance achieved with different LLM models. The selection of the analyzed models is done with the help of the huggingface LLM leaderboard <ref type="foot" target="#foot_12">13</ref> . Many of those models are based on LLama2 <ref type="bibr" target="#b27">[27]</ref> and fine-tuned on a specialized dataset. As of 01/09/2023, model jondurbin/airoboros-l2-70b-2.1 is the leading system whereas upstage/Llama-2-70binstruct-v2 is a general model which was also the leader of the board at the time of release. It can be observed that the F-measure increases with the model size except for the chat variant of LLama2. The reason might be that prompt 7 is more designed for completion than a chat. Model upstage/Llama-2-70b-instruct-v2 is selected due to a high Fmeasure as well as a low runtime.</p><p>For all models, the following parameters for loading the models are used: device_map is set to " auto", torch_dtype is set to "float16", and load_in_8bit is set to "true". With those settings, the memory footprint of the models is reduced such that the 7B and 13B variants fit on one A100 (40GB) GPU and the 70B variants on 2 GPUs of the same type.   <ref type="table" target="#tab_3">4</ref> shows the results if the text extractor is modified. The OnlyLabel extractor is the worst in terms of F-Measure but it is also the fastest one (due to the small size of the input that needs to be processed). It is nice to see that the LLM can easily deal with RDF serializations (as produced by Description-InRDF extractor) and achieve an even higher F-Measure than SEB-Matcher and close to Matcha. For the final configuration, the Only-Label extractor is used to decrease the runtime even though other extractors could improve the final results. The few-shot prompts also contain verbalizations of concepts. Those are created according to the selected text extractor. We also tested to keep the original prompt but achieved better results by using the same text extractor for example creation and testing. <ref type="table">6</ref> shows the prompts used. Prompts 0-4 are zero-shot, meaning that no examples were provided. Prompt one tests if additional context information (e.g. what are the topics of the ontologies) improves the results. Prompts 2, 3, and 4 further try to guide the model to answer with yes/no. Prompt 5 uses one positive and one negative correspondence whereas prompt 6 uses three positives and three negatives. With those added examples it is possible to reach the best precision but the overall best F-Measure is achieved by adding a description of the task at the very beginning (prompt 7). However, it is remarkable that the second best results are achieved with a simple zero-shot prompt (prompt 0). Prompts 8 and 9 are multiple-choice decisions, which are observed to be inferior to single decision ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Prompts. Table</head><p>The runtimes vary drastically. The main reason is that for some prompts the target tokens (like yes/no etc.) are generated very late or not at all. In such cases, the text completion takes rather long (even though the maximum number of new tokens is set to 10). <ref type="bibr">Overall 22,</ref><ref type="bibr">288</ref> examples are classified whereas the multiple choice decisions only need to predict 6,035 examples. Multiple choice prompts can reduce the runtimes, but achieve less good results. 4.3.5 Postprocessing. In this section, the influence of the postprocessing is analyzed. Table <ref type="table" target="#tab_4">5</ref> shows the results when only the candidate generation step is executed and when each filter is additionally added. Without the LLM model, we achieve an F-Measure of 0.497 when the full filter chain is applied.</p><p>When using the LLM and the cardinality filter, the F-Measure is already increased to 0.719. Still, there are a lot of incorrect correspondences even though one entity is only mapped to a maximum of one other entity. Thus, the confidence filter is applied which lifts the F-Measure to 0.9. Adding the results of the high-precision matcher provides a slight increase in both precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND OUTLOOK</head><p>In this paper, we presented OLaLa, an ontology matching system that is built on top of open-source large language models. We have shown that using such a model, especially in a few-shot setting, can yield competitive results, even if only based on textual descriptions.</p><p>In our ablation study, we have observed that model and parameter combinations can have a strong impact on the overall results, and it is likely that there is no one-parameterization-fits-all solution, i.e., different parameter sets might deliver optimal results for different matching problems. Therefore, we plan to more closely examine the automatic parameterization of our system.</p><p>OLaLa provides an experimentation base for different variations, such as new prompts (prompt engineering), and also prompting techniques, like generating knowledge in the form of text that is used as additional information during classification <ref type="bibr" target="#b13">[14]</ref> or Chainof-Thought prompting <ref type="bibr" target="#b10">[11]</ref> that also allows to generate an explanation why two concepts are the same. In early experiments, we have observed that generating additional explanations for all candidates results in large runtimes (for anatomy, the expected runtime exceeds four days) but it could be useful to generate explanations for the final alignment which contains way less correspondences, or creating explanations on demand.</p><p>As already shown, the text extractors make a huge difference in terms of F-Measure. The RDF serialization works best but also generates a lot of tokens which could be reduced by selecting important properties to be included. Finally, the system should be more scalable such that it can also be applied to large KGs with instance matching (which is technically possible, but with large runtimes). This could be achieved, e.g., by using a fast high-precision matcher to first find easy matches, and applying the LLM model only to edge cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the OLaLa system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall Results of the default configuration of OLaLa, compared to the respective three best systems in different OAEI test cases</figDesc><table><row><cell>Test case</cell><cell>System</cell><cell>Prec</cell><cell>Rec</cell><cell>𝐹 1</cell><cell cols="2">Size Time</cell></row><row><cell></cell><cell></cell><cell>Anatomy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Matcha</cell><cell cols="5">0.951 0.930 0.941 1482 0:00:37</cell></row><row><cell></cell><cell>SEBMatcher</cell><cell cols="5">0.945 0.874 0.908 1402 9:53:22</cell></row><row><cell>mouse-human</cell><cell>OLaLa</cell><cell cols="5">0.914 0.891 0.902 1478 2:41:23</cell></row><row><cell></cell><cell>LogMapBio</cell><cell cols="5">0.873 0.919 0.895 1596 0:19:43</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">0.997 0.622 0.766 946</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Common KG</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>OLaLa</cell><cell cols="4">1.000 0.922 0.960 120</cell><cell>0:06:34</cell></row><row><cell></cell><cell>KGMatcher+</cell><cell cols="4">1.000 0.910 0.950 117</cell><cell>2:43:50</cell></row><row><cell>nell-dbpedia</cell><cell>Matcha</cell><cell cols="4">1.000 0.910 0.900 104</cell><cell>0:01:00</cell></row><row><cell></cell><cell>ATMatcher</cell><cell cols="4">1.000 0.800 0.890 104</cell><cell>0:03:10</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">1.000 0.600 0.750 78</cell><cell>0:00:37</cell></row><row><cell></cell><cell cols="4">Knowledge Graph (only class matches)</cell><cell></cell></row><row><cell></cell><cell>OLaLa</cell><cell cols="4">1.000 1.000 1.000 11</cell><cell>0:17:40</cell></row><row><cell>marvel-</cell><cell>ATMatcher</cell><cell cols="4">1.000 1.000 1.000 11</cell><cell>0:04:36</cell></row><row><cell>cinematic-</cell><cell>LogMap</cell><cell cols="4">1.000 1.000 1.000 10</cell><cell>0:32:40</cell></row><row><cell>marvel</cell><cell>LSMatch</cell><cell cols="4">1.000 1.000 1.000 8</cell><cell>1:46:01</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">1.000 0.600 0.750 8</cell><cell>0:02:40</cell></row><row><cell></cell><cell>ATMatcher</cell><cell cols="4">0.830 0.710 0.770 39</cell><cell>0:03:23</cell></row><row><cell>memoryalpha-memorybeta</cell><cell>LogMap OLaLa LSMatch</cell><cell cols="4">0.880 0.500 0.640 21 1.000 0.350 0.530 24 1.000 0.290 0.440 26</cell><cell>0:05:09 0:35:03 0:57:37</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">1.000 0.290 0.440 19</cell><cell>0:01:50</cell></row><row><cell></cell><cell>ATMatcher</cell><cell cols="4">1.000 0.770 0.870 34</cell><cell>0:02:04</cell></row><row><cell>memoryalpha-stexpanded</cell><cell>OLaLa KGMatcher LSMatch</cell><cell cols="4">1.000 0.540 0.700 28 1.000 0.540 0.700 29 1.000 0.540 0.700 25</cell><cell>0:29:41 0:25:42 0:20:38</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">1.000 0.460 0.630 19</cell><cell>0:01:11</cell></row><row><cell></cell><cell>LogMap</cell><cell cols="4">1.000 0.800 0.890 12</cell><cell>0:07:44</cell></row><row><cell>starwars-swg</cell><cell>OLaLa ATMatcher LSMatch</cell><cell cols="4">1.000 0.600 0.750 13 1.000 0.600 0.750 13 1.000 0.600 0.750 19</cell><cell>0:38:49 0:04:24 0:38:50</cell></row><row><cell></cell><cell>String Baseline</cell><cell cols="4">1.000 0.400 0.570 9</cell><cell>0:02:52</cell></row><row><cell></cell><cell>ATMatcher</cell><cell cols="4">1.000 0.870 0.930 31</cell><cell>0:04:20</cell></row><row><cell>starwars-swtor</cell><cell>KGMatcher String Baseline OLaLa</cell><cell cols="4">1.000 0.870 0.930 30 1.000 0.800 0.890 27 0.920 0.800 0.860 30</cell><cell>0:43:57 0:02:51 0:45:47</cell></row><row><cell></cell><cell>LogMap</cell><cell cols="4">1.000 0.730 0.850 28</cell><cell>0:07:10</cell></row><row><cell></cell><cell></cell><cell>Biodiv</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>LogMap</cell><cell cols="4">0.781 0.656 0.713 676</cell><cell>0:00:25</cell></row><row><cell>envo-</cell><cell>LogMapBio</cell><cell cols="4">0.753 0.652 0.699 697</cell><cell>1:00:03</cell></row><row><cell>sweet</cell><cell>LogMapLt</cell><cell cols="4">0.829 0.594 0.692 576</cell><cell>0:07:32</cell></row><row><cell></cell><cell>OLaLa</cell><cell cols="5">0.431 0.613 0.510 1145 6:55:19</cell></row><row><cell></cell><cell>AML (2021)</cell><cell cols="4">0.976 0.764 0.839 359</cell><cell>0:00:21</cell></row><row><cell>gemet-</cell><cell cols="5">ATMatcher (2021) 0.631 0.919 0.748 486</cell><cell>0:00:08</cell></row><row><cell>anaee</cell><cell>OLaLa</cell><cell cols="4">0.565 0.916 0.699 542</cell><cell>4:28:07</cell></row><row><cell></cell><cell>LogMapLt</cell><cell cols="4">0.840 0.458 0.593 182</cell><cell>0:00:03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of zero-shot bi-encoders (SBERT models) on the anatomy track. The best recall per 𝑘 is highlighted with bold print. Time is measured in seconds.</figDesc><table><row><cell cols="2">k Model</cell><cell>Prec</cell><cell>Rec</cell><cell>𝐹 1</cell><cell>Size Time</cell></row><row><cell></cell><cell cols="5">multi-qa-mpnet-base-dot-v1 0.034 0.985 0.066 43,786</cell><cell>8</cell></row><row><cell></cell><cell>all-mpnet-base-v2</cell><cell cols="4">0.034 0.983 0.066 43,625</cell><cell>8</cell></row><row><cell>10</cell><cell>multi-qa-distilbert-cos-v1</cell><cell cols="4">0.035 0.985 0.067 43,071</cell><cell>8</cell></row><row><cell></cell><cell>all-distilroberta-v1</cell><cell cols="4">0.034 0.981 0.066 43,567</cell><cell>8</cell></row><row><cell></cell><cell>all-MiniLM-L12-v2</cell><cell cols="4">0.034 0.983 0.066 43,399</cell><cell>8</cell></row><row><cell></cell><cell cols="5">multi-qa-mpnet-base-dot-v1 0.066 0.978 0.124 22,338</cell><cell>8</cell></row><row><cell></cell><cell>all-mpnet-base-v2</cell><cell cols="4">0.066 0.972 0.124 22,204</cell><cell>8</cell></row><row><cell>5</cell><cell>multi-qa-distilbert-cos-v1</cell><cell cols="4">0.067 0.973 0.125 22,025</cell><cell>7</cell></row><row><cell></cell><cell>all-distilroberta-v1</cell><cell cols="4">0.066 0.968 0.123 22,366</cell><cell>6</cell></row><row><cell></cell><cell>all-MiniLM-L12-v2</cell><cell cols="4">0.066 0.974 0.124 22,229</cell><cell>6</cell></row><row><cell></cell><cell cols="5">multi-qa-mpnet-base-dot-v1 0.107 0.964 0.193 13,649</cell><cell>8</cell></row><row><cell></cell><cell>all-mpnet-base-v2</cell><cell cols="4">0.108 0.966 0.194 13,543</cell><cell>7</cell></row><row><cell>3</cell><cell>multi-qa-distilbert-cos-v1</cell><cell cols="4">0.108 0.963 0.194 13,553</cell><cell>6</cell></row><row><cell></cell><cell>all-distilroberta-v1</cell><cell cols="4">0.106 0.958 0.191 13,696</cell><cell>7</cell></row><row><cell></cell><cell>all-MiniLM-L12-v2</cell><cell cols="4">0.107 0.964 0.193 13,611</cell><cell>7</cell></row><row><cell></cell><cell cols="4">multi-qa-mpnet-base-dot-v1 0.306 0.931 0.461</cell><cell>4,612</cell><cell>8</cell></row><row><cell></cell><cell>all-mpnet-base-v2</cell><cell cols="3">0.307 0.935 0.463</cell><cell>4,615</cell><cell>14</cell></row><row><cell>1</cell><cell>multi-qa-distilbert-cos-v1</cell><cell cols="3">0.307 0.935 0.463</cell><cell>4,620</cell><cell>6</cell></row><row><cell></cell><cell>all-distilroberta-v1</cell><cell cols="3">0.292 0.904 0.442</cell><cell>4,692</cell><cell>6</cell></row><row><cell></cell><cell>all-MiniLM-L12-v2</cell><cell cols="3">0.306 0.933 0.461</cell><cell>4,620</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance impact of using different LLM models on the anatomy track.</figDesc><table><row><cell>Model</cell><cell>Prec</cell><cell>Rec</cell><cell>𝐹 1</cell><cell>Size</cell><cell>Time</cell></row><row><cell>meta-llama/ Llama-2-7b-hf</cell><cell cols="5">0.932 0.640 0.759 1,041 7:50:33</cell></row><row><cell>meta-llama/ Llama-2-13b-hf</cell><cell cols="5">0.806 0.820 0.813 1,543 1:35:15</cell></row><row><cell>meta-llama/ Llama-2-70b-hf</cell><cell cols="5">0.946 0.860 0.901 1,378 6:45:13</cell></row><row><cell>meta-llama/ Llama-2-70b-chat-hf</cell><cell cols="5">0.663 0.801 0.725 1,832 3:55:57</cell></row><row><cell>jondurbin/ airoboros-l2-70b-2.1</cell><cell cols="5">0.804 0.877 0.839 1,654 4:00:12</cell></row><row><cell>upstage/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Llama-2-70b-</cell><cell cols="5">0.914 0.891 0.902 1,479 2:40:18</cell></row><row><cell>instruct-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance impact of using different text extraction strategies on the anatomy track.</figDesc><table><row><cell>Text Extractor</cell><cell>Prec</cell><cell>Rec</cell><cell>𝐹 1</cell><cell>Size Time</cell></row><row><cell>OnlyLabels</cell><cell cols="4">0.914 0.891 0.902 1478 2:41:23</cell></row><row><cell>VerbalizedRDF</cell><cell cols="4">0.929 0.884 0.906 1443 3:57:46</cell></row><row><cell cols="5">DescriptionInRDF 0.943 0.915 0.929 1471 9:02:24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Impact of the LLM and the different post processing pipelines on the anatomy track. HP represents the highprecision matcher.</figDesc><table><row><cell>Postprocessing</cell><cell>Prec</cell><cell>Rec</cell><cell>𝐹 1</cell><cell>Size</cell><cell>Time</cell></row><row><cell>Candidates</cell><cell cols="5">0.066 0.978 0.125 22,289 0:00:37</cell></row><row><cell>+ Cardinality</cell><cell cols="4">0.385 0.693 0.495 2,731</cell><cell>0:00:37</cell></row><row><cell>+ Confidence</cell><cell cols="4">0.387 0.693 0.497 2,715</cell><cell>0:00:37</cell></row><row><cell>+ LLM + Cardinality</cell><cell cols="4">0.591 0.919 0.719 2,357</cell><cell>2:37:51</cell></row><row><cell>+ Confidence</cell><cell cols="4">0.911 0.889 0.900 1,480</cell><cell>2:37:51</cell></row><row><cell cols="5">+ LLM + HP + Cardinality 0.593 0.921 0.721 2,356</cell><cell>2:37:51</cell></row><row><cell>+ Confidence</cell><cell cols="4">0.914 0.891 0.902 1,478</cell><cell>2:37:51</cell></row></table><note><p>4.3.3 Text Extractors. Table</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://chat.openai.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/dwslab/melt/tree/master/examples/llm-transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We also explored prompt engineering as an alternative to get to confidences, using prompts such as "and also provide a confidence score with your answer", but we observed that the LLM will often respond that it is not able to provide a specific confidence value, and even if it does, it is not easy to extract it out of the generated text. Therefore, we discarded that idea again.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/docs/transformers/main/en/generation_strategies</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://dwslab.github.io/melt/matcher-components/full-matcher-list</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://oaei.ontologymatching.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://huggingface.co/upstage/Llama-2-70b-instruct-v2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>The positive and negative examples are taken from the anatomy track and used across all tracks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://huggingface.co/docs/transformers/main/en/main_classes/text_generation# transformers.GenerationConfig</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>For reasons of completeness, we should mention that we use three examples from the anatomy track for our few-shot prompt. Thus, one could argue that there is minimal information leakage for the anatomy track. However, given the alignment size, we consider this neglectable. Moreover, we could have used examples from other tracks for anatomy, but we wanted to keep the prompt constant across all tracks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://www.sbert.net/docs/pretrained_models.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors acknowledge support by the state of <rs type="person">Baden-Württemberg</rs> through bwHPC and the <rs type="funder">German Research Foundation (DFG)</rs> through grant <rs type="grantNumber">INST 35/1597-1 FUGG</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qAXbxdC">
					<idno type="grant-number">INST 35/1597-1 FUGG</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international semantic web conference</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient Selection of Mappings and Automatic Quality-Driven Combination of Matching Methods</title>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">F</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Palandri Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Stroe</surname></persName>
		</author>
		<idno>OM&apos;09). CEUR-WS.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Ontology Matching -Volume</title>
		<meeting>the 4th International Conference on Ontology Matching -Volume<address><addrLine>Chantilly; Aachen, DEU</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">551</biblScope>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/V1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERTMap: a BERT-based ontology alignment system</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denvar</forename><surname>Antonyrajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Palo Alto, California USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5684" to="5691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><surname>Jiménez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2022</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="575" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language Model Analysis for Ontology Subsumption Inference</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><surname>Jimenez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.213</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.findings-acl.213" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3439" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Knowledge Graph Track at OAEI -Gold Standards, Baselines, and the Golden Hammer Bias</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-49461-2_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-49461-2_20" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -17th International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece; Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-31">2020. May 31-June 4, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="343" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer Based Semantic Relation Typing for Knowledge Graph Integration</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-33455-9_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-33455-9_7" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -20th International Conference, ESWC 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Hersonissos, Crete, Greece; Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023-05-28">2023. May 28 -June 1, 2023</date>
			<biblScope unit="volume">13870</biblScope>
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MELT -Matching EvaLuation Toolkit</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Portisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Systems. The Power of AI and Knowledge Graphs</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="231" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">KERMIT -A Transformer-Based Approach for Knowledge Graph Matching</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Portisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.13931</idno>
		<idno type="arXiv">arXiv:2204.13931</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2204.13931" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22199" to="22213" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Entity Matching with Pre-Trained Language Models</title>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.14778/3421424.3421431</idno>
		<ptr target="https://doi.org/10.14778/3421424.3421431" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-Alignment Pretraining for Biomedical Entity Representations</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2021.NAACL-MAIN.334</idno>
		<ptr target="https://doi.org/10.18653/V1/2021.NAACL-MAIN.334" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="4228" to="4238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generated Knowledge Prompting for Commonsense Reasoning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.225</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-long.225" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics ACL. ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3154" to="3169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ontology engineering with Large Language Models</title>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Mateiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Groza</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.16699</idno>
		<idno type="arXiv">ARXIV.2307.16699arXiv:2307.16699</idno>
		<ptr target="https://doi.org/10.48550/" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Automatic Ontology Alignment using BERT</title>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Neutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Maaike</surname></persName>
		</author>
		<author>
			<persName><surname>De Boer</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-2846/paper28.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2021 Spring Symposium on Combining Machine Learning and Knowledge Engineering (AAAI-MAKE 2021)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the AAAI 2021 Spring Symposium on Combining Machine Learning and Knowledge Engineering (AAAI-MAKE 2021)<address><addrLine>Palo Alto, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Stanford University</publisher>
			<date type="published" when="2021-03-22">2021. March 22-24, 2021</date>
			<biblScope unit="volume">2846</biblScope>
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conversational Ontology Alignment with ChatGPT</title>
		<author>
			<persName><forename type="first">Saki</forename><surname>Sanaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Saeid Mahdavinejad</surname></persName>
		</author>
		<author>
			<persName><surname>Hitzler</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2308.09217</idno>
		<idno type="arXiv">arXiv:2308.09217</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2308.09217" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large Language Models and Knowledge Graphs: Opportunities and Challenges</title>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Dietze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajira</forename><surname>Jabeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janna</forename><surname>Omeliyanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Lissandrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russa</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Bonifati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edlira</forename><surname>Vakaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Dragoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Graux</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2308.06374</idno>
		<idno type="arXiv">arXiv:2308.06374</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2308.06374" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03423</idno>
		<title level="m">Using ChatGPT for Entity Matching</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mina</forename><surname>Abd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikooie</forename><surname>Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alsayed</forename><surname>Algergawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Buche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyla</forename><forename type="middle">Jael</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omaima</forename><surname>Fallatah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irini</forename><surname>Fundulaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Hertling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Huschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liliana</forename><surname>Ibanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><surname>Jiménez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naouel</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Laadhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lambrix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Michel</surname></persName>
		</author>
		<imprint>
			<publisher>Engy Nasr, Heiko Paulheim, Catia Pesquita</publisher>
			<pubPlace>Tzanina Saveta, Pavel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Results of the Ontology Alignment Evaluation Initiative 2022</title>
		<author>
			<persName><forename type="first">Cássia</forename><surname>Shvaiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chantelle</forename><surname>Trojahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfang</forename><surname>Verhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beyza</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Yaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zamazal</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Workshop on Ontology Matching (OM 2022) co-located with the 21th International Semantic Web Conference (ISWC 2022)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the 17th International Workshop on Ontology Matching (OM 2022) co-located with the 21th International Semantic Web Conference (ISWC 2022)<address><addrLine>Hangzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="84" to="128" />
		</imprint>
	</monogr>
	<note>CEUR-WS.org oaei22_paper0.pdf</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/V1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subsumption Prediction for E-Commerce Taxonomies</title>
		<author>
			<persName><forename type="first">Jingchuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizzie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qunzhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-33455-9_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-33455-9_15" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -20th International Conference, ESWC 2023</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Hersonissos, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023-05-28">2023. May 28 -June 1, 2023</date>
			<biblScope unit="volume">13870</biblScope>
			<biblScope unit="page" from="244" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A contrastive framework for neural text generation</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21548" to="21561" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01137</idno>
		<title level="m">Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
