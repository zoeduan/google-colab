<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FOODGPT: A LARGE LANGUAGE MODEL IN FOOD TESTING DOMAIN WITH INCREMENTAL PRE-TRAINING AND KNOWLEDGE GRAPH PROMPT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhixiao</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Beijing</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts and Telecommunications Beijing</orgName>
								<orgName type="institution" key="instit3">Tsinghua University Beijing</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yijiong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Beijing</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts and Telecommunications Beijing</orgName>
								<orgName type="institution" key="instit3">Tsinghua University Beijing</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meiqi</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Beijing</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts and Telecommunications Beijing</orgName>
								<orgName type="institution" key="instit3">Tsinghua University Beijing</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junyi</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Beijing</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts and Telecommunications Beijing</orgName>
								<orgName type="institution" key="instit3">Tsinghua University Beijing</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
							<email>yfhuang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">School of Information and Communication Engineering</orgName>
								<orgName type="department" key="dep3">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Tsinghua University Beijing</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts and Telecommunications Beijing</orgName>
								<orgName type="institution" key="instit3">Tsinghua University Beijing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FOODGPT: A LARGE LANGUAGE MODEL IN FOOD TESTING DOMAIN WITH INCREMENTAL PRE-TRAINING AND KNOWLEDGE GRAPH PROMPT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5559440DCC9D2DF8C818487068963126</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Food testing</term>
					<term>Large language models</term>
					<term>Incremental pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, the construction of large language models in specific domains is done by fine-tuning on a base model. Some models also incorporate knowledge bases without the need for pre-training. This is because the base model already contains domain-specific knowledge during the pre-training process. We build a large language model for food testing. Unlike the above approach, a significant amount of data in this domain exists in Scanning format for domain standard documents. In addition, there is a large amount of untrained structured knowledge. Therefore, we introduce an incremental pre-training step to inject this knowledge into a large language model. In this paper, we propose a method for handling structured knowledge and scanned documents in incremental pre-training. To overcome the problem of machine hallucination, we constructe a knowledge graph to serve as an external knowledge base for supporting retrieval in the large language model. It is worth mentioning that this paper is a technical report of our pre-release version, and we will report our specific experimental data in future versions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLM) <ref type="bibr" target="#b0">[1]</ref> have gained significant research importance in the field of natural language processing. Models such as ChatGPT, LLaMA <ref type="bibr" target="#b1">[2]</ref>, GPT-4, ChatGLM <ref type="bibr" target="#b2">[3]</ref>, and PaLM <ref type="bibr" target="#b3">[4]</ref> have demonstrated outstanding performance in downstream tasks. The powerful ability of LLM in understanding human instructions has led to continuous research on LLMs in various vertical domains.</p><p>ChatLaw <ref type="bibr" target="#b4">[5]</ref> is based on Ziya-LLaMA-13B and utilizes legal data for instruction fine-tuning, incorporating vector database retrieval to create a legal LLM. DoctorGLM <ref type="bibr" target="#b5">[6]</ref> is built upon ChatGLM-6B and fine-tuned using Chinese medical dialogue datasets to create a Chinese medical consultation model. BenTsao is based on LLaMA-7B and constructs a Chinese medical LLM by leveraging a medical knowledge graph and the GPT-3.5 API to build a Chinese medical instruction dataset. Cornucopia, on the other hand, is based on LLaMA-7B and constructs an instruction dataset using Chinese financial public data and crawled financial data, focusing on question-answering in the financial domain.</p><p>Previous research assume that the base models have already injected the corresponding domain knowledge, hence no incremental pre-training is performed on the base models. However, in certain domains, such as food testing, a large arXiv:2308.10173v1 [cs.CL] 20 Aug 2023 amount of knowledge exists in non-textual images, scanned documents and private structured data. To build LLMs in these domains, we believe that the knowledge in the base models is insufficient, making incremental pre-training essential. Therefore, we begin constructing a Chinese food testing LLM from the incremental pre-training stage, using the food testing domain as a representative example. In summary, this paper contributes in the following ways:</p><p>• We develope a LLM called FoodGPT for food testing, which, to our knowledge, is the first LLM in the field of food testing.</p><p>• We start building vertical domain LLM from incremental pre-training, which is currently a rare effort to our knowledge.</p><p>• For scanned format domain standard documents and structured data, we propose a new data collection method that successfully injects the above knowledge into the FoodGPT during the incremental pre-training stage.</p><p>• The food testing domain LLM requires precise numerical outputs. Hence, we construct a knowledge graph as a retrieval library for the FoodGPT to reduce machine hallucination phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Incremental Pre-training</head><p>In the field of food testing, a significant amount of data is present in images or scanned documents, while another portion is stored in private structured databases. These data have not been utilized in the pre-training of LLMs. If we directly fine-tune the base model, it may struggle to perform well due to a lack of domain knowledge. Therefore, in the training of FoodGPT, we incorporate an incremental pre-training step to inject domain knowledge into FoodGPT. Below, we will describe the approach to handling different types of pre-training data.</p><p>Figure <ref type="figure">1</ref>: Processing pipeline of images and scanned documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Images and Scanned Documents</head><p>The storage of images and scanned documents contains a majority of domain standard document information. We categorize these two types of data together because we use optical character recognition (OCR) technology to process both. Due to the inability to directly extract text from these images and scanned documents, we apply OCR to over ten thousand of them. The specific processing pipeline is illustrated in Figure <ref type="figure">1</ref>. Each document represents a domain standard document and exceeds the maximum sequence length used for training the model. Therefore, we split the documents into chapters based on their sections. It is discovered that descriptions of the same testing item varied among different documents, as different food items are targeted by the same testing item in different documents. To prevent conflicts in these descriptions, we add a prefix before each chapter of data to indicate the corresponding document. We fine-tune a UIE <ref type="bibr" target="#b6">[7]</ref> model to extract document names from text, and construct the prefixes for the extracted document names using a heuristic generation method, which are then concatenated with the text. Additionally, each chapter of text may include data such as tables and formulas. These types of data, when processed by OCR, significantly affect the fluidity of the text. Therefore, we used BERT <ref type="bibr" target="#b7">[8]</ref> and GPT-2 <ref type="bibr" target="#b8">[9]</ref> to calculate the perplexity of each sentence in each chapter of text, and exclude sentences with high perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured Knowledge</head><p>A large amount of knowledge in the field of food testing is also present in private structured databases. These data consist of manually entered tables obtained from extensive food testing and are stored in private databases of testing institutions or intermediaries. They contain a wealth of learnable knowledge. In addition to integrating this knowledge into an external knowledge base, we believe that it should also be incorporated into the incremental pre-training of LLM. The way we handle structured data in incremental pre-training is illustrated in Figure <ref type="figure">2</ref>.</p><p>Figure <ref type="figure">2</ref>: There are two ways we handle structured data.</p><p>We create two versions of structured data, Datav1 and Datav2, for incremental pre-training. In Datav1, we first remove the confidential and privacy information fields from the table. Then, we construct each data by dict. It's worth noting that in this domain, one food item can correspond to multiple testing items. We use the "testing item" as the key and represent the corresponding multiple specific testing items using a table in markdown format as the value.</p><p>We consider that this structured data could potentially harm the model, so it is necessary to serialize the structured knowledge. As far as we know, currently structured data is manually converted into natural language using templates. However, the number of manual templates is quite limited, and there may be ambiguous repetitions and a decrease in model performance when serializing a large amount of structured knowledge. Therefore, our Datav2 employs a novel method to serialize structured data. The construction process of Datav2 is as follows:</p><p>• Similar to Datav1, we remove confidential and private information fields from the table.</p><p>• Because some fields in the original data do not have individual meanings, we merge certain fields to ensure each field has a separate meaning.</p><p>• We input each data point into ChatGPT to generate text randomly according to specific rules. The rules for random text generation are as follows: 1) Randomly select fields to input for each data point. 2) Randomly choose the temperature parameter within the range of [0.5, 1.0] for text generation. 3) Ensure that each field in a data point is selected at least once and no more than twice during text generation, in order to include all field information and avoid excessive repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other Types of Data</head><p>In addition to the two types of data mentioned above, we incorporate other data to construct the incremental pre-training dataset for FoodGPT. The specific sources are as follows:</p><p>• Food detection dictionary. Each data entry provides an explanation of specialized terms.</p><p>• Chinese tutorials and research papers in the field of food testing. We split them into individual data entries based on paragraphs. • Food sentiment data. Each data entry consists of news articles related to public opinions on food, covering the past five years. • Food safety-related laws. Each data entry represents a provision within the laws.</p><p>• Food safety-related exam questions. Each data entry corresponds to a question, and we make efforts to include detailed explanations for each question.</p><p>We choose Chinese-LLaMA2-13B as the base model and used the LoRA <ref type="bibr" target="#b9">[10]</ref> method for incremental pre-training. We will elaborate on the specific experimental results in future versions of the technical report. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Instruction Fine-tuning</head><p>During the instruction fine-tuning phase, we construct a dataset for instruction fine-tuning through two channels. The LoRA approach was employed to fine-tune FoodGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We utilize two methods to construct the instruction fine-tuning dataset for FoodGPT. Firstly, we select relevant topics from food forums and scraped a large number of question-answer pairs. To ensure high-quality answers, we prioritize users with higher posting frequency, aiming to include answers from users who frequently post quality content. Secondly, we collaborate with industry experts in the food testing field to design 100 high-quality seed instructions. These seed instructions are further expanded and diversified using the evol-instruct <ref type="bibr" target="#b10">[11]</ref> method, resulting in a more comprehensive and extensive dataset of instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Process</head><p>We utilize the LoRA method to fine-tune the instructions for Chinese-LLaMA2-13B. The entire pipeline of instruction fine-tuning is illustrated in Figure <ref type="figure" target="#fig_0">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">External Knowledge Graph Retrieval</head><p>The requirements for food testing are very strict in terms of output. For instance, inaccurate data output could lead to severe consequences. In order to ensure the quality of the data generated by FoodGPT and reduce machine hallucinations, we construct a knowledge graph by extracting a large amount of knowledge from structured data and text. This knowledge graph serves as an external knowledge base to support the retrieval and output of FoodGPT. The process of incorporating the knowledge graph is depicted in Figure <ref type="figure" target="#fig_1">4</ref>. When the user inputs a query to FoodGPT, we utilize a retrieval model to parse the query and retrieve relevant knowledge from the knowledge graph. The retrieved knowledge is then concatenated with the query and inputted into FoodGPT. FoodGPT, equipped with parameterized knowledge and external knowledge, comprehends the user's intent and generates responses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present FoodGPT, a large language model for the field of food testing. FoodGPT is built upon the Chinese-LLaMA2-13B base model, with incremental pre-training, instruction fine-tuning, and external knowledge graph integration. Since a significant amount of knowledge exists in images, scanned documents, and private structured knowledge bases, which the base model lacks, we deem it necessary to perform incremental pre-training. We propose new approaches to handle these data and incorporate them into our incremental pre-training database along with other data. In the instruction fine-tuning stage, we crawl question-answer pairs from forums and employ evol-instruct to construct the fine-tuning dataset, guided by seed instructions provided by domain experts. Given the stringent requirements for output metrics in the field of food testing, we additionally construct a knowledge graph to serve as an external database to assist FoodGPT in generating outputs.</p><p>It is worth mentioning that this paper is a technical report of the pre-release version of FoodGPT, and we will elaborate on experimental details and analysis in future versions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The entire pipeline of instruction fine-tuning.</figDesc><graphic coords="4,142.20,218.57,327.60,188.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Retrieval Method for External Knowledge Graph.</figDesc><graphic coords="5,189.00,139.71,234.00,140.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,72.00,354.53,468.00,185.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,72.00,198.08,468.02,221.10" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Glm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<title level="m">General language model pretraining with autoregressive blank infilling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Chatlaw: Open-source legal large language model with integrated external knowledge bases</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16092</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Doctorglm: Fine-tuning your chinese doctor is not a herculean task</title>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01097</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unified structure generation for universal information extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12277</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Wizardlm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12244</idno>
		<title level="m">Empowering large language models to follow complex instructions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
