<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models as Zero-Shot Conversational Recommenders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-19">19 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhouhang</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
							<email>rahuljha@netflix.com</email>
						</author>
						<author>
							<persName><forename type="first">Harald</forename><surname>Steck</surname></persName>
							<email>hsteck@netflix.com</email>
						</author>
						<author>
							<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
							<email>dliang@netflix.com</email>
						</author>
						<author>
							<persName><forename type="first">Bodhisattwa</forename><forename type="middle">Prasad</forename><surname>Majumder</surname></persName>
							<email>bmajumde@eng.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
							<email>nkallus@netflix.com</email>
						</author>
						<author>
							<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
							<email>jmcauley@ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Netflix Inc. Los Gatos</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Netflix Inc. Los Gatos</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Netflix Inc. Los Gatos</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Netflix Inc. Los Gatos</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Netflix Inc. Los Gatos</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Cornell University New York</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Language Models as Zero-Shot Conversational Recommenders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-19">19 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">897AE1D56BE1C7E391997DC87116F086</idno>
					<idno type="DOI">10.1145/3583780.3614949</idno>
					<idno type="arXiv">arXiv:2308.10053v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>conversational recommendation</term>
					<term>large language model</term>
					<term>datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data:</p><p>To gain insights into model behavior in "in-the-wild" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. This is the largest public real-world conversational recommendation dataset to date. (2) Evaluation: On the new dataset and two existing conversational recommendation datasets, we observe that even without fine-tuning, large language models can outperform existing fine-tuned conversational recommendation models. (3) Analysis: We propose various probing tasks to investigate the mechanisms behind the remarkable performance of large language models in conversational recommendation. We analyze both the large language models' behaviors and the characteristics of the datasets, providing a holistic understanding of the models' effectiveness, limitations and suggesting directions for the design of future conversational recommenders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Personalization; • Computing methodologies → Natural language generation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conversational recommender systems (CRS) aim to elicit user preferences and offer personalized recommendations by engaging in interactive conversations. In contrast to traditional recommenders that primarily rely on users' actions like clicks or purchases, CRS possesses the potential to: <ref type="bibr" target="#b0">(1)</ref> understand not only users' historical actions but also users' (multi-turn) natural-language inputs; (2) provide not only recommended items but also human-like responses for multiple purposes such as preference refinement, knowledgable discussion or recommendation justification. Towards this objective, a typical conversational recommender contains two components <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74]</ref>: a generator to generate natural-language responses and a recommender to rank items to meet users' needs.</p><p>Recently, significant advancements have shown the remarkable potential of large language models (LLMs) <ref type="foot" target="#foot_0">1</ref> , such as ChatGPT <ref type="bibr" target="#b29">[30]</ref>, in various tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b70">71]</ref>. This has captured the attention of the recommender systems community to explore the possibility of leveraging LLMs in recommendation or more general personalization tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56</ref>]. Yet, current efforts generally concentrate on evaluating LLMs in traditional recommendation settings, where only users' past actions like clicks serve as inputs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>. The conversational recommendation scenario, though involving more natural language interactions, is still in its infancy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b62">63]</ref>. Yes, there is a new Terminator movie. : <ref type="bibr">[System]</ref> Have you seen the trailer for it? . [User]: I also need a sci-fi movie with my family, it should be lighthearted and enjoyable.</p><p>[BLANK] :[System] 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>Pretend you are a movie recommender system. I will give you a conversation between a user and you (a recommender system).</p><p>Based on the conversation, you reply me with 20 recommendations without extra sentences.</p><p>Here is the conversation: {}</p><p>T F S Prompting 1.Guardians of the Galaxy 2.The Lego Movie 3.Men in Black 4.WALL-E 5.The Fifth Element ... 1. MOVIE_320442 2. MOVIE_352933 3. MOVIE_435849 4. MOVIE_235802 5. MOVIE_239823 ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>🤖</head><p>Figure <ref type="figure">1</ref>: Large Language Models (LLMs) as Zero-Shot Conversational Recommenders (CRS).We introduce a simple prompting strategy to define the task description 𝑇 , format requirement 𝐹 and conversation context 𝑆 for a LLM, denoted as F , we then post-process the generative results into ranked item lists with processor Φ.</p><p>In this work, we propose to use large language models as zeroshot conversational recommenders and then empirically study the LLMs' <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b67">68]</ref> recommendation abilities. Our detailed contributions in this study include three key aspects regarding data, evaluation, and analysis.</p><p>Data. We construct Reddit-Movie, a large-scale conversational recommendation dataset with over 634k naturally occurring recommendation seeking dialogs from users from Reddit 2 , a popular discussion forum. Different from existing crowd-sourced conversational recommendation datasets, such as ReDIAL <ref type="bibr" target="#b40">[41]</ref> and IN-SPIRED <ref type="bibr" target="#b21">[22]</ref>, where workers role-play users and recommenders, the Reddit-Movie dataset offers a complementary perspective with conversations where users seek and offer item recommendation in the real world. To the best of our knowledge, this is the largest public conversational recommendation dataset, with 50 times more conversations than ReDIAL.</p><p>Evaluation. By evaluating the recommendation performance of LLMs on multiple CRS datasets, we first notice a repeated item shortcut in current CRS evaluation protocols. Specifically, there exist "repeated items" in previous evaluation testing samples serving as ground-truth items, which allows the creation of a trivial baseline (e.g., copying the mentioned items from the current conversation history) that outperforms most existing models, leading to spurious conclusions regarding current CRS recommendation abilities. After removing the "repeated items" in training and testing data, we reevaluate multiple representative conversational recommendation models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74]</ref> on ReDIAL, INSPIRED and our Reddit dataset. With this experimental setup, we empirically show that LLMs can outperform existing fine-tuned conversational recommendation models even without fine-tuning.</p><p>Analysis. In light of the impressive performance of LLMs as zeroshot CRS, a fundamental question arises: What accounts for their remarkable performance? Similar to the approach taken in <ref type="bibr" target="#b52">[53]</ref>, we posit that LLMs leverage both content/context knowledge (e.g., "genre", "actors" and "mood") and collaborative knowledge (e.g., 2 <ref type="url" target="https://www.reddit.com/">https://www.reddit.com/</ref> "users who like A typically also like B") to make conversational recommendations. We design several probing tasks to uncover the model's workings and the characteristics of the CRS data. Additionally, we present empirical findings that highlight certain limitations of LLMs as zero-shot CRS, despite their effectiveness.</p><p>We summarize the key findings of this paper as follows:</p><p>• CRS recommendation abilities should be reassessed by eliminating repeated items as ground truth. • LLMs, as zero-shot conversational recommenders, demonstrate improved performance on established and new datasets over fine-tuned CRS models. • LLMs primarily use their superior content/context knowledge, rather than their collaborative knowledge, to make recommendations. • CRS datasets inherently contain a high level of content/context information, making CRS tasks better-suited for LLMs than traditional recommendation tasks. • LLMs suffer from limitations such as popularity bias and sensitivity to geographical regions.</p><p>These findings reveal the unique importance of the superior content/context knowledge in LLMs for CRS tasks, offering great potential to LLMs as an effective approach in CRS; meanwhile, analyses must recognize the challenges in evaluation, datasets, and potential problems (e.g., debiasing) in future CRS design with LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LLMS AS ZERO-SHOT CRS 2.1 Task Formation</head><p>Given a user set U, an item set I and a vocabulary V, a conversation can be denoted as 𝐶 = (𝑢 𝑡 , 𝑠 𝑡 , I 𝑡 ) 𝑇 𝑡 =1 . That means during the 𝑡 th turn of the conversation, a speaker 𝑢 𝑡 ∈ U generates an utterance 𝑠 𝑡 = (𝑤 𝑖 ) 𝑚 𝑖=1 , which is a sequence of words 𝑤 𝑖 ∈ V. This utterance 𝑠 𝑡 also contains a set of mentioned items I 𝑡 ⊂ I (I 𝑡 can be an empty set if no items mentioned). Typically, there are two users in the conversation 𝐶 playing the role of seeker and recommender respectively. Let us use the 2 nd conversation turn in Figure <ref type="figure">1</ref> as an example. Here 𝑡 = 2, 𝑢 𝑡 is [System], 𝑠 𝑡 is "You would love Terminator !" and I 2 is a set containing the movie Terminator. Following many CRS papers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74]</ref>, the recommender component of a CRS is specifically designed to optimize the following objective: during the 𝑘 th turn of a conversation, where 𝑢 𝑘 is the recommender, the recommender takes the conversational context (𝑢 𝑡 , 𝑠 𝑡 , I 𝑡 ) 𝑘 -1 𝑡 =1 as its input, and generate a ranked list of items Î𝑘 that best matches the ground-truth items in I 𝑘 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework</head><p>Prompting. Our goal is to utilize LLMs as zero-shot conversational recommenders. Specifically, without the need for fine-tuning, we intend to prompt an LLM, denoted as F , using a task description template 𝑇 , format requirement 𝐹 , and conversational context 𝑆 before the 𝑘 th turn. This process can be formally represented as:</p><formula xml:id="formula_0">Î𝑘 = Φ (F (𝑇 , 𝐹, 𝑆)) .<label>(1)</label></formula><p>To better understand this zero-shot recommender, we present an example in Figure <ref type="figure">1</ref> with the prompt setup in our experiments. <ref type="foot" target="#foot_1">3</ref>Models. We consider several popular LLMs F that exhibit zero-shot prompting abilities in two groups. To try to ensure deterministic results, we set the decoding temperature to 0 for all models.</p><p>• GPT-3.5-turbo <ref type="bibr" target="#b29">[30]</ref> <ref type="foot" target="#foot_2">foot_2</ref> and GPT-4 <ref type="bibr" target="#b50">[51]</ref> from OPENAI with abilities of solving many complex tasks in zero-shot setting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51]</ref> but are closed-sourced.</p><p>• BAIZE <ref type="bibr" target="#b67">[68]</ref>  <ref type="foot" target="#foot_3">5</ref> and Vicuna <ref type="bibr" target="#b10">[11]</ref>, which are representative open-sourced LLMs fine-tuned based on LLAMA-13B <ref type="bibr" target="#b60">[61]</ref>.</p><p>Processing. We do not assess model weights or output logits from LLMs. Therefore, we apply a post-processor Φ (e.g., fuzzy matching) to convert a recommendation list in natural language to a ranked list Î𝑘 . The approach of generating item titles instead of ranking item IDs is referred to as a generative retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b59">60]</ref> paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>Ideally, a large-scale dataset with diverse interactions and realworld conversations is needed to evaluate models' ability in conversational recommendation. Existing conversational recommendation datasets are usually crowd-sourced <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b74">75]</ref> and thus only partially capture realistic conversation dynamics. For example, a crowd worker responded with "Whatever Whatever I'm open to any suggestion." when asked about movie preferences in ReDIAL; this happens since crowd workers often do not have a particular preference at the time of completing a task. In contrast, a real user could have a very particular need, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. ), an existing CRS dataset (Re-DIAL <ref type="bibr" target="#b40">[41]</ref>), and our Reddit-Movie dataset. The Reddit-Movie dataset contains more information in its textual content compared to existing datasets where users often explicitly specify their preference. See Section 5.2 for quantitative analysis.</p><p>To complement crowd-sourced CRS datasets, we present the Reddit-Movie dataset, the largest-scale conversational movie recommendation dataset to date, with naturally occurring movie recommendation conversations that can be used along with existing crowd-sourced datasets to provide richer perspectives for training and evaluating CRS models. In this work, we conduct our model evaluation and analysis on two commonly used crowd-sourcing datasets: ReDIAL <ref type="bibr" target="#b40">[41]</ref> and INSPIRED <ref type="bibr" target="#b21">[22]</ref>, as well as our newly collected Reddit dataset. We show qualitative examples from the Reddit dataset as in Figure <ref type="figure" target="#fig_2">2</ref> and quantitative analysis in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Construction</head><p>To construct a CRS dataset from Reddit, we process all Reddit posts from 2012 Jan to 2022 Dec from pushshift.io <ref type="foot" target="#foot_4">6</ref> . We consider movie recommendation scenarios <ref type="foot" target="#foot_5">7</ref> and extract related posts from five related subreddits: r/movies, r/bestofnetflix, r/moviesuggestions, r/netflixbestof and r/truefilm. We process the raw data with the pipeline of conversational recommendation identification, movie mention recognition and movie entity linking <ref type="foot" target="#foot_6">8</ref> . In our following evaluation, we use the most recent 9k conversations in Reddit-Movie base from December 2022 as the testing set since these samples occur after GPT-3.5-t's release. Meanwhile, GPT-4 <ref type="bibr" target="#b50">[51]</ref> also mentioned its pre-training data cut off in Sept. 2021 <ref type="foot" target="#foot_7">9</ref> . For other compared models, we use the remaining 76k conversations in Reddit-Movie base dataset for training and validation. Table 1, we observe: (1) The dataset Reddit-Movie stands out as the largest conversational recommendation dataset, encompassing 634,392 conversations and covering 51,203 movies. (2) In comparison to ReDIAL [41] and IN-SPIRED [22], Reddit-Movie contains fewer multi-turn conversations, mainly due to the inherent characteristics of Reddit posts. (3) By examining representative examples depicted in Figure <ref type="figure" target="#fig_2">2</ref>, we find that Reddit-Movie conversations tend to include more complex and detailed user preference in contrast to ReDIAL, as they originate from real-world conversations on Reddit, enriching the conversational recommendation datasets with a diverse range of discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we evaluate the proposed LLMs-based frameowrk on ReDIAL <ref type="bibr" target="#b40">[41]</ref>, INSPIRED <ref type="bibr" target="#b21">[22]</ref> and our Reddit datasets. We first explain the evaluation setup and a repeated item shortcut of the previous evaluation in Sections 4.1 and 4.2. Then, we re-train models and discuss LLM performance in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head><p>Repeated vs. New Items. Given a conversation 𝐶 = (𝑢 𝑡 , 𝑠 𝑡 , I 𝑡 ) 𝑇 𝑡 =1 , it is challenging to identify the ground-truth recommended items, i.e., whether the mentioned items I 𝑘 at the 𝑘 th (𝑘 ≤ 𝑇 ) turn are used for recommendation purposes. A common evaluation setup assumes that when 𝑢 𝑘 is the recommender, all items 𝑖 ∈ I 𝑘 serve as ground-truth recommended items.</p><p>In this work, we further split the items 𝑖 ∈ I 𝑘 into two categories: repeated items or new items. Repeated items are items that have appeared in previous conversation turns, i.e., {𝑖 | ∃𝑡 ∈ [1, 𝑘), 𝑖 ∈ I 𝑡 }; and new items are items not mentioned in previous conversation turns. We explain the details of this categorization in Section 4.2.</p><p>Evaluation Protocol. On those three datasets, we evaluate several representative CRS models and several LLMs on their recommendation abilities. For baselines, after re-running the training code provided by the authors, we report the prediction performance using Recall@K <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b73">74]</ref> (i.e., HIT@K). We consider the means and the standard errors 10 of the metric with 𝐾 = {1, 5}. 10 We show standard errors as error bars in our figures and gray numbers in our tables.</p><p>Compared CRS Models. We consider several representative CRS models. For baselines which rely on structured knowledge, we use the entity linking results of ReDIAL and INSPIRED datasets provided by UniCRS <ref type="bibr" target="#b63">[64]</ref>. Note that we do not include more works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref> because UniCRS <ref type="bibr" target="#b63">[64]</ref> is representative with similar results.</p><p>• ReDIAL <ref type="bibr" target="#b40">[41]</ref>: This model is released along with the ReDIAL dataset with an auto-encoder <ref type="bibr" target="#b57">[58]</ref>-based recommender.</p><p>• KBRD <ref type="bibr" target="#b9">[10]</ref>: This model proposes to use the DBPedia <ref type="bibr" target="#b0">[1]</ref> to enhance the semantic knowledge of items or entities.</p><p>• KGSF <ref type="bibr" target="#b73">[74]</ref>: This model incorporates two knowledge graphs to enhance the representations of words and entities, and uses the Mutual Information Maximization method to align the semantic spaces of those two knowledge graphs. • UniCRS <ref type="bibr" target="#b63">[64]</ref>: This model uses pre-trained language model, DialoGPT <ref type="bibr" target="#b68">[69]</ref>, with prompt tuning to conduct recommendation and conversation generation tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Repeated Items Can Be Shortcuts</head><p>Current evaluation for conversational recommendation systems does not differentiate between repeated and new items in a conversation. We observed that this evaluation scheme favors systems that optimize for mentioning repeated items. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, a trivial baseline that always copies seen items from the conversation history has better performance than most previous models under the standard evaluation scheme. This phenomenon highlights the risk of shortcut learning <ref type="bibr" target="#b17">[18]</ref>, where a decision rule performs well against certain benchmarks and evaluations but fails to capture the true intent of the system designer. Indeed, the #HIT@1 for the models tested dropped by more than 60% on average when we focus on new item recommendation only, which is unclear from the overall recommendation performance. After manually checking, we observe a typical pattern of repeated items, which is shown in the example conversation in Figure <ref type="figure">1</ref>. In this conversation, Terminator at the 6 th turn is used as the ground-truth item. The system repeated this Terminator because the system quoted this movie for a content-based discussion during the conversation rather than making recommendations. Given the nature of recommendation conversations between two users, it is more probable that items repeated during a conversation are intended for discussion rather </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LLMs Performance</head><p>Finding 1 -LLMs outperform fine-tuned CRS models in a zero-shot setting. For a comparison between models' abilities to recommend new items to the user in conversation, we re-train existing CRS models on all datasets for new item recommendation only.</p><p>The evaluation results are as shown in Figure <ref type="figure">4</ref>. Large language models, although not fine-tuned, have the best performance on all datasets. Meanwhile, the performance of all models is uniformly lower on Reddit compared to the other datasets, potentially due to the large number of items and fewer conversation turns, making recommendation more challenging.</p><p>Finding 2 -GPT-based models achieve superior performance than open-sourced LLMs. As shown in Figure <ref type="figure">4</ref>, large language models consistently outperform other models across all three datasets, while GPT-4 is generally better than GPT-3.5-t. We hypothesize this is due to GPT-4's larger parameter size enables it to retain more correlation information between movie names and user preferences that naturally occurs in the language models' pre-training data. Vicuna and BAIZE, while having comparable performance to prior models on most datasets, have significantly lower performance than its teacher, GPT-3.5-t. This is consistent with previous works' finding that smaller distilled models via imitation learning cannot fully inherit larger models ability on downstream tasks <ref type="bibr" target="#b19">[20]</ref>.</p><p>Finding 3 -LLMs may generate out-of-dataset item titles, but few hallucinated recommendations. We note that language models trained on open-domain data naturally produce items out of the allowed item set during generation. In practice, removing these items improves the models' recommendation performance. Large language models outperform other models (with GPT-4 being the best) consistently regardless of whether these unknown items are removed or not, as shown in Table <ref type="table" target="#tab_2">2</ref>. Meanwhile, Table <ref type="table" target="#tab_3">3</ref> shows that around 95% generated recommendations from GPT-based models (around 81% from BAIZE and 87% from Vicuna) can be found in IMDB 11 by string matching. Those lower bounds of these matching rates indicate that there are only a few hallucinated item titles in the LLM recommendations in the movie domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DETAILED ANALYSIS</head><p>Observing LLMs' remarkable conversational recommendation performance for zero-shot recommendation, we are interested in what accounts for their effectiveness and what their limitations are. We aim to answer these questions from both a model and data perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge in LLMs</head><p>Experiment Setup. Motivated by the probing work of <ref type="bibr" target="#b52">[53]</ref>, we posit that two types of knowledge in LLMs can be used in CRS:</p><p>• Collaborative knowledge, which requires the model to match items with similar ones, according to community interactions like "users who like A typically also like B". In</p><p>Vicuna GPT-3.</p><p>5-t GPT-4 0.000 0.025 0.050 0.075 0.100 0.125 0.150 Recal@5 INSPIRED Vicuna GPT-3.5-t GPT-4 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 ReDIAL Vicuna GPT-3.5-t GPT-4 0.00 0.02 0.04 0.06 0.08 Reddit S 0 , 1 S 1 , 1 S 2 , 1 S 2 , 2 S 3 , 1 S 3 , 2 Figure 5: Ablation studies for the research question about the primary knowledge used by LLMs for CRS. Here Φ 1 is the post-processor which only considers in-dataset item titles; Φ 2 is the post-processor based on Φ 1 and further excludes all seen items in conversational context from generated recommendation lists. For inputs like Original (𝑆 0 ) and ItemOnly (𝑆 1 ), LLMs show similar performance with Φ 1 or Φ 2 , so we only keep Φ 1 here. We consider Φ 2 because ItemRemoved (𝑆 2 ) and ItemRandom (𝑆 3 ) have no information about already mentioned items, which may cause under-estimated accuracy using Φ 1 compared to Original. 0.00 0.05 0.10 0.15 0.20 0.25 Recall@5 INSPIRED 0.0 0.1 0.2 0.3 0.4 ReDIAL 0.00 0.02 0.04 0.06 0.08 Reddit S0, 1 S1, 1 S2, 2 S3, 2 {0} [1,5) [5,10) [10,+ ) 0 100 Counts 49 150 25 4 {0} [1,5) [5,10) [10,+ ) 0 2000 643 2416 486 7 {0} [1,5) [5,10) [10,+ ) 0 2500 2635 4573 1676 526 our experiments, we define the collaborative knowledge in LLMs as the ability to make accurate recommendations using item mentions in conversational contexts. • Content/context knowledge, which requires the model to match recommended items with their content or context information. In our experiments, we define the content/context knowledge in LLMs as the ability to make accurate recommendations based on all other conversation inputs rather than item mentions, such as contextual descriptions, mentioned genres, and director names. To understand how LLMs use these two types of knowledge, given the original conversation context 𝑆 (Example in Figure <ref type="figure">1</ref>), we perturb 𝑆 with three different strategies as follows and subsequently re-query the LLMs. We denote the original as 𝑆 0 :</p><p>• S 0 (Original): we use the original conversation context.</p><p>• S 1 (ItemOnly): we keep mentioned items and remove all natural language descriptions in the conversation context. • S 2 (ItemRemoved): we remove mentioned items and keep other content in the conversation context.</p><p>Table <ref type="table">4</ref>: To understand the content/context knowledge in LLMs and existing CRS models, we re-train the existing CRS models using the same perturbed conversation context Item-Removed (𝑆 2 ). We include the results of the representative CRS model UniCRS (denoted as CRS*) as well as a representative text-encoder BERT-small <ref type="bibr" target="#b14">[15]</ref> (denoted as TextEnc*).</p><formula xml:id="formula_1">INSPIRED ReDIAL Reddit Model R@1 R@5 R@1 R@5 R@1 R@</formula><p>5 Vicuna .024 .010 .062 .017 .014 .002 .053 .003 .008 .001 .025 .001 GPT-3.5-t .057 .016 .123 .023 .030 .003 .105 .005 .018 .001 .068 .002 GPT-4 .062 .017 .128 .023 .032 .003 .102 .005 .019 .001 .075 .002 CRS* .039 .011 .087 .014 .015 .002 .058 .003 .001 .000 .008 .001 TextEnc* .038 .015 .090 .016 .013 .002 .053 .004 .002 .000 .009 .001</p><p>• S 3 (ItemRandom): we replace the mentioned items in the conversation context with items that are uniformly sampled from the item set I of this dataset, to eliminate the potential influence of 𝑆 2 on the sentence grammar structure.</p><p>Finding 4 -LLMs mainly rely on content/context knowledge to make recommendations. Figure <ref type="figure">5</ref> shows a drop in performance for most models across various datasets when replacing the original conversation text Original (𝑆 0 ) with other texts, indicating that LLMs leverage both content/context knowledge and collaborative knowledge in recommendation tasks. However, the importance of these knowledge types differs. Our analysis reveals that content/context knowledge is the primary knowledge utilized by LLMs in CRS. When using ItemOnly (𝑆 1 ) as a replacement for Original, there is an average performance drop of more than 60% in terms of Recall@5. On the other hand, GPT-based models experience only a minor performance drop of less than 10% on average when using ItemRemoved (𝑆 2 ) or ItemRandom (𝑆 3 ) instead of Original. Although the smaller-sized model Vicuna shows a higher performance drop, it is still considerably milder compared to using ItemOnly.</p><p>To accurately reflect the recommendation abilities of LLMs with ItemRemoved and ItemRandom, we introduce a new post-processor Table <ref type="table">5</ref>: To understand the collaborative knowledge in LLMs and existing CRS models, we re-train the existing CRS models using the same perturbed conversation context ItemOnly (𝑆 1 ). We include the results of the representative CRS model Uni-CRS (denoted as CRS*) as well as a representative item-based collaborative model FISM <ref type="bibr" target="#b30">[31]</ref> (denoted as ItemCF*).</p><formula xml:id="formula_2">INSPIRED ReDIAL Reddit Model R@1 R@5 R@1 R@5 R@1 R@</formula><p>5 Vicuna .005 .005 .024 .010 .011 .002 .039 .003 .005 .000 .015 .001 GPT-3.5-t .024 .010 .052 .015 .021 .002 .063 .004 .007 .001 .026 .001 GPT-4 .014 .008 .052 .015 .025 .002 .069 .004 .007 .001 .028 .001 CRS* .038 .013 .085 .019 .025 .002 .072 .004 .003 .000 .015 .001 ItemCF* .042 .012 .087 .016 .029 .003 .088 .004 .004 .001 .018 .001</p><p>denoted as Φ 2 (describe in the caption of Figure <ref type="figure">5</ref>). By employing Φ 2 , the performance gaps between Original and ItemRemoved (or ItemRandom) are further reduced. Furthermore, Figure <ref type="figure" target="#fig_4">6</ref> demonstrates the consistent and close performance gap between Original and ItemRemoved (or ItemRandom) across different testing samples, which vary in size and the number of item mentions in Original.</p><p>These results suggest that given a conversation context, LLMs primarily rely on content/context knowledge rather than collaborative knowledge to make recommendations. This behavior interestingly diverges from many traditional recommenders like collaborative filtering <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58]</ref> or sequential recommenders <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b72">73]</ref>, where user-interacted items are essential.</p><p>Finding 5 -GPT-based LLMs possess better content/context knowledge than existing CRS. From Table <ref type="table">4</ref>, we observe the superior recommendation performance of GPT-based LLMs against representative conversational recommendation or text-only models on all datasets, showing the remarkable zero-shot abilities in understanding user preference with the textual inputs and generating correct item titles. We conclude that GPT-based LLMs can provide more accurate recommendations than existing trained CRS models in an ItemRemoved (𝑆 2 ) setting, demonstrating better content/context knowledge.</p><p>Finding 6 -LLMs generally possess weaker collaborative knowledge than existing CRS. In Table <ref type="table">5</ref>, the results from IN-SPIRED and ReDIAL indicate that LLMs underperform existing representative CRS or ItemCF models by 30% when using only the item-based conversation context ItemOnly (𝑆 1 ). It indicates that LLMs, trained on a general corpus, typically lack the collaborative knowledge exhibited by representative models trained on the target dataset. There are several possible reasons for this weak collaborative knowledge in LLMs. First, the training corpus may not contain sufficient information for LLMs to learn the underlying item similarities. Second, although LLMs may possess some collaborative knowledge, they might not align with the interactions in the target datasets, possibly because the underlying item similarities can be highly dataset-platform-dependent.</p><p>However, in the case of the Reddit dataset, LLMs outperform baselines in both Recall@1 and Recall@5, as shown in Table <ref type="table">5</ref>. This outcome could be attributed to the dataset's large number of rarely interacted items, resulting in limited collaborative information. The Reddit dataset contains 12,982 items with no more than 3 mentions as responses. This poses a challenge in correctly ranking these items within the Top-5 or even Top-1 positions. LLMs, which possess at least some understanding of the semantics in item titles, have the chance to outperform baselines trained on datasets containing a large number of cold-start items.</p><p>Recent research on LLMs in traditional recommendation systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> also observes the challenge of effectively leveraging collaborative information without knowing the target interaction data distribution. Additionally, another study <ref type="bibr" target="#b2">[3]</ref> on traditional recommendation systems suggests that LLMs are beneficial in a setting with many cold-start items. Our experimental results support these findings within the context of conversational recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Information from CRS Data</head><p>Experimental Setup for Finding 7. To understand LLMs in CRS tasks from the data perspective, we first measure the content/context information in CRS datasets. Content/context information refers to the amount of information contained in conversations, excluding the item titles, which reasonably challenges existing CRS and favors LLMs according to the findings in Section 5.1. Specifically, we conduct an entropy-based evaluation for each CRS dataset and compare the conversational datasets with several popular conversation and question-answering datasets, namely DailyDialog (chit chat) <ref type="bibr" target="#b44">[45]</ref>, MsMarco (conversational search) <ref type="bibr" target="#b1">[2]</ref>, and HotpotQA (question answering). We use ItemRemoved (𝑆 2 ) conversation texts like Section 5.1, and adopt the geometric mean of the entropy distribution of 1,2,3-grams as a surrogate for the amount of information contained in the datasets, following previous work on evaluating information content in text <ref type="bibr" target="#b28">[29]</ref>. However, entropy naturally grows with the size of a corpus, and each CRS dataset has a different distribution of words per sentence, sentences per dialog, and corpus size. Thus, it would be unfair to compare entropy between corpus on a per-dialog, per-turn, or per-dataset basis. To ensure a fair comparison, we repeatedly draw increasingly large subsets of texts from each of the datasets, compute the entropy of these subsets, and report the trend of entropy growth with respect to the size of the subsampled text for each CRS dataset.</p><p>Finding 7 -Reddit provides more content/context information than the other two CRS datasets. Based on the results in Figure <ref type="figure" target="#fig_5">7a</ref>, we observe that the Reddit dataset has the most content/context information among the three conversational recommendation datasets. Those observations are also aligned with the results in Figure <ref type="figure">5</ref> and table <ref type="table">4</ref>, where LLMs -which possess better content/context knowledge than baselines -can achieve higher relative improvements compared to the other two datasets. Meanwhile, the content/context information in Reddit is close to question answering and conversational search, which is higher than existing conversational recommendation and chit-chat datasets.</p><p>Finding 8 -Collaborative information is insufficient for satisfactory recommendations, given the current models. Quantifying the collaborative information in datasets is challenging. Instead of proposing methods to measure collaborative information, we aim to make new observations based on general performance results presented in Figure <ref type="figure">4</ref> and recommendation results using only collaborative information in Table <ref type="table">5</ref>. Comparing the performance of the best models in Table <ref type="table">5</ref> under an ItemOnly (𝑆 1 ) setting with the performance of the best models in Figure <ref type="figure">4</ref> under an Original (𝑆 0 ) setting reveals a significant disparity. For instance, on ReDIAL, the Recall@1 performance is 0.029 for ItemCF* compared to 0.046 for GPT-4, representing a 39.96% decrease. Similarly, for Reddit, the Recall@1 performance is 0.007 compared to 0.023 for GPT-4 both, which is 69.57% lower. We also experimented with other recommender systems, such as transformer-based models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b58">59]</ref> to encode the item-only inputs and found similar results. Based on the current performance gap, we find that using the existing models, relying solely on collaborative information, is insufficient to provide satisfactory recommendations. We speculate that either (1) more advanced models or training methods are required to better comprehend the collaborative information in CRS datasets, or (2) the collaborative information in CRS datasets is too limited to support satisfactory recommendations.</p><p>USA UK AUS CAN ESP FRA DEU ITA JPN RUS 10 2 10 3 Ground-Truth Freq By Country USA UK AUS CAN ESP FRA DEU ITA JPN RUS 0.00 0.01 0.02 0.03 0.04 0.05</p><p>Recall@1 with GPT-4 Experimental Setup for Finding 9. To understand whether the collaborative information from CRS datasets are aligned with pure interaction datasets, we conduct an experiment on the Reddit dataset.</p><p>In this experiment, we first process the dataset to link the items to a popular interaction dataset ML-25M <ref type="bibr" target="#b20">[21]</ref> <ref type="foot" target="#foot_8">foot_8</ref> . We then experiment with two representative encoders for item-based collaborative filtering based on FISM <ref type="bibr" target="#b30">[31]</ref> and Transformer <ref type="bibr" target="#b58">[59]</ref> (TRM), respectively. We report the testing results on Reddit, with fine-tuning on Reddit (FT), pre-training on ML-25M (PT), and pre-training on ML-25M then fine-tuning Reddit (PT+FT). Note that since it is a linked dataset with additional processing, the results are not comparable with beforementioned results on Reddit.</p><p>Finding 9 -Collaborative information can be dataset-or platform-dependent. From Figure <ref type="figure" target="#fig_5">7b</ref> shows that the models solely pre-trained on ML-25M (PT) outperform a random baseline, indicating that the data in CRS may share item similarities with pure interaction data from another platform to some extent. However, Figure <ref type="figure" target="#fig_5">7b</ref> also shows a notable performance gap between PT and fine-tuning on Reddit (FT). Additionally, we do not observe further performance improvement when pre-training on ML-25M then fine-tuning on Reddit (PT+FT). These observations indicate that the collaborative information and underlying item similarities, even when utilizing the same items, can be largely influenced by the specific dataset or platform. The finding also may partially explain the inferior zero-shot recommendation performance of LLMs in Table 5 and suggest the necessity of further checking the alignment of collaborative knowledge in LLMs with the target datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Limitations of LLMs as Zero-shot CRS</head><p>Finding 10 -LLM recommendations suffer from popularity bias in CRS. Popularity bias refers to a phenomenon that popular items are recommended even more frequently than their popularity would warrant <ref type="bibr" target="#b7">[8]</ref>. Figure <ref type="figure" target="#fig_6">8</ref> shows the popularity bias in LLM recommendations, though it may not be biased to the popular items in the target datasets. On ReDIAL, the most popular movies such as Avengers: Infinity War appear around 2% of the time over all ground-truth items; On Reddit, the most popular movies such as Everything Everywhere All at Once appears less than 0.3% of the time over ground-truth items. But for the generated recommendations from GPT-4 (other LLMs share a similar trend), the most popular items such as The Shawshank Redemption appear around 5% times on ReDIAL and around 1.5% times on Reddit. Compared to the target datasets, LLMs recommendations are more concentrated on popular items, which may cause further issues like the bias amplification loop <ref type="bibr" target="#b7">[8]</ref>. Moreover, the recommended popular items are similar across different datasets, which may reflect the item popularity in the pre-training corpus of LLMs.</p><p>Finding 11 -Recommendation performance of LLMs is sensitive to geographical regions. Despite the effectiveness in general, it is unclear whether LLMs can be good recommenders across various cultures and regions. Specifically, pre-trained language models' strong open-domain ability can be attributed to pre-training from massive data <ref type="bibr" target="#b4">[5]</ref>. But it also leads to LLMs' sensitivity to data distribution. To investigate LLMs recommendation abilities for various regions, we take test instances from the Reddit dataset and obtain the production region of 7,476 movies from a publicly available movie dataset 13 by exact title matching, then report the Recall@1 for the linked movies grouped by region. We only report regions with more than 300 data points available to ensure enough data to support the result. As shown in Figure <ref type="figure" target="#fig_8">9</ref> the current best model, GPT-4's performance on recommendation is higher for movies produced in English-speaking regions. This could be due to bias in the training data -the left of Figure <ref type="figure" target="#fig_8">9</ref> show item on Reddit forums are dominated by movies from English-speaking regions. Such a result highlights large language model's recommendation performance varies by region and culture and demonstrates the importance of cross-regional analysis and evaluation for language model-based conversational recommendation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Conversational Recommendation. Conversational recommender systems (CRS) aim to understand user preferences and provide personalized recommendations through conversations. Typical traditional CRS setups include template-based CRS <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b69">70]</ref> and critiquing-based CRS <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67]</ref>. More recently, as natural language processing has advanced, the community developed "deep" CRS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64]</ref> that support interactions in natural language. Aside from collaborative filtering signals, prior work shows that CRS models benefit from various additional information. Examples include knowledge-enhanced models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b73">74]</ref> that make use of external knowledge bases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47]</ref>, review-aware models <ref type="bibr" target="#b48">[49]</ref>, and session/sequence-based models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b75">76]</ref>. Presently, UniCRS <ref type="bibr" target="#b63">[64]</ref>, a model built on DialoGPT <ref type="bibr" target="#b68">[69]</ref> with prompt tuning <ref type="bibr" target="#b3">[4]</ref>, stands as the state-of-the-art approach on CRS datasets such as ReDIAL <ref type="bibr" target="#b40">[41]</ref> and INSPIRED <ref type="bibr" target="#b21">[22]</ref>. Currently, by leveraging LLMs, <ref type="bibr" target="#b15">[16]</ref> proposes a new CRS pipeline but does not provide quantitative results, and <ref type="bibr" target="#b62">[63]</ref> proposes better user simulators to improve evaluation strategies in LLMs. Unlike those papers, we uncover a repeated item shortcut in the previous evaluation protocol, and propose a framework where LLMs serve as zero-shot CRS with detailed analyses to support our findings from both model and data perspectives.</p><p>Large Language Models. Advances in natural language processing (NLP) show that large language models (LLMs) exhibit strong 13 <ref type="url" target="https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset">https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset</ref> generalization ability towards unseen tasks and domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>In particular, existing work reveals language models' performance and sample efficiency on downstream tasks can be improved simply through scaling up their parameter sizes <ref type="bibr" target="#b34">[35]</ref>. Meanwhile, language models could further generalize to a wide range of unseen tasks by instruction tuning, learning to follow task instructions in natural language <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b56">57]</ref>. Following these advances, many works successfully deploy large language models to a wide range of downstream tasks such as question answering, numerical reasoning, code generation, and commonsense reasoning without any gradient updates <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b71">72]</ref>. Recently, there have been various attempts by the recommendation community to leverage large language models for recommendation, this includes both adapting architectures used by large language models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> and repurposing existing LLMs for recommendation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b61">62]</ref>. However, to our best knowledge, we are the first work that provides a systematic quantitative analysis of LLMs' ability on conversational recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSION</head><p>We investigate Large Language Models (LLMs) as zero-shot Conversational Recommendation Systems (CRS). Through our empirical investigation, we initially address a repetition shortcut in previous standard CRS evaluations, which can potentially lead to unreliable conclusions regarding model design. Subsequently, we demonstrate that LLMs as zero-shot CRS surpass all fine-tuned existing CRS models in our experiments. Inspired by their effectiveness, we conduct a comprehensive analysis from both the model and data perspectives to gain insights into the working mechanisms of LLMs, the characteristics of typical CRS tasks, and the limitations of using LLMs as CRS directly. Our experimental evaluations encompass two publicly available datasets, supplemented by our newly-created dataset on movie recommendations collected by scraping a popular discussion website. This dataset is the largest public CRS dataset and ensures more diverse and realistic conversations for CRS research. We also discuss the future directions based on our findings in this section.</p><p>On LLMs. Given the remarkable performance even without finetuning, LLMs hold great promise as an effective approach for CRS tasks by offering superior content/contextual knowledge. The encouraging performance from the open-sourced LLMs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b67">68]</ref> also opens up the opportunities to further improve CRS performance via efficient tuning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref> and collaborative filtering <ref type="bibr" target="#b35">[36]</ref> ensembling. Meanwhile, many conventional tasks, such as debiasing <ref type="bibr" target="#b7">[8]</ref> and trustworthy <ref type="bibr" target="#b16">[17]</ref> need to be revisited in the context of LLMs.</p><p>On CRS. Our findings suggest the systematic re-benchmarking of more CRS models to understand their recommendation abilities and the characteristics of CRS tasks comprehensively. Gaining a deeper understanding of CRS tasks also requires new datasets from diverse sources e.g., crowd-sourcing platforms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>, discussion forums, and realistic CRS applications with various domains, languages, and cultures. Meanwhile, our analysis of the information types uncovers the unique importance of the superior content/context knowledge in LLMs for CRS tasks; this distinction also sets CRS tasks apart from traditional recommendation settings and urges us to explore the interconnections between CRS tasks and traditional recommendation <ref type="bibr" target="#b20">[21]</ref> or conversational search <ref type="bibr" target="#b1">[2]</ref> tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><figDesc>User]: I love Back to the Future, any recommendations? You would love Terminator! :[System] [User]: Who is in it? Arnold Schwarzenegger! :[System] [User]: Did they make a new Terminator?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>IFigure 2 :</head><label>2</label><figDesc>Figure2: Typical model inputs from a traditional recommendation dataset (MovieLens<ref type="bibr" target="#b20">[21]</ref>), an existing CRS dataset (Re-DIAL<ref type="bibr" target="#b40">[41]</ref>), and our Reddit-Movie dataset. The Reddit-Movie dataset contains more information in its textual content compared to existing datasets where users often explicitly specify their preference. See Section 5.2 for quantitative analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: To show the repeated item shortcut, we count CRS recommendation hits using the Top-K ranked list 𝐾 = {1, 5}. We group the ground-truth hits by repeated items (shaded bars) and new items (not shaded bars). The trivial baseline copies existing items from the current conversation history in chronological order, from the most recent and does not recommend new items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: GPT-3.5-t Recall@5 results grouped by the occurrences of items in conversation context, and count the conversations per dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: The left subfigure shows the entropy of the frequency distribution of 1,2,3-grams with respect to number of words drawn from each dataset (item names excluded) to measure the content/context information across datasets. The right subfigure shows the results of processed Reddit collaborative dataset aligned to ML-25M<ref type="bibr" target="#b20">[21]</ref>. RAND denotes random baseline, FT denotes fine tuning on Reddit, PT denotes pre-training on ML-25M, PT+FT means FT after PT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Scatter plots of the frequency of LLMs (GPT-4) generated recommendations and ground-truth items.</figDesc><graphic coords="8,59.81,83.69,228.08,132.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Ground-truth item counts in Reddit by country (in log scale) and the corresponding Recall@1 by country.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics. We denote a subset of Reddit-Movie in 2022 as base, and the entire ten-year dataset as large.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Conv. #Turns #Users #Items</cell></row><row><cell>INSPIRED [22]</cell><cell>999</cell><cell>35,686</cell><cell>999</cell><cell>1,967</cell></row><row><cell>ReDIAL [41]</cell><cell>11,348</cell><cell>139,557</cell><cell>764</cell><cell>6,281</cell></row><row><cell>Reddit-Movie base</cell><cell>85,052</cell><cell>133,005</cell><cell>10,946</cell><cell>24,326</cell></row><row><cell cols="4">Reddit-Movie large 634,392 1,669,720 36,247</cell><cell>51,203</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>CRS recommendation performance on New Items in terms of Recall@K, with 𝐾 = {1, 5}. To exclude the influence of repeated items in CRS evaluation, we remove all repeated items in training and testing datasets and re-train all baselines.Recall@1 results of considering all generated item titles (Φ 0 ) and only considering in-dataset item titles (Φ 1 ). BAIZE .019 .019 .028 .011 .021 .002 .021 .002 .012 .001 .013 .008 Vicuna .028 .011 .033 .012 .020 .002 .020 .002 .012 .001 .012 .001 GPT-3.5-t .047 .015 .052 .015 .041 .003 .043 .003 .022 .001 .023 .001 GPT-4 .062 .017 .066 .017 .043 .003 .046 .004 .022 .001 .023 .001</figDesc><table><row><cell>Recall@1</cell><cell>0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08</cell><cell></cell><cell>INSPIRED</cell><cell>0.01 0.02 0.03 0.04 0.05</cell><cell>ReDIAL</cell><cell></cell><cell>0.005 0.010 0.015 0.020 0.025</cell><cell>Reddit</cell><cell>Recall@5</cell><cell>0.02 0.04 0.06 0.08 0.10 0.12 0.14</cell><cell>INSPIRED</cell><cell>0.02 0.04 0.06 0.08 0.10 0.12 0.14</cell><cell>ReDIAL</cell><cell>0.02 0.04 0.06 0.08</cell><cell>Reddit</cell></row><row><cell></cell><cell>0.00</cell><cell cols="2">ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell><cell>0.00</cell><cell cols="2">ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell><cell>0.000</cell><cell cols="2">ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell><cell>0.00</cell><cell>ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell><cell>0.00</cell><cell>ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell><cell>0.00</cell><cell>ReDIAL KBRD KGSF UniCRS BAIZE Vicuna GPT-3.5-t GPT-4</cell></row><row><cell cols="5">Figure 4: INSPIRED</cell><cell>ReDIAL</cell><cell></cell><cell></cell><cell>Reddit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Model</cell><cell>Φ 0</cell><cell>Φ 1</cell><cell>Φ 0</cell><cell>Φ 1</cell><cell></cell><cell>Φ 0</cell><cell>Φ 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>than serving as recommendations. We argue that considering the large portion of repeated items (e.g., more than 15% ground-truth items are repeated items in INSPIRED), it is beneficial to remove repeated items and re-evaluate CRS models to better understand models' recommendation ability. It is worth noting that the repetition patterns have also been investigated in evaluating other recommender systems such as next-basket recommendation<ref type="bibr" target="#b39">[40]</ref></p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Fraction of Top-K (𝐾 = 20 in our prompt setup) recommendations (#rec) that can be string matched in the IMDB movie database (%imdb) for the different models, which shows a lower bound of non-hallucinated movie titles.</figDesc><table><row><cell cols="2">BAIZE</cell><cell cols="2">Vicuna</cell><cell cols="2">GPT-3.5-t</cell><cell cols="2">GPT-4</cell></row><row><cell>#rec</cell><cell>%imdb</cell><cell>#rec</cell><cell>%imdb</cell><cell>#rec</cell><cell>%imdb</cell><cell>#rec</cell><cell>%imdb</cell></row><row><cell cols="8">259,333 81.56% 258,984 86.98% 321,048 95.51% 322,323 94.86%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We refer to LLMs as the large-sized pre-trained language models with exceptional zero-shot abilities as defined in<ref type="bibr" target="#b70">[71]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We leave more prompting techniques such as CoT<ref type="bibr" target="#b65">[66]</ref> in future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Referred as GPT-3.5-t hereafter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We use BAIZE-V2 in https://huggingface.co/project-baize/baize-v2-13b</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://pushshift.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Other domains like songs, books can potentially be processed in a similar way</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Check our evaluation data, LLMs scripts, results and the links of Reddit-Movie datasets in https://github.com/AaronHeee/LLMs-as-Zero-Shot-Conversational-RecSys.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>We note that there is a possibility that GPT-4's newest checkpoint might include a small amount of more recent data<ref type="bibr" target="#b50">[51]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>We only use items that can be linked to ML-25M in this experiment. Here 63.32% items are linked using the links.csv file from ML-25M.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-11-11">2007. November 11-15, 2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268[cs.CL]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00447</idno>
		<title level="m">TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoregressive Entity Retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=5k8F6UU39V" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bias and debias in recommender system: A survey and future directions</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Critiquing-based recommenders: survey and emerging trends</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearl</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="125" to="150" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards Knowledge-Based Recommender Dialog System</title>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1803" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://lmsys.org/blog/2023-03-30-vicuna/" />
		<title level="m">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benton</forename><forename type="middle">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno>ArXiv abs/2204.02311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards conversational recommender systems</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08084[cs.IR]</idno>
		<title level="m">M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakim</forename><surname>Sidahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changbo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Schubiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Lara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07961</idno>
		<title level="m">Leveraging Large Language Models in Conversational Recommender Systems</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12515</idno>
		<title level="m">A survey on trustworthy recommender systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5)</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqiang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In RecSys &apos;22: Sixteenth ACM Conference on Recommender Systems</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Golbeck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Harper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ekstrand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bracha</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Basilico</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Keld</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Even</forename><surname>Lundgaard</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oldridge</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="299" to="315" />
			<date type="published" when="2022-09-18">2022. September 18 -23, 2022</date>
			<publisher>ACM</publisher>
			<pubPlace>Seattle, WA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Gudibande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15717[cs.CL]</idno>
		<title level="m">The False Promise of Imitating Proprietary LLMs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">INSPIRED: Toward Sociable Recommendation Dialog Systems</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiaoyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8142" to="8152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial personalized ranking for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR conference on research &amp; development in information retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
	<note>Xiaoyu Du, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locker: Locally constrained self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3088" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bundle MCR: Towards Conversational Bundle Recommendation</title>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Conference on Recommender Systems</title>
		<meeting>the 16th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="288" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08845</idno>
		<title level="m">Large Language Models are Zero-Shot Rankers for Recommender Systems</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 56th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chatgpt: Optimizing language models for dialogue</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Felipe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceron</forename><surname>Uribe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><forename type="middle">Metz</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pokorny</forename><surname>Rapha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fism: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1951" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06474</idno>
		<title level="m">Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scaling Laws for Neural Language Models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv abs/2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="304" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive path reasoning on graph for conversational recommendation</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03879[cs.IR]</idno>
		<title level="m">GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A next basket recommendation reality check</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Jullien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhdeh</forename><surname>Ariannezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards deep conversational recommendations</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-Supervised Bot Play for Conversational Recommendation with Justifications</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhisattwa</forename><surname>Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05197</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xiang Ao, Fuzhen Zhuang, and Qing He. 2022. User-centric conversational recommendation with multi-aspect user modeling</title>
		<author>
			<persName><forename type="first">Shuokai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Competition-level code generation with AlphaCode</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Leblond</surname></persName>
		</author>
		<author>
			<persName><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agustin</forename><forename type="middle">Dal</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cherepanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Jaymin</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Sutherland Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="1092" to="1097" />
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset</title>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/I17-1099" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
		<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ConceptNet-a practical commonsense reasoning tool-kit</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Push</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT technology journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Junling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10149[cs.IR]</idno>
		<title level="m">Is ChatGPT a Good Recommender? A Preliminary Study</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RevCore: Review-Augmented Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1161" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774[cs.CL]</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/b" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>1efde53be364a73914f58805a001731-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">What does bert know about books, movies and music? probing bert for conversational recommendation</title>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Penha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Conference on Recommender Systems</title>
		<meeting>the 14th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="388" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Variational Reasoning about User Preferences for Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasheng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheshera</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11406</idno>
		<title level="m">LaMP: When Large Language Models Meet Personalization</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trishala</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=9Vrb9D0WI4" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
		<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on information and knowledge management</title>
		<meeting>the 28th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transformer memory as a differentiable search index</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21831" to="21843" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03516[cs.IR]</idno>
	</analytic>
	<monogr>
		<title level="m">Generative Recommendation: Towards Next-generation Recommender Paradigm</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13112</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1929" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/file/9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
		</imprint>
	</monogr>
	<note>d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep language-based critiquing for recommender systems</title>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Baize: An opensource chat model with parameter-efficient tuning on self-chat data</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01196</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multiple Choice Questions based Multi-Interest Policy Learning for Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17568[cs.LG]</idno>
		<title level="m">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM international conference on information &amp; knowledge management</title>
		<meeting>the 29th ACM international conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Improving conversational recommender systems via knowledge graph based semantic fusion</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards Topic-Guided Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4128" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Improving conversational recommender systems via transformerbased sequential modelling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2319" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
