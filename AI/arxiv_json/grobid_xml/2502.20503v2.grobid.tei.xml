<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Protecting multimodal large language models against misleading visualizations</title>
				<funder>
					<orgName type="full">National Research Center for Applied Cybersecurity ATHENE</orgName>
				</funder>
				<funder ref="#_SaxNEsW">
					<orgName type="full">LOEWE initiative (Hesse, Germany)</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder>
					<orgName type="full">Accelerate Foundation Model Academic Research)</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-03-05">5 Mar 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Tonglet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Hessian Center for AI (hessian.AI)</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP Lab)</orgName>
								<address>
									<country>TU Darmstadt</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">KU Leuven</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">KU Leuven</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Hessian Center for AI (hessian.AI)</orgName>
								<orgName type="laboratory">Ubiquitous Knowledge Processing Lab (UKP Lab)</orgName>
								<address>
									<country>TU Darmstadt</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Protecting multimodal large language models against misleading visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-03-05">5 Mar 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">2C54DFDAD119FACAD7AA154D6D66777F</idno>
					<idno type="arXiv">arXiv:2502.20503v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>large language models</term>
					<term>question answering</term>
					<term>visualization 10K</term>
					<term>20K</term>
					<term>[…] 40K X-axis label: &quot;Year&quot; -ticks 2000</term>
					<term>2001</term>
					<term>[…] 2009</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We assess the vulnerability of multimodal large language models to misleading visualizations -charts that distort the underlying data using techniques such as truncated or inverted axes, leading readers to draw inaccurate conclusions that may support misinformation or conspiracy theories. Our analysis shows that these distortions severely harm multimodal large language models, reducing their question-answering accuracy to the level of the random baseline. To mitigate this vulnerability, we introduce six inference-time methods to improve performance of MLLMs on misleading visualizations while preserving their accuracy on non-misleading ones. The most effective approach involves (1) extracting the underlying data table and (2) using a text-only large language model to answer questions based on the table. This method improves performance on misleading visualizations by 15.4 to 19.6 percentage points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visualizations are widely used to convey data insights efficiently. However, design flaws -whether intentional or not -can distort the correct interpretation of the underlying data <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. These design flaws, or misleaders, include truncated, inverted, and dual axes, 3D effects, or inconsistent tick intervals. Misleading visualizations pose a serious threat to our society, as they are used to support misinformation and conspiracy Fig. <ref type="figure">1</ref> Four examples of real-world misleading visualizations <ref type="bibr" target="#b0">[1]</ref> with QA pairs. The correct answer is colored in green, while the wrong answer supported by the misleader is colored in purple.</p><p>theories <ref type="bibr" target="#b2">[3]</ref>. Figure <ref type="figure">1</ref> shows real-world examples of misleading visualizations used to misinform readers on sensitive topics such as access to safe drinking water, politics, COVID-19, or abortion statistics. The CALVI test <ref type="bibr" target="#b7">[8]</ref> demonstrated the impact of misleading visualizations in decreasing human readers' ability to interpret the underlying data accurately.</p><p>The visualization reasoning abilities of multimodal large language models (MLLMs) have advanced rapidly, as demonstrated on the reference benchmark ChartQA <ref type="bibr" target="#b8">[9]</ref>, suggesting their potential as chart reasoning assistants. However, it remains uncertain whether they are sensitive to misleaders, as humans are. If vulnerable, MLLMs used as chart reasoning assistants risk amplifying both the spread of misinformation and human belief in it. Recent work has reported a sensitivity of GPT4 <ref type="bibr" target="#b9">[10]</ref> to misleaders, but their analysis was limited to a single MLLM evaluated on six misleading visualizations <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this study, we compare the question-answering (QA) performance of 16 MLLMs of varying sizes across three datasets: (a) a misleading visualization dataset containing n = 143 instances and featuring 17 distinct misleaders, defined in Table <ref type="table">A1</ref>. It combines the existing synthetic CALVI (n = 45) <ref type="bibr" target="#b7">[8]</ref> and CHARTOM (n = 56) <ref type="bibr" target="#b11">[12]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>datasets with</head><p>The tick values on one axis are not equally spaced, e.g., the ticks are 20, 30, 50.</p><p>False Standard prompting Visualization + Misleader Warning Axes extraction</p><p>Table extraction Redraw visualization Table-based QA False True False False Visualization + Axes + Table False Visualization + Table False Visualization + Axes Standard prompting Our evaluation reveals that misleaders substantially degrade MLLM QA performance, reducing accuracy by up to 34.8 percentage points compared to non-misleading visualizations. To mitigate this vulnerability, we explore six inference-time correction methods that modify the input image and/or prompt without requiring additional training data or fine-tuning. These methods aim to reduce the impact of misleaders while minimizing any negative effect on accuracy for non-misleading visualizations. We compare the following methods, illustrated in Figure <ref type="figure" target="#fig_1">2:</ref> (1) including a warning message in the prompt to alert the MLLM to specific misleaders in the visualization;</p><p>(2) extracting the axes using the MLLM and incorporating them into the prompt; <ref type="bibr" target="#b2">(3)</ref> extracting the underlying data table and incorporating it into the prompt; (4) combining both extracted axes and table into the prompt; (5) providing the extracted table without the visualization to a text-only LLM, reframing the task as table-based QA; and (6) using a text-only LLM to generate code for a new visualization based on the extracted table, replacing the original one as input.</p><p>1 Results First, MLLMs perform substantially worse on misleading visualizations than on non-misleading ones, with accuracy dropping by up to 34.8 percentage points. The decline is even more pronounced compared to ChartQA, reaching up to 65.5 percentage points. Furthermore, the mean MLLM accuracy on misleading visualizations (24.8%) is close to the random baseline (25.6%). This suggests that MLLMs struggle to correctly interpret the distorted data, making them highly vulnerable to misleaders. This aligns with findings from the CALVI study <ref type="bibr" target="#b7">[8]</ref>, where human accuracy averaged 80% on non-misleading visualizations but dropped to 39% on misleading ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Assessing the vulnerability to misleading visualizations</head><p>Second, accuracy on misleading visualizations does not follow the upward performance trend observed in ChartQA, and to a lower extent, non-misleading visualizations. From these results, we conclude that reducing MLLM vulnerability to misleading visualizations will not happen naturally as a by-product of improving performance on standard benchmarks like ChartQA. This makes dedicated mitigation methods all the more necessary.</p><p>Third, the best-performing MLLMs on misleading visualizations, GPT-4o and InternVL2.5-38B, primarily outperform other MLLMs on the real-world subset. We assume this is due to their large parametric knowledge, which spans beyond the end date of the real-world subset (2022). This knowledge about real-world events and statistics allows them to answer some questions without leveraging the visualization. However, this advantage does not extend to synthetic data in CALVI and CHARTOM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Mitigating the impact of misleading visualizations</head><p>We evaluate the impact of the correction methods with three open-weight, mid-sized MLLMs which all perform below the random baseline on misleading visualizations: Qwen2VL-7B, Ovis1.6-9B, and InternVL2.5-8B. Figure <ref type="figure" target="#fig_3">4</ref> presents the change in accuracy across the six correction methods: (1) inclusion of a warning message, inclusion of the (2) axes, (3) table, or (4) both, (5) table-based QA, and (6) redrawing the visualization. Qwen2.5-7B <ref type="bibr" target="#b13">[14]</ref> serves as the text-only LLM for table-based QA and redrawing the visualization.</p><p>The most effective approach by far is table-based QA, yielding significant improvements of 15.4 to 19.6 percentage points. While incorrect or incomplete table extractions decrease performance on non-misleading visualizations -particularly for line charts, scatter plots, and maps -these losses are not significant.</p><p>Another promising correction method is redrawing the visualization. It achieves significant but more modest improvements for two MLLMs. This approach is effective only when the generated code compiles on a Python interpreter; otherwise, it defaults</p><formula xml:id="formula_0">OO D Y D % OO D Y D % T Z H Q Y O % WL Q \ F K D U W % J S W F K D U WL Q V WU X F WL R Q % LQ WH U Q Y O % F K D U WJ H P P D % R Y LV % T Z H Q Y O % LQ WH U Q Y O % LQ WH U Q Y O % R Y LV % J S W R LQ WH U Q Y O % LQ WH U Q Y O % $FFXUDF\ 'DWDVHW 0LVOHDGLQJYLVXDOL]DWLRQV 1RQPLVOHDGLQJYLVXDOL]DWLRQV &amp;KDUW4$ OO D Y D % OO D Y D % T Z H Q Y O % WL Q \ F K D U W % J S W F K D U WL Q V WU X F WL R Q % LQ WH U Q Y O % F K D U WJ H P P D % R Y LV % T Z H Q Y O % LQ WH U Q Y O % LQ WH U Q Y O % R Y LV % J S W R LQ WH U Q Y O % LQ WH U Q Y O % $FFXUDF\ 0LVOHDGLQJYLVXDOL]DWLRQVVXEVHW &amp;$/9, &amp;+$5720 5HDOZRUOG</formula><p>Fig. <ref type="figure" target="#fig_2">3</ref> Top: Accuracy (%) of various MLLMs on misleading visualization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>, non-misleading visualization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, and ChartQA datasets <ref type="bibr" target="#b8">[9]</ref>. The horizontal dashed line indicates the accuracy of the random baseline on misleading visualizations. Models are sorted by increasing accuracy on ChartQA. Bottom: Accuracy (%) of various MLLMs on subsets of the misleading visualizations.</p><p>to using the original visualization. Across both datasets, the lowest redrawing success rates are observed for scatter plots (79%), and stacked bar charts (80%), which we explain by their high number of visual items and complex layouts, while the success rates for all other chart types are above 90%. By either removing or modifying the visualization, these two methods effectively neutralize misleaders that exploit visual perception errors, such as inverted axes and inconsistent tick intervals. In Figure <ref type="figure" target="#fig_1">2</ref>, redrawing the visualization corrects the misleading trend line, eliminating its deceptive elements and ensuring a more accurate representation of the data.</p><p>Other correction methods do not yield significant improvements. Including the extracted table in the prompt even has a significant negative impact on non-misleading visualizations for one MLLM. Adding a warning message provides non-significant changes of at most 4.9 percentage points, with most improvements observed on the real-world subset. This method assumes prior knowledge of the misleader present in the visualization, making the reported results an upper bound on its effectiveness. In practice, a classifier needs to detect first the presence of misleaders, which remains a challenging task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, as MLLMs struggle not only to distinguish non-misleading visualizations from misleading ones but also to correctly identify the specific misleader in a given visualization. Given the already low results obtained with ground-truth misleader labels, this correction method appears unpromising overall. However, training a highly accurate misleader detection model could enable the selective application of other correction methods, eliminating their negative impact on non-misleading visualizations.</p><p>We examine further the quality of the intermediate table extraction step using CHARTOM <ref type="bibr" target="#b11">[12]</ref>, which pairs each question with two visualizations of the same dataone misleading and one non-misleading. Ideally, the extracted tables should be identical for both. However, across all MLLMs, the extracted tables have a perfect match in only 4 out of 56 pairs (7.1%), and a partial match for 13 to 14 other pairs (23 to 25%). Notably, 3D effects achieve their deceptive purpose, as MLLMs systematically produce different tables when the visualization is presented with or without 3D effects. Since table-based QA and redrawing the visualization highly depend on this intermediate table extraction step, improving it is an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discussion</head><p>Our findings highlight the vulnerability of MLLMs to misleading visualizations, reducing their QA performance compared to non-misleading ones, down to the level of the random baseline. To address this vulnerability, we explored six inference-time correction methods. The most effective approach involves extracting the underlying table and performing the QA task using only the table, improving accuracy by up to 19.6 percentage points. While recent work in visualization comprehension with MLLMs primarily treats visualizations as images <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, our findings challenge this direction and underscore the merits of earlier approaches, such as DePlot <ref type="bibr" target="#b19">[20]</ref>, which use table extraction as an intermediate step, followed by table-based QA with a text-only model. By making MLLMs more robust to misleading visualizations, correction methods help ensure that AI-assisted chart reasoning does not yield inaccurate interpretations of the underlying data, reducing the risk that human users believe in and propagate misinformation.</p><p>We identify two limitations to our work. First, the visualization redrawing method, which uses the package Matplotlib, does not support maps, as Matplotlib lacks sufficient functionality for rendering high-quality maps. Second, we assume prior knowledge of the chart type (bar, line, ...) for generating the prompts for axes extraction and visualization redrawing. This is a reasonable assumption, as the chart type can either be provided by a human user or accurately predicted by a classifier as a preprocessing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The misleading and non-misleading visualization datasets combine three existing resources and one introduced in this work. First, CALVI <ref type="bibr" target="#b7">[8]</ref> includes 45 misleading and 15 non-misleading visualizations based on synthetic data, each paired with a multiplechoice question (MCQ) with three to four choices. Second, CHARTOM <ref type="bibr" target="#b11">[12]</ref> contains 56 samples, including 28 MCQs, 20 free-text questions, and 8 ranking questions. The MCQs provide two to four choices. CHARTOM is the only dataset where each question is linked to two visualizations -one misleading and one non-misleading. Like CALVI, the underlying data is synthetic. Third, VLAT <ref type="bibr" target="#b12">[13]</ref>, the reference dataset to evaluate the human comprehension of visualizations, provides 12 non-misleading visualizations, each paired with three to seven MCQs, for a total of 53 instances. The visualizations are based on real-world data. MCQs have two to four choices. Fourth, we introduce a dataset of 42 real-world misleading visualizations, each annotated with a MCQ with three to four choices. The real-world visualizations come from a collection annotated with misleader labels <ref type="bibr" target="#b0">[1]</ref>, which inspired the synthetic examples in CALVI. We manually create MCQs, using those from CALVI and CHARTOM as inspiration. The motivation for creating this additional resource is that CALVI and CHARTOM rely both on synthetic data. By incorporating questions about real-world data, we introduce direct conflicts with MLLMs' parametric knowledge, allowing us to assess their vulnerability in real-world scenarios.</p><p>We also report the performance of MLLMs on the test set of ChartQA <ref type="bibr" target="#b8">[9]</ref>, which contains 2500 real-world visualizations paired with free-text questions, of which half are human-written and the others are AI-generated.</p><p>The datasets cover together 11 chart types: line charts, area charts, stacked area charts, bar charts, stacked bar charts, histograms, pie charts, scatter plots, bubble charts, maps, treemaps.</p><p>For free-text questions where the expected answer is a number, we adopt the relaxed accuracy metric of ChartQA <ref type="bibr" target="#b8">[9]</ref>. Specifically, a prediction is considered correct if it falls within a ±5% interval of the ground truth. For MCQs, we present the choices in their default order. In Table <ref type="table">B2</ref>, we report the mean accuracy of the MLLMs over three different choice orders. The choices are shuffled using three randomly generated numbers as seeds <ref type="bibr">(654,</ref><ref type="bibr">114,</ref><ref type="bibr" target="#b24">25)</ref>. Standard deviations for the accuracy range from 0.0 to 4.06 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference with (M)LLMs</head><p>We evaluate 11 open-weight MLLMs from the Llava-Next <ref type="bibr" target="#b20">[21]</ref>, Qwen2VL <ref type="bibr" target="#b21">[22]</ref>, Ovis-1.6 <ref type="bibr" target="#b22">[23]</ref>, and InternVL2.5 <ref type="bibr" target="#b23">[24]</ref> families, ranging from 2 to 38 billion parameters. Additionally, we include two commercial models with undisclosed number of parameters, GPT-4 and GPT-4o <ref type="bibr" target="#b9">[10]</ref>, as well as three open-weight MLLMs specifically trained for visualization reasoning: ChartInstruction <ref type="bibr" target="#b16">[17]</ref>, TinyChart <ref type="bibr" target="#b17">[18]</ref>, and ChartGemma <ref type="bibr" target="#b18">[19]</ref> ranging from 3 to 13 billion parameters. We use the Hugging Face transformers <ref type="bibr" target="#b24">[25]</ref> implementation for all open-weight (M)LLMs and access GPT models via the Azure OpenAI API. To ensure deterministic outputs, we fix the random seed at 42, set the temperature to 0, and use a top-p value of 1. Following the standard ChartQA evaluation setup, all (M)LLMs are prompted in a zero-shot manner. For TinyChart, we report results using the Direct approach <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation of correction methods</head><p>All prompts are in the code, which is provided as supplementary material. The significance of correction methods is assessed using the McNemar test (p ≤ 0.05) <ref type="bibr" target="#b25">[26]</ref>. The p-values are reported in Table <ref type="table">C3</ref>.</p><p>Misleader warning: we insert in the prompt a short warning message based on the definitions of misleaders. The message is the same for all instances with the same type of misleader. There are no messages for the five types of misleaders where the visualization is deceiving only in the context of a specific question, making a standardized warning message impossible: cherry picking, misleading annotations, concealed uncertainty, missing normalization, and missing data.</p><p>Axes and table extraction: we prompt the MLLM to extract the axes or underlying table in a zero-shot setting. The axes and tables are formatted as text strings. We do not impose constraints on the delimiters used to indicate new rows and columns.</p><p>Table -based QA: the extracted table is provided as input with the question and an instruction to answer it based on the table.</p><p>Redrawn visualization: the text-only LLM receives the extracted table and the chart type as text input and generates Python code to create a visualization using Matplotlib. The code is then executed to produce a new visualization. If the code compiles successfully, the newly generated visualization replaces the original one in the QA prompt; otherwise, the original visualization remains in use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Definition of misleaders</head><p>Table <ref type="table">A1</ref> The 17 types of misleader included in the misleading visualizations dataset, with their number of occurrences (n) and definitions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misleader Definition</head><p>Inverted axis (n=26) An axis is oriented in an unconventional direction and the perception of the data is reversed <ref type="bibr" target="#b0">[1]</ref>. Truncated axis (n=21)</p><p>The axis does not start from zero or is truncated in the middle resulting in an exaggerated difference between the two bars <ref type="bibr" target="#b0">[1]</ref>. Inappropriate axis range (n=15)</p><p>The axis range is either too broad or too narrow to accurately visualize the data, allowing changes to be minimized or maximized depending on the author's intention <ref type="bibr" target="#b0">[1]</ref>. Inconsistent tick intervals (n=12)</p><p>Cases with varying intervals between the ticks <ref type="bibr" target="#b0">[1]</ref>. 3D effects (n=12)</p><p>The closer something is, the larger it appears, despite being the same size in 3D perspective <ref type="bibr" target="#b0">[1]</ref>. Inappropriate item order (n=9)</p><p>The axis labels or legends appear to be in a random order due to manipulation of data ordering <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inappropriate aggregation (n=8)</head><p>Aggregating data in an improper way that leads to inaccurate conclusions <ref type="bibr" target="#b7">[8]</ref>. Dual axis (n=8)</p><p>Two independent axes are layered on top of each other with inappropriate scaling <ref type="bibr" target="#b0">[1]</ref>. Misrepresentation (n=7)</p><p>The value labels provided do not match the visual encoding <ref type="bibr" target="#b0">[1]</ref>. Cherry picking (n=6)</p><p>Selecting only a subset of data to display, which can be misleading if one is asked to infer something about the whole set of data <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Misleading annotations (n=5)</head><p>Annotations that contradict or make it harder to read the visualization <ref type="bibr" target="#b7">[8]</ref>. Area encoding (n=5)</p><p>Linearly encoding the values as areas leads the readers to consistently underestimate the values <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concealed uncertainty (n=3)</head><p>Not displaying uncertainty in visualizations may misrepresent the certainty in the underlying data.</p><p>In the case of prediction making, this can misguide the viewers to falsely overconfident conclusions <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing normalization (n=3)</head><p>Displaying unnormalized data in absolute quantity when normalized data in relative quantity is of interest <ref type="bibr" target="#b7">[8]</ref>. Inappropriate use of pie chart (n=1) When a pie chart is used for non-part-to-whole data, it creates confusion for the audience, who may misinterpret the significance of a given section <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing data (n=1)</head><p>A visual representation implies data exist but the data is actually missing <ref type="bibr" target="#b7">[8]</ref>. Overplotting (n=1)</p><p>Displaying too many things on a plot can obscure parts of the data <ref type="bibr" target="#b7">[8]</ref>.</p><p>Table B2 Mean accuracy and standard deviation (%) when shuffling MCQ choices with three different random seeds. Model Misleading visualizations Non-misleading visualizations llava-7B 24.48 ± 0.70 44.35 ± 2.79 llava-13B 24.47 ± 2.42 37.36 ± 4.06 qwen2vl-2B 24.71 ± 3.85 38.44 ± 1.23 tinychart-3B 22.15 ± 1.76 22.04 ± 1.23 gpt4 30.54 ± 2.14 47.58 ± 2.91 chartinstruction-13B 27.97 ± 2.52 34.14 ± 2.33 internvl2.5-2B 19.58 ± 3.21 43.28 ± 1.23 chartgemma-3B 22.61 ± 2.82 26.61 ± 2.91 ovis1.6-9B 23.55 ± 0.40 52.15 ± 2.83 qwen2vl-7B 20.75 ± 2.25 48.93 ± 3.26 internvl2.5-4B 19.35 ± 2.25 46.50 ± 2.46 internvl2.5-8B 22.61 ± 1.07 55.38 ± 1.68 ovis1.6-27B 28.44 ± 0.40 53.76 ± 1.23 gpt4o 37.76 ± 0.00 59.95 ± 1.86 internvl2.5-26B 20.05 ± 2.46 55.38 ± 1.86 internvl2.5-38B 32.64 ± 1.07 62.63 ± 0.46 Appendix B Impact of choices order in MCQs Appendix C P-values of significance tests</p><p>Table C3 P-values for the McNemar test assessing the significance of the correction methods. Significant results (p≤0.05) are marked in bold. Method Model Misleading p-value Non-misleading p-value qwen2vl 1.00 1.00 Misleader warning ovis1.6 0.17 0.25 internvl2.5 1.00 1.00 qwen2vl 0.58 0.63 Axes ovis1.6 0.38 0.08 internvl2.5 0.65 0.10 qwen2vl 0.21 1.00 Table ovis1.6 0.40 0.04 internvl2.5 0.80 0.66 qwen2vl 0.17 1.00 Table + Axes ovis1.6 1.00 0.09 internvl2.5 1.00 1.00 qwen2vl 1e-4 0.14 Table-based QA ovis1.6 3e-3 0.07 internvl2.5 5e-5 0.34 qwen2vl 0.01 0.45 Redrawn visualization ovis1.6 2e-4 0.45 internvl2.5 0.68 0.33</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig.2Illustration of the six inference-time correction methods applied on a misleading visualization from CALVI<ref type="bibr" target="#b7">[8]</ref>. The visualization suffers from inconsistent tick intervals on the y-axis. The correct answer is True. In this case, it is only predicted after redrawing the visualization.</figDesc><graphic coords="3,301.75,286.01,88.86,80.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 presents the QA accuracy of 16 MLLMs across the three datasets. The results reveal three key trends.First, MLLMs perform substantially worse on misleading visualizations than on non-misleading ones, with accuracy dropping by up to 34.8 percentage points. The decline is even more pronounced compared to ChartQA, reaching up to 65.5 percentage points. Furthermore, the mean MLLM accuracy on misleading visualizations (24.8%) is close to the random baseline (25.6%). This suggests that MLLMs struggle to correctly interpret the distorted data, making them highly vulnerable to misleaders. This aligns with findings from the CALVI study<ref type="bibr" target="#b7">[8]</ref>, where human accuracy averaged 80% on non-misleading visualizations but dropped to 39% on misleading ones.Second, accuracy on misleading visualizations does not follow the upward performance trend observed in ChartQA, and to a lower extent, non-misleading visualizations. From these results, we conclude that reducing MLLM vulnerability to misleading visualizations will not happen naturally as a by-product of improving performance on standard benchmarks like ChartQA. This makes dedicated mitigation methods all the more necessary.Third, the best-performing MLLMs on misleading visualizations, GPT-4o and InternVL2.5-38B, primarily outperform other MLLMs on the real-world subset. We assume this is due to their large parametric knowledge, which spans beyond the end date of the real-world subset (2022). This knowledge about real-world events and statistics allows them to answer some questions without leveraging the visualization. However, this advantage does not extend to synthetic data in CALVI and CHARTOM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Change (∆) in accuracy (percentage points) on misleading and non-misleading visualization datasets using different inference-time correction methods to mitigate misleading visualizations. Statistically significant changes (p≤0.05) are hashed.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work has been funded by the <rs type="funder">LOEWE initiative (Hesse, Germany)</rs> within the emergenCITY center (Grant Number: <rs type="grantNumber">LOEWE/1/12/519/03/05.001(0016)/72</rs>) and by the <rs type="funder">German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science</rs> and the Arts within their joint support of the <rs type="funder">National Research Center for Applied Cybersecurity ATHENE</rs>. We gratefully acknowledge the support of <rs type="funder">Microsoft</rs> with a grant for access to OpenAI GPT models via the Azure cloud (<rs type="funder">Accelerate Foundation Model Academic Research)</rs>. We want to express our gratitude to <rs type="person">Niklas Traser</rs> for conducting an initial exploration of the real-world data, to <rs type="person">Jan Zimny</rs> for our insightful discussions on the topic of misleading visualizations, and to <rs type="person">Germàn Ortiz</rs>, <rs type="person">Manisha Venkat</rs>, and <rs type="person">Max Glockner</rs> for their feedback on a draft of this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SaxNEsW">
					<idno type="grant-number">LOEWE/1/12/519/03/05.001(0016)/72</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Misinformed by visualization: What do we learn from misinformative visualizations?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shigyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14559</idno>
		<ptr target="https://doi.org/10.1111/cgf.14559.WileyOnlineLibrary" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="515" to="525" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How deceptive are deceptive visualizations? an empirical analysis of common distortion techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Satterthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702608</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702608" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. CHI &apos;15</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems. CHI &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1469" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Misleading beyond visual tricks: How people actually lie with charts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lisnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Polychronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kogan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580910</idno>
		<ptr target="https://doi.org/10.1145/3544548.3580910" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI &apos;23. Association for Computing Machinery</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems. CHI &apos;23. Association for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">i came across a junk&quot;: Understanding design flaws of data visualization from the public&apos;s perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2024.3456341</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2024.3456341" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="393" to="403" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Surfacing visualization mirages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376420</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376420" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. CHI &apos;20</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems. CHI &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The deceptive potential of common design tactics used in data visualizations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>O'brien</surname></persName>
		</author>
		<idno type="DOI">10.1145/3380851.3416762</idno>
		<ptr target="https://doi.org/10.1145/3380851.3416762" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th ACM International Conference on Design of Communication. SIGDOC &apos;20</title>
		<meeting>the 38th ACM International Conference on Design of Communication. SIGDOC &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Truncating bar graphs persistently misleads viewers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vargas Restrepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Marsh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jarmac.2020.10.002</idno>
		<ptr target="https://doi.org/10.1016/j.jarmac.2020.10.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Research in Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="311" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Calvi: Critical thinking assessment for literacy in visualizations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3581406</idno>
		<ptr target="https://doi.org/10.1145/3544548.3581406" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. CHI &apos;23</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems. CHI &apos;23<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ChartQA: A benchmark for question answering about charts with visual and logical reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.177</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.findings-acl.177" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Villavicencio</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2263" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<title level="m">OpenAI: Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical evaluation of the gpt-4 multimodal language model on visualization literacy tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bendeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2024.3456155</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2024.3456155" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1105" to="1115" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Chartom: A visual theoryof-mind benchmark for multimodal large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.14419</idno>
		<idno type="arXiv">arXiv:2408.14419abs/2408.14419</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2408.14419" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vlat: Development of a visualization literacy assessment test</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598920</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2016.2598920" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="551" to="560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Team</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.15115</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2412.15115" />
		<title level="m">Qwen2.5 technical report</title>
		<imprint>
			<publisher>Alibaba Cloud</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How good (or bad) are llms at detecting misleading visualizations?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2024.3456333</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2024.3456333" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1116" to="1125" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can gpt-4 models detect misleading visualizations?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarvghad</surname></persName>
		</author>
		<idno type="DOI">10.1109/VIS55277.2024.00029</idno>
		<ptr target="https://doi.org/10.1109/VIS55277.2024.00029" />
	</analytic>
	<monogr>
		<title level="m">2024 IEEE Visualization and Visual Analytics (VIS)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advancing multimodal large language models in chart question answering with visualization-referenced instruction tuning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2024.3456159</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2024.3456159" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="525" to="535" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TinyChart: Efficient chart understanding with program-of-thoughts learning and visual token merging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.emnlp-main.112</idno>
		<ptr target="https://doi.org/10.18653/v1/2024.emnlp-main.112" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Al-Onaizan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</editor>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1882" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ChartGemma: Visual instruction-tuning for chart reasoning in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Masry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kartha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schockaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Darwish</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2025.coling-industry.54" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Computational Linguistics: Industry Track</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Computational Linguistics: Industry Track<address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="625" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DePlot: One-shot visual language reasoning by plot-to-table translation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-acl.660</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.findings-acl.660" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10381" to="10399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52733.2024.02484</idno>
		<ptr target="https://doi.org/10.1109/CVPR52733.2024.02484" />
	</analytic>
	<monogr>
		<title level="m">2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26286" to="26296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2409.12191</idno>
		<idno type="arXiv">arXiv:2409.12191abs/2409.12191</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2409.12191" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.20797</idno>
		<idno type="arXiv">arXiv:2405.20797abs/2405.20797</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.20797" />
		<title level="m">Ovis: Structural embedding alignment for multimodal large language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.05271</idno>
		<idno type="arXiv">arXiv:2412.05271abs/2412.05271</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2412.05271" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schlangen</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Note on the sampling error of the difference between correlated proportions or percentages</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mcnemar</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02295996</idno>
		<ptr target="https://doi.org/10.1007/BF02295996" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
