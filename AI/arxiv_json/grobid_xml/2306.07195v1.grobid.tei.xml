<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_kpuuM5A #_jTntuWX">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D9B4C5DFE0EAB67122DA33D7F9B3B68</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recursion is one of the hallmarks of human language. While many design features of language have been shown to exist in animal communication systems, recursion has not. Previous research shows that GPT-4 is the first large language model (LLM) to exhibit metalinguistic abilities <ref type="bibr" target="#b12">(Beguš, Dąbkowski, and Rhodes, 2023)</ref>. Here, we propose several prompt designs aimed at eliciting and analyzing recursive behavior in LLMs, both linguistic and non-linguistic. We demonstrate that when explicitly prompted, GPT-4 can both produce and analyze recursive structures. Thus, we present one of the first studies investigating whether meta-linguistic awareness of recursion-a uniquely human cognitive property-can emerge in transformers with a high number of parameters such as GPT-4.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">introduction</head><p>Recursion is a process or pattern that repeats in a self-similar or self-referential manner. In linguistics, recursion refers to the embedding of phrases within phrases of the same type. This provides linguistic units with potentially infinite layers of depth and complexity.</p><p>Recursion is-to the best of our knowledge-one of the few properties entirely unique to human language <ref type="bibr" target="#b22">(Hockett, 1960)</ref>. Despite previous claims to the contrary <ref type="bibr" target="#b18">(Fitch and Hauser, 2004;</ref><ref type="bibr" target="#b19">Gentner et al., 2006)</ref>, no other animal communication system has been convincingly shown to feature recursion <ref type="bibr" target="#b11">(Beecher, 2021;</ref><ref type="bibr" target="#b15">Corballis, 2007)</ref>. For this reason, recursion has become one of the most widely studied, but also hotly debated aspects of language <ref type="bibr" target="#b14">(Chomsky, 2014;</ref><ref type="bibr" target="#b21">Hauser, Chomsky, and</ref><ref type="bibr">Fitch, 2002 vs. Everett, 2005;</ref><ref type="bibr" target="#b24">Jackendoff and Pinker, 2005)</ref>.</p><p>At the same time, recent advances in the development of large language models (henceforth LLMs)-notably OpenAI's (2023) release of GPT-4-have sparked a discussion about the nature of LLMs' generativity and its relation to the human language faculty. <ref type="bibr" target="#b20">Haider (2023)</ref> demonstrates that ChatGPT behaves like a native speaker with respect to a number of English and German grammatical competence tasks. <ref type="bibr" target="#b29">Piantadosi (2023)</ref> argues that the success of LLMs, capable of remarkable language performance despite a lack of innate biases, refutes <ref type="bibr" target="#b13">Chomsky's (1957)</ref> nativist theory of language. <ref type="bibr" target="#b25">Katzir (2023)</ref> counters by pointing out that LLMs do not (i) develop human-like linguistic constraints, (ii) derive the attested linguistic typology, (iii) show competence beyond initial performance, (iv) distinguish between likelihood and grammaticality, or (v) reason in a human-like way. <ref type="bibr" target="#b23">Hu and Levy (2023)</ref> show that LLMs' probability distributions over strings is a better measure of language performance than prompting (i. e. prompt-based methodologies may underestimate the models' language competence). <ref type="bibr" target="#b12">Beguš, Dąbkowski, and Rhodes (2023)</ref> demonstrate that GPT-4 may be the first LLM capable not only of using language, but also being able to perform meta-linguistic tasks by applying linguistic theory to construct valid analyses.</p><p>In this paper, we connect the two lines of research by asking whether two recent GPT models (GPT-4 and GPT-3.5) are capable of producing, identifying, and analyzing recursive structures. Previous research focuses on LLMs' implicit capacity for recursion. For example, <ref type="bibr" target="#b26">Lakretz, Desbordes, et al. (2022)</ref> and <ref type="bibr" target="#b27">Lakretz, Hupkes, et al. (2021)</ref> look at the output probabilities for correct subject-verb agreement in embedded dependencies. They find that the performance of LLMs on long-range dependencies is below chance and conclude that the syntactic processing in transformers is much less robust than in humans. Our methodology is novel in that we ask the models to generate and analyze recursive structures explicitly.</p><p>We design prompts aimed at eliciting and analyzing recursion in the domains of language and visual representation. We find that GPT-4 largely succeeds on the tasks in both domains. We compare the performance of GPT-4 with GPT-3.5. We find that the older model succeeds only on the simpler linguistic tasks and more often fails at generating recursive images. We conclude that GPT-4 may be the first LLM capable of general recursive reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">linguistic recursion</head><p>In this section, we test GPT-4's performance on tasks related to linguistic recursion. The tasks involve defining and illustrating recursion, adding more layers of embedding, drawing syntactic trees which capture recursive structure, and writing code that mimics linguistic recursion. We find that GPT-4 performance on the tasks is very good, although the model makes mistakes in syntactic analyses of recursive structures. We compare the performance of GPT-4 and GPT-3.5. We find that while GPT-3.5 can illustrate recursion and has a limited ability to embed clauses, only GPT-4 succeeds on the more elaborate tasks.</p><p>First, we prompted the model to provide a definition of recursion and illustrate it (1). 1 GTP-4's answer is correct and fully satisfactory.</p><p>(1) What is recursion in natural language? Define and illustrate.</p><p>Recursion in natural language is a property that allows a sentence or a phrase to be expanded indefinitely by embedding the same structure within itself. This linguistic principle is one of the foundations of generative grammar, a theory of language pioneered by Noam Chomsky.</p><p>To understand this better, let's consider a simple sentence:</p><p>• "John likes pizza." Now, let's embed another sentence structure within this sentence:</p><p>• "John, who lives in New York, likes pizza."</p><p>We've inserted another sentence or clause ("who lives in New York") within the original sentence. This clause has the same structure as a sentence -it has a subject ("who") and a verb ("lives").</p><p>And you can keep embedding more structures indefinitely:</p><p>• "John, who lives in New York, which is a city in the United States, likes pizza."</p><p>1 Some of the reported prompts and replies have been edited and simplified for the sake of conciseness and ease of presentation. For full documentation of the chat data presented here, see <ref type="bibr" target="#b16">Dąbkowski and Beguš (2023)</ref>.</p><p>• "John, who lives in New York, which is a city in the United States, that was discovered by the Europeans, likes pizza."</p><p>• ...and so on.</p><p>We then asked the model to identify and label instances of recursion in eleven sentences <ref type="bibr" target="#b5">(2)</ref>. GPT-4 performance is often perfect; the model correctly labels all the non-recursive sentences, and correctly classifies each instance of recursion in the data set.</p><p>(2) For each of the sentences below, tell me if the sentence contains an instance of recursion. If so, identify it and say what kind of recursion it is. Remember that there are different types of recursion.</p><p>In addition to clausal recursion, there is also recursive application of adjectival modification, recursive adverbial modification, recursive genitive (possessive) recursion, and so on.</p><p>1. Mark said hi.</p><p>2. Mark said that Jolene said hi.</p><p>3. I saw a big blue elephant.</p><p>4. I saw a big elephant.</p><p>5. Martha's dog barked.</p><p>6. A cat bit Martha's dog's tail. 7. The light flashed quickly. 8. The light flashed incredibly quickly. 9. A boy who had a lollipop yelled my name. 10. A boy yelled my name. 11. A boy who I saw a girl liked yelled my name. 1. "Mark said hi." -This sentence does not contain any form of recursion. 2. "Mark said that Jolene said hi." -This sentence contains clausal recursion, with one clause embedded inside another ("Mark said" containing "that Jolene said hi"). 3. "I saw a big blue elephant." -This sentence contains recursive adjectival modification. The adjective "big" modifies "blue elephant", and "blue" further modifies "elephant". 4. "I saw a big elephant." -This sentence does not contain any form of recursion. 5. "Martha's dog barked." -This sentence does not contain any form of recursion. 6. "A cat bit Martha's dog's tail." -This sentence contains recursive genitive (possessive) recursion. "Martha's dog's tail" involves two layers of possessive marking: Martha possessing the dog, and the dog possessing the tail. 7. "The light flashed quickly." -This sentence does not contain any form of recursion. 8. "The light flashed incredibly quickly." -This sentence contains recursive adverbial modification. The adverb "incredibly" modifies another adverb "quickly".</p><p>The model correctly adds layers of causal complementation (3.2), adjectival modification <ref type="bibr">(3.3)</ref>, adverbial modification <ref type="bibr">(3.6)</ref>, possession <ref type="bibr">(3.8)</ref>, and clausal modification <ref type="bibr">(3.9,11)</ref>. We bring special attention to the last example <ref type="bibr">(3.11)</ref>, where GPT-4 constructs a triply center-embedded sentence.</p><p>Center embedding, while grammatical, is difficult to process. As such, center-embedding sentences are exceedingly rare and were most likely absent from (or only sparsely present in) the model's training data. GPT-4's ability to construct center-embedded sentences without being explicitly prompted to do so thus suggests that the model acquired grammatical structure beyond simple distributional tendencies of its training data set.</p><p>To test the consistency of the model's responses, we repeated the same prompt multiple times. GPT-4 makes occasional errors. However, when prompted to evaluate its own reply, the model often identifies the problems and fixes them. On one occasion, GPT-4 attempts to increase the depth of possessive embedding by adding an adjective instead of a Saxon genitive (4). When prompted to evaluate its answer, the model recognizes it mistakenly changed the type of recursion (instead of adding depth to the existing type) and corrects its response ( <ref type="formula">5</ref>).</p><p>(4) For each of the sentences you identified as recursive, first add one layer of recursion, and ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>A cat bit Martha's dog's tail.</p><p>• One layer added: "A cat bit Martha's dog's fluffy tail."</p><p>• Two layers added: "A cat bit Martha's dog's fluffy tail's tip."</p><p>(5) Are there any mistakes in your previous response? If so, correct them.</p><p>My apologies for the confusion. In an effort to maintain the same kind of recursion, some of my examples changed type of recursion rather than added depth to the existing type. Let's correct these:</p><p>6. A cat bit Martha's dog's tail.</p><p>• One layer added: "A cat bit Martha's neighbor's dog's tail."</p><p>• Two layers added: "A cat bit Martha's neighbor's brother's dog's tail."</p><p>We then asked GPT-4 to draw trees for some of the simpler recursive structures it generated using the forest package <ref type="bibr">(Živanović, 2017)</ref> in L A T E X <ref type="bibr" target="#b7">(6)</ref>. The analyses provided by the model are generally correct, although there are minor mistakes. For example, the model represents proper nouns and pronouns and unary branching NP → N structures (as opposed to treating them as lexical NPs). In (6b), GPT-4 labels each projection of adjectival adjunction as AP (instead of the correct N'). Despite these errors, however, the model correctly represents the hierarchical recursive structure of the sentences at hand. <ref type="bibr" target="#b7">(6)</ref> Using the forest package in L A T E X, draw a syntax tree for the sentence: GPT-4 has a harder time drawing correct syntactic trees for some of the other sentences. For example, when prompted to represent possessive recursion, the model generates an incorrect flat structure (7a). However, the model is capable of refining its answers. Simply asking it if the tree is correct results in a remarkable improvement; GPT-4 redraws the tree with the complex possessor represented as a left-branching structure (7b). The tree still leaves much to be desiredthe possessive clitic 's is represented as a node dominating the possessor and the input chunk dog's tail has been turned into a compound noun dog tail. At last, prompting the model to evaluate the second tree results in another round of improvement, with each level or recursive possession represented as left-branching and binary (7c).</p><p>(7) a. Using the forest package in L A T E X, draw a syntax tree for the sentence "A cat bit Martha's neighbor's sister's dog's tail." Assume X-bar theory.</p><p>b. Is the tree you drew correct? Finally, we prompted the model to write a piece of code isomorphic to how recursion works in natural languages. GPT-4 succeeded on the task by writing a function that models English recursive speech report <ref type="bibr">(9)</ref>. ( <ref type="formula">9</ref>) Write a piece of recursive code that is isomorphic to how recursion works in natural languages.</p><p>In natural languages, recursion is often represented in nested structures like subordinate clauses. For instance, consider this sentence:</p><p>"Alice said that Bob said that Charlie said that he didn't take the cake."</p><p>The structure of this sentence can be represented recursively as "Person said that [sentence]". This pattern can continue to nest indefinitely.</p><p>Here's a Python function that simulates this kind of recursion:</p><p>def recursive_sentence(names, message, depth=0):</p><p># Base case: if depth == 0, return the message if depth == 0: return message # Recursive case: "{name} said that {sentence}" else:</p><p>return f"{names[depth % len(names)]} said that {recursive_sentence(names, message, depth -1)}" names = ["Alice", "Bob", "Charlie"] message = "he didn't take the cake" print(recursive_sentence(names, message, 3)) # Output: "Alice said that Bob said that Charlie said that he didn't take the cake"</p><p>In interim summary, GPT-4 is capable of identifying, producing, and analyzing recursive structures.</p><p>While GPT-4's performance is not always perfect, the model can often auto-correct when prompted to evaluate its own output.</p><p>Next, we compared the performance of GPT-4 with an older version of the model, GPT-3.5. GPT-3.5 performs well on the basic tasks of defining and illustrating recursion <ref type="bibr" target="#b1">(10)</ref><ref type="bibr">(11)</ref>. <ref type="bibr" target="#b1">(10)</ref> What is recursion in natural language? Define and illustrate.</p><p>Recursion in natural language refers to the linguistic phenomenon where a sentence or phrase contains a repeated structure or pattern within itself. It involves the repetition of a particular linguistic element or construction within a larger linguistic context. In simpler terms, recursion is a form of self-embedding, where a linguistic unit is embedded within another unit of the same kind.</p><p>To illustrate recursion, let's consider an example sentence: "The cat that caught the mouse that scared the dog that chased the squirrel that climbed the tree..."</p><p>In this sentence, the relative clause "that caught the mouse" is embedded within the main clause "The cat," creating a recursive structure. Furthermore, the relative clause itself contains another embedded clause "that scared the dog," which is embedded within the previous relative clause. This pattern of embedding continues as each new clause is introduced.</p><p>(11) Write a complete recursive sentence with multiple levels of recursion.</p><p>"The book that the librarian recommended to the student who borrowed it from the library that the author praised for its comprehensive content that inspired the professor to write a research paper that the journal published and gained recognition for its insightful analysis that influenced the readers to explore new perspectives on the subject."</p><p>However, GPT-3.5 is markedly inferior to the latest model when it comes to identifying and labeling instances of recursion. Given the same (below elided) prompt as GPT-4 in ( <ref type="formula">2</ref>), GPT-3.5 fails on many of the same example sentences that GPT-4 succeeded at (12).</p><p>(12) For each of the sentences below, tell me if the sentence contains an instance of recursion. If ...</p><p>The model correctly identifies clausal recursion in (12.2) and (12.11), as well as lack of recursion in (12.1), (12.4), (12.5), (12.7), and (12.10). However, GPT-3.5 fails to note adjectival recursion in (12.3) and adverbial recursion in (12.8). The model recognizes possessive recursion in (12.6), but gives a garbled explanation of the recursive structure, saying that "the possessive form Martha's" is "applied twice to indicate the possession relationship between Martha, dog, and tail." GPT-3.5 observes recursion in (12.9), but calls it "recursive adjectival modification." The model correctly notes that the "phrase who had a lollipop ... modifies the noun boy," but also splices in a comment about a previous sentence, saying that the "adjective big modifies the noun elephant."</p><p>We then asked GPT-3.5 to add additional layers of recursion to all the sentences previously identified as recursive (13). Here again, we see that GPT-3.5 is outperformed by GPT-4.</p><p>(13) For each of the sentences you identified as recursive, first add one layer of recursion, and then add another one. Make sure that in adding layers of recursion, you embed structures of the same type. For example, if you identified possessive recursion, add an additional layer of possession. If you identified causal recursion, add a further embedded clause, etc.</p><p>2. Mark said that Jolene said that Mark said hi. This sentence contains clausal recursion. The main clause "Mark said" embeds the subordinate clause "that Jolene said," and within that subordinate clause, there is another embedded clause "that Mark said hi."</p><p>6. A cat bit Martha's dog's tail's end. This sentence contains recursive genitive (possessive) recursion. The phrase "Martha's dog's tail" involves the possessive form "Martha's" applied twice. By adding another layer, we have "tail's end," where the possessive form "tail's" is applied to indicate the end of the tail. 9. A boy who had a lollipop that a girl who had a hat gave yelled my name. This sentence contains recursive adjectival modification and clausal recursion. The adjective phrase "who had a lollipop" modifies the noun "boy," and within that phrase, there is another embedded clause "that a girl who had a hat gave."</p><p>First, despite having identified four recursive sentences (12. <ref type="bibr" target="#b5">2,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr">9,</ref><ref type="bibr">11)</ref>, GPT-3.5 only attempts the task on the first three of them. Second, the model ignores the request to "first add one layer of recursion, and then add another one;" instead, the model only adds one layer of recursion. GPT-3.5 correctly adds further embedding in (13.2) and (13.6). However, in (13.6), the model still claims that the "phrase Martha's dog's tail involves the possessive form Martha's applied twice." In (13.9), GPT-3.5 attempts to embed another relative clause but fails to do. The model generates the ungrammatical "*A boy who had a lollipop that a girl who had a hat gave yelled my name," where the ditransitive verb give is used monotranstively. (To make the sentence grammatical, one could, for example, use a pronoun, i. e. "A boy who had a lollipop that a girl who had a hat gave him yelled my name.")</p><p>Next, we tested GPT-3.5's performance on generating tree diagrams which represent syntactic structures. The model succeeds on simpler structures, such as those discussed in ( <ref type="formula">6</ref>). (GPT-3.5's outputs are not presented here for reasons of brevity.) However, the model fails at left-branching possessive recursion. When first prompted to draw a syntax tree for the sentence "A cat bit Martha's neighbor's sister's dog's tail," GPT-3.5 generates a right-branching structure and fails to identify the terminal nodes in the recursive NP (14a). (Also observe other errors. For example, the top node is labeled as TP, branching into a single DP. The verb is analyzed as rising to T and no V node is given. Instead, the unary-branching VP dominates the object DP.) Prompting the model to evaluate its previous response does not result in improved performance. When asked whether the tree is correct, GPT-3.5 generates another right-branching structure (14b). When asked to evaluate the tree again, the model generates a baroque confused mess (14c).</p><p>(14) a. Using the forest package in L A T E X, provide code for a syntax tree for the sentence: "A cat bit Martha's neighbor's sister's dog's tail." Assume X-bar theory.</p><p>b. Is the tree you drew correct?</p><p>c. Is the tree you drew correct?</p><p>a. b. c. TP DP DP D A NP cat T ′ T bit VP DP D N NP NP D N NP D N NP D N NP D N NP tail TP DP DP D A NP cat T ′ T bit VP DP DP D N NP Martha's N ′ N neighbor's NP N ′ N sister's NP N ′ N dog's NP tail TP DP D ′ D A NP cat T ′ T bit VP DP D ′ D N NP DP D ′ D N NP DP D ′ D N NP D ′ D N NP tail N ′ N dog's NP DP D ′ D N NP sister's N ′ N neighbor's NP DP D ′ D N NP Martha's N ′ N A NP cat</p><p>Finally, we tested GPT-3.5's performance on capturing language structure with recursive code. When prompted to generate recursive code, GPT-3.5 implements a factorial function similar to GPT-4's (again, not reported here for brevity). However, when prompted to write a piece of code that is isomorphic natural language recursion (15), the model fumbles and generates a function that takes "Hello, how are you?" as input and returns "Hello, how are you?"</p><p>(15) Write a piece of recursive code that is isomorphic to how recursion works in natural languages.</p><p>def speak_recursively(sentence): if len(sentence) == 0: return "" else: first_word = sentence[0] rest_of_sentence = sentence[1:] return first_word + " " + speak_recursively(rest_of_sentence) Here's an example usage of the speak_recursively function: sentence = "Hello, how are you?" result = speak_recursively(sentence) print(result)</p><p>Output:</p><p>Hello, how are you?</p><p>In summary, we tested GPT-4 and GPT-3.5 on a number of language-related tasks involving explicit command of recursion. GPT-4 succeeds at defining, illustrating, identifying different types of recursion, and generating deeper levels of syntactic embedding. The model is capable of providing theoretical analyses of various syntactic structures and writing code that mimics linguistic recursion.</p><p>While GPT-4 occasionally falters, the model is capable of self-correction-oftentimes simply asking it to evaluate its previous answer results in a dramatic improvement.</p><p>GPT-3.5 succeeds at defining and illustrating linguistic recursion. However, the model struggles with identifying different types of recursion and generating further layers of recursive structure. GPT-3.5 only seems capable of providing syntactic analyses of the simpler structures. The model does not capture linguistic recursion with computer code. When GPT-3.5 fails at a task, prompting the model to evaluate its output seldom improves performance, and often has the opposite effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">visual recursion</head><p>In this section, we test GPT-4 and GPT-3.5's performance on a number of tasks related to recursion in the visual domain by prompting the models to generate recursive images and diagrams. A recursive image is an image that contains the entire image as a part of itself. We find that GPT-4 succeeds on generating consistently recursive art. The outputs of GPT-3.5 appear more creative regarding the range of generated structures, more often fail to exhibit recursion.</p><p>First, we prompted GPT-4 to produce recursive ASCII art. We repeated each prompt several times to observe the diversity of structures generated by each model. When asked for a recursive ASCII diagram, the model outputs variations on a binary-branching tree (16).</p><p>We then prompted GPT-4 to draw recursive ASCII images (17). The model often responds with variations on the Sierpiński triangle (17a-b). Other structures include a rectangle with smaller rectangles protruding rightwards (17c), a Christmas-like tree (17d), a rhomboidal fractal, and a wave of increasing amplitude (17e). GPT-4's performance on the task is good; most of the images-perhaps with the exception of (17d)-are clearly recursive. While Sierpiński triangles are almost certainly present in the training and therefore likely memorized, the other structures generated by the model are possibly novel. <ref type="foot" target="#foot_1">2</ref>Next, we tested the model's performance with the charting and diagramming plugin Show Me.</p><p>The plugin works by taking the user's request for a visual representation of a concept or process and generating a diagram using Mermaid syntax. To test the plugin's functionality, we repeatedly asked the model for recursive diagrams (18). GPT-4 succeeds on the task and generates a wide variety thereof. The model's outputs include a simple loop (18a), a basic recursive function schema (18b), a simple simple representation of factorialization (18c), a more complex representation of factorialization (18d), and a binary-branching tree (18e). d. e.</p><p>Our last task for GPT-4 was to draw a recursive image using the tikz package <ref type="bibr" target="#b31">(Tantau, 2007)</ref> in L A T E X. The model struggles with the task and requires multiple rounds of reprompting. Oftentimes the images generated by the model's code are not recursive, are trivially recursive, or do not compile. Some replies, however, are remarkably successful. In (19), GPT-4 outputs tikz code which contains smaller X is one-third the size of the larger one, following a simple recursive pattern." GPT-4 considers the pattern in (17f) recursive "because it reproduces the original X in each subsequent line, and if you continue the pattern, it will repeat itself infinitely." Sometimes the model's explanations are more successful. For example, GPT-4 characterized the "smaller diamonds" in as "recursively placed within the larger diamond." For complete responses of both models to each of the discussed queries, see <ref type="bibr" target="#b16">Dąbkowski and Beguš (2023)</ref>.</p><p>the recursive function fractal producing an image with seven levels of ever-smaller triangles. The compiled image is given on the right.</p><p>(19) Draw a recursive image using L A T E X's tikz package.</p><p>\begin{tikzpicture} \draw[very thick] (0,0) --(0,2); \newcommand{\Y}[2]{ \draw[#1] (0,0) --(45:#2) --(-45:#2) --cycle; } \newcommand{\fractal}[2]{ \Y{scale=#2}{#2} \ifnum#1&gt;0 \begin{scope}[shift={(45:#2)},rotate=45] \fractal{\the\numexpr#1-1\relax}{#2/2} \end{scope} \begin{scope}[shift={(-45:#2)},rotate=-45] \fractal{\the\numexpr#1-1\relax}{#2/2} \end{scope} \fi } \begin{scope}[yshift=2cm] \fractal{7}{2} \end{scope} \end{tikzpicture}</p><p>In interim summary, GPT-4 is capable of generating recursive images and diagrams using ASCII, as well as Mermaid and tikz syntax. The tikz package is the most versatile tool for diagramming and at the same time most challenging for the model.</p><p>Finally, we compared the performance of GPT-4 with a previous iteration of the model, GPT-3.5. When asked to draw a recursive ASCII diagram (20), GPT-3.5 comes up with a wider range of structures than GPT-4. In (20a), the model generates a classic Sierpiński triangle. In (20b), the model outputs four connected rectangles. In (20c), the model replies with a honeycomb-like structure.</p><p>While GPT-3.5's diagrams are generally more varied than GPT-4's, the generated structures are often non-recursive (e. g. 20b).</p><p>Next, we prompted GPT-3.5 to draw recursive ASCII images (21). Here again, the model's creations are more diverse and creative than those of GPT-4. While GPT-3.5 often generates Sierpiński triangles (21a), and other triangles (21b), the model also outputs a "house" (a trapezoid on top of a rectangle) within a "house" (21c), a vaguely cloud-like shape (21d), nested rectangles (21e), and two trapezoids within a "house" (a rectangle with yet another trapezoid on top) (21f). Note, however, that although GPT-3.5's outputs are more diverse than GPT-4's, only the (likely memorized) Sierpiński triangle d. e. f.</p><formula xml:id="formula_0">_ _ _ _ _ _ / \ _ _ _ / \ _ _ _ / / \ / \ \ _ _ / / \ / \ \ _ _ / \ _ _ / _ _ _ _ _ \ _ _ / \ _ / \ \ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _</formula><p>_ / ********* * * * ***** * * * * * * * * * * * * * * * ***** * * * *********</p><formula xml:id="formula_1">_ _ _ _ _ _ / \ / \ / \ / \ / _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ | | | _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ | | / \ / \| | / \ / | | / _ _ _ _ _ _ _ _ _ _ _ _ V _ _ _ _ _ _ _ _ _ _ |</formula><p>Finally, we prompted GPT-3.5 to draw a recursive image in tikz. Similarly to the newer model, GPT-3.5 struggles with tikz syntax and most of its outputs do not compile without errors. When successful upon reprompting, the model outputs code which makes use of a recursive function to generate an image of a triangle made up of green upward-pointing arrows. Notwithstanding, while the code itself is indisputably recursive, the resulting image is not.</p><p>(22) Draw a recursive image using L A T E X's tikz package.</p><p>\begin{tikzpicture} % Define the recursive function for drawing the fractal tree \def\recursive#1#2{ \ifnum#1&gt;0 \draw[green] (0,0) --(0, #2); \draw[green] (0, #2) --(-0.5 * #2, 0.8 * #2); \draw[green] (0, #2) --(0.5 * #2, 0.8 * #2); \recursive{\numexpr#1-1\relax}{0.8 * #2} \begin{scope}[shift={(-0.5 * #2, 0.8 * #2)}] \recursive{\numexpr#1-1\relax}{0.6 * #2} \end{scope} \begin{scope}[shift={(0.5 * #2, 0.8 * #2)}] \recursive{\numexpr#1-1\relax}{0.6 * #2} \end{scope} \fi } % Call the recursive function with initial values \recursive{5}{4} \end{tikzpicture}</p><p>In summary, we tested the performance of GPT-4 and GPT-3.5 on tasks that involved the use of recursion in the visual domain. We find that GPT-4 can generate recursive diagrams and images using ASCII characters, Mermaid syntax (using the Show Me plugin), and tikz syntax. GPT-3.5 can also draw diagrams in ASCII and tikz, but more frequently fails to generate recursive structures. Both models struggle with the syntax of the tikz package and require reprompting to fix their code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">discussion and conclusion</head><p>In conclusion, we tested GPT-4 and GPT-3.5's explicit command of recursion on a series of linguistic and visual tasks. We find that GPT-4 is capable of identifying, creating, and analyzing recursive structures in the linguistic domain. The model is also capable of creating recursive art in ASCII, illustrating recursive processes with Mermaid syntax, and creating fractal graphs using the tikz package in L A T E X. GPT-4's success at producing recursion across domains suggests that it may be the first language model with emergent and general (i. e. non-domain-specific) recursive ability. In comparison, GPT-3.5's performance on the same tasks is more limited. While the older model is capable of identifying recursion and illustrating it with linguistic examples, it often fails at analyzing it and representing it visually.</p><p>One could object to our findings by noting that the GPT training data almost certainly contains linguistics textbooks (and other pedagogical and research materials) that explicitly discuss recursion. Therefore, the models' understanding of recursion can be attributed to memorization. We respond by observing that humans are not intuitively aware of recursion either and need to be explicitly instructed as well. Thus, there are no fundamental asymmetries in the prior training necessary for humans and LLMs to succeed on the tasks presented in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>a. "Mark said that Jolene said that John said that Lisa said hi." b. "I saw a big blue majestic beautiful elephant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>c.#</head><figDesc>Is the tree you drew correct? task, we tested GPT-4's ability to model linguistic recursion with code. First, we asked the model to volunteer a piece of recursive code<ref type="bibr" target="#b8">(8)</ref>. The model offered a simple recursive function in Python calculating the factorial of a number. (8) Write a piece of recursive code. def factorial(n): # Base case: factorial of 0 or 1 is 1 if n == 0 or n == 1: return 1 Recursive case: n * factorial of n-1 else: return n * factorial(n-1) print(factorial(5)) # Output: 120</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 16 )</head><label>16</label><figDesc>Draw a recursive ASCII diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 18 )</head><label>18</label><figDesc>Generate a recursive diagram using the Show Me plugin. The diagram can be of anything you'd like. However, you have to make sure the diagram itself is recursive.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>linguistic recursion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We note that although GPT-4 is able to generate recursive images rather successfully, it does not describe them well. For example, the model says that the image in (17c) "shows a larger X shape, with a smaller X shape nested inside it. The</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Intrigued by the different character of the ASCII art generated by GPT-4 and GPT-3.5, we tested both models' performance with different temperature values. Lowering the temperature values below 1.0 results in more conservative output (and more Sierpiński triangles). Raising the temperature above 1.0 increases the frequency with which elaborate original structures are generated. However, the elaborate structures are less frequently recursive. We conclude that changing the temperature setting does not considerably improve either model's performance.</p></note>
		</body>
		<back>

			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>gašper beguš university of california, berkeley manuscript as of june 12, 2023</p></div>
			</div>


			<div type="funding">
<div><p>1. This sentence does not contain recursion.</p><p>2. This sentence contains clausal recursion. The main clause "<rs type="programName">Mark said" embeds the subordinate clause "that Jolene said hi</rs>."</p><p>3. This sentence does not contain recursion.</p><p>4. This sentence does not contain recursion.</p><p>5. This sentence does not contain recursion.</p><p>6. This sentence contains recursive genitive (possessive) recursion. The phrase "<rs type="projectName">Martha's dog's tail" involves the possessive form "Martha's" applied twice to indicate the possession relationship between "Martha," "dog</rs>," and "tail."</p><p>7. This sentence does not contain recursion.</p><p>8. This sentence does not contain recursion.</p><p>9. This sentence contains recursive adjectival modification. The adjective "big" modifies the noun "elephant," and the adjectival phrase "who had a lollipop" further modifies the noun "boy."</p><p>10. This sentence does not contain recursion.</p><p>11. This sentence contains clausal recursion. The main clause "A boy liked" embeds the subordinate clause "who I saw a girl."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kpuuM5A">
					<orgName type="program" subtype="full">Mark said&quot; embeds the subordinate clause &quot;that Jolene said hi</orgName>
				</org>
				<org type="funded-project" xml:id="_jTntuWX">
					<orgName type="project" subtype="full">Martha&apos;s dog&apos;s tail&quot; involves the possessive form &quot;Martha&apos;s&quot; applied twice to indicate the possession relationship between &quot;Martha,&quot; &quot;dog</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(21a), the house within a house (21c), and the nested rectangles (21e) are clearly compliant with the prompt, which is to say-recursive. 3   (20) Draw a recursive ASCII diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a.</head><p>b. c.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">This sentence contains clausal recursion, with the relative clause</title>
		<imprint/>
	</monogr>
	<note>A boy who had a lollipop yelled my name who had a lollipop&quot; modifying &quot;boy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">This sentence does not contain any form of recursion</title>
		<imprint/>
	</monogr>
	<note>A boy yelled my name</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">This sentence contains clausal recursion. The main clause &quot;A boy ... yelled my name&quot; contains an embedded clause &quot;who I saw a girl liked&quot; which itself contains another embedded clause &quot;a girl liked</title>
	</analytic>
	<monogr>
		<title level="m">The recursion here involves embedding of relative clauses within relative clauses</title>
		<imprint/>
	</monogr>
	<note>A boy who I saw a girl liked yelled my name</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">we asked GPT-4 to add additional layers of recursion to all the sentences previously identified as recursive (3). The model&apos;s performance on all of the examples is perfect</title>
		<author>
			<persName><surname>Next</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Make sure that in adding layers of recursion, you embed structures of the same type. For example, if you identified possessive recursion, add an additional layer of possession. If you identified causal recursion</title>
		<imprint/>
	</monogr>
	<note>For each of the sentences you identified as recursive, first add one layer of recursion, and then add another one</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mark said that Jolene said that John said hi</title>
	</analytic>
	<monogr>
		<title level="m">Mark said that Jolene said that John said that Lisa said hi</title>
		<imprint/>
	</monogr>
	<note>+1 layer Mark said that Jolene said hi +2 layers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">+2 layers: &quot;I saw a big blue majestic beautiful elephant</title>
		<imprint/>
	</monogr>
	<note>I saw a big blue elephant +1 layer: &quot;I saw a big blue majestic elephant</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">+2 layers: &quot;A cat bit Martha&apos;s neighbor&apos;s sister&apos;s dog&apos;s tail</title>
		<imprint/>
	</monogr>
	<note>+1 layer A cat bit Martha&apos;s dog&apos;s tail A cat bit Martha&apos;s neighbor&apos;s dog&apos;s tail</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The light flashed incredibly almost blindingly quickly</title>
		<author>
			<persName><surname>Original</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The light flashed incredibly almost blindingly astoundingly quickly</title>
		<imprint/>
	</monogr>
	<note>+1 layer The light flashed incredibly quickly +2 layers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A boy who had a lollipop that was cherry-flavored that his grandmother gave him yelled my name</title>
		<imprint/>
	</monogr>
	<note>+1 layer A boy who had a lollipop yelled my name +2 layers A boy who had a lollipop that was cherry-flavored yelled my name</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">These results are at odds with Şahin et al.&apos;s (2020) assertion that language models &quot;lack the skill of iterative reasoning upon knowledge&quot; (p. 1241), and challenge Katzir (2023), who shows a case where reprompting does not result in an improvement and argues that &quot;further time and resources are of no use to ChatGPT</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>+1 layer A boy who I saw a girl liked yelled my name +2 layers A boy who I saw a girl that a dog followed liked yelled my name A boy who I saw a girl that a dog that a cat chased followed liked yelled my name Finally, we observe that while GPT-4&apos;s initial replies to a prompt are not always correct, the model&apos;s performance improves considerably when asked to evaluate its own output Instead, our findings suggest that-given the right experimental set-up-GPT-4 is capable of emergent iterative reasoning which is a prerequisite for complex problem-solving</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why are no animal communication systems simple languages?</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Beecher</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.602635</idno>
		<ptr target="https://www.frontiersin.org/articles/10.3389/fpsyg.2021.602635" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<idno type="ISSN">1664-1078</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Beguš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksymilian</forename><surname>Gašper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Dąbkowski</surname></persName>
		</author>
		<author>
			<persName><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00948[cs.CL</idno>
		<title level="m">Large linguistic models: Analyzing theoretical linguistic abilities of LLMs</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Syntactic Structures</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Mouton and Co</publisher>
			<pubPlace>The Hague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimal recursion: Exploring the prospects</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recursion: Complexity in Cognition. Studies in Theoretical Psycholinguistics (SITP)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursion, language, and starlings</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Corballis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="697" to="704" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large language models and (non-)linguistic recursion</title>
		<author>
			<persName><forename type="first">Maksymilian</forename><surname>Dąbkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gašper</forename><surname>Beguš</surname></persName>
		</author>
		<ptr target="https://osf.io/kjpq8/" />
	</analytic>
	<monogr>
		<title level="j">OSF</title>
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cultural constraints on grammar and cognition in pirahã: Another look at the design features of human language</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">L</forename><surname>Everett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Anthropology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="621" to="646" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computational constraints on syntactic processing in a nonhuman primate</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Tecumseh</surname></persName>
		</author>
		<author>
			<persName><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="377" to="380" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive syntactic pattern learning by songbirds</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">Q</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kimberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">C</forename><surname>Margoliash</surname></persName>
		</author>
		<author>
			<persName><surname>Nusbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">440</biblScope>
			<biblScope unit="page" from="1204" to="1207" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is Chat-GPT a grammatically competent informant?</title>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Haider</surname></persName>
		</author>
		<ptr target="https://lingbuzz.net/lingbuzz/007285" />
	</analytic>
	<monogr>
		<title level="m">Manuscript. Salzburg University</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The faculty of language: What is it, who has it, and how did it evolve?</title>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Tecumseh</forename><surname>Fitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="1569" to="1579" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Origin of Speech</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Hockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Prompt-based methods may underestimate large language models&apos; linguistic generalizations</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://lingbuzz.net/lingbuzz/007313" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Manuscript. Massachusetts Institute of</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky)</title>
		<author>
			<persName><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Why large language models are poor theories of human linguistic cognition. a reply to Piantadosi (2023)</title>
		<author>
			<persName><forename type="first">Roni</forename><surname>Katzir</surname></persName>
		</author>
		<ptr target="https://lingbuzz.net/lingbuzz/007190" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Manuscript. Tel Aviv University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can transformers process recursive nested constructions, like humans?</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Lakretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Desbordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3226" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mechanisms for handling nested dependencies in neural-network language models and humans</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Lakretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieuwke</forename><surname>Hupkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Vergallito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislas</forename><surname>Dehaene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page">104699</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774[cs.CL</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Gpt-4 Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Modern language models refute Chomsky&apos;s approach to language</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Piantadosi</surname></persName>
		</author>
		<ptr target="https://lingbuzz.net/lingbuzz/007180" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>Manuscript. University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PuzzLing Machines: A challenge on learning from small data</title>
		<author>
			<persName><forename type="first">Gözde</forename><surname>Şahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yova</forename><surname>Gül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.115.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1241" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">TikZ and pgf: Manual for version 1</title>
		<author>
			<persName><forename type="first">Till</forename><surname>Tantau</surname></persName>
		</author>
		<ptr target="https://www.bu.edu/math/files/2013/08/tikzpgfmanual.pdf" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Version 2.1.5. LaTeX package for drawing (linguistic) trees</title>
		<author>
			<persName><forename type="first">Sašo</forename><surname>Živanović</surname></persName>
		</author>
		<ptr target="https://ctan.org/pkg/forest" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
