<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models</title>
				<funder ref="#_VqnfTh2 #_Z7djVae">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_fZSk2UP">
					<orgName type="full">Strategic Priority Research Program of Chinese Academy of Sciences</orgName>
				</funder>
				<funder ref="#_mMnzXvS">
					<orgName type="full">Youth Innovation Promotion Association CAS and Yunnan Provincial Major Science and Technology Special Plan Projects</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
							<email>wengsyx@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqi</forename><surname>Wang</surname></persName>
							<email>wangzhiqi2022@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huanxuan</forename><surname>Liao</surname></persName>
							<email>liaohuanxuan2023@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<email>shizhu.he@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
							<email>liushengping@unisound.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Unisound</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">The Laboratory of Cognition and Decision Intelligence for Complex Systems</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1B52F6A1B7B75C7ADA921A388A4DFCE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present "LMTuner", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules -the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage 1 and screencast video 2 are now publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are demonstrating unprecedented performance in numerous natural language understanding and generation tasks <ref type="bibr">(Wei et al., 2022b,a;</ref><ref type="bibr">Weng et al., 2023a)</ref> thanks to their capacity to learn from extensive text data with generative manner <ref type="bibr" target="#b1">(Brown et al., 2020;</ref><ref type="bibr" target="#b31">Scao et al., 2022)</ref>. This has led to a rising number of researchers and engineers embarking on the training of their own language models for specific industries and domains <ref type="bibr" target="#b36">(Taori et al., 2023;</ref><ref type="bibr" target="#b54">Xu et al., 2023;</ref><ref type="bibr" target="#b9">Cui et al., 2023)</ref>. However, training LLMs imposes high demands on engineering skills <ref type="bibr" target="#b58">(Zhang et al., 2022)</ref>, and the various techniques applicable 1 <ref type="url" target="https://wengsyx.github.io/LMTuner/">https://wengsyx.github.io/LMTuner/</ref> 2 <ref type="url" target="https://youtu.be/nsXmWOmN3rE">https://youtu.be/nsXmWOmN3rE</ref> </p><p>Interaction Module Training Module Inference Module Man-Machine Interactive Environment Interaction Let_Tune() Dataset GPT-4 Model PEFT MEFT Hyper-Parameters Medical Low Own Llama Llama2 ChatGLM GLM-130B LoRA LOMO QLoRA Learning Rate Epochs Warmup to such training are mostly disparate <ref type="bibr" target="#b10">(Dao et al., 2022;</ref><ref type="bibr" target="#b51">Xi et al., 2023;</ref><ref type="bibr" target="#b22">Luo et al., 2023)</ref>. This not only increases the complexity of related projects but also adds to the learning cost required for training language models.</p><p>With the progressive development of generative large language model technology, various techniques have emerged, incorporated into different toolkits <ref type="bibr">(Zhao et al., 2023b)</ref>. As shown in Table <ref type="table">1</ref>, when aiming for model parallelism, MegatronLM <ref type="bibr" target="#b33">(Shoeybi et al., 2020)</ref> stands as a preferred choice, while bitsandbytes <ref type="bibr" target="#b11">(Dettmers et al., 2022)</ref> serves the purpose of model quantization, and Opendelta <ref type="bibr" target="#b18">(Hu et al., 2023)</ref> facilitates the implementation of Efficient Fine-Tuning technology. Nonetheless, the flexibility offered by these individual tools comes at the cost of developers spending significant time coordinating the usage of diverse tool modules, rendering direct application challenging. To address these challenges, several frameworks, such as h2oGPT <ref type="bibr" target="#b3">(Candel et al., 2023)</ref> and Lamini <ref type="bibr" target="#b13">(Diamos et al., 2023)</ref>, have attempted to consolidate some of these functionalities. However, these frameworks typically integrate only a subset of the available techniques, often lacking comprehensive coverage of commonly used model technologies.</p><p>Therefore, in this paper, we present LMTuner, a novel framework that significantly minimizes these</p><p>arXiv:2308.10252v1 [cs.CL] 20 Aug 2023 Highly-integrable User-friendly Model Parallelism Quantization PEFT MEFT ZeRO Load Dataset Position Interpolation AI Assisstent Code Concise MegatronLM <ref type="bibr" target="#b33">(Shoeybi et al., 2020)</ref> ✓ Huggingface <ref type="bibr" target="#b50">(Wolf et al., 2020)</ref> ✓ ✓ ✓ ✓ ✓ bitsandbytes <ref type="bibr" target="#b11">(Dettmers et al., 2022)</ref> ✓ OpenDelta <ref type="bibr" target="#b18">(Hu et al., 2023)</ref> ✓ ✓ Lamini <ref type="bibr" target="#b13">(Diamos et al., 2023)</ref> ✓ ✓ h2oGPT <ref type="bibr" target="#b3">(Candel et al., 2023</ref>)</p><formula xml:id="formula_0">✓ ✓ ✓ ✓ LMTuner (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓</formula><p>Table 1: Compared to other commonly used language model training systems, LMTuner system has highly integrable and user-friendly.</p><p>obstacles by offering an easy-to-use, scalable, and integrative modular system. As depicted in Figure <ref type="figure" target="#fig_0">1</ref> • LMTuner boasts high usability, only needing a single code ( ) to be launched. It facilitates quick start-up for large language model training by allowing users to interact using natural language with LMTuner.</p><p>• We have incorporated a wide range of techniques suited for training large language models, including models, domain QA datasets, Efficient Fine-Tuning methods, and specific hyperparameters. This integration fosters the research and development of large language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The development of pre-training language models has brought about numerous language model tools and a flourishing NLP community, among which are "Transformers" <ref type="bibr" target="#b50">(Wolf et al., 2020)</ref>. It establishes a range of model classes and provides APIs for implementing easily extendable transformer models. Other tools closely associated with language model building include Fairseq <ref type="bibr" target="#b26">(Ott et al., 2019)</ref>, MegatronLM <ref type="bibr" target="#b33">(Shoeybi et al., 2020)</ref>, Med-ConQA <ref type="bibr">(Xia et al., 2022a)</ref>, OpenDelta <ref type="bibr" target="#b18">(Hu et al., 2023)</ref> and h2oGPT <ref type="bibr" target="#b3">(Candel et al., 2023)</ref>. Differing from these, LMTuner is specifically designed for training auto-regressive LLMs. With its modular design, it allows for a free combination of different pre-training models, datasets, model frameworks, automatic length extrapolation settings, and PEFT methods within one framework. With the assistance of dialogue-type LLMs such as GPT-4 (Ope-nAI, 2023), users can accomplish the entire training process with just a single line of code. Recent researches have proposed many directions for training LLMs <ref type="bibr" target="#b19">(Ignat et al., 2023)</ref>, including high-quality data <ref type="bibr" target="#b61">(Zhou et al., 2023;</ref><ref type="bibr" target="#b16">Gunasekar et al., 2023)</ref>, efficient fine-tuning methods <ref type="bibr" target="#b24">(Mangrulkar et al., 2022)</ref>, model structures <ref type="bibr" target="#b32">(Shazeer, 2019;</ref><ref type="bibr" target="#b10">Dao et al., 2022)</ref>, and length extrapolation <ref type="bibr" target="#b7">(Chi et al., 2023;</ref><ref type="bibr" target="#b39">Tworkowski et al., 2023;</ref><ref type="bibr" target="#b34">Su et al., 2022;</ref><ref type="bibr">Chen et al., 2023a</ref>). The LMTuner system integrates the most advanced techniques in these fields, allowing users to make choices according to their needs. LMTuner integrates these techniques into separate modules, facilitating user selection and usage <ref type="bibr">(Wang et al., 2023a;</ref><ref type="bibr" target="#b10">Dao et al., 2022;</ref><ref type="bibr" target="#b12">Dettmers et al., 2023)</ref>.</p><p>LLMs undergoing instruction pre-training can align with human instructions <ref type="bibr">(Ouyang et al., 2022b;</ref><ref type="bibr" target="#b8">Chung et al., 2022)</ref> and have been discovered to possess many capabilities not present in smaller language models <ref type="bibr">(Zhao et al., 2023a)</ref>, such as tool creation <ref type="bibr" target="#b2">(Cai et al., 2023)</ref>, environment Based on your GPU memory, we recommend training Llama2-7B or using LoRA to train Llama2-13B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>It's okay to use Llama2-13B.</p><p>I have set up Llama2-13B with LoRA for you.</p><p>Based on your GPU memory, we recommend using a batch size of 8 and setting the max text length to 1024, does that work for you?</p><p>In addition, you will need to set the number of training epochs and how often to save the weights, such as every N steps. The defaults are 10 and 1000, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>Right. A: Lactose intolerance is a common digestive disorder where the body is unable to completely digest lactose, a sugar found in milk and dairy products. This condition results from a deficiency of the enzyme lactase, which is produced in the small intestine.\n\nSymptoms\nThe symptoms of lactose intolerance typically develop within a ... exploration <ref type="bibr">(Wang et al., 2023b)</ref>, self-verification <ref type="bibr" target="#b48">(Weng et al., 2022)</ref>, and complex reasoning <ref type="bibr">(Weng et al., 2023b;</ref><ref type="bibr" target="#b62">Zhu et al., 2023)</ref>. Leveraging these capabilities of LLMs, LMTuner's Interaction Module can help users analyze their needs and recommend necessary settings through a nature-language-based interaction method. This practice of using LLMs as interactive agents <ref type="bibr">(Wang et al., 2023c)</ref> have also been applied in interactive decision making <ref type="bibr" target="#b55">(Yao et al., 2023)</ref> and functioning as research assistants <ref type="bibr">(Ren et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{ 'GPU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LMTuner</head><p>LMTuner is an open-source system that offers a command-line interface (CLI) for training LLMs: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interaction Module</head><p>The Interaction Module of LMTuner, utilizing GPT-4's (or ChatGPT) System Message and Function features, serves as an LLM training assistant. This module, during initialization, incorporates common training issues, parameter configurations, and selectable methods into System Messages, thereby streamlining the training process. The essential function of GPT-4 within this context is to ascertain the requisite training configurations. A function, Set_ARGS, is available to amend the parameters in the training configuration. To enable GPT-4 to evaluate various parameter settings, we utilize pynvml to monitor server GPUs, integrating this information within the System Message content.</p><p>The Interaction Module helps prevent user configuration mistakes through its user-friendly interface and high adaptability. As shown in Figure <ref type="figure" target="#fig_6">3</ref>, users can state their needs conversationally. LM-</p><p>Tuner then analyzes their words and suggests appropriate training settings. This works because the System Message contains knowledge about LLM training, compensating for the common lack of Llama2 7B Llama 33B Llama2 70B GLM-130B ...... Models Chinese Common Law English Common PEFT and MEFT Datsets Hyper-Parameters Interaction Module Learning Reat=1e-5 Warmup=0.02 Batch Size=24 Seq Length=1024 I have collected one million legal texts and corresponding question-answer dialogues, and I would like to use them to train a Chinese language model. Since I have a large amount of data, I don't need the model to be too large, and I hope to perform incremental training on all parameters in My 8 * A6000 server. I have a strong desire to train a massive model on my personal dataset, but unfortunately, I am limited to only eight 3090s. Could you lend me a hand? I would like to use Llama2-7B to train a medical dialogue chatbot, And I have two A100 GPUs. A B C Custom Medical ChatGLM2 6B Normal LoRA QLoRA ...... Learning Reat=1e-4 Warmup=0.02 Batch Size=6 Seq Length=1024 LOMO Learning Reat=1e-4 Warmup=0.02 Batch Size=32 Seq Length=2048 such expertise among researchers and engineers without backgrounds in large language model training.</p><p>The Interaction Module, while offering immense flexibility and ease of use, prevents potential issues such as user configuration errors. As shown in Figure <ref type="figure" target="#fig_6">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train the model on another server</head><p>Welcome to LMTuner we can provide you with various training methods you want</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Installation</head><p>For details, please refer to Github Guide</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operation Login wandb</head><p>Please run the following code in shell to login wandb</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copy ds_config</head><p>Please move the originally generated ds_config.json to the now training path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training operation</head><p>Please manually execute the following code on the server to start training: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Module</head><p>The training module in LMTuner has highly integrated, easy to invoke, and extensible features. Currently, the mainstream LLMs are mostly similar in architecture, and the training process and loss calculation are generally consistent. Therefore, we construct the required techniques in a modularized manner at the code level according to different requirements. Meanwhile, such design also facilitates engineers with coding skills to directly replace the corresponding modules through hooks. In the rest of this section, we will introduce each technical module separately.</p><p>Datasets.</p><p>The availability of high-quality training data is crucial for developing capable LLMs. To facilitate affordable access to suitable datasets for question answering, we have curated and prepared a collection of QA datasets covering diverse domains, including English, Chinese, medical, and legal fields. Furthermore, to enable the development of models with customizable names and personas, the datasets have been augmented with synthetic question-answer pairs inquiring about the model's identity (e.g. "Hi I'm [MODEL NAME]"). During training, the [MODEL NAME] tokens can be dynamically substituted with the preferred name for each model instance. 1 from LMTuner . dataset import LMTunerDataset 2 3 dataset = LMTunerDataset () 4 5 # Give your model a name 6 dataset . set_model_name ( ' LMTuner ') 7 8 # Add QA dataset samples 9 dataset . add_sample ([ ' Who are you ? ', 10 "I 'm LMTuner , your personal sidekick !" ])</p><p>However, despite the richness of our curated datasets, they inherently possess certain limitations in coverage and diversity. To augment the builtin datasets and account for user-specific needs, our system also provides seamless support for customized training data. Users can simply provide the local path to their own JSONL-formatted question answering data files. This design choice provides greater flexibility to users, empowering them to tailor the training distribution to their unique application requirements. For instance, users can provide proprietary datasets containing sensitive or confidential information not suitable for public release. The ability to directly use local JSONL files avoids the need to port datasets to external platforms.</p><p># Pretrained Custom-Dataset Format { "input" : " " , "output" : " With t h e b u r g e o n i n g d e v e l o p m e n t i n t h e r e a l m o f . . . " , } # Instruct Custom-Dataset Format { "input" : " Human : Who a r e you ? " , "output" : " A s s i s t a n t : I 'm LMTuner , y o u r p e r s o n a l s i d e k i c k ! " , } Models. Recent advancements in natural language processing have been enabled by the transformer ar-chitecture <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>. The SwissArmy-Transformer<ref type="foot" target="#foot_0">foot_0</ref> framework facilitates efficient development of diverse transformer models by decoupling reusable core components from interchangeable model-specific modules. These lightweight modules attach to the shared backbone via hooks, enabling rapid iteration and customization. In contrast, Transformers <ref type="bibr" target="#b50">(Wolf et al., 2020)</ref> provides optimized implementations of canonical architectures and pretrained models for production use.</p><p>The LMTuner system combines these complementary strengths for flexible model development and deployment. It utilizes SwissArmyTransformer to construct tailored architectures and seamlessly integrates Transformers' pretrained models. This synthesis of code modularization and extensive pretrained models promises to enhance productivity, accelerate innovation, and improve real-world language understanding. LMTuner promotes exploratory modeling by innovating new designs built on transformer infrastructure, while benefiting from cutting-edge advancements in language model pretraining. Selectively utilizing both libraries stands to meaningfully advance natural language processing systems through rapid prototyping of specialized models and accessible deployment of state-of-the-art capabilities. Efficient Fine-Tuning. LMTuner uses ZeRO technology of Deepspeed by default to unload parameters and improve training throughput. In addition, LMTuner provides parameters-efficient fine-tuning (PEFT) methods including LoRA <ref type="bibr" target="#b17">(Hu et al., 2022)</ref> and QLoRA <ref type="bibr" target="#b12">(Dettmers et al., 2023)</ref>, and memoryefficient fine-tuning (MEFT) methods including LOMO <ref type="bibr" target="#b18">(Lv et al., 2023)</ref> and Quantization <ref type="bibr" target="#b15">(Gholami et al., 2021)</ref>. They support training LLMs with low memory usage. These methods are implemented in a modularized manner at the code level of LM-Tuner, so they can be easily combined and used freely. Position Interpolation. To better support longcontext modeling, LMTuner has integrated some scaling of RoPE. We implemented Xpos <ref type="bibr" target="#b35">(Sun et al., 2022)</ref> and some recent position interpolation methods like linear interpolation <ref type="bibr">(Chen et al., 2023a)</ref>, dynamic interpolation, NTK-Aware Scaled RoPE (NTKv1) and NTK-By-Parts (NTKv2). The dynamic methods choose the correct scale parameter based on sequence length, rather than having to settle for a fixed trade-off between maximum sequence length and performance on shorter sequences, i.e., use the exact position values for the first 2048 contexts and then recalculate the position vectors for each new sequence length as the model generates the markers one by one. A certain degree of long-context modeling can be achieved by choosing different scaling methods. Other Details. We use the probability distribution over sequences of tokens as the optimization objective with cross-entropy loss <ref type="bibr" target="#b29">(Radford et al., 2019)</ref> and use Lion optimizer <ref type="bibr">(Chen et al., 2023b</ref>) by default to optimize LLMs, because it has been proven to be more memory-efficient than Adam <ref type="bibr" target="#b20">(Kingma and Ba, 2017)</ref>. During training, we record the loss, learning rate, and number of tokens respectively at each step using wandb<ref type="foot" target="#foot_1">foot_1</ref> , and display them in the browser through line charts, which helps users observe the training status during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference Module</head><p>LMTuner loads the final model weights after training and generates continuations conditioned on given contexts until reaching the maximum length. To speed up inference, LMTuner provides model quantization methods including INT8 and INT4 quantization <ref type="bibr" target="#b56">(Zeng et al., 2022)</ref>. By quantizing 16-bit floating-point weights into lower bitwidth integers such as 8-bit or 4-bit, the computation time and memory usage during inference can be reduced. LMTuner quantizes weights of selected layers in a trained model, while keeping activations in 16-bit floating-point format. After quantization, the inference latency on CPU and throughput on GPU can be improved significantly with little degradation in model quality <ref type="bibr" target="#b21">(Liu et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Running Case</head><p>The target audience of LMTuner is machine learning engineers and researchers across academia and industry. Novice users can leverage LMTuner's guided interaction while experts retain full control over implementation details. It compares favorably to current systems by combining user-friendliness, scalability, and integrability within a unified interface. LMTuner is open-source under the Apache 2.0 license, allowing free use in commercial products. By open-sourcing LMTuner, we hope to catalyze progress in large language model training and lower the barriers to leveraging these transformative technologies. The availability of an easy-to-use, highly customizable system should benefit the broader community.</p><p>Assuming we need to train a medical LLM that can assist in patient diagnosis, and we have two A6000 GPUs with 48GB VRAM each, as well as a medical QA dataset (MedDialog <ref type="bibr" target="#b57">(Zeng et al., 2020)</ref>). Using LMTuner, we can automatically determine the training process, including the selection of the LLama-7B model and a set of corresponding hyperparameters<ref type="foot" target="#foot_2">foot_2</ref> .</p><p>while Table <ref type="table">2</ref> showcases the performance of the MedDialog test set (without manual model selection). Instead, we chose the final model obtained after LMTuner training, which completed 10 epochs. We observed that the LMTuner-trained model surpasses existing models in metrics such as Bleu, Meteor, and NIST. This indicates that the trained model is ready for direct utilization.</p><p>Method MedDialog BLEU-2 BLEU-4 Meteor Nist-2 Nist-4 GPT-3 (175B) + Direct 14.93 7.52 4.55 0.814 0.852 GPT-3 (175B) + CoT 9.18 4.61 3.54 0.546 0.569 Instruct-GPT (175B) + Direct 15.93 8.21 5.74 0.874 0.913 Instruct-GPT (175B) + CoT 16.49 7.80 4.94 1.020 1.059 GLM (130B) + Direct 12.16 6.02 5.31 0.577 0.601 GLM (130B) + CoT 30.02 15.61 8.73 1.909 2.017 Zeng et al. (2020) 31.67 16.88 9.57 1.981 2.076 LMTuner (Ours) 35.62 19.02 9.47 2.377 2.517</p><p>Table <ref type="table">2</ref>: We present the results of several language models on the MedDialog task, including the zeroshot (Direct) and CoT <ref type="bibr">(Wei et al., 2022b</ref><ref type="bibr" target="#b44">(Wei et al., , 2021) )</ref> performance of some LLMs (GPT-3:code-davinci-001 <ref type="bibr">(Chen et al., 2021)</ref>; Instruct-GPT: code-davinci-002 <ref type="bibr">(Ouyang et al., 2022a)</ref>), as well as the performance of previous Finetune state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>LMTuner represents a pioneering effort to facilitate large language model training through enhanced usability and modularity. We believe LMTuner represents a significant step towards realizing the full potential of large language models. By continuing to integrate emerging techniques and community feedback, its capabilities will only grow over time.</p><p>The availability of LMTuner as an open source project presents exciting opportunities for LLMs enhancement. We hope its emphasis on usability and extensibility will meaningfully accelerate future work in this paradigm-defining domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>While LMTuner is designed to be user-friendly and intuitive, it may not capture all specific user requirements in its current version. The training process might need to be adjusted or additional techniques might need to be integrated to achieve optimal performance for certain specialized tasks. Some complex requirements could necessitate manual code modification, which might increase the learning curve for non-expert users. However, this limitation is also being actively addressed through continuous development and updates to the system to enhance its comprehension of user requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Support Models</head><p>LMTuner enables rapid deployment of large language models ranging from 300 million to 130 billion parameters, facilitating their application to textual data tasks. We list the models supported for training in LMTuner in Table <ref type="table">4</ref>. (Please note that some of these models are not fully allowed for commercial use.)</p><p>LMTuner also provides capabilities to load pretrained models from Transformers, however this necessitates redefining the data tokenizer. In addition, LMTuner facilitates training models from scratch to incorporate specialized architectures such as FlashAttention and Multi-Query Attention through its flexible hook. The modular hook-based approach streamlines implementing customized attentional mechanisms and training objectives. This balancing of usability and flexibility makes LM-Tuner a promising platform for rapidly prototyping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Support Datasets</head><p>We have supported multiple QA datasets in different fields for quick training. The provision of ready-to-use domain-specific datasets eliminates the need for users to invest significant time and effort in dataset curation and preprocessing. With high-quality training data covering key domains, users can promptly initialize model training and optimization workflows.</p><p>• For English datasets, we default to loading the LIMA<ref type="foot" target="#foot_3">foot_3</ref>  <ref type="bibr" target="#b61">(Zhou et al., 2023)</ref> dataset, which includes 1,030 diverse and high-quality QA samples.</p><p>• For Chinese datasets, we manually translated the LIMA dataset and added an additional 60 Chinese samples covering different fields such as Chinese history, Marxism, essay writing, and Chinese humor, making it a QA dataset with 1,090 samples covering multiple fields and Chinese characteristics.</p><p>• For Chinese Medical datasets, We selected about 60,000 medical consultation dialogues from CMCQA <ref type="bibr">(Xia et al., 2022b)</ref>, which contained QA samples from over 30 different departments, including pediatrics, gynecology, internal medicine, oncology and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LMTuner's GPU Memory Usage</head><p>Normally, training LLMs requires a huge amount of GPU memory. Recent advancements in EFT techniques such as LoRA and LOMO have enabled the training of extremely large language models on consumer GPUs with as little as 24GB of memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the LMTuner. LMTuner can be invoked through a single line of code . The overall process is top-down, sequentially going through the Interaction Module, Training Module, and Inference Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>, LMTuner is comprised of three modules. 1) The Interaction Module enables user-friendly communication, automatically adjusting parameters based on user needs and context, ideal for non-technical users. 2) The Training Module autonomously processes training using these parameters, saving users the complexity of setup. 3) The Inference Module utilizes the trained models for various tasks upon training completion. With these modules, developers can effortlessly generate their desired models and automatically pass on the parameters to the next module for seamless training. This streamlines the entire process, allowing the system to be readily deployed and utilized upon completion of training.In summary, our proposed LMTuner system offers greater usability and flexibility, allowing users to swiftly configure parameters for training language models according to specific needs and effectively initiate the training. Our contributions are as follows:• We have proposed LMTuner, which is the highly usable and integrated training system for LLMs. It is free to use and license-friendly (Apache 2.0). And we open source code at https://github.com/WENGSYX/LMTuner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>You are an AI assistant capable of training large language models. Firstly, you need to ask the user questions in order ... ARGS['model']: Default: None; Here are some available models to choose from: GLM-130B, ChatGLM2-6B, llama-7b... ARGS['learning rate']: Default 1e-5; If using LoRA or QLoRA as the method, please set it to 1e-3... want to train a medical model on this machine. Hello there! I"m your AI assistant, and I"m here to help you train your model. Before we get started, it"s important to have a clear plan and goal in mind User I want to train a medical model on this machine. Perfect! We can definitely assist you with that. To start with, let's choose the dataset for your model. Since you mentioned you're interested in training a medical model, we can recommend the 'LM-Medical-v1' dataset to you. Would that be OK? User ok Great! Your dataset has been set to 'LM-Medical-v1'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>deepspeed --include localhost:0,1 main.py --model Llama2-13B --use_lora 1 --lora_rank 16 --fp16 --dataset LM-Medical-v1 --max_seq_length 1024 --deepspeed_config ./ds_config.json ... are the manifestations of lactose intolerance? What are the treatment methods?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the LMTuner system. When a user makes a request by , the LMTuner system automatically detects the GPUs available in the environment and engages in detailed discussions with the user to determine the configuration. The configuration is then automatically sent to the Training Module to initiate training. During the training process, wandb is enabled to save training logs. The final trained model is sent to the Inference Module for deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>#</head><figDesc>Official Launch for LMTuner . from LMTuner import Let_Tune Let_Tune () &gt;&gt;&gt; [ AI ] Greetings ! I am your AI assistant . Present assist training your model . Necessary possess clear plan , goal first . &gt;&gt;&gt; [ ANSWER ] : As shown in Figure 2, LMTuner allows for the development of LLMs training through simple con-versations. This can improve engineering efficiency when training LLMs and reduce code burden.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Depending on the requirements specified, LMTuner can automatically suggest different training plans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>, users with different requirements only need to express their needs in natural language. The LMTuner system is capable of analyzing and recommending appropriate training parameters. If the user is not inclined to train the model on the current device, LMTuner will automatically generate a Readme.md file that includes environment configuration, model processing, and training code instructions like Figure4, facilitating a swift setup for the user on a new device. Once all training parameters are finalized, LM-Tuner saves a copy of these in an ARGS.json file. If one wishes to initiate training quickly with the same parameters, they only need to pass the path name of ARGS.json to the function, thereby avoiding redundant repeated dialogues. 1 # Quickly Launch for LMTuner . 2 from LMTuner import Let_Tune 3 Let_Tune ( ARGS = './ ARGS . config ') 4 5 &gt;&gt;&gt; [ LMTuner ] We will train the model ~Go ! 6 &gt;&gt;&gt; [2023 -07 -19 05:18:34 ,778] [ INFO ] [ runner . py :555: main ] cmd = python -u -m deepspeed . launcher . launch --world_info = xxxxx main . py --seed 1234 ......</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Figure 4: An example of the Readme.md automatically generated by LMTuner.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/THUDM/SwissArmyTransformer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://wandb.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We released the code at https://github.com/ WENGSYX/LMTuner/tree/main/Example/English_ Medical</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://huggingface.co/datasets/GAIR/lima</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">Strategic Priority Research Program of Chinese Academy of Sciences</rs> (No. <rs type="grantNumber">XDA27020100</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">U1936207</rs>, No.<rs type="grantNumber">61976211</rs>). This work was supported by the <rs type="funder">Youth Innovation Promotion Association CAS and Yunnan Provincial Major Science and Technology Special Plan Projects</rs> (No.<rs type="grantNumber">202202AD080004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fZSk2UP">
					<idno type="grant-number">XDA27020100</idno>
				</org>
				<org type="funding" xml:id="_VqnfTh2">
					<idno type="grant-number">U1936207</idno>
				</org>
				<org type="funding" xml:id="_Z7djVae">
					<idno type="grant-number">61976211</idno>
				</org>
				<org type="funding" xml:id="_mMnzXvS">
					<idno type="grant-number">202202AD080004</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In addition, tensor parallelism techniques can split a model across different GPUs, making it possible to scale up training of even larger LLMs. In Table <ref type="table">3</ref> we list the minimum recommended GPU memory configuration by LMTuner for training models of different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D System Configuration and Requirements</head><p>LMTuner (<ref type="url" target="https://github.com/WENGSYX/LMTuner">https://github.com/WENGSYX/  LMTuner</ref>) is an open-source system that offers a command-line interface for training LLMs, without requiring any coding experience. The system requirements include: Ubuntu 14.04+, Debian 8+, CentOS 6+, or Fedora 27+. And an NVIDIA GPU with driver version &gt;= 460.32.03 or AMD GPU with ROMc &gt;= 4.0.</p><p>In addition, a series of Python libraries, including Apex 7 , Pytorch 8 and DeepSpeed 9 , are also required. For details, please refer to <ref type="url" target="https://wengsyx.github.io/LMTuner/install.html">https://  wengsyx.github.io/LMTuner/install.html</ref>.</p><p>After installation is complete, you can directly launch via Command-Line Interface with-7 <ref type="url" target="https://github.com/NVIDIA/apex">https://github.com/NVIDIA/apex</ref> 8 <ref type="url" target="https://pytorch.org/">https://pytorch.org/</ref> 9 <ref type="url" target="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</ref> out modifying any code by using the one-line code Let_Lingo() .</p><p>LMTuner requires the provision of an OpenAI Key to facilitate the collaborative determination of training parameters with GPT-4 through dialogue. However, we have also pre-defined ten sets of configuration questions related to training LLMs in a format similar to a questionnaire, to enable users in countries that do not support GPT-4 to quickly initiate LMTuner 10 .</p><p>Finally, LMTuner will automatically generate and run a command line to invoke deepspeed according to the requirements without needing to modify at the code level. This not only simplifies the training process, but also facilitates modifying and recording the configuration of hyperparameters, helping users without basic knowledge to also train LLMs. 10 Take a look at the current list of Supported Countries and Territories for OpenAI: <ref type="url" target="https://platform.openai.com/docs/supported-countries">https://platform.openai.com/  docs/supported-countries</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Large language models as tool makers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Arno</forename><surname>Candel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Jeblick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithvi</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Gambera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Landry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Chesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasha</forename><surname>Stetsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srisatish</forename><surname>Ambati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>h2ogpt: Democratizing large language models</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fotios</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hebgen Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">2023a. Extending context window of large language models via positional interpolation</title>
		<author>
			<persName><forename type="first">Shouyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherman</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<title level="m">Symbolic discovery of optimization algorithms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dissecting transformer length extrapolation via the lens of receptive field analysis</title>
		<author>
			<persName><forename type="first">Ta-Chung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">I</forename><surname>Ting-Han Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName><surname>Ramadge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chatlaw: Open-source legal large language model with integrated external knowledge bases</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Llm.int</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Qlora: Efficient finetuning of quantized llms</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Lamini: The llm engine for rapidly customizing models</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samee</forename><surname>Ibraheem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Daniel</surname></persName>
		</author>
		<ptr target="https://github.com/lamini-ai/lamini" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<editor>Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Textbooks are all you need</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">OpenDelta: A plug-and-play library for parameterefficient adaptation of pre-trained models</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingtai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
	<note>System Demonstrations)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Oana</forename><surname>Ignat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Abzaliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Biester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naihao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Gunal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacky</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namho</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinka</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronica</forename><surname>Nwatu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Perez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winston</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Mihalcea</surname></persName>
		</author>
		<title level="m">A phd student&apos;s perspective on</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>research in nlp in the era of very large language models</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ze-Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<title level="m">Do emergent abilities exist in quantized large language models: An empirical study</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Came: Confidenceguided adaptive memory efficient optimization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Qi jie Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter finetuning for large language models with limited resources</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengxiao</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Peft: Stateof-the-art parameter-efficient fine-tuning methods</title>
		<author>
			<persName><forename type="first">Sourab</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<ptr target="https://github.com/huggingface/peft" />
	</analytic>
	<monogr>
		<title level="m">Lysandre Debut, Younes Belkada, and Sayak Paul</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Training language models to follow instructions with human feedback</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2022b. Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Crest-copilot for real-world experimental scientist</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhichu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>A length-extrapolatable transformer</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<title level="m">Igor Molybog</title>
		<editor>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</editor>
		<editor>
			<persName><surname>Narang</surname></persName>
		</editor>
		<meeting><address><addrLine>Robert Stojnic, Sergey Edunov</addrLine></address></meeting>
		<imprint>
			<publisher>Aurelien Rodriguez</publisher>
		</imprint>
	</monogr>
	<note>and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Focused transformer: Contrastive training for context scaling</title>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Tworkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Staniszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikołaj</forename><surname>Pacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Miłoś</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Guanhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">Ade</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Lei Yang, and Yuxiong He. 2023a. Zero++: Extremely efficient collective communication for giant model training</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Linxi Fan, and Anima Anandkumar. 2023b. Voyager: An open-ended embodied agent with large language models</title>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Mandlekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaochun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mong</forename><surname>Yuan Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Nik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Jie Fu. 2023c. Interactive natural language processing</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07682</idno>
		<title level="m">Emergent abilities of large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">2023a. Large language models need holistically thought in medical conversational qa</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno>ArXiv, abs/2305.05410</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Large language models are reasoners with self-verification</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09561</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01665</idno>
		<title level="m">Neural comprehension: Language models with compiled neural networks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Training transformers with 4-bit integers</title>
		<author>
			<persName><forename type="first">Haocheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">2022a. Med-ConQA: Medical conversational question answering system based on knowledge graphs</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>UAE. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">2022b. Medconqa: Medical conversational question answering system based on knowledge graphs</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</title>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Meddialog: Large-scale medical dialogue dataset</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2023a. A survey of large language models</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">2023b. Ten-centPretrain: A scalable and flexible toolkit for pretraining models of different modalities</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiquan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigang</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimmo</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
	<note>System Demonstrations)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<title level="m">Less is more for alignment</title>
		<meeting><address><addrLine>Lima</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Minjun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14211</idno>
		<title level="m">Towards graph-hop retrieval and reasoning in complex question answering over textual database</title>
		<imprint>
			<date type="published" when="2023-06">Jun Zhao. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
