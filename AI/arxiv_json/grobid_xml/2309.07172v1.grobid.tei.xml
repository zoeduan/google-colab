<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Large Language Models for Ontology Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-12">12 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
							<email>yuan.he@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
							<email>jiaoyan.chen@manchester.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
							<email>hang.dong@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
							<email>ian.horrocks@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Large Language Models for Ontology Alignment</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
						<imprint>
							<date type="published" when="2023-09-12">12 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">DE090B033C4B397841A657D511F88287</idno>
					<idno type="arXiv">arXiv:2309.07172v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ontology Alignment, Ontology Matching, Large Language Model, GPT, Flan-T5 I. Horrocks) 0000-0002-4486-1262 (Y. He)</term>
					<term>0000-0003-4643-6750 (J. Chen)</term>
					<term>0000-0001-6828-6891 (H. Dong)</term>
					<term>0000-0002-2685-7462 (I. Horrocks)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot 1 performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design. 2   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Ontology alignment, also known as ontology matching (OM), is to identify semantic correspondences between ontologies. It plays a crucial role in knowledge representation, knowledge engineering and the Semantic Web, particularly in facilitating semantic interoperability across heterogeneous sources. This study focuses on equivalence matching for named concepts.</p><p>Previous research has effectively utilised pre-trained language models like BERT and T5 for OM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, but recent advancements in large language models (LLMs) such as ChatGPT <ref type="bibr" target="#b2">[3]</ref> and Flan-T5 <ref type="bibr" target="#b3">[4]</ref> necessitate further exploration. These LLMs, characterised by larger parameter sizes and task-specific fine-tuning, are typically guided by task-oriented prompts in a zero-shot setting or a small set of examples in a few-shot setting when applying to downstream tasks.</p><p>This work explores the feasibility of employing LLMs for zero-shot OM. Given the significant computational demands of LLMs, it is crucial to conduct experiments with smaller yet representative datasets before full deployment. To this end, we extract two challenging subsets from the NCIT-DOID and the SNOMED-FMA (Body) equivalence matching datasets, both part of Bio-ML 1  [5] -a track of the Ontology Alignment Evaluation Initiative (OAEI) that is compatible with machine learning-based OM systems. Notably, the extracted subsets exclude "easy" mappings, i.e., concept pairs that can be aligned through string matching.</p><p>We mainly evaluate the open-source LLM, Flan-T5-XXL, the largest version of Flan-T5 containing 11B parameters <ref type="bibr" target="#b3">[4]</ref>. We assess its performance factoring in the use of concept labels, score thresholding, and structural contexts. For baselines, we adopt the previous top-performing OM system BERTMap and its lighter version, BERTMapLt. Preliminary tests are also conducted on GPT-3.5-turbo; however, due to its high cost, only initial results are reported. Our findings suggest that LLM-based OM systems hold the potential to outperform existing ones, but require efforts in prompt design and exploration of optimal presentation methods for ontology contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>Task Definition The task of OM can be defined as follows. Given the source and target ontologies, denoted as 𝒪 𝑠𝑟𝑐 and 𝒪 𝑡𝑔𝑡 , and their respective sets of named concepts 𝒞 𝑠𝑟𝑐 and 𝒞 𝑡𝑔𝑡 , the objective is to generate a set of mappings in the form of (𝑐 ∈ 𝒞 𝑠𝑟𝑐 , 𝑐 ′ ∈ 𝒞 𝑡𝑔𝑡 , 𝑠 𝑐≡𝑐 ′ ), where 𝑐 and 𝑐 ′ are concepts from 𝒞 𝑠𝑟𝑐 and 𝒞 𝑡𝑔𝑡 , respectively, and 𝑠 𝑐≡𝑐 ′ ∈ [0, 1] is a score that reflects the likelihood of the equivalence 𝑐 ≡ 𝑐 ′ . From this definition, we can see that a paramount component of an OM system is its mapping scoring function 𝑠 : 𝒞 𝑠𝑟𝑐 × 𝒞 𝑡𝑔𝑡 → [0, 1]. In the following, we formulate a sub-task for LLMs regarding this objective.</p><p>Concept Identification This is essentially a binary classification task that determines if two concepts, given their names (multiple labels per concept possible) and/or additional structural contexts, are identical or not. As LLMs typically work in a chat-like manner, we need to provide a task prompt that incorporates the available information of two input concepts, and gather classification results from the responses of LLMs. To avoid excessive prompt engineering, we present the task description (as in previous sentences) and the available input information (such as concept labels and structural contexts) to ChatGPT based on GPT-4<ref type="foot" target="#foot_0">foot_0</ref> , and ask it to generate a task prompt for an LLM like itself. The resulting template is as follows:</p><p>Given the lists of names and hierarchical relationships associated with two concepts, your task is to determine whether these concepts are identical or not. Consider the following: Analyze the names and the hierarchical information provided for each concept and provide a conclusion on whether these two concepts are identical or different ("Yes" or "No") based on their associated names and hierarchical relationships.</p><note type="other">Source</note><p>where the italic part is generated in the second round when we inform ChatGPT parent/child contexts can be considered. Since the prompt indicates a yes/no question, we anticipate the generation of "Yes" or "No" tokens in the LLM responses. For simplicity, we use the generation probability of the "Yes" token as the classification score. Note that this score is proportional to the final mapping score but is not normalised. For ranking-based evaluation, given a source concept, we also consider candidate target concepts with the "No" answer as well as their "No" scores, placing them after the candidate target concepts with the "Yes" answer in an ascending order -a larger "No" score implies a lower rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>Dataset Construction Evaluating LLMs with the current OM datasets of normal or large scales can be time and resource intensive. To yield insightful results prior to full implementation, we leverage two challenging subsets extracted from the NCIT-DOID and the SNOMED-FMA (Body) equivalence matching datasets of the OAEI Bio-ML track. We opt for Bio-ML as its ground truth mappings are curated by humans and derived from dependable sources, Mondo and UMLS. We choose NCIT-DOID and SNOMED-FMA (Body) from five available options because their ontologies are richer in hierarchical contexts. For each original dataset, we first randomly select 50 matched concept pairs from ground truth mappings, but excluding pairs that can be aligned with direct string matching (i.e., having at least one shared label) to restrict the efficacy of conventional lexical matching. Next, with a fixed source ontology concept, we further select 99 unmatched target ontology concepts, thus forming a total of 100 candidate mappings (inclusive of the ground truth mapping). This selection is guided by the sub-word inverted index-based idf scores as in He et al. <ref type="bibr" target="#b0">[1]</ref>, which are capable of producing target ontology concepts lexically akin to the fixed source concept. We finally randomly choose 50 source concepts that do not have a matched target concept according to the ground truth mappings, and create 100 candidate mappings for each. Therefore, each subset comprises 50 source ontology concepts with a match and 50 without. Each concept is associated with 100 candidate mappings, culminating in a total extraction of 10,000, i.e., (50+50)*100, concept pairs. Evaluation Metrics From all the 10,000 concept pairs in a given subset, the OM system is expected to predict the true mappings, which can be compared against the 50 available ground truth mappings using Precision, Recall, and F-score defined as:</p><formula xml:id="formula_0">𝑃 = |ℳ 𝑝𝑟𝑒𝑑 ∩ ℳ 𝑟𝑒𝑓 | |ℳ 𝑝𝑟𝑒𝑑 | , 𝑅 = |ℳ 𝑝𝑟𝑒𝑑 ∩ ℳ 𝑟𝑒𝑓 | |ℳ 𝑟𝑒𝑓 | , 𝐹 1 = 2𝑃 𝑅 𝑃 + 𝑅</formula><p>where ℳ 𝑝𝑟𝑒𝑑 refers to the set of concept pairs (among the 10,000 pairs) that are predicted as true mappings by the system, and ℳ 𝑟𝑒𝑓 refers to the 50 ground truth (reference) mappings.</p><p>Given that each source concept is associated with 100 candidate mappings, we can calculate ranking-based metrics based on their scores. Specifically, we calculate Hits@1 for the 50 matched source concepts, counting a hit when the top-scored candidate mapping is a ground truth mapping. The MRR score is also computed for these matched source concepts, summing the inverses of the ground truth mappings' relative ranks among candidate mappings. These two scores are formulated as:</p><formula xml:id="formula_1">𝐻𝑖𝑡𝑠@𝐾 = ∑︁ (𝑐,𝑐 ′ )∈ℳ 𝑟𝑒𝑓 I 𝑟𝑎𝑛𝑘 𝑐 ′ ≤𝐾 /|ℳ 𝑟𝑒𝑓 |, 𝑀 𝑅𝑅 = ∑︁ (𝑐,𝑐 ′ )∈ℳ 𝑟𝑒𝑓 𝑟𝑎𝑛𝑘 -1 𝑐 ′ /|ℳ 𝑟𝑒𝑓 |</formula><p>For the 50 unmatched source concepts, we compute the Rejection Rate (RR), considering a successful rejection when all the candidate mappings are predicted as false mappings by the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results on the challenging subset of the SNOMED-FMA (Body) equivalence matching dataset of Bio-ML.</p><p>system. The unmatched source concepts are assigned a "null" match, denoted as 𝑐 𝑛𝑢𝑙𝑙 . This results in a set of "unreferenced" mappings, represented as ℳ 𝑢𝑛𝑟𝑒𝑓 . We can then define RR as:</p><formula xml:id="formula_2">𝑅𝑅 = ∑︁ (𝑐,𝑐 𝑛𝑢𝑙𝑙 )∈ℳ 𝑢𝑛𝑟𝑒𝑓 ∏︁ 𝑑∈𝒯𝑐 (1 -I 𝑐≡𝑑 )/|ℳ 𝑢𝑛𝑟𝑒𝑓 |</formula><p>where 𝒯 𝑐 is the set of target candidate classes for a source concept 𝑐, and I 𝑐≡𝑑 is a binary indicator that outputs 1 if the system predicts a match between 𝑐 and 𝑑, and 0 otherwise. It is worth noting that the product term becomes 1 only when all target candidate concepts are predicted as false matches, i.e., ∀𝑑 ∈ 𝒯 𝑐 .I 𝑐≡𝑑 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Settings</head><p>We examine Flan-T5-XXL under various settings: (i) the vanilla setting, where a mapping is deemed true if it is associated with a "Yes" answer; (ii) the threshold <ref type="foot" target="#foot_1">3</ref> setting, which filters out the "Yes" mappings with scores below a certain threshold; (iii) the parent/child setting, where sampled parent and child concept names are included as additional contexts; and (iv) parent/child+threshold setting, incorporating both structural contexts and thresholding. We also conduct experiments for GPT-3.5-turbo, the most capable variant in the GPT-3.5 series, using the same prompt. However, only setting (i) is reported due to a high cost of this model. For the baseline models, we consider BERTMap and BERTMapLt <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, where the former uses a fine-tuned BERT model for classification and the latter uses the normalised edit similarity. Note that both BERTMap and BERTMapLt inherently adopt setting (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As shown in Table <ref type="table" target="#tab_0">1</ref>-2, we observe that the Flan-T5-XXL (+threshold) obtains the best F-score among its settings. While it outpaces BERTMap by 0.093 in F-score on the NCIT-DOID subset but lags behind BERTMap and BERTMapLt by 0.206 and 0.049, respectively, on the SNOMED-FMA (Body) subset. Regarding MRR, BERTMap leads on both subsets. Among Flan-T5-XXL settings, using a threshold enhances precision but reduces recall. Incorporating parent/child contexts does not enhance matching results -this underscores the need for a more in-depth examination of strategies for leveraging ontology contexts. GPT-3.5-turbo <ref type="foot" target="#foot_2">4</ref> does not perform well with the given prompt. One possible reason is the model's tendency to furnish extended explanations for its responses, making it challenging to extract straightforward yes/no answers. Besides, no ranking scores are presented for GPT-3.5-turbo because it does not support extracting generation probabilities. The suboptimal performance of BERTMapLt is as expected because we exclude concept pairs that can be string-matched from the extracted datasets while BERTMapLt relies on the edit similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>This study presents an exploration of LLMs for OM in a zero-shot setting. Results on two challenging subsets of OM datasets suggest that using LLMs can be a promising direction for OM but various problems need to be addressed including, but not limited to, the design of prompts and overall framework <ref type="foot" target="#foot_3">5</ref> , and the incorporation of ontology contexts. Future studies include refining prompt-based approaches, investigating efficient few-shot tuning, and exploring structure-informed LLMs. The lessons gleaned from these OM studies can also offer insights into other ontology engineering tasks such as ontology completion and embedding, and pave the way for a broader study on the integration of LLMs with structured data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Concept Names: &lt;list of concept names&gt; Parent Concepts of the Source Concept: &lt;list of concept names&gt; Child Concepts of the Source Concept: &lt;list of concept names&gt; ... (same for the target concept)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Results on the challenging subset of the NCIT-DOID equivalence matching dataset of Bio-ML.</figDesc><table><row><cell>System</cell><cell cols="2">Precision Recall</cell><cell cols="2">F-score Hits@1</cell><cell>MRR</cell><cell>RR</cell></row><row><cell>Flan-T5-XXL</cell><cell>0.643</cell><cell>0.720</cell><cell>0.679</cell><cell>0.860</cell><cell>0.927</cell><cell>0.860</cell></row><row><cell>+ threshold</cell><cell>0.861</cell><cell>0.620</cell><cell>0.721</cell><cell>0.860</cell><cell>0.927</cell><cell>0.940</cell></row><row><cell>+ parent/child</cell><cell>0.597</cell><cell>0.740</cell><cell>0.661</cell><cell>0.880</cell><cell>0.926</cell><cell>0.760</cell></row><row><cell>+ threshold &amp; parent/child</cell><cell>0.750</cell><cell>0.480</cell><cell>0.585</cell><cell>0.880</cell><cell>0.926</cell><cell>0.920</cell></row><row><cell>GPT-3.5-turbo</cell><cell>0.217</cell><cell>0.560</cell><cell>0.313</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BERTMap</cell><cell>0.750</cell><cell>0.540</cell><cell>0.628</cell><cell>0.900</cell><cell>0.940</cell><cell>0.920</cell></row><row><cell>BERTMapLt</cell><cell>0.196</cell><cell>0.180</cell><cell>0.187</cell><cell>0.460</cell><cell>0.516</cell><cell>0.920</cell></row><row><cell>System</cell><cell cols="2">Precision Recall</cell><cell cols="2">F-score Hits@1</cell><cell>MRR</cell><cell>RR</cell></row><row><cell>Flan-T5-XXL</cell><cell>0.257</cell><cell>0.360</cell><cell>0.300</cell><cell>0.500</cell><cell>0.655</cell><cell>0.640</cell></row><row><cell>+ threshold</cell><cell>0.452</cell><cell>0.280</cell><cell>0.346</cell><cell>0.500</cell><cell>0.655</cell><cell>0.820</cell></row><row><cell>+ parent/child</cell><cell>0.387</cell><cell>0.240</cell><cell>0.296</cell><cell>0.540</cell><cell>0.667</cell><cell>0.900</cell></row><row><cell>+ threshold &amp; parent/child</cell><cell>0.429</cell><cell>0.120</cell><cell>0.188</cell><cell>0.540</cell><cell>0.667</cell><cell>0.940</cell></row><row><cell>GPT-3.5-turbo</cell><cell>0.075</cell><cell>0.540</cell><cell>0.132</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BERTMap</cell><cell>0.485</cell><cell>0.640</cell><cell>0.552</cell><cell>0.540</cell><cell>0.723</cell><cell>0.920</cell></row><row><cell>BERTMapLt</cell><cell>0.516</cell><cell>0.320</cell><cell>0.395</cell><cell>0.340</cell><cell>0.543</cell><cell>0.960</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>ChatGPT (GPT-4 version): https://chat.openai.com/?model=gpt-4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The thresholds are empirically set to 0.650, 0.999, and 0.900 for Flan-T5-XXL, BERTMap, and BERTMapLt in a pioneer experiment concerning small fragments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The experimental trials for text-davinci-003 and GPT-4 also showed suboptimal results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>This work focuses on the mapping scoring, but the searching (or candidate selection) part of OM is also crucial, especially considering that LLMs are highly computationally expensive.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">BERTMap: A BERT-based ontology alignment system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Antonyrajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eslamialishah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahramali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naddaf-Sh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zarandioon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Truveta mapper: A zero-shot ontology alignment framework</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Machine learning-friendly biomedical datasets for equivalence and subsumption ontology matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiménez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ISWC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allocca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sapkota</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03067</idno>
		<title level="m">Deeponto: A python package for ontology engineering with deep learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
