<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner&apos;s Students</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
							<email>merlin.carl@uni-flensburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut für Mathematik Europa</orgName>
								<orgName type="institution">Universität Flensburg Flensburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Euf</forename><surname>Flensburg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Mathematik Europa</orgName>
								<orgName type="institution">Universität Flensburg Flensburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Germany</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Mathematik Europa</orgName>
								<orgName type="institution">Universität Flensburg Flensburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Large Language Models for (De-)Formalization and Natural Argumentation Exercises for Beginner&apos;s Students</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">162B9292B959A442E1055D0370AA6368</idno>
					<idno type="DOI">10.4204/EPTCS.400.3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-04-29T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">92ea31e</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe two systems currently being developed that use large language models for the automatized correction of (i) exercises in translating back and forth between natural language and the languages of propositional logic and first-order predicate logic and (ii) exercises in writing simple arguments in natural language in non-mathematical scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Using Large Language Models for Autoformalization Autoformalization, i.e., the automated translation from natural language to formal logic, is a natural language processing task that has been adressed in a number of ways; natural language proof checking systems such as Naproche (see, e.g., Cramer, [9]) and SAD (Verchinine et al., [15]) make use of grammar-based approaches, with natural-language parsers producing intermediate formats between natural language and formal logic. Recently, the interest in machine learning approaches has increased, including the use of neural networks (Azerbayev et al., [2], Wang et al., [14]) and large language models (Wu et al., [16]); autoformalization for mathematics at the undergraduate level was explored in Azerbayev et al. [2]. Two strong large language model that are publically available are OpenAI's GPT-3.5, with the associated text completion model text-davinci-003 <ref type="bibr" target="#b0">1</ref> , and the more recent GPT-4-Turbo. <ref type="bibr" target="#b1">2</ref> One advantage of pretrained models over training neural networks is that frequently, a few examples are sufficient to obtain a stable and usable model. It has been observed in [2] that GPT can successfully be used for autoformalization. Not surprisingly, the performance improves drastically when one merely demands autoformalization of expressions in a controlled natural language (CNL) rather than arbitrary sentences in natural (mathematical) language, which can be used, e.g., for automated proof checking in teaching contexts, see [7].</p><p>For the applications we describe in this article, however, we are interested not in translating mathematical sentences into formal logic, but rather statements in natural language on everyday matters. To this end, we wrote prompts for text-davinci-003 consisting of different numbers of examples, depending on the complexity of the task:</p><p>• 30 examples for the translation of natural language sentences into formulas in propositional logic with a given notation.</p><p>• 44 examples for the translation of natural language sentences into formulas in propositional logic, combined with a classification determining whether that sentences formulates a claim or an assumption.</p><p>• 56 examples for the translation of natural language sentences into formulas in first-order predicate logic.</p><p>Typical example prompts for the three tasks looked like this (original German; translated to English for the convenience of the reader):</p><p>• notation:{S:Fritz takes a boat;F:Fritz takes a plane;A:Fritz arrives in America;K:Fritz tries to swim}Fritz arrives in America if and only if he takes a boat or a plane, but not if he tries to swim. <ref type="bibr" target="#b3">3</ref> #((S ∨ F) ↔ A) ∧ (K → ¬A) §</p><p>• notation:{W:This is supposed to be a joke;L:This is supposed to be funny;N:This is new}If this is supposed to be a joke, it is neither funny nor new.♯[claim <ref type="bibr" target="#b4">4</ref> , [W, →, [neg, [L, or, N]]]] §</p><p>• notation:{B(x,y):x is the brother of y;S(x,y):x is the sister of y}The sister of someone's brother is that someone's sister.♯∀x : ∀z : (∃y : (B(x, y) ∧ S(y, z)) → S(x, z)) §</p><p>As one can see, these examples are structured as follows: the first part of the form "notation:{...}" introduces a number of propositional letters, predicate letters and constant symbols, along with their intended semantics; this is followed by a sentence in natural language, a sharp serving as a separation symbol, a formalization of the natural language sentence in the given notation and a stop symbol. In each case, a few examples were added where either the natual language sentence was not a sentence at all, but rather some nonsense string, or could not be expressed in the given notation.</p><p>Requests to the model prompted in this way can then be made by expanding the prompt by a string of the form "notation:{...}φ ♯", where φ is a natural language sentence. Experiments showed a satisfying performance on the intended kind of (simple) example sentences. An impressive feature was that formalizations worked well even when the precise formulation did not use the expressions given in the prompt. Thus, in the notation "R: It rains, S: There is a storm; P-the party will be cancelled", the sentence "If it pours or there is a strong wind, there will be no feast" was (correctly) formalized as (R ∨ S) → P.</p><p>The main drawbacks were the following:</p><p>• The model showed a tendency to report sentences with strange, wrong or absurd content, such as "If the moon is made of green cheese, then there is a giraffe on the moon" are as erroneous, even though they could easily be formalized within the given notation. Adding an explanation of the specific meaning of "error" in this context and several further examples did not solve this issue. Consequently, such examples, which are somewhat typical for logic classes, should at the moment be avoided when using the model.</p><p>• The model showed a certain tendency to use all pieces of the given notation in its formalization, so that, e.g., "Barking dogs don't bark" <ref type="bibr" target="#b5">5</ref> was formalized as ∀x((D(x) ∧ B(x)) → S(x)) in the notation where D(x) stood for "x is a dog", B(x) for "x barks" and S(x) for "x bites". This was considerably improved by adding several examples with superfluous notation.</p><p>• The model showed a tendency to "project" expressions onto the given notation; for example, in the notation from the last bullet point, "Barking cats don't bite" was formalized as ∀x((D(x)∧B(x)) → S(x)), even though no predicate letter for "cat" was contained in the notation ("meowing cats don't bite", however, led to an error message). Whether or not this presents a serious issue for the intended application remains to be seen.</p><p>• For sentences of high logical complexity (e.g., containing several quantifier alternations or junctors), the formalizations were frequently wrong. This, however, does not present much of an issue for the intended application.</p><p>• The disadvantages of using cloud-based LLMs include (i) their lack of stability (continued training can lead to a much worse performance, rendering working applications unusable), (ii) their lack of reliable availability (models may stop being available altogether, and temporarily become unavailable due to server capacity issues) and (iii) pricing. For these reasons, we expect our system to remain in an experimental state until workable local alternatives become available.</p><p>By now, text-davinci-003, which was state of the art when our system was developed early in 2023, is considered "legacy" by OpenAI and its use deprecated. Thus, additionally, we tried the same task with an AI-"assistant" based on the more recent model GPT-4-Turbo. Here, we achieved a surprisingly good performance using merely a prompt explaining the notation to be used (i.e., without offering any initial example cases); in contrast to the above examples, however, the notation -i.e., the abbreviations for the atomic propositions, predicate and constant symbols to be used in the formalization -was fixed and the same for all the examples. The precise prompts, along with the example sentences and the obtained output for 50 instances of first-order-formalization and 57 instances of formalization in propositional logic, can be found in the appendix. As can be seen from the examples (54)-(57) for propositional logic, "nonsense" sentences concerning strongly counterfactual scenarious did no longer pose an issue for GPT-4-Turbo. Concerning logically absurd sentences, such as (43), (44), (47), (48), one of these (44) was still (wrongly) labeled as "not expressable", <ref type="bibr" target="#b6">6</ref> while the other two were processed correctly. In formalization in quantifier logic, all "absurd" examples, namely (24), (26), (33), were processed correctly. While a certain flexibility of expression was retained -for example, the sentence "In terms of size, Fritz surpasses himself" was formalized correctly given a notation that contained a two-place predicate for "larger than" -the issue of replacing non-expressible terms by terms in the vocabulary did no longer show up: Thus, the propositional examples (20), (21), ( <ref type="formula">27</ref>), (57) were correctly identified as "not expressable".</p><p>For the task of formalization in propositional logic, we also evaluated several large language models that are available locally, i.e., they can be run on the user's local machine rather than remotely. The performance of general local LLMs and also of LLMs focusing on mathematics -such as WizardMath-70B <ref type="bibr" target="#b7">7</ref> -turned out to be poor; the only reasonable results were obtained with models trained on the task of code-writing, in particular WizardCoder-34B. <ref type="bibr" target="#b8">8</ref> For propositional logic, GPT-4-Turbo formalized 55 of the 57 sample sentences correctly, or about 96.5 percent. The best performance of a local large language model for the same set of sentences was successful for 40 sentences, corresponding to a success rate of about 69 percent. As can be seen in lines (54)-(57), highly counterfactual content -which was a serious issue for davinci-003 -was consistently <ref type="bibr" target="#b6">6</ref> When asked for the reason why (44) would not be expressable, the model replied that "the statement is self-contradictory and cannot be expressed meaningfully using the given notation or any standard logical operators because it violates the law of non-contradiction. Thus, the proper response is "not expressable".", thus explicitly defending the (wrong) claim that logically inconsistent statements cannot be expressed in the formalism of propositional logic. <ref type="bibr" target="#b7">7</ref> See <ref type="url" target="https://huggingface.co/TheBloke/WizardMath-70B-V1.0-GGML">https://huggingface.co/TheBloke/WizardMath-70B-V1.0-GGML</ref>. <ref type="bibr" target="#b8">8</ref> See <ref type="url" target="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0">https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0</ref>.</p><p>processed correctly by both models. Logically absurd statements, such as (43), ( <ref type="formula">44</ref>), (48) were falsely labeled as "not expressible" once by both models; here, GPT-4 generated a lengthy explanation claiming that contradictory statements cannot be expressed in formal logic. Tautologous statements, such as (47) were unproblematic, as were factually wrong statements for both models. Both models could successfully handle a certain freedom of expression, as is demonstrated by the sentences (17), ( <ref type="formula">18</ref>), ( <ref type="formula">19</ref>), ( <ref type="formula">31</ref>), (32), (34). Both models still had an issue with intricate logical structure, such as triple negation (38);</p><p>as such examples are likely to arise rarely in the training data, this is to be expected. While GPT-4-Turbo clearly outperforms the local model, both models show usable results for the intended application, namely simple exercises of limited logical complexity.</p><p>For the task of autoformalizing natural language sentences in first-order logic, no local LLM showed a satisfying performance. The best results were obtained with GPT-4-Turbo, where 46 out of 50 provided example sentences were formalized correctly, i.e., 92 percent. The unexpressable sentences (18), ( <ref type="formula">32</ref>), (36) were correctly identified. In cases where the content was counterfactual or absurd, the model stated this fact in a text comment accompanying the formalization, but still provided a correct formalization (see ( <ref type="formula">24</ref>), ( <ref type="formula">26</ref>), ( <ref type="formula">27</ref>), ( <ref type="formula">33</ref>)). For some of those sentences that were not formalized as expected, the model provided explanations: In the case of (13) ("If Hector barks, he is not a real dog"), the model explained that the notation did not allow for expressing "is not a real dog"; given common ways of using "real XYZ", this is arguably correct; a similar point can be made for (46). Without these examples, the success rate would be about 96 percent. An actual mistake is (34), which was not expressible, as no 2-place-predicate "x bites y" was given in the notation; in (35), the formulation "baying canine" was apparently too "off" in order to be identified as "barking dog". However, such formulations are not to be expected from someone seriously attempting to solve a deformalization exercise, so that the practical relevance of this mistake for the intended application appear limited.</p><p>We point out that the system is currently in an experimental stage. In particular, several practical challenges in terms of pricing and stability of the underlying LLMs will need to be overcome before the system can be actually employed (and tested) in teaching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automatized Correction of (De-)Formalization Exercises</head><p>Beginner students frequently have difficulties both with expressing statements in the language of logic and with interpreting statements written in the language of formal logic. Even after becoming acquainted to the meaning of the logical symbols, extracting the meaning remains a challenge: Frequently, a sentence such as ∃x : child(x) ∧ swims(x) will be read out as "There is x such that x is a child and x swims", remaining close to the surface structure of the formula, rather than something like "Some child swims". Indeed, the relation between "natural" ways of thinking of something and the way it is expressed in formal logic is quite intricate; one notorious example of this being the way of expressing dynamics via quantifier changes. <ref type="bibr" target="#b9">9</ref> Formalization and deformalization exercises are meant to practice this crucial skill in university education in mathematics. In a formalization exercise -also known as "math dictation", see, e.g., [4] 10 -a sentence in natural language is given, together with some formal vocabulary, and the student's task is to produce a logical formula expressing this sentence. Thus, a typical formalization exercise could look like this:</p><p>• Let S stand for the statement "The sun shines", W for the statement "I go out for a walk". Formalize the statement "I won't go for a walk unless the sun shines" in the language of propositional logic.</p><p>• Let C(x) stand for "x is a child", S(x) for "x swims". Formalize the statement "Some children swim" in the language of first-order predicate logic.</p><p>It is not too hard to provide automated feedback for such formalization exercises: One can, for example, store the exercise in the form of a pair (η, φ ) consisting of the natural language statement η to be displayed to the user and a correct formalization φ of it, take the user's input, determine whether it is a well-formed formula at all (and, if not, provide according feedback), and, in case it is, pass the tasks φ → η and η → φ to an automated theorem prover, such as the PyProver, which can be imported into Python code as a package. Systems that work roughly in this way include the "mathematical logic tutor" of Moreno et al. [13] or the formalization exercises in Edukera [1].</p><p>We remark here <ref type="bibr" target="#b11">11</ref> that the concept of formalization is rather intricate. Indeed, a formalization in the sense above is a special case of a translation, and so the difficult question about the adequacy of translations also applies to formalizations: When is a formula φ an adequate formalization of a natural language sentence S? A naive approach might be to say that φ and S must be in some sense "provably equivalent"; this, however, leads to several difficulties: First, one would have to make precise the informal notion of proof required to make sense of this criterion. Second, such a notion would necessarily be relative to the set of accepted background assumptions, which are usually left implicit and may be highly context-dependent. For example, when asked to formalize "If a, b are the lengths of the catheti of a right triangle and c is the length of its hypotenuse, then</p><formula xml:id="formula_0">c 3 &gt; a 3 + b 3 ", it should be acceptable -if theory-laden -to respond with ∀a, b, c ∈ R + ((c &gt; a + b ∧ a 2 + b 2 = c 2 ) → c 3 &gt; a 3 + b 3</formula><p>), thus presupposing Pythagoras' theorem; but if the sentence would be to formalize "If a, b are the lengths of the catheti of a right triangle and c is the length of its hypotenuse, then a 2 + b 2 = c 2 ", it would be most inadequate to formalize this in the same way, thus leading to a tautology. (One might even ask whether it can be formalized adequately in the language of real closed fields at all without circularity.) Third, mere provable equivalence is a poor criterion for the adequacy of a formalization; for example, formalizing either of the preceding statements as 1 = 1 should certainly not be counted as correct. <ref type="bibr" target="#b12">12</ref> Other great examples of formalization, such as Gödel's arithmetization of the concept of first-order provability require a creative mastering of some background theory. This kind of formalizing is a task resembling programming more than translation, and it is not what the system sketched in this paper is meant to teach. The difference between "translation-like formalization" and "programming-like formalization", although didactically quite obvious, is not easy to pin down logically. However, by considering everyday rather than mathematical contexts, this issue is mostly avoided: There is not much background theory that could be used in expressing a sentence such as "Barking dogs don't bite" into a first-order language with a vocabulary for "barks", "bites" and "is a dog", let alone for interpreting the respective first-order formula in natural language. We will thus accept a formula φ as a correct formalization of a sentence S with a fixed given formalization ψ if and only if the equivalence φ ↔ ψ is a tautology in the respective logic (that is, propositional logic or first-order logic), i.e., using the empty background theory. This seems to yield a didactically acceptable notion of adequacy, as long as the sentences under consideration are neither tautological nor contradictory -although "weird" formalizations accepted by a system working with this premise are still possible (e.g. by writing something like A ∨ (A ∧ A) or ¬¬¬¬A rather than A, or by forming a conjunction with a number of unrelated tautologies), they are unlikely to actually be proposed by students, except by those who have already mastered the subject at hand anyway.</p><p>Deformalization exercises, on the other hand, are a far more delicate matter. In a deformalization exercise, a student is given a formal vocabulary together with a logical formula using that vocabulary, and is asked to express it in natural language in the simplest possible terms. Thus, typical deformalization exercises look as follows:</p><p>• Let S stand for the statement "The sun shines", W for the statement "I go out for a walk". Express the statement W → S in natural language.</p><p>• Let C(x) stand for "x is a child", S(x) for "x swims". Express the statement ∃x : C(x) ∧ S(x) in natural language.</p><p>The automatized correction of such exercises is challenging in at least to ways: First, one needs to automatically translate the user's natural language input into the appropriate formal language, using the specified vocabulary. Second, one needs to grade the "naturalness" of the user's input, so that expressions such as "There is x such that x is a child and x swims" mentioned above receive a different feedback than "Some children swim".</p><p>In our system, which is currently in an experimental stage but will eventually be integrated into the Diproche system ([8], [5], [7], [4], [6]), exercises are stored as triples (n, η, φ ), where n is the available vocabulary, η is a natural language sentence and φ is the formalization thereof, which should be "optimal" in terms of naturalness. This kind of representation has the advantage that the same exercise can both be used as a formalization exercise (displaying n and η, then asking for φ ) and as a deformalization exercise (displaying n and φ , then asking for η).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Checking Deformalizations</head><p>In this section, we will explain the architecture of the checking routine for deformalization exercises. As explained above, users are given a set of notations and a formula using these notations. They can then enter an arbitrary string in a text window in a web interface. This input string is passed on to the formalization routine, which uses the input and the training examples to generate a request for a large language model (such as text-davinci-003) and passes it on to this LLM; if this yields the output "error", an error message is displayed to the user and no further processing takes place; otherwise, the formalization of the users input and the formula given in the problem statement are passed on to the PyProver, which determines whether the implications between them are provable (i.e., propositional or first-order tautologies). More precisely, if ψ is the formal expression fixed in the problem statement and φ is the result of autoformalizing the user's input, the PyProver is asked to prove both φ → ψ and ψ → φ . The result of this is then passed on to the feedback creation, which will report which of these implications could be verified.</p><p>However, a merely logically correct answer does not imply that the input string is a good deformalization of the given formula, or that the user has understood this formula. Typically, beginners will translate a formula such as ∀x((D(x) ∧ B(x)) → ¬S(x)) -in the vocabulary given above -"word for word" into something like "For all x, if x is a dog and x barks, then x does not bite". Thus, in order to provide a meaningful assessment of the input, the mere logical correctness needs to be supplemented with an evaluation of the "naturalness" or "simplicity" of the input string. This is the task of the "Grader" module.</p><p>Our attempts to train language models to provide such an assessment were utterly unsuccessful so far: Even with a considerable number of examples, the model appeared to be unable to distinguish unnaturally formulated sentences from perfectly naturally formulated sentences with an unusual content. We thus resorted to a rather simple-minded solution, which, however, turned out to work quite well for our purpose: We measure the complexity of the input by relating its length to the length of the template solution entered as part of the problem statement (although not displayed to the user) and normalize the result. A "word for word translation" of logical syntax into natural language will usually be considerably longer than the shortest formulation in natural language, so that this can be expected to approximate the degree of naturalness satisfyingly well. More precisely, if |s| denotes the length of a string, η denotes the template solution in the problem statement and φ is the user's input, the degree of simplicity is given by 10σ (10(</p><formula xml:id="formula_1">|η| |φ | -0.7))</formula><p>where σ is the usual sigmoid function, given by σ (x) := 1 1+e -x . This yields a measure of the input's simplicity on a scale from 0 (not natural at all) to 10 (very natural); feedback is provided depending on whether this value is less than or equal to 5, strictly between 5 and 8 or equal to or above 8. Thus, a good solution, which will have |η| ≈ |φ |, will have σ (10( |η| |φ | -0.7)) close to (but below) 1, while a overcomplicated solution with |φ | much larger than η will lead to a value close to 0. A score of at least 5 is awarded if the template solution has a length that is 70 percent (or less) of the length of the input string.</p><p>Finally, the feedback creation either reports that the input is logically incorrect (not equivalent to the formula to be expressed) and, in this case, whether it is necessary, but not sufficient, sufficient, but not necessary, or neither; in this case, no evaluation of the simplicity of the input is given. If, on the other hand, the input was logically correct, then the user is provided with feedback reporting this, accompanied by the system's evaluation of its simplicity, asking the user, if necessary, to try her hand at further simplifications of her expression.</p><p>There are at least two sources of potential mistakes in this approach: First, the LLM may provide a formalization not faithful to the user's input, either by an actual formalization mistake or by interpreting the natural language semantics in a subtly different way, and second, the ATP may fail to verify the equivalence even if the input is correct. Due to the decidability of propositional logic (and the fact that the expressions arising in the given contexts will use less than 10 propositional variables and be of surveyable length, so that no resource issues can occur), the latter kind of mistake can only occur for first-order logic. Since the formulas coming up in this context will be logically rather simple -typically restricted to a single quantifier change -this is not likely to be a frequent issue; however, a substantial evaluation of this point will have to wait until the system can be actually employed with student users. <ref type="bibr" target="#b13">13</ref> Potentially, it cannot lead to wrong solutions being reported as correct, but it could lead to correct solutions being reported as incorrect. In order not to confuse users, the feedback should thus be formulated as a "failure to verify" rather than as a claim that the solution is wrong. The user may then attempt to reformulate her solution to make it easier processable. Another option would be to switch to an ATP that can generate countermodels to non-verifable statements and report these back to the user. The former type of mistake can lead both to correct solutions being reported as incorrect and to incorrect solutions being reported as correct. The latter issue may in particular come up for sentences that deviate from, but strongly resemble, very common natural language sentences that are likely to occur in the training data. Also, issues are <ref type="bibr" target="#b13">13</ref> One way to exclude such difficulties altogether would be to restrict exercises to decidable fragments of first-order logic, such as monadic first-order logic (see, e.g., [3]), formulas with at most two variables (see, e.g., [11]) etc.; these classes already lead to a rich supply of exercises. If this path is taken, one needs to ensure that the user's input will also fall into the relevant class.</p><p>Interface Formalize text-davinci-003 Prover Grader Feedback Creation Figure <ref type="figure">1</ref>: Flowchart for the correction routine for deformalization exercises. (Generated with the help of GPT-3.5.) likely to occur when students write ungrammatical inputs. Again, an evaluation of the seriousness of such issues will have to wait until the system can actually be employed. To counter such difficulties, the formalization obtained by the system could be reported back to the user to check whether it faithfully captures what she intended to express. However, since the system is intended to teach basic skills in translating back and forth between formal logic and natural language, it is not clear that the intended users will be able to determine whether the formalization was done correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Natural Language Argumentation</head><p>Besides formalization, learning how to prove is another challenge for beginner students. The Diproche system ([8], [5]) is designed to provide automated feedback on the several aspects -including logical correctness -for solutions to simple beginner proving exercises written in a controlled natural language (CNL). Recently, large language models have been integrated in the Diproche architecture to provide a more "liberal" CNL than the original parser-based one (see [7]).</p><p>Diproche is focused on mathematical argumentation in areas such as propositional logic, Boolean set theory or elementary number theory. As a preliminary exercise, it may also be helpful to practice logical argumentation in non-mathematical contexts, putting aside the difficulties students may have with the mathematical content and symbolism. Thus, a natural argumentation exercise might look like this:</p><p>• Suppose that the following is true: If the sun shines, Hans goes for a walk. When Hans goes for a walk, he takes his dog with him. When Hans takes his dog for a walk, the dog barks at the cat on the neighbour's roof. When the dog barks at the cat on the roof, the cat runs away. However, the cat still sits on the roof. Show that the sun does not shine.</p><p>Using the same prompts discussed in the last section, such statements can automatically be translated into propositional logic <ref type="bibr">14</ref> ; the resulting formal representation can then be passed on to the checking components of the Diproche system, which will provide feedback on the logical correctness of the argument. Thus, an accepted solution to the above exercise could look like this:</p><p>• The cat still sits on the roof. Hence the dog did not bark. Consequently, Hans did not take his dog for a walk. So Hans did not go for a walk. Thus the sun does not shine.</p><p>For mathematical exercises, the Diproche system gives feedback on linguistical correctness, logical correctness, type mistakes (using variables without introducing them before, or in a wrong way, such as adding two propositions), success in achieving proof obligations and supposed logical or algebraic fallacies. For the "natural language argumentation" exercises, the feedback on type mistakes will be suppressed, since this should not be an issue for the intended kind of argumentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Further Work</head><p>Clearly, a system for didactical uses should be employed in teaching and evaluated in terms of usability and effectiveness. We plan to do so in the near future. On the technical side, the autoformalization with text-davinci-003 or GPT-4-Turbo, although impressive in many respects, still leaves some things to be desired. <ref type="bibr" target="#b15">15</ref> We plan to gather a substantial amount of training data and use it to fine-tune pretrained language models. Whether this will lead to improved performance remains to be seen. Concerning the formalization exercises ("math dictations"), the LLM technology could be used for automatically deformalizing the user's formal input and reporting it back to her in the case the formalization is not correct, thus making it clearer where the mistake lies; here, more traditional NLP techniques, such as the "pretty printer" described in [10], may also be useful. <ref type="bibr" target="#b16">16</ref> Meanwhile, we believe that good use could be made of the "natural argumentation" framework in other subjects, in particular in philosophy: Here, it is a basic type of exercise to reconstruct plain text arguments in a semi-formal style where assumptions and consequences are explicitly labeled, and the approach discussed here for the verification of natural language argumentation could then be used to verify whether an argument written in this way is in fact logically cogent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>In this appendix, we give the detailed results of testing the performance of GPT-4-Turbo and WizardCoder-34B in autoformalization of natural language sentence in propositional logic and of GPT-4-Turbo in first-order logic. Although text-davinci-003 was used in the version of the system described above, it is now considered legacy and its use is deprecated; moreover, its performance in autoformalization tasks appears to have considerably dropped since spring 2023; therefore, no results for text-davinci-003 are listed. We first give the prompt given to the LLM, followed by a table listing the input sentences, the model's output and an evaluation of correctness. It should be noted that the sentences were tested in the order in which they are listed in the table and that other orders may lead to different results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Propositional Logic</head><p>For propositional logic, we used the following prompt; in WizardCoder-34B, temperature was set to 0, repitition penalty to 1 (the minimum). Below, ⊻ is used for exclusive disjunction. In the case of missing brackets, formulas were read according to the usual priority rules. In (31) and (56), a missing closing bracket was added in the WizardCoder output, which was still counted as correct, as this can easily be done automatically.</p><p>Express the sentence as a formula in propositional logic, using the given notation. Notation:</p><p>• S:"The sun shines"</p><p>• R:"It rains"</p><p>• P:"There will be a party" • M:"It is Monday"</p><p>• A:"Andreas will attend the party" • B:"Barbara will attend the party"</p><p>• C:"The moon is made of green cheese" • G:"There is a giraffe on the moon" For the GPT-4-Turbo assistant, the following was added:</p><p>If the given sentence cannot be expressed with the given notation, return "not expressable".</p><p>Nr Input GPT-4 +/-WizardCoder-34B +/-1 It does not rain and the sun shines. ¬R ∧ S + example 2 It's neither sunny nor rainy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>¬S ∧ ¬R</head><formula xml:id="formula_2">+ ¬(S ∨ R) +<label>3</label></formula><p>If there is a party on Monday, Andreas will attend it.</p><formula xml:id="formula_3">(P ∧ M) → A + M → A -<label>4</label></formula><p>If it rains on Monday, Andreas will stay away from the party.</p><formula xml:id="formula_4">(R ∧ M) → ¬A + (M ∧ R) → ¬A) +<label>5</label></formula><p>Andreas and Barbara will only attend the party together.</p><formula xml:id="formula_5">(A ↔ B) + (A ↔ B) + 6</formula><p>There will be a party if the sun shines on Monday.</p><p>(S ∧ M) → P + (S → P) -7</p><p>It is not true that Andreas will abstain from the party when it rains.</p><formula xml:id="formula_6">¬(R → ¬A) + ¬((R → ¬A) ∧ (M ∧ R)) -<label>8</label></formula><p>On rainy Mondays, Barbara does not go to parties.</p><formula xml:id="formula_7">(R ∧ M) → ¬B + ((M ∧ R) → ¬B) +<label>9</label></formula><p>It is a sunny, rainy Monday.</p><formula xml:id="formula_8">S ∧ R ∧ M + (S ∧ R ∧ M) +<label>10</label></formula><p>On sunny Mondays, it rains.</p><formula xml:id="formula_9">(S ∧ M) → R + (M ∧ S) → R) +</formula><p>11 Andreas and Barbara attend the party.</p><formula xml:id="formula_10">A ∧ B + (A ∧ B) +</formula><p>12 At least one of Andreas and Barbara attends the party.</p><formula xml:id="formula_11">A ∨ B + (A ∨ B) +</formula><p>13 Exactly one of Andreas and Barbara attends the party.</p><formula xml:id="formula_12">(A ∧ ¬B) ∨ (¬A ∧ B) + (A ⊻ B) +<label>14</label></formula><p>Andreas parties provided Barbara doesn't. ¬B → A + (→ ¬A) → P -15 Andreas goes to the party provided Barbara doesn't. ¬B → A + (B → ¬A) + 16 Barbara and Andreas only visit Monday parties. (B ∨ A) → M + (M → (A ∨ B)) ∧ (¬A → M)∧(¬B → M) -17 On Monday, there is good or bad weather. M → (S ∨ R) + (M → (S ∨ R)) + 18 When it is wet, Andreas does not go to the party, but Barbara does. R → (¬A ∧ B) + ((R → ¬B) ∧ B) -19 It is a bright Monday, and Andreas and Barbara jointly visit the celebration. (S ∧ M) ∧ (A ∧ B) + (M ∧ S ∧ (A ∨ B)) -20 On Tuesday, there is a thunderstorm. not expressable + not expressable + 21 Christa exercises on Tuesday. not expressable + not expressable + 22 It's a bright Monday. S ∧ M + (M ∧ S) + 23 It rains if and only if the sun shines. R ↔ S + (S → R) ∧ (R → S) + 24 Sunshine and rain do not occur together ¬(S ∧ R) + not expressible -25 Monday is no day for parties. M → ¬P + ¬M -26 Andreas is not a party person. ¬A / not expressable + ¬A + 27 Andreas is not a Tyrannosaurus. not expressable + not expressable + 28 Andreas stays away from the party. ¬A + ¬A + 29 Andreas does not stay away from the party. A + A + 30 If Andreas stays away from the party, then so does Barbara. ¬A → ¬B + (¬A → ¬B) + 31 On sunny days, Barbara and Andreas celebrate. S → (A ∧ B) + (S → (A ∧ B) + 32 Barbara feasts in the rain, but Andreas doesn't. R → (B ∧ ¬A) + (R → (B ∧ ¬A)) + 33 It's not Monday, but Barbara still parties. ¬M ∧ B + ¬M -34 In spite of the downpour, Andreas celebrates. R ∧ A + (R → A) -35 It's sun or rain, but not both. (S ∨ R) ∧ ¬(S ∧ R) + (S ⊻ R) + 36 On sunny and also on rainy days, no parties take place. (S → ¬P) ∧ (R → ¬P) + not expressible -37 On sunny and also on rainy days, there are no parties. (S → ¬P) ∧ (R → ¬P) + ((S ⊻ R) → ¬P) -38 It is wrong that the party has not been canceled. ¬¬P / P -¬¬P -39 Mondays are rainy. M → R + M ∧ R -40 Rainy days are Andreas' party days. R → A + (R → A) + 41 It is a lie that Andreas and Barbara attended the party together. ¬(A ∧ B) + ¬(A ∧ B) + 42 Barbara parties on rainy Mondays. If the given sentence cannot be expressed with the given notation, return "not expressible". No comment or explanation; only return the formula.</p><p>Nr Input Output GPT-4 +/-? 1 Fritz is a dog. D( f r) + 2 Hector is taller than Fritz. L(he, f r) + 3 If Hector barks, then he bites. B(he) → S(he) + 4 There is a dog. ∃xD(x) + 5 Every dog barks. ∀x(D(x) → B(x)) + 6 Some dogs bite. ∃x(D(x) ∧ S(x)) + 7 Hector does not bite. ¬S(he) + 8 Barking dogs bite. ∀x((D(x) ∧ B(x)) → S(x)) + 9 Barking dogs don't bite. ∀x((D(x) ∧ B(x)) → ¬S(x)) + 10 All dogs bite. ∀x(D(x) → S(x)) + 11 Everything bites. ∀xS(x) + 12 Barking dogs don't bark. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><figDesc>R ∧ M) → B + (R ∧ M) → B + 43 It's Monday and it's not. M ∧ ¬M + (M ∧ ¬M) + Express the given sentence as a formula in first-order logic, using the following notation: • D(x):x is a dog • B(b):b barks • S(a):a bites • L(a, b):a is larger than b • fr:Fritz • he:Hector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>∀x((D(x) ∧ B(x)) → ¬B(x)) + 13 If Hector barks, he is not a real dog. not expressible -14 If Hector barks, he is not a dog. B(he) → ¬D(he) + 15 Fritz and Hector are barking dogs. (D( f r) ∧ B( f r)) ∧ (D(he) ∧ B(he)) + 16 If a dog barks, then Fritz barks. ∀x((D(x) ∧ B(x)) → B( f r)) + 17 No dog is bigger than Fritz. ∀x(D(x) → ¬L(x, f r)) + 18 There is a god. not expressible. + 19 Fritz is the tallest dog. D( f r) ∧ ¬∃x(D(x) ∧ L(x, f r)) + 20 No dog that is larger than Fritz will bite. ∀x(D(x) ∧ L(x, f r) → ¬S(x)) + 21 Fritz is smaller than Hector. L(he, f r) + 22 One of Hector and Fritz barks. B(he) ∨ B( f r) + 23 Some dogs are larger than both Fritz and Hector. ∃x(D(x) ∧ L(x, f r) ∧ L(x, he)) + 24 In terms of size, Fritz surpasses himself. L( f r, f r) + 25 Some dogs are larger than others. ∃x∃y(D(x) ∧ D(y) ∧ L(x, y)) + 26 Each dog is larger than each dog. ∀x∀y(D(x) ∧ D(y) → L(x, y)) + 27 Fritz is a dog and he isn't. D( f r) ∧ ¬D( f r) + 28 Fritz is the only dog. D( f r) ∧ ∀x(D(x) → x = f r) + 29 Every dog is larger than every other dog. ∀x∀y((D(x) ∧ D(y) ∧ x ̸ = y) → L(x, y)) +</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See https://platform.openai.com/docs/models/gpt-3-5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that the system is to be used by students in Germany.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Translated for the convenience of the reader; in the actual prompt, we used "beh" for "Behauptung", which represents claims in the internal Diproche format, see<ref type="bibr" target="#b5">[5]</ref>. For assumptions, it would have been "vss" for "Voraussetzung".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Note that this is not a typo. In order to evaluate how true the model formalizes the given sentence, rather than some more common substitute for it, we deliberately used this (absurd) sentence rather than the well-known proverb that barking dogs don't bite.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>For example, in the definition of continuity, the idea of "moving one point closer to another" is a dynamical conception expressed via quantifier changes; for a detailed discussion, see the considerations of the concept of continuity in Lakoff and Nunez<ref type="bibr" target="#b12">[12]</ref>, p. 309-315.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>We originally learned this term from Michael Junk.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>We thank one of our referees for pointing out that this point warrants a discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>Note that the former statement is correct, asa 3 + b 3 &lt; a 2 c + b 2 c = c 2 • c = c 3 .</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head><p>We thank our three anonymous referees for several comments that helped in improving the presentation of the paper, along with constructive criticism concerning its content.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>53 Given that it rains, the sun shines.</p><p>54 If the moon is made of green cheese, then there is a giraffe on it.</p><p>55 There are no moon giraffes.</p><p>¬G + ¬G + 56 On Mondays, the moon is made of green cheddar and populated by giraffes.</p><p>57 There is an elephant on the moon. not expressable + not expressible + mistakes (total) 2 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">First-Order Logic</head><p>For first-order logic, WizardCoder-34B showed no satisfying performance; for GPT-4-Turbo, we used the following prompt:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Edukera</forename><surname>Homepage</surname></persName>
		</author>
		<ptr target="https://www.edukera.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartosz</forename><surname>Piotrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Avigad</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.12433</idno>
	</analytic>
	<monogr>
		<title level="m">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The same can, of course, easily be done with first-order logic</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beiträge zur Algebra der Logik, insbesondere zum Entscheidungsproblem</title>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Behmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01457985</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="163" to="229" />
			<date type="published" when="1922">1922</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.17513</idno>
		<idno>ArXiv:2006.01800v2</idno>
		<title level="m">Automatized Evaluatoin of Formalization Exercises in Mathematics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Number Theory and Axiomatic Geometry in the Diproche System</title>
		<author>
			<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
		</author>
		<idno type="DOI">10.4204/EPTCS.328.4</idno>
	</analytic>
	<monogr>
		<title level="m">Electronic Proceedings in Theoretical Computer Science</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="page" from="56" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2002.05083</idno>
		<idno>ArXiv:2002.05083v1</idno>
		<title level="m">Using Automated Theorem Provers for Mistake Diagnosis in the Didactics of Mathematics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:2303.17513</idno>
		<title level="m">Improving the Diproche CNL through Autoformalization via Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diproche -ein automatisierter Tutor für den Einstieg ins Beweisen</title>
	</analytic>
	<monogr>
		<title level="j">Digitale Kompetenzen und Curriculare Konsequenzen</title>
		<editor>
			<persName><forename type="first">Merlin</forename><surname>Carl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; Regula</forename><surname>Krapf</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Proof-checking mathematical texts in controlled natural language</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Cramer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Rheinische Friedrich-Wilhelms-Universität Bonn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic parsing of geometry statements using supervised machine learning on synthetic data</title>
		<author>
			<persName><forename type="first">Tabet</forename><surname>Salwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Graham-Lengrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natarajan</forename><surname>Narboux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Blanchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Naumowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
		<ptr target="https://ceur-ws.org/Vol-3377/natfom5.pdf" />
	</analytic>
	<monogr>
		<title level="m">Joint Proceedings of the FMM, FVPS, MathUI,NatFoM, and OpenMath Workshops, Doctoral Program, and Work in Progress at the Conference on Intelligent Computer Mathematics 2021 co-located with the 14th Conference on Intelligent Computer Mathematics (CICM 2021), Virtual Event</title>
		<editor>
			<persName><forename type="first">Yasmine</forename><surname>Sharoda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudio</forename><surname>Sacerdoti Coen</surname></persName>
		</editor>
		<meeting><address><addrLine>Timisoara, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07-26">2021. July 26 -31, 2021</date>
			<biblScope unit="volume">3377</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Logical Systems Containing Only a Finite Number of Symbols</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Henkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Presses de l&apos;Universite de Montreal</publisher>
			<pubPlace>Montreal</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Where mathematics comes from : how the embodied mind brings mathematics into being</title>
		<author>
			<persName><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Rafel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Núñez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mathematical Logic Tutor-Propositional Calculus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Budesca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Congress on Tools for Teaching Logic</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Qingxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372885.3373827</idno>
	</analytic>
	<monogr>
		<title level="m">CPP 2020: Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">System for Automated Deduction (SAD): A Tool for Proof Verification</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Verchinine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Lyaletski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Paskevich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-73595-3_29</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4603</biblScope>
			<biblScope unit="page">6915907</biblScope>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>Automated Deduction -CADE-21</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoformalization with Large Language Models</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Staats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.12615</idno>
	</analytic>
	<monogr>
		<title level="m">36th Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
