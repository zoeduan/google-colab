{
  "title": "RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS",
  "authors": [
    "Zhangyin Feng",
    "Xiaocheng Feng",
    "Dezhi Zhao",
    "Maojin Yang",
    "Bing Qin"
  ],
  "abstract": "\n Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledgeintensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multihop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines. \n",
  "references": [
    {
      "id": null,
      "title": "RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS",
      "authors": [
        "Zhangyin Feng",
        "Xiaocheng Feng",
        "Dezhi Zhao",
        "Maojin Yang",
        "Bing Qin"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "",
      "authors": [
        "References"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Advances in neural information processing systems",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann"
      ],
      "year": "2022",
      "venue": "Training compute-optimal large language models",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "A Zeng"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery"
      ],
      "year": "2022",
      "venue": "Palm: Scaling language modeling with pathways",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Latent retrieval for weakly supervised open domain question answering",
      "authors": [
        "K Lee",
        "M.-W Chang",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "authors": [
        "R Zellers",
        "Y Bisk",
        "R Schwartz",
        "Y Choi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "In-context retrieval-augmented language models",
      "authors": [
        "O Ram"
      ],
      "year": "2023",
      "venue": "In-context retrieval-augmented language models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
      "authors": [
        "O Khattab"
      ],
      "year": "2023",
      "venue": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Replug: Retrieval-augmented black-box language models",
      "authors": [
        "W Shi"
      ],
      "year": "2023",
      "venue": "Replug: Retrieval-augmented black-box language models",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Generate rather than retrieve: Large language models are strong context generators",
      "authors": [
        "W Yu"
      ],
      "year": "2023",
      "venue": "Generate rather than retrieve: Large language models are strong context generators",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Recitationaugmented language models",
      "authors": [
        "Z Sun",
        "X Wang",
        "Y Tay",
        "Y Yang",
        "D Zhou"
      ],
      "year": "2023",
      "venue": "Recitationaugmented language models",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Leveraging passage retrieval with generative models for open domain question answering",
      "authors": [
        "G Izacard",
        "E Grave"
      ],
      "year": "2020",
      "venue": "Leveraging passage retrieval with generative models for open domain question answering",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Natural questions: A benchmark for question answering research",
      "authors": [
        "T Kwiatkowski"
      ],
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "authors": [
        "M Joshi",
        "E Choi",
        "D Weld",
        "L Zettlemoyer"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps",
      "authors": [
        "X Ho",
        "A.-K Duong Nguyen",
        "S Sugawara",
        "A Aizawa"
      ],
      "year": "",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "International Committee on Computational Linguistics",
      "authors": [
        "Spain Barcelona"
      ],
      "year": "2020",
      "venue": "International Committee on Computational Linguistics",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
      "authors": [
        "Z Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
      "authors": [
        "H Trivedi",
        "N Balasubramanian",
        "T Khot",
        "A Sabharwal"
      ],
      "year": "2022",
      "venue": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Active retrieval augmented generation",
      "authors": [
        "Z Jiang"
      ],
      "year": "2023",
      "venue": "Active retrieval augmented generation",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Chain of thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei"
      ],
      "year": "2022",
      "venue": "Chain of thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Few-shot learning with retrieval augmented language models",
      "authors": [
        "G Izacard"
      ],
      "year": "2022",
      "venue": "Few-shot learning with retrieval augmented language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Retrieval-Generation Synergy Augmented Large Language Models",
      "text": ""
    },
    {
      "title": "Abstract",
      "text": "Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledgeintensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines. Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin Harbin Institute of Technology, China large language models, retrieval augmented, question answering large language models, retrieval augmented, question answering large"
    },
    {
      "title": "1 Introduction",
      "text": "Large Language models (LLMs) have demonstrated impressive performance on diverse language tasks through in-context learning [1, 2, 3, 4, 5, 6]. However, they still struggle with knowledge-intensive tasks that require access to a large amount of knowledge, such as open-domain question answering [7] and commonsense reasoning [8], since the implicit knowledge preserved in the parameters may be partial and insufficient. As shown in the top of Figure 1, one promising direction is to incorporate non-parametric knowledge to help alleviate this problem with large language models. Recent research shows that retrieving relevant documents from an external datastore [9, 10, 11] or directly generating contextual documents from LLMs [12, 13] both can improve LLMs' performance on knowledge-intensive tasks. The former, called retrieve-then-read, requires a retriever to retrieve relevant documents. The latter, known as generate-then-read, leverages large language models to generate relevant documents before answering questions. However, as shown in Figure 1, the above two methods are isolated and lack coordination with each other. To fill this gap, in this paper, we explore an effective retrieval-generation collaboration framework to further improve the ability of large language models to solve knowledge-intensive tasks. In this work, we present ITRG, an **IT**erative **R**etrieval-**G**eneration synergy framework to generate relevant documents that simultaneously exploits parametric and non-parametric knowledge. In each iteration, ITRG consists of two important steps: generation augmented retrieval (GAR) and retrieval augmented generation (RAG). In the GAR step, we propose a simple and effective method to expand queries by concatenating pseudo-documents generated from large language models and original questions. And expanded queries improve the accuracy of retrieving relevant documents. In the RAG step, we use large language models to comprehensively understand retrieved documents to generate new documents for answering questions. We repeat these steps until we reach the maximum allowed number of iterations. Through multiple retrieval generation collaborations, our method aids in discovering the appropriate reasoning path and providing correct answers to questions. We evaluate the efficacy of our method on 4 question answering datasets, including Natural Questions, TriviaQA, 2WikiMultiHopQA, and HotpotQA. Experimental results show that our method performs better than previous baselines on all datasets. In summary, our main contributions can be summarized as follows: (1) We propose ITRG, an iterative retrieval-generation synergy framework using both parametric and non-parametric knowledge. (2) We propose a simple and effective generation-augmented retrieval strategy and two retrieval-augmented generation strategies. (3) Empirical results show that ITRG outperforms previous retrieval-augmented methods. Figure 1: The top is the standard method utilizing LLMs for question answering with relevant documents. The bottom shows three methods to generate relevant documents."
    },
    {
      "title": "2 Iterative Retrieval-Generation Synergy",
      "text": "In this section, we first introduce the overall framework, and then introduce the retrieval-generation collaboration framework in detail, including generation augmented retrieval and retrieval augmented generation."
    },
    {
      "title": "Overview",
      "text": "We show the framework of ITRG in Figure 2. Given a user question \\(q\\) and a document corpus \\(\\mathcal{D}=\\{d_{i}\\}_{i=1}^{|\\mathcal{D}|}\\) (i.e, \\(d_{i}\\) is a Wikipedia paragraph.), ITRG repeats generation augmented retrieval (GAR) and retrieval augmented generation (RAG) for \\(T\\) iterations. In the GAR process of iteration \\(t\\), we concatenate the output \\(y_{t-1}\\) of the last iteration and question \\(q\\) to form a new query, and then use a dense retriever to retrieve top-\\(k\\) paragraphs. In the first iteration, we only use the question as the query. In the RAG process of iteration \\(t\\), based on the question \\(q\\) and the retrieved top-\\(k\\) paragraphs, we exploit large language models to generate new paragraphs to answer questions. Specifically, we propose two methods to generate new paragraphs, which will be introduced in detail in SS2.3."
    },
    {
      "title": "Generation Augmented Retrieval",
      "text": "Knowledge-intensive tasks (e.g., open-domain question answering) often require access to additional documents. A common approach is to directly employ the question as the query, and then equip a sparse or dense retriever to retrieve relevant documents. In practice, we find that in some cases using the question directly as the query fails to retrieve relevant documents because there may exist semantic gaps between them. To alleviate this problem, we propose a simple query expansion method. At the first iteration (\\(t=1\\)), we use the original question \\(q\\) as the query. At iteration \\(t\\) (\\(t>1\\)), we concatenate the original question \\(q\\) and the document generated \\(y_{t-1}\\) in the last iteration as the new query \\(q_{t}=[q;y_{t-1}]\\). Then, we utilize a pre-trained dense retriever to retrieve top-\\(k\\) documents, which are denoted as \\(R_{t}=\\{d\\}\\). Given an input question \\(q\\), the retriever aims to retrieve a small set of documents from a corpus \\(\\mathcal{D}=\\{d_{i}\\}_{i=1}^{|\\mathcal{D}|}\\) that are relevant to \\(q\\). Following prior work [14], we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context \\(q\\) and the document \\(d\\). Specifically, the encoder maps each document \\(d\\in\\mathcal{D}\\) to an embedding \\(\\mathbf{E}(d)\\) by taking the mean pooling of the last hidden representation over the tokens in \\(d\\). At query time, the same encoder is applied to the input context \\(q\\) to obtain a query embedding \\(\\mathbf{E}(q)\\). The similarity between the query embedding and the document embedding is computed by their cosine similarity: \\(s(d,q)=\\cos(\\mathbf{E}(d),\\mathbf{E}(q))\\). The top-\\(k\\) documents that have the highest similarity scores are retrieved."
    },
    {
      "title": "Retrieval Augmented Generation",
      "text": "Following previous work [13], for a given question \\(q\\), we could directly prompt large language models to generate related documents without retrieving them from an external corpus. However, we find that if only the parametric knowledge learned by the large model in the pre-training stage is used, the generated documents may be incomplete. Retrieval augmented generation (RAG) aims to comprehensively understand the retrieved non-parametric knowledge and the parametric knowledge inside large language models to generate more accurate factual knowledge. Specifically, we propose two strategies, which will be described in detail below."
    },
    {
      "title": "2.3.1 Refine",
      "text": "An intuitive idea is to refine the previously generated document \\(y_{t-1}\\) based on the original question \\(q\\) and the retrieved top-\\(k\\) documents at the current iteration step \\(R_{t}\\) to obtain a new document \\(y_{t}\\). We call this method refine. Considering that the document retrieved in the last iteration \\(R_{t-1}\\) has been used to generate the last document \\(y_{t-1}\\), we refine the previous output \\(y_{t-1}\\) with updated documents \\(R_{update}\\). \\[R_{update}=R_{t}-R_{t-1}, \\tag{1}\\] \\[y_{t}=\\mathcal{M}\\left(\\mathrm{prompt}\\left(y_{t-1},q,R_{update} \\right)\\right), \\tag{2}\\] Figure 2: Iterative retrieval-generation synergy framework contains two steps in each iteration: (1) generation augmented retrieval (GAR): utilize the output of the previous iteration to expand the query to help retrieve more relevant documents; (2) retrieval augmented generation (RAG): utilize retrieved documents to generate new documents to answer questions. We only show three iterations in this figure for brevity. Solid arrows indicate RAG within an iteration, and dashed arrows indicate GAR between iterations. Purple represents correct and useful information, and red represents wrong or invalid information. [MISSING_PAGE_FAIL:3] and improves by 6.8 points in absolute gains. Compared to vanilla LM, ITRG (refresh) can improve the EM score by 9.4, 7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot settings on the Hotpotqa dataset."
    },
    {
      "title": "Performance At Different Iterations",
      "text": "In this section, we analyze the performance of our model and the quality of the generated documents during the iteration process. Specifically, we present the results of ITRG (refresh) at different iterations in 5-shot setting in Table 3. We measure the answer recall of generated documents at different iteration steps and present results in Table 4. Table 3 shows that the performance of the model gradually improves with iteration. And Table 4 shows that the quality of the generated documents also gradually improves with iteration. These results verify that our iterative retrieval-generation collaborative framework is effective and can further enhance the reasoning capabilities of large language models."
    },
    {
      "title": "5 Conclusion",
      "text": "In this paper, we present ITRG, which is an iterative retrieval-generation synergy framework, containing two important steps: generation-augmented retrieval and retrieval-augmented generation. They form a closed loop, and can improve each other via multiple iterations. We propose a simple and effective generation-augmented retrieval strategy and two retrieval-augmented generation strategies. Empirical results show our approach significantly exceeds several strong baselines, including GPT 3.5, on four open domain question answering datasets, which indicates that our method can significantly improve the reasoning ability of large language models. \\begin{table} \\begin{tabular}{l l c c c c c} \\hline \\hline & \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{Natural Questions} & \\multicolumn{3}{c}{TriviaQA} \\\\ \\cline{3-7} & & 0-shot & 1-shot & 5-shot & 0-shot & 1-shot & 5-shot \\\\ \\hline \\multirow{2}{*}{GPT 3.5} & Text-davinci-002 & 12.0 & 24.6 & 33.0 & 46.0 & 74.2 & 76.0 \\\\ & Text-davinci-003 & 29.4 & 33.0 & 33.8 & 75.8 & 78.6 & 77.8 \\\\ \\hline \\multirow{4}{*}{LLaMA 33B} & Vanilla LM & 27.0 & 29.4 & 32.4 & 74.8 & 70.8 & 75.8 \\\\ & Retrieve-then-Read & 27.8 & 30.6 & 29.8 & 74.6 & 76.0 & 76.0 \\\\ \\cline{1-1} & Generate-then-Read & 28.0 & 31.4 & 31.0 & 73.6 & 77.2 & 77.6 \\\\ \\cline{1-1} & ITRG (refine) & 34.4 & 34.6 & 34.8 & **79.0** & **79.4** & **80.6** \\\\ \\cline{1-1} & ITRG (refresh) & **37.6** & **38.4** & **38.0** & 77.0 & 78.6 & 79.4 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: Exact match performance on single-hop question answering. All ITRG results are from the last iteration (\\(T=5\\)). \\begin{table} \\begin{tabular}{l l c c c c c} \\hline \\hline & \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{2WikiMultiHopQA} & \\multicolumn{3}{c}{HotpotQA} \\\\ \\cline{3-7} & & 0-shot & 1-shot & 5-shot & 0-shot & 1-shot & 5-shot \\\\ \\hline \\multirow{2}{*}{GPT 3.5} & Text-davinci-002 & 16.4 & 27.6 & 30.8 & 12.2 & 20.2 & 22.2 \\\\ & Text-davinci-003 & 27.2 & 27.0 & 29.8 & 25.0 & 25.8 & 26.6 \\\\ \\hline \\multirow{8}{*}{LLaMA 33B} & Vanilla LM & 24.4 & 27.6 & 31.8 & 22.6 & 25.0 & 27.0 \\\\ & COT & - & - & 32.2 & - & - & 28.6 \\\\ \\cline{1-1} & Retrieve-then-Read & 27.4 & 29.2 & 32.0 & 28.4 & 29.8 & 30.4 \\\\ \\cline{1-1} & Generate-then-Read & 30.0 & 30.4 & 31.6 & 25.0 & 27.0 & 27.0 \\\\ \\cline{1-1} & ITRG (refine) & **33.0** & 33.6 & 37.0 & 28.8 & 29.6 & 30.6 \\\\ \\cline{1-1} & ITRG (refresh) & 32.2 & **36.2** & **38.6** & **31.0** & **32.6** & **33.4** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 2: Exact match performance on multi-hop question answering. All ITRG results are from the last iteration (\\(T=5\\)). \\begin{table} \\begin{tabular}{l c c c c c} \\hline \\hline Iteration & 1 & 2 & 3 & 4 & 5 \\\\ \\hline Natural Questions & 44.0 & 46.4 & 48.4 & 48.8 & 48.0 \\\\ TriviaQA & 18.8 & 19.0 & 20.2 & 19.2 & 19.2 \\\\ 2WikiMultiHopQA & 34.2 & 36.6 & 35.0 & 40.0 & 37.0 \\\\ HotpotQA & 34.2 & 34.8 & 35.6 & 33.8 & 33.6 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 4: Answer recall of generated documents at different iterations with ITRG (refresh). \\begin{table} \\begin{tabular}{l l c c c c c} \\hline \\hline & \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{2WikiMultiHopQA} & \\multicolumn{3}{c}{HotpotQA} \\\\ \\cline{3-7} & & 0-shot & 1-shot & 5-shot & 0-shot & 1-shot & 5-shot \\\\ \\hline \\multirow{2}{*}{GPT 3.5} & Text-davinci-002 & 16.4 & 27.6 & 30.8 & 12.2 & 20.2 & 22.2 \\\\ & Text-davinci-003 & 27.2 & 27.0 & 29.8 & 25.0 & 25.8 & 26.6 \\\\ \\hline \\multirow{4}{*}{LLaMA 33B} & Vanilla LM & 24.4 & 27.6 & 31.8 & 22.6 & 25.0 & 27.0 \\\\ & COT & - & - & 32.2 & - & - & 28.6 \\\\ \\cline{1-1} & Retrieve-then-Read & 27.4 & 29.2 & 32.0 & 28.4 & 29.8 & 30.4 \\\\ \\cline{1-1} & Generate-then-Read & 30.0 & 30.4 & 31.6 & 25.0 & 27.0 & 27.0 \\\\ \\cline{1-1} & ITRG (refine) & **33.0** & 33.6 & 37.0 & 28.8 & 29.6 & 30.6 \\\\ \\cline{1-1} & ITRG (refresh) & 32.2 & **36.2** & **38.6** & **31.0** & **32.6** & **33.4** \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 3: Exact match performance of ITRG (refresh) at different iterations in 5-shot setting."
    },
    {
      "title": "References",
      "text": "* [1] T. Brown _et al._, \"Language models are few-shot learners,\" _Advances in neural information processing systems_, vol. 33, pp. 1877-1901, 2020. * [2] J. Hoffmann _et al._, \"Training compute-optimal large language models,\" 2022. * [3] A. Zeng _et al._, \"Glm-130b: An open bilingual pre-trained model,\" _arXiv preprint arXiv:2210.02414_, 2022. * [4] A. Chowdhery _et al._, \"Palm: Scaling language modeling with pathways,\" _arXiv preprint arXiv:2204.02311_, 2022. * [5] OpenAI, \"Gpt-4 technical report,\" 2023. * [6] H. Touvron _et al._, \"Llama: Open and efficient foundation language models,\" 2023. * [7] K. Lee, M.-W. Chang, and K. Toutanova, \"Latent retrieval for weakly supervised open domain question answering,\" in _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 6086-6096. [Online]. Available: [https://aclanthology.org/P19-1612](https://aclanthology.org/P19-1612) * [8] R. Zellers, Y. Bisk, R. Schwartz, and Y. Choi, \"SWAG: A large-scale adversarial dataset for grounded commonsense inference,\" in _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 93-104. [Online]. Available: [https://www.aclweb.org/anthology/D18-1009](https://www.aclweb.org/anthology/D18-1009) * [9] O. Ram _et al._, \"In-context retrieval-augmented language models,\" _arXiv preprint arXiv:2302.00083_, 2023. * [10] O. Khattab _et al._, \"Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp,\" 2023. * [11] W. Shi _et al._, \"Replug: Retrieval-augmented black-box language models,\" _arXiv preprint arXiv:2301.12652_, 2023. * [12] W. Yu _et al._, \"Generate rather than retrieve: Large language models are strong context generators,\" 2023. * [13] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, \"Recitation-augmented language models,\" 2023. * [14] G. Izacard and E. Grave, \"Leveraging passage retrieval with generative models for open domain question answering,\" _arXiv preprint arXiv:2007.01282_, 2020. * [15] T. Kwiatkowski _et al._, \"Natural questions: A benchmark for question answering research,\" _Transactions of the Association for Computational Linguistics_, vol. 7, pp. 452-466, 2019. [Online]. Available: [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026) * [16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, \"TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension,\" in _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601-1611. [Online]. Available: [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147) * [17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa, \"Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps,\" in _Proceedings of the 28th International Conference on Computational Linguistics_. Barcelona, Spain (Online): International Committee on Computational Linguistics, Dec. 2020, pp. 6609-6625. [Online]. Available: [https://aclanthology.org/2020.coling-main.580](https://aclanthology.org/2020.coling-main.580) * [18] Z. Yang _et al._, \"HotpotQA: A dataset for diverse, explainable multi-hop question answering,\" in _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2369-2380. [Online]. Available: [https://aclanthology.org/D18-1259](https://aclanthology.org/D18-1259) * [19] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \"Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,\" _arXiv preprint arXiv:2212.10509_, 2022. * [20] Z. Jiang _et al._, \"Active retrieval augmented generation,\" _arXiv preprint arXiv:2305.06983_, 2023. * [21] L. Ouyang _et al._, \"Training language models to follow instructions with human feedback,\" _Advances in Neural Information Processing Systems_, vol. 35, pp. 27 730-27 744, 2022. * [22] J. Wei _et al._, \"Chain of thought prompting elicits reasoning in large language models,\" _arXiv preprint arXiv:2201.11903_, 2022. * [23] G. Izacard _et al._, \"Few-shot learning with retrieval augmented language models,\" _arXiv preprint arXiv:2208.03299_, 2022."
    }
  ]
}