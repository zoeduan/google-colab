{
  "title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
  "authors": [
    "Chuang Liu",
    "Renren Jin",
    "Yuqi Ren",
    "Linhao Yu",
    "Tianyu Dong",
    "Xiaohan Peng",
    "Shuting Zhang",
    "Jianxiang Peng",
    "Peiyi Zhang",
    "Qingqing Lyu",
    "Xiaowen Su",
    "Qun Liu",
    "Deyi Xiong"
  ],
  "abstract": "\n Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zeroand few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-theart open-source Chinese large language models on the proposed benchmark. The size of these models varies from 335M to 130B parameters. Experiment results demonstrate that they perform significantly worse than GPT-3.5 that reaches an accuracy of ∼ 48% on M3KE. The dataset is available at  https://github.  com/tjunlp-lab/M3KE . \n",
  "references": [
    {
      "id": null,
      "title": "M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
      "authors": [
        "Chuang Liu",
        "Renren Jin",
        "Yuqi Ren",
        "Linhao Yu",
        "Tianyu Dong",
        "Xiaohan Peng",
        "Shuting Zhang",
        "Jianxiang Peng",
        "Peiyi Zhang",
        "Qingqing Lyu",
        "Xiaowen Su",
        "Qun Liu",
        "Deyi Xiong"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Planning for agi and beyond",
      "authors": [
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Planning for agi and beyond",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Promptsource: An integrated development environment and repository for natural language prompts",
      "authors": [
        "H Stephen",
        "Victor Bach",
        "Zheng Xin Sanh",
        "Albert Yong",
        "Colin Webson",
        "Raffel",
        "V Nihal",
        "Abheesht Nayak",
        "Taewoon Sharma",
        "M Kim",
        "Saiful",
        "Thibault Bari",
        "Zaid Févry",
        "Manan Alyafeai",
        "Andrea Dey",
        "Zhiqing Santilli",
        "Srulik Sun",
        "Canwen Ben-David",
        "Gunjan Xu",
        "Han Chhablani",
        "Jason Wang",
        "Alan Fries",
        "Maged Saeed Alshaibani",
        "Shanya Sharma",
        "Urmish Thakker",
        "Khalid Almubarak",
        "Xiangru Tang",
        "Dragomir R Radev",
        "Mike Tian-Jian",
        "Alexander M Jiang",
        "Rush"
      ],
      "year": "2022",
      "venue": "ACL (demo)",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "A cookbook of self-supervised learning",
      "authors": [
        "Randall Balestriero",
        "Mark Ibrahim",
        "Vlad Sobal",
        "Ari Morcos",
        "Shashank Shekhar",
        "Tom Goldstein",
        "Florian Bordes",
        "Adrien Bardes",
        "Gregoire Mialon",
        "Yuandong Tian"
      ],
      "year": "2023",
      "venue": "A cookbook of self-supervised learning",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Language models are few-shot learners",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel M Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Benjamin Gray",
        "Jack Chess",
        "Christopher Clark",
        "Sam Berner",
        "Alec Mc-Candlish",
        "Ilya Radford",
        "Dario Sutskever",
        "Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Sparks of artificial general intelligence: Early experiments with",
      "authors": [
        "Varun S'ebastien Bubeck",
        "Ronen Chandrasekaran",
        "Johannes Eldan",
        "Eric Gehrke",
        "Ece Horvitz",
        "Peter Kamar",
        "Yin Lee",
        "Yuanzhi Tat Lee",
        "Scott Li",
        "Harsha Lundberg",
        "Hamid Nori",
        "Marco Palangi",
        "Yi Tulio Ribeiro",
        "Zhang"
      ],
      "year": "2023",
      "venue": "Sparks of artificial general intelligence: Early experiments with",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "A comprehensive survey of ai-generated content",
      "authors": [
        "Yihan Cao",
        "Siyu Li",
        "Yixin Liu",
        "Zhiling Yan",
        "Yutong Dai",
        "Philip S Yu",
        "Lichao Sun"
      ],
      "year": "2023",
      "venue": "A history of generative ai from gan to chatgpt",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Phoenix: Democratizing chatgpt across languages",
      "authors": [
        "Zhihong Chen",
        "Feng Jiang",
        "Junying Chen",
        "Tiannan Wang",
        "Fei Yu",
        "Guiming Chen",
        "Hongbo Zhang",
        "Juhao Liang",
        "Chen Zhang",
        "Zhiyi Zhang"
      ],
      "year": "2023",
      "venue": "Phoenix: Democratizing chatgpt across languages",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Deep reinforcement learning from human preferences",
      "authors": [
        "Paul F Christiano",
        "Jan Leike",
        "Tom B Brown",
        "Miljan Martic",
        "Shane Legg",
        "Dario Amodei"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shane Shixiang",
        "Zhuyun Gu",
        "Mirac Dai",
        "Xinyun Suzgun",
        "Aakanksha Chen",
        "Sharan Chowdhery",
        "Gaurav Narang",
        "Adams Mishra",
        "Vincent Y Yu",
        "Yanping Zhao",
        "Andrew M Huang",
        "Hongkun Dai",
        "Slav Yu",
        "Ed H Petrov",
        "Jeff Chi",
        "Jacob Dean",
        "Adam Devlin",
        "Denny Roberts",
        "Quoc V Zhou",
        "Jason Le",
        "Wei"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "A survey for in-context learning",
      "authors": [
        "Qingxiu Dong",
        "Lei Li",
        "Damai Dai",
        "Ce Zheng",
        "Zhiyong Wu",
        "Baobao Chang",
        "Xu Sun",
        "Jingjing Xu",
        "Lei Li",
        "Zhifang Sui"
      ],
      "year": "2023",
      "venue": "A survey for in-context learning",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "2022a. Glam: Efficient scaling of language models with mixture-of-experts",
      "authors": [
        "Nan Du",
        "Yanping Huang",
        "Andrew M Dai",
        "Simon Tong",
        "Dmitry Lepikhin",
        "Yuanzhong Xu",
        "Maxim Krikun",
        "Yanqi Zhou",
        "Adams Wei Yu",
        "Orhan Firat",
        "Barret Zoph",
        "Liam Fedus",
        "Maarten P Bosma",
        "Zongwei Zhou",
        "Tao Wang",
        "Yu Emma Wang",
        "Kellie Webster",
        "Marie Pellat",
        "Kevin Robinson",
        "Kathleen S Meier-Hellstern",
        "Toju Duke",
        "Lucas Dixon",
        "Kun Zhang",
        "Quoc V Le",
        "Yonghui Wu",
        "Zhifeng Chen",
        "Claire Cui"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "GLM: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.26"
    },
    {
      "id": "b12",
      "title": "Openwebtext corpus",
      "authors": [
        "Aaron Gokaslan",
        "Vanya Cohen",
        "Ellie Pavlick",
        "Stefanie Tellex"
      ],
      "year": "2019",
      "venue": "Openwebtext corpus",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Domain mastery benchmark: An ever-updating benchmark for evaluating holistic domain knowledge of large language model-a preliminary release",
      "authors": [
        "Zhouhong Gu",
        "Xiaoxuan Zhu",
        "Haoning Ye",
        "Lin Zhang",
        "Zhuozhi Xiong",
        "Zihan Li",
        "Qianyu He",
        "Sihang Jiang",
        "Hongwei Feng",
        "Yanghua Xiao"
      ],
      "year": "2023",
      "venue": "Domain mastery benchmark: An ever-updating benchmark for evaluating holistic domain knowledge of large language model-a preliminary release",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Measuring massive multitask language understanding",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andy Zou",
        "Mantas Mazeika",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "Measuring massive multitask language understanding",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "",
      "authors": [
        "Jordan Hoffmann",
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Elena Buchatskaya",
        "Trevor Cai",
        "Eliza Rutherford",
        "Diego De Las",
        "Lisa Anne Casas",
        "Johannes Hendricks",
        "Aidan Welbl",
        "Tom Clark",
        "Eric Hennigan",
        "Katie Noland",
        "George Millican",
        "Bogdan Van Den Driessche",
        "Aurelia Damoc",
        "Simon Guy",
        "Karen Osindero",
        "Erich Simonyan",
        "Jack W Elsen",
        "Oriol Rae",
        "Laurent Vinyals",
        "Sifre"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "Shaohan Huang",
        "Li Dong",
        "Wenhui Wang",
        "Yaru Hao",
        "Saksham Singhal",
        "Shuming Ma",
        "Tengchao Lv",
        "Lei Cui",
        "Owais Khan Mohammed",
        "Barun Patra",
        "Qiang Liu",
        "Kriti Aggarwal",
        "Zewen Chi",
        "Johan Bjorck",
        "Vishrav Chaudhary",
        "Subhojit Som",
        "Xia Song",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Language is not all you need: Aligning perception with language models",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "QASC: A dataset for question answering via sentence composition",
      "authors": [
        "Tushar Khot",
        "Peter Clark",
        "Michal Guerquin",
        "Peter Jansen",
        "Ashish Sabharwal"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Multi-task deep neural networks for natural language understanding",
      "authors": [
        "Xiaodong Liu",
        "Pengcheng He",
        "Weizhu Chen",
        "Jianfeng Gao"
      ],
      "year": "2019",
      "venue": "ACL (1)",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "A robustly optimized BERT pretraining approach",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Crosslingual generalization through multitask finetuning",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful",
        "Sheng Bari",
        "Shen"
      ],
      "year": "2022",
      "venue": "Crosslingual generalization through multitask finetuning",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll L Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul F Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context",
      "authors": [
        "Denis Paperno",
        "Germán Kruszewski",
        "Angeliki Lazaridou",
        "Ngoc Quan",
        "Raffaella Pham",
        "Sandro Bernardi",
        "Marco Pezzelle",
        "Gemma Baroni",
        "Raquel Boleda",
        "Fernández"
      ],
      "year": "2016",
      "venue": "ACL (1)",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "",
      "authors": [
        "Jack W Rae",
        "Sebastian Borgeaud",
        "Trevor Cai",
        "Katie Millican",
        "Jordan Hoffmann",
        "H Francis Song",
        "John Aslanides",
        "Sarah Henderson",
        "Roman Ring",
        "Susannah Young",
        "Eliza Rutherford",
        "Tom Hennigan",
        "Jacob Menick",
        "Albin Cassirer",
        "Richard Powell",
        "George Van Den Driessche",
        "Lisa Anne Hendricks",
        "Maribeth Rauh",
        "Po-Sen Huang",
        "Amelia Glaese",
        "Johannes Welbl",
        "Sumanth Dathathri",
        "Saffron Huang",
        "Jonathan Uesato",
        "John Mellor",
        "Irina Higgins",
        "Antonia Creswell",
        "Nat Mcaleese",
        "Amy Wu",
        "Erich Elsen",
        "M Siddhant",
        "Elena Jayakumar",
        "David Buchatskaya",
        "Esme Budden",
        "Karen Sutherland",
        "Michela Simonyan",
        "Laurent Paganini",
        "Lena Sifre",
        "Martens",
        "Lorraine Xiang",
        "Adhiguna Li",
        "Aida Kuncoro",
        "Elena Nematzadeh",
        "Domenic Gribovskaya",
        "Angeliki Donato",
        "Arthur Lazaridou",
        "Jean-Baptiste Mensch",
        "Maria Lespiau",
        "Nikolai Tsimpoukelli",
        "Doug Grigorev",
        "Thibault Fritz",
        "Mantas Sottiaux",
        "Toby Pajarskas",
        "Zhitao Pohlen",
        "Daniel Gong",
        "Cyprien Toyama",
        "Yujia De Masson D'autume",
        "Tayfun Li",
        "Vladimir Terzi",
        "Igor Mikulik",
        "Aidan Babuschkin",
        "Diego Clark",
        "De Las",
        "Aurelia Casas",
        "Chris Guy",
        "James Jones",
        "Matthew J Bradbury",
        "Blake A Johnson",
        "Laura Hechtman",
        "Iason Weidinger",
        "William S Gabriel",
        "Edward Isaac",
        "Lockhart"
      ],
      "year": "",
      "venue": "Simon Osindero",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Squad: 100, 000+ questions for machine comprehension of text",
      "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Pangu-Σ: Towards trillion parameter language model with sparse heterogeneous computing",
      "authors": [
        "Xiaozhe Ren",
        "Pingyi Zhou",
        "Xinfan Meng",
        "Xinjing Huang",
        "Yadao Wang",
        "Weichao Wang",
        "Pengfei Li",
        "Xiaoda Zhang",
        "Alexander Podolskiy",
        "Grigory Arshinov",
        "Andrey Bout",
        "Irina Piontkovskaya",
        "Jiansheng Wei",
        "Xin Jiang",
        "Teng Su",
        "Qun Liu",
        "Jun Yao"
      ],
      "year": "2023",
      "venue": "Pangu-Σ: Towards trillion parameter language model with sparse heterogeneous computing",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Multitask prompted training enables zero-shot task generalization",
      "authors": [
        "Victor Sanh",
        "Albert Webson",
        "Colin Raffel",
        "Stephen H Bach",
        "Lintang Sutawika",
        "Zaid Alyafeai",
        "Antoine Chaffin",
        "Arnaud Stiegler",
        "Arun Raja",
        "Manan Dey",
        "M Saiful Bari",
        "Canwen Xu",
        "Urmish Thakker",
        "Shanya Sharma Sharma",
        "Eliza Szczechla",
        "Taewoon Kim",
        "Gunjan Chhablani",
        "V Nihal",
        "Debajyoti Nayak",
        "Jonathan Datta",
        "Mike Chang",
        "Tian-Jian",
        "Han Jiang",
        "Matteo Wang",
        "Sheng Manica",
        "Zheng Xin Shen",
        "Harshit Yong",
        "Rachel Pandey",
        "Thomas Bawden",
        "Trishala Wang",
        "Jos Neeraj",
        "Abheesht Rozen",
        "Andrea Sharma",
        "Thibault Santilli",
        "Jason Févry",
        "Alan Fries",
        "Ryan Teehan",
        "Le Teven",
        "Stella Scao",
        "Leo Biderman",
        "Thomas Gao",
        "Alexander M Wolf",
        "Rush"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "",
      "authors": [
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilic",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Matthias Yvon",
        "Jonathan Gallé",
        "Alexander M Tow",
        "Stella Rush",
        "Albert Biderman",
        "Pawan Webson",
        "Thomas Sasanka Ammanamanchi",
        "Benoît Wang",
        "Niklas Sagot",
        "Albert Muennighoff",
        "Olatunji Villanova Del Moral",
        "Rachel Ruwase",
        "Stas Bawden",
        "Angelina Bekman",
        "Iz Mcmillan-Major",
        "Huu Beltagy",
        "Lucile Nguyen",
        "Samson Saulnier",
        "Pedro Ortiz Tan",
        "Victor Suarez",
        "Hugo Sanh",
        "Yacine Laurençon",
        "Julien Jernite",
        "Margaret Launay",
        "Mitchell"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "",
      "authors": [
        "Aarohi Srivastava",
        "Abhinav Rastogi",
        "Abhishek Rao",
        "Abu Awal",
        "Md Shoeb",
        "Abubakar Abid",
        "Adam Fisch",
        "Adam R Brown",
        "Adam Santoro",
        "Aditya Gupta",
        "Adrià Garriga-Alonso",
        "Agnieszka Kluska",
        "Aitor Lewkowycz",
        "Akshat Agarwal",
        "Alethea Power",
        "Alex Ray",
        "Alex Warstadt",
        "Alexander W Kocurek",
        "Ali Safaya",
        "Ali Tazarv",
        "Alice Xiang",
        "Alicia Parrish",
        "Allen Nie",
        "Aman Hussain",
        "Amanda Askell",
        "Amanda Dsouza",
        "Ameet Rahane",
        "Anantharaman S Iyer",
        "Anders Andreassen",
        "Andrea Santilli",
        "Andreas Stuhlmüller",
        "Andrew M Dai",
        "Andrew La",
        "Andrew K Lampinen",
        "Andy Zou",
        "Angela Jiang"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Learning to summarize from human feedback",
      "authors": [
        "Nisan Stiennon",
        "Long Ouyang",
        "Jeff Wu",
        "Daniel M Ziegler",
        "Ryan Lowe",
        "Chelsea Voss",
        "Alec Radford",
        "Dario Amodei",
        "Paul F Christiano"
      ],
      "year": "2020",
      "venue": "Learning to summarize from human feedback",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Large-scale knowledge enhanced pretraining for language understanding and generation",
      "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Shikun Feng",
        "Siyu Ding",
        "Chao Pang",
        "Junyuan Shang",
        "Jiaxiang Liu",
        "Xuyi Chen",
        "Yanbin Zhao",
        "Yuxiang Lu",
        "Weixin Liu",
        "Zhihua Wu",
        "Weibao Gong",
        "Jianzhong Liang",
        "Zhizhou Shang",
        "Peng Sun",
        "Wei Liu",
        "Xuan Ouyang",
        "Dianhai Yu",
        "Hua Hao Tian",
        "Haifeng Wu",
        "Wang"
      ],
      "year": "",
      "venue": "Large-scale knowledge enhanced pretraining for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Tianyi Zhang",
        "Yann Dubois",
        "Xuechen Li",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori B Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurélien Rodriguez",
        "Armand Joulin"
      ],
      "year": "",
      "venue": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "authors": [
        "Alex Wang",
        "Yada Pruksachatkun",
        "Nikita Nangia",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2019",
      "venue": "NeurIPS",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "authors": [
        "Alex Wang",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2018",
      "venue": "Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, Black-boxNLP@EMNLP 2018",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation",
      "authors": [
        "Shuohuan Wang",
        "Yu Sun",
        "Yang Xiang",
        "Zhihua Wu",
        "Siyu Ding",
        "Weibao Gong",
        "Shikun Feng",
        "Junyuan Shang",
        "Yanbin Zhao",
        "Chao Pang",
        "Jiaxiang Liu",
        "Xuyi Chen",
        "Yuxiang Lu",
        "Weixin Liu",
        "Xi Wang",
        "Yangfan Bai",
        "Qiuliang Chen",
        "Li Zhao",
        "Shiyong Li",
        "Peng Sun",
        "Dianhai Yu",
        "Yanjun Ma",
        "Hua Hao Tian",
        "Tian Wu",
        "Wei Wu",
        "Ge Zeng",
        "Wen Li",
        "Haifeng Gao",
        "Wang"
      ],
      "year": "2021",
      "venue": "titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "Self-instruct: Aligning language model with self generated instructions",
      "authors": [
        "Yizhong Wang",
        "Yeganeh Kordi",
        "Swaroop Mishra",
        "Alisa Liu",
        "Noah A Smith",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "year": "2022",
      "venue": "Self-instruct: Aligning language model with self generated instructions",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks",
      "authors": [
        "Yizhong Wang",
        "Swaroop Mishra",
        "Pegah Alipoormolabashi",
        "Yeganeh Kordi",
        "Amirreza Mirzaei",
        "Atharva Naik",
        "Arjun Ashok",
        "Arut Selvan Dhanasekaran",
        "Anjana Arunkumar",
        "David Stap",
        "Eshaan Pathak",
        "Giannis Karamanolakis",
        "Gary Haizhi",
        "Ishan Lai",
        "Ishani Purohit",
        "Jacob Mondal",
        "Kirby Anderson",
        "Krima Kuznia",
        "Doshi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Wei Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew M Du",
        "Dai",
        "V Quoc",
        "Le"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning",
      "authors": [
        "Shaohua Wu",
        "Xudong Zhao",
        "Tong Yu",
        "Rongguo Zhang",
        "Chong Shen",
        "Hongli Liu",
        "Feng Li",
        "Hong Zhu",
        "Jiangang Luo",
        "Liang Xu"
      ],
      "year": "2021",
      "venue": "Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "An explanation of in-context learning as implicit bayesian inference",
      "authors": [
        "Sang Michael Xie",
        "Aditi Raghunathan",
        "Percy Liang",
        "Tengyu Ma"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "CLUE: A chinese language understanding evaluation benchmark",
      "authors": [
        "Liang Xu",
        "Hai Hu",
        "Xuanwei Zhang",
        "Lu Li",
        "Chenjie Cao",
        "Yudong Li",
        "Yechen Xu",
        "Kai Sun",
        "Dian Yu",
        "Cong Yu",
        "Yin Tian",
        "Qianqian Dong",
        "Weitang Liu",
        "Bo Shi",
        "Yiming Cui",
        "Junyi Li",
        "Jun Zeng",
        "Rongzhao Wang",
        "Weijian Xie",
        "Yanting Li",
        "Yina Patterson",
        "Zuoyu Tian",
        "Yiwen Zhang",
        "He Zhou",
        "Shaoweihua Liu",
        "Zhe Zhao",
        "Qipeng Zhao",
        "Cong Yue",
        "Xinrui Zhang",
        "Zhengliang Yang",
        "Kyle Richardson",
        "Zhenzhong Lan"
      ],
      "year": "2020",
      "venue": "COLING",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "Be everyone's large language model engine",
      "authors": [
        "Yan Gong",
        "Yiping Peng",
        "Qiang Niu",
        "Baochang Ma Yunjie",
        "Ji",
        "Yong Deng",
        "Xiangang Li"
      ],
      "year": "2023",
      "venue": "Be everyone's large language model engine",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "Defending against neural fake news",
      "authors": [
        "Rowan Zellers",
        "Ari Holtzman",
        "Hannah Rashkin",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Franziska Roesner",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "GLM-130B: an open bilingual pre-trained",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia",
        "Weng Lam Tam",
        "Zixuan Ma",
        "Yufei Xue",
        "Jidong Zhai",
        "Wenguang Chen",
        "Peng Zhang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "GLM-130B: an open bilingual pre-trained",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Measuring massive multitask chinese understanding",
      "authors": [
        "Hui Zeng"
      ],
      "year": "2023",
      "venue": "Measuring massive multitask chinese understanding",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "",
      "authors": [
        "Wei Zeng",
        "Xiaozhe Ren",
        "Teng Su",
        "Hui Wang",
        "Yi Liao",
        "Zhiwei Wang",
        "Xin Jiang",
        "Zhenzhang Yang",
        "Kaisheng Wang",
        "Xiaoda Zhang",
        "Chen Li",
        "Ziyan Gong",
        "Yifan Yao",
        "Xinjing Huang",
        "Jun Wang",
        "Jianfeng Yu",
        "Qi Guo",
        "Yue Yu",
        "Yan Zhang",
        "Jin Wang",
        "Hengtao Tao",
        "Dasen Yan",
        "Zexuan Yi",
        "Fang Peng",
        "Fangqing Jiang",
        "Han Zhang",
        "Lingfeng Deng",
        "Yehong Zhang",
        "Zhe Lin",
        "Chao Zhang",
        "Shaojie Zhang",
        "Mingyue Guo",
        "Shanzhi Gu",
        "Gaojun Fan",
        "Yaowei Wang",
        "Xuefeng Jin",
        "Qun Liu",
        "Yonghong Tian"
      ],
      "year": "2021",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "OPT: open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona T Diab",
        "Xian Li",
        "Xi Victoria Lin",
        "Todor Mihaylov",
        "Myle Ott",
        "Sam Shleifer",
        "Kurt Shuster",
        "Daniel Simig",
        "Punit Singh Koura",
        "Anjali Sridhar",
        "Tianlu Wang",
        "Luke Zettlemoyer"
      ],
      "year": "2022",
      "venue": "OPT: open pre-trained transformer language models",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "CPM-2: large-scale cost-effective pre-trained language models",
      "authors": [
        "Zhengyan Zhang",
        "Yuxian Gu",
        "Xu Han",
        "Shengqi Chen",
        "Chaojun Xiao",
        "Zhenbo Sun",
        "Yuan Yao",
        "Fanchao Qi",
        "Jian Guan",
        "Pei Ke",
        "Yanzheng Cai",
        "Guoyang Zeng",
        "Zhixing Tan",
        "Zhiyuan Liu",
        "Minlie Huang",
        "Wentao Han",
        "Yang Liu",
        "Xiaoyan Zhu",
        "Maosong Sun"
      ],
      "year": "2021",
      "venue": "CPM-2: large-scale cost-effective pre-trained language models",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Yifan Dong",
        "Chen Du",
        "Yushuo Yang",
        "Zhipeng Chen",
        "Jinhao Chen",
        "Ruiyang Jiang",
        "Yifan Ren",
        "Xinyu Li",
        "Zikang Tang",
        "Peiyu Liu",
        "Jian-Yun Liu",
        "Ji-Rong Nie",
        "Wen"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Agieval: A humancentric benchmark for evaluating foundation models",
      "authors": [
        "Wanjun Zhong",
        "Ruixiang Cui",
        "Yiduo Guo",
        "Yaobo Liang",
        "Shuai Lu",
        "Yanlin Wang",
        "Amin Saied",
        "Weizhu Chen",
        "Nan Duan"
      ],
      "year": "2023",
      "venue": "Agieval: A humancentric benchmark for evaluating foundation models",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt",
      "authors": [
        "Ce Zhou",
        "Qian Li",
        "Chen Li",
        "Jun Yu",
        "Yixin Liu",
        "Guangjing Wang",
        "Kai Zhang",
        "Cheng Ji",
        "Qiben Yan",
        "Lifang He",
        "Hao Peng",
        "Jianxin Li",
        "Jia Wu",
        "Ziwei Liu",
        "Pengtao Xie",
        "Caiming Xiong",
        "Jian Pei",
        "Philip S Yu",
        "Lichao Sun"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "authors": [
        "Yukun Zhu",
        "Ryan Kiros",
        "Richard S Zemel",
        "Ruslan Salakhutdinov",
        "Raquel Urtasun",
        "Antonio Torralba",
        "Sanja Fidler"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chinese large language models on the proposed benchmark. The size of these models varies from 335M to 130B parameters. Experiment results demonstrate that they perform significantly worse than GPT-3.5 that reaches an accuracy of \\(\\sim\\) 48\\(\\%\\) on M3KE. The dataset is available at [https://github.com/tjunlp-lab/M3KE](https://github.com/tjunlp-lab/M3KE)."
    },
    {
      "title": "1 Introduction",
      "text": "Large Language Models (LLMs) [12, 13, 14, 15, 16] have achieved remarkable progress in recent years, especially with the release of ChatGPT1, which is widely acknowledged to revolutionize the world of natural language processing and to transform AI and society [1, 11, 12, 13]. Generally, LLMs are trained via self-supervised learning [14] on a huge amount of unlabeled data [15, 16, 17, 18], which cover a wide range of genres, e.g., encyclopedias, news, books, social medias, etc. Many studies have demonstrated that LLMs are able to acquire broad knowledge of many types and subjects [15, 16, 17, 18, 19, 20, 21]. Footnote 1: [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt) The paradigms that elicit and apply the acquired knowledge in LLMs onto downstream tasks have shifted from fine-tuning to instruction-tuning. Early LLMs usually adopt fine-tuning, which, however, suffers from lack of cross-task generalization as the fine-tuned LLMs are often task-specific and not being parameter-efficient as all pre-trained LLM parameters are usually required to be updated on downstream tasks. As LLMs reach the scale of billions of parameters, a more efficient alternative to elicit knowledge, in-context Learning (ICL) [15, 16, 17] has emerged, which uses only a few demonstration examples concatenated in a prompt. In order to enhance the cross-task generalization of LLMs to a variety of downstream tasks, instruction-tuning [14, 15, 16], which is performed via multi-task learning [13, 15] has been proposed. In instruction-tuning, the instructions for different tasks are different, but in a unified form. Supervised Fine-tuning (SFT) [18] and Reinforcement Learning from Human Feedback (RLHF) [16, 17, 19] are successful methods of instruction-tuning, which not only achieve generalization to unseen instructions but also align LLMs with human values and intents [14, 15, 16, 17, 18]. 2022). As the capability of knowledge acquisition and application in LLMs is constantly and rapidly evolving, a natural question which arises, is how we can assess such knowledge. Traditional single-task evaluation benchmarks Rajpurkar et al. (2016); Khot et al. (2020) are no longer adequate for evaluating them. Multi-task benchmarks like GLUE Wang et al. (2018), SuperGLUE Wang et al. (2019) and BIG-bench Srivastava et al. (2022) aggregate multiple NLP tasks to evaluate LLMs, which, however, are not sufficient either to assess knowledge acquired by LLMs. To address this issue, Hendrycks et al. (2021) propose MMLU, a widely used benchmark to test the knowledge acquisition and application capability of LLMs, which uses test questions across multiple subjects that humans lean to assess LLMs in zero- and few-shot settings. As MMLU is an English benchmark, it cannot be directly used for measuring LLMs trained with data in other languages. Even if it is translated into other languages, like the way used in evaluating GPT-4 OpenAI (2023), there are still gaps in knowledge across different languages as they usually have different education systems and knowledge structures. Similar to LLMs in English, LLMs dedicated in Chinese have also achieved rapid advances recently Du et al. (2022); Zeng et al. (2021); Zhang et al. (2021); Sun et al. (2021); Zeng et al. (2022); Ren et al. (2023); Wu et al. (2021); Wang et al. (2021); Chen et al. (2023). However, a massive knowledge evaluation benchmark that measures Chinese LLMs in line with Chinese education system is a desideratum. To bridge this gap, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is designed to measure the knowledge acquired by Chinese LLMs by testing their multitask accuracy in zero- and few-shot settings. M3KE contains 20,477 questions collected from 71 tasks. In particular, unlike recent benchmarks MMCU Zeng (2023) and AGIEval Zhong et al. (2023), M3KE covers all major levels of Chinese education system, ranging from primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence ensuring a standardized and unified assessment process. Table 1 shows the comparison between M3KE and other related benchmarks. With M3KE, we have tested recently released Chinese LLMs, to track the progress of Chinese LLMs in knowledge acquisition and application. The evaluated models are either pre-trained on massive data or pre-trained + fine-tuned with SFT or RLHF. The model sizes vary from 335M to 130B parameters. With extensive experiments, we observe that most evaluated Chinese LLMs have near random-chance accuracy, even for primary school tasks. The best performance is achieved by an SFT model built on the open-source BLOOM Scao et al. (2022), which is 14.8 points lower than the accuracy of GPT-3.5-turbo. Our main contributions are summarized as follows. * We propose M3KE, a knowledge evaluation benchmark for Chinese LLMs, which to date covers the largest number of tasks in line with Chinese education system. * We have tested a wide range of open-source Chinese LLMs, with model sizes varying from 335M to 130B, against GPT-3.5-turbo. * We have analyzed the performance of each model on different subject clusters and education levels in both zero- and five-shot settings."
    },
    {
      "title": "2 Related Work",
      "text": "Chinese Large Language Models.Recent years have witnessed a rapid development of Chinese LLMs, following the efforts of their English counterparts, e.g., GPT-3 Brown et al. (2020), Gopher Rae et al. (2021), LLaMA Touvron et al. (2023). Chinese LLMs, such as Pangu-\\(\\alpha\\) with 200B parameters Zeng et al. (2021), Yuan 1.0 with 245B parameters Wu et al. (2021), ERNIE 3.0 Titan with 260B parameters Sun et al. (2021), have been trained on Chinese textual data that contain tokens ranging from 180B to 329B. These models are developed in industry, which are usually not open-source. With the success of open-source LLMs \\begin{table} \\begin{tabular}{l c c c} \\hline \\hline **Benchmark** & **Language** & **\\# Tasks** & **\\# Questions** \\\\ \\hline MMLU Hendrycks et al. (2021) & En & 57 & 15,508 \\\\ AGIEval Zhong et al. (2023) & En \\& Zh & 20 & 8,062 \\\\ MMCU Zeng (2023) & Zh & 51 & 11,900 \\\\ M3KE & Zh & 71 & 20,477 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 1: The comparison between M3KE and other related benchmarks. (Taori et al., 2023; Peng et al., 2023) based on LLaMA, Chinese versions, such as ChatGLM-6B2, MOSS3, Phoenix (Chen et al., 2023), have emerged very recently. These models usually contain less than 20 billion parameters and are supervised fine-tuned on instructions that are either distilled from models of GPT-3.5 or learned in a self-instructing manner (Wang et al., 2022). Footnote 2: [https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) Footnote 3: [https://github.com/OpenLMLab/MOSS](https://github.com/OpenLMLab/MOSS) Benchmarks.The capability of eliciting and applying knowledge acquired during training is an important indicator for measuring LLMs. However, existing evaluation benchmarks (Wang et al., 2018, 2019; Srivastava et al., 2022; Xu et al., 2020) are normally designed to evaluate LLMs on various NLP tasks, not tailored for knowledge acquisition and application assessment. To comprehensively measure knowledge in LLMs, MMLU (Hendrycks et al., 2021) is proposed, which collects multiple-choice questions from 57 tasks that humans learn. As a different education system is used, on the one side, knowledge in Chinese LLMs may not exhibit in the translated-into-Chinese version of MMLU, e.g., Chinese Medicine, Chinese Legal System. On the other side, knowledge to be assessed in MMLU may be absent in Chinese textual data used to train Chinese LLMs. Our work is related to 3 datasets that have been developed concurrently with M3KE. MMCU (Zeng, 2023) is a Chinese benchmark that assesses knowledge in four domains: medicine, education, law, and psychology. AGIEval (Zhong et al., 2023) is a bilingual benchmark that measures the capability of LLMs on tasks of the Chinese college entrance exam and American college admission test, for high-school graduates. DomMa (Gu et al., 2023) is another Chinese benchmark that focuses on domain-specific knowledge. In contrast to these benchmarks, M3KE is a comprehensive Chinese benchmark that spans major stages of Chinese education system, from primary school to college with a broader range of subject categories, such as art, religion, traditional Chinese medicine, and classical literature."
    },
    {
      "title": "3 M3Ke",
      "text": "M3KE covers major Chinese education levels, including primary school, middle school, high school, college and professional exams, as well as multiple tasks as shown in Figure 1 while the detailed subjects are listed in A. We collect and organize multiple-choice questions from public websites. To ensure the quality and comprehensiveness of the questions, entrance exam questions are selected as much as possible. For the primary school, middle school and high school education level, we choose the subjects according to the corresponding entrance exams for Chinese students. For the college level, we select subjects according to the national entrance exam for master's degree in China. In addition to subjects under the major Chinese education system, we also collect comprehensive tasks to expand the knowledge coverage in M3KE, including computer grade exam, ancient Chinese language, novels and Chinese national civil service exam which covers commonsense knowledge, arts, religion, etc. In total, we have 71 tasks and 20,477 questions. We divide each task into a test set and a few-shot set, where the few-shot set includes 5 questions for each task for the few-shot evaluation setting. The test set includes 20,122 questions, and each task contains at least 100 questions. Instances of M3KE are listed in Table 2."
    },
    {
      "title": "Arts & Humanities",
      "text": "Arts & Humanities comprise a range of disciplines that cover Chinese, literature, arts and history. These disciplines focus on the analysis and interpretation of literary and cultural artifacts, rather than on practical applications. For instance, the Chinese in primary school aims to evaluate the students' proficiency in language use and literary apprecia Figure 1: The distribution of tasks in M3KE. \\begin{table} \\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \\hline Arts \\& Humanities & \\multicolumn{1}{p{142.3pt}}{**Which statement about the Lascaux cave murals is incorrect?**} \\\\ \\cline{3-3} & A & This fresco was found in France \\\\ & B & There are more than 100 animal images found \\\\ & C & The discovery was made in 1940 \\\\ & **D** & **Mural color is mainly black** \\\\ \\hline Social Sciences & & A & **A wants to kill B, and puts poison into B’s food. After B consumed it, A regretted it and rushed to explain the situation and sent B to the hospital for rescue. The hospital found that the poison was not toxic at all and B was unharmed. A’s behavior belongs to?** \\\\ \\cline{3-3} & A & Not a crime \\\\ & B & Attempted crime \\\\ & **C** & **Crime suspension** \\\\ & D & Crime reached \\\\ \\hline Natural Sciences & & **Which characteristic of nerve fiber conduction excitation is affected by the use of cocaine anesthesia?** \\\\ \\cline{3-3} & A & **Physiological integrity** \\\\ & B & **Insulation** \\\\ & C & Bidirectional conduction \\\\ & D & **Relative non-fatigability** \\\\ \\hline Other & & **Several studies have previously suggested that consuming chocolate increases the likelihood of developing heart disease. However, a recent and more reliable study concluded that there is no association between chocolate consumption and incidence of heart disease. It is estimated that the consumption of chocolate will significantly increase after the publication of this research. The above inference is based on the assumption that the reliability of the previous studies was lower than that of the latest study.** \\\\ \\hline & A & Although some people are aware that consuming chocolate increases the likelihood of developing heart disease, they still indulge in it. People have never believed the claim that eating chocolate makes it more likely to develop heart disease. Nowadays, many people eat chocolate because they have not heard of the claim that chocolate can lead to heart disease. \\\\ \\cline{3-3} & D & **Newadays, many people abstain from eating chocolate solely because they believe that chocolate can trigger heart disease.** \\\\ \\hline \\end{tabular} \\end{table} Table 2: Examples from M3KE. Bolded items represent correct answers. Examples from top to bottom are from Fine Arts, Criminal Jurisprudence, Animal Physiology and Chinese Civil Service Examination task, respectively. tion for ages 7 to 13, such as the usage of synonyms and antonyms. The historical studies cover both Chinese and world history from ancient to modern times. M3KE also incorporates artistic subjects, such as dance, fine arts, music and film, because we believe that art is an essential aspect of human culture and should be relevant to LLMs as well."
    },
    {
      "title": "Social Sciences",
      "text": "Social sciences differ from Arts & Humanities in that they emphasize practical aspects of humanistic studies, such as law, politics, education and psychology. These subjects are mainly taught at the college level. Although ideological and political courses are also part of the Chinese middle school and high school curriculum, they primarily involve moral education. Social sciences also encompass economic and management studies, which largely consist of questions from the joint exams for graduate students majoring in these fields in China. These studies include microeconomics, macroeconomics, management and logic at the undergraduate level."
    },
    {
      "title": "Natural Sciences",
      "text": "Natural sciences encompass engineering, science, medicine and fundamental disciplines such as math, physics, chemistry, biology and so on. These subjects often require a high degree of computation, analysis and logical reasoning skills. The same subject may assess different types of knowledge at different levels according to the Chinese education system. For instance, primary school math mainly tests the basic arithmetic operations, while high school math covers more advanced mathematical concepts, such as sequences, derivatives and geometry."
    },
    {
      "title": "Other",
      "text": "Other types of tasks include religion, Chinese civil service exam, and specialized tasks, like ancient Chinese language and novel reasoning task. These tasks require knowledge that is not limited to a single level or subject as described above. The Chinese civil service exam involves knowledge in commonsense, humanities, logic and other domains, which we can consider as an assessment of the comprehensive knowledge for LLMs. Similarly, in the novel task, these questions involve a lot of information from many classical novels."
    },
    {
      "title": "Overall Statistics",
      "text": "Table 3 shows the overall statistics of M3KE. The numbers of tasks in the four subject clusters described above are 12, 21, 31 and 7, respectively, while the numbers of questions in the four subject clusters are 3,612, 6,222, 8,162 and 2,126, respectively. The maximum number of questions is 425 while the minimum number is 100. Questions in social and natural sciences are usually longer than those in arts & humanities and other while their answer choices are shorter."
    },
    {
      "title": "4 Experiments",
      "text": "We assessed state-of-the-art large language models recently developed for Chinese on M3KE, attempting to understand and track the progress of Chinese LLMs in learning and applying knowledge from massive data."
    },
    {
      "title": "Assessed Models",
      "text": "The assessed Chinese LLMs can be divided into two categories: models being only pre-trained and models that are instruction-tuned with SFT/RLHF. For the former, we selected GLM-335M (Du et al., 2022), GLM-10B (Du et al., 2022), GLM-130B (Zeng et al., 2022) and BLOOM-7.1B (Scao et al., 2022). For the latter, we included ChatGLM-6B4, MOSS-SFT-16B5, BELLE-7B (Yunjie Ji and Li, \\begin{table} \\begin{tabular}{l c c c c} \\hline & **Arts \\& Humanities** & **Social Sciences** & **Natural Sciences** & **Other** \\\\ \\hline Tasks & 12 & 21 & 31 & 7 \\\\ Q Numbers & 3,612 & 6,222 & 8,162 & 2,126 \\\\ Avg.Q Numbers & 301 & 296 & 263 & 303 \\\\ Max.Q Numbers & 352 & 374 & 347 & 425 \\\\ Min.Q Numbers & 190 & 190 & 100 & 129 \\\\ Avg.Q Tokens & 30.33 & 38.75 & 38.54 & 33.21 \\\\ Avg.C Tokens & 53.92 & 30.99 & 44.57 & 52.53 \\\\ \\hline \\end{tabular} \\end{table} Table 3: Overall statistics of M3KE. Q: question. C: answer choices2023), where BELLE-7B is the SFT version based on BLOOMZ-7.1B-MT (Muennighoff et al., 2022). We used the two variants of BELLE fine-tuned on 200K and 2M instructions, namely BELLE-7B-0.2M6 and BELLE-7B-2M7. We also evaluated GPT-3.5-turbo8 from OpenAI as a reference. Footnote 6: [https://huggingface.co/BelleGroup/BELLE-7B-0.2M](https://huggingface.co/BelleGroup/BELLE-7B-0.2M) Footnote 7: [https://huggingface.co/BelleGroup/BELLE-7B-2M](https://huggingface.co/BelleGroup/BELLE-7B-2M) Footnote 8: [https://openai.com/product](https://openai.com/product)"
    },
    {
      "title": "Prompts",
      "text": "All models were tested using the \\(n\\)-shot setting with a unified prompt, where \\(n\\) is an integer from 0 to 5. For the zero-shot setting (i.e., \\(n=0\\)), the unified prompt provided to all models is \"Please choose the correct option from 'A', 'B', 'C', 'D' based on the following question\". For few-shot setting (i.e., \\(n>0\\)), the unified prompt is \"Please choose the correct option from 'A', 'B', 'C', 'D' based on the following examples and question\". The input to all LLMs consists of the prompt, question, answer choices and suffix, which is \"the correct option is: \". Even we tell models to only output the correct answer choice indicator (i.e., \\(\\in\\{A,B,C,D\\}\\)) in the prompt, not all models can follow this instruction. Sometimes they output both answer choice and rationale to the answer choice (the order of these two types of outputs are random). We hence keep only the output answer choice indicator as the final answer to calculate accuracy."
    },
    {
      "title": "Results",
      "text": "We compared the zero-shot accuracy of each model in Table 4 in terms of subject clusters. For the pre-trained models, there is a clear positive correlation between accuracy and model size, where the model with 130B parameters significantly outperforms the models with 335M/7B/10B parameters, even though they have different backbones. The accuracy of GPT-3.5-turbo is significantly higher than those of the evaluated Chinese LLMs, which currently provides an upper bound for open-source Chinese LLMs. All pretrained LLMs with \\(\\leq 10B\\) parameters achieve an accuracy lower than random-chance accuracy (i.e., 25\\(\\%\\)), indicating that knowledge acquired by these models is not adequate for M3KE. In addition, we observe that the number of instructions used for SFT is an important factor, as the BELLE model fine-tuned with 2M instructions is significantly better than that with 0.2M instructions. The zero-shot performance of GPT-3.5-turbo is much higher than the compared open-source \\begin{table} \\begin{tabular}{l c c c c c} \\hline Models & Arts \\& Humanities & Social Sciences & Natural Sciences & Other & Average \\\\ \\hline GLM-335M & 0.070 & 0.046 & 0.084 & 0.044 & 0.062 \\\\ BLOOM-7.1B & 0.163 & 0.159 & 0.161 & 0.158 & 0.161 \\\\ GLM-10B & 0.180 & 0.229 & 0.219 & 0.150 & 0.197 \\\\ GLM-130B & 0.326 & 0.352 & 0.274 & 0.359 & 0.328 \\\\ \\hline ChatGLM-6B & 0.246 & 0.267 & 0.168 & 0.263 & 0.236 \\\\ MOSS-SFT-16B & 0.260 & 0.263 & 0.207 & 0.275 & 0.251 \\\\ BELLE-7B-0.2M & 0.247 & 0.296 & 0.260 & 0.260 & 0.266 \\\\ BELLE-7B-2M & 0.328 & 0.367 & 0.282 & 0.355 & 0.333 \\\\ GPT-3.5-turbo & 0.460 & 0.538 & 0.444 & 0.481 & 0.481 \\\\ \\hline \\end{tabular} \\end{table} Table 4: Average zero-shot accuracy for each model on the four subject clusters. \\begin{table} \\begin{tabular}{l c c c c c} \\hline Models & Arts \\& Humanities & Social Sciences & Natural Sciences & Other & Average \\\\ \\hline GLM-335M & 0.220 & 0.247 & 0.193 & 0.126 & 0.196 \\\\ BLOOM-7.1B & 0.247 & 0.260 & 0.235 & 0.246 & 0.247 \\\\ GLM-10B & 0.294 & 0.304 & 0.232 & 0.211 & 0.260 \\\\ GLM-130B & 0.297 & 0.329 & 0.246 & 0.228 & 0.275 \\\\ \\hline ChatGLM-6B & 0.188 & 0.175 & 0.121 & 0.198 & 0.171 \\\\ MOSS-SFT-16B & 0.266 & 0.264 & 0.258 & 0.284 & 0.268 \\\\ BELLE-7B-0.2M & 0.292 & 0.327 & 0.273 & 0.307 & 0.299 \\\\ BELLE-7B-2M & 0.287 & 0.309 & 0.284 & 0.313 & 0.298 \\\\ GPT-3.5-turbo & 0.453 & 0.540 & 0.464 & 0.476 & 0.483 \\\\ \\hline \\end{tabular} \\end{table} Table 5: Average five-shot accuracy for each model on the four subject clusters. Chinese LLMs, but still lower than 50\\(\\%\\) accuracy, suggesting that M3KE is a very challenging benchmark. We further compared the accuracy of different models under the 5-shot setting. Results are shown in Table 5. For pre-trained models, ICL in the few-shot setting significantly improves the performance and the smaller the pretrained model is, the larger the achieved improvement is. The exception is GLM-130B, which performs significantly worse under the 5-shot setting than the zero-shot setting. We conjecture that GLM-130B already has the ability to understand questions without examples because it uses instances in the instruction format as part of the pre-training corpus (Zeng et al., 2022), and demonstrations may bring interference to the final prediction of the model. The 5-shot results of the SFT models are mixed in comparison to those in the zero-shot setting. We find that for ChatGLM-6B and BELLE-7B-2M, 5-shot is worse than zero-shot setting, similar to the results observed on GLM-130B. In contrast, 5-shot has a positive impact on MOSS-SFT-16B and BELLE-7B-0.2M. As these models are different from each other in terms of model size, training data, instruction data, etc., we leave the in-depth analysis on the mixed results to our future work. We finally provide the results of each model on different education levels in Table 6 for the zero-shot setting and Table 7 for the few-shot setting. Interestingly, we observe that LLMs do not reach higher performance at lower education levels than higher education levels, even for GPT-3.5-turbo. This suggests that tasks from lower education levels remain challenging for these state-of-the-art Chinese LLMs."
    },
    {
      "title": "5 Conclusion",
      "text": "We have presented a new benchmark M3KE, to assess the capability of Chinese LLMs in learning and applying knowledge in multiple subjects at multiple levels of Chinese education system. M3KE contains 71 tasks and 20,447 questions. We find that all evaluated state-of-the-art open-source Chinese LLMs significantly lag behind GPT-3.5. We hope that this benchmark can be used to track and promote further progress in Chinese LLMs. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline Models & Primary School & Middle School & High School & College & Other & Average \\\\ \\hline GLM-335M & 0.075 & 0.099 & 0.099 & 0.054 & 0.046 & 0.075 \\\\ BLOOM-7.1B & 0.173 & 0.142 & 0.173 & 0.160 & 0.164 & 0.163 \\\\ GLM-10B & 0.190 & 0.199 & 0.197 & 0.213 & 0.152 & 0.190 \\\\ GLM-130B & 0.243 & 0.303 & 0.229 & 0.324 & 0.359 & 0.292 \\\\ \\hline ChatGLM-6B & 0.180 & 0.243 & 0.191 & 0.213 & 0.250 & 0.216 \\\\ MOSS-SFT-16B & 0.224 & 0.223 & 0.213 & 0.242 & 0.260 & 0.232 \\\\ BELLE-7B-0.2M & 0.233 & 0.269 & 0.259 & 0.268 & 0.263 & 0.258 \\\\ BELLE-7B-2M & 0.248 & 0.313 & 0.263 & 0.332 & 0.349 & 0.301 \\\\ GPT-3.5-turbo & 0.328 & 0.403 & 0.395 & 0.509 & 0.484 & 0.435 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 6: Average zero-shot accuracy for each model on five major education levels. \\begin{table} \\begin{tabular}{l c c c c c c} \\hline \\hline Models & Primary School & Middle School & High School & College & Other & Average \\\\ \\hline GLM-335M & 0.206 & 0.229 & 0.232 & 0.223 & 0.114 & 0.201 \\\\ BLOOM-7.1B & 0.262 & 0.222 & 0.245 & 0.249 & 0.246 & 0.245 \\\\ GLM-10B & 0.229 & 0.263 & 0.270 & 0.278 & 0.197 & 0.248 \\\\ GLM-130B & 0.268 & 0.293 & 0.272 & 0.294 & 0.208 & 0.267 \\\\ \\hline ChatGLM-6B & 0.089 & 0.150 & 0.137 & 0.155 & 0.196 & 0.146 \\\\ MOSS-SFT-16B & 0.272 & 0.223 & 0.263 & 0.266 & 0.281 & 0.261 \\\\ BELLE-7B-0.2M & 0.260 & 0.256 & 0.273 & 0.298 & 0.310 & 0.280 \\\\ BELLE-7B-2M & 0.258 & 0.264 & 0.268 & 0.306 & 0.299 & 0.279 \\\\ GPT-3.5-turbo & 0.308 & 0.565 & 0.373 & 0.517 & 0.475 & 0.448 \\\\ \\hline \\hline \\end{tabular} \\end{table} Table 7: Average five-shot accuracy for each model on five major education levels."
    },
    {
      "title": "References",
      "text": "* [1]S. Altman (2023) Planning for agi and beyond. OpenAI Blog. Cited by: SS1. * [2]S. H. Bach, V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. Bari, T. Fevry, Z. Allyafaei, M. Dey, A. Santtilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. Saeed AlShaibani, S. Sharma, U. Thakker, K. Almuabarak, X. Tang, D. R. Radev, M. Tian-Jian Jiang, and A. M. Rush (2022) Promptsource: an integrated development environment and repository for natural language prompts. In ACL (demo), pp. 93-104. Cited by: SS1. * [3]R. Balestriero, M. Ibrahim, V. Sobal, A. Borces, S. S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, et al. (2023) A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210. Cited by: SS1. * [4]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020) Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Cited by: SS1. * [5]S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. Tulio Ribeiro, and Y. Zhang (2023) Sparks of artificial general intelligence: early experiments with gpt-4. volume abs/2303.12712. Cited by: SS1. * [6]Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun (2023) A comprehensive survey of ai-generated content (aigc): a history of generative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226. Cited by: SS1. * [7]Z. Chen, F. Jiang, J. Chen, T. Wang, F. Yu, G. Chen, H. Zhang, J. Liang, C. Zhang, Z. Zhang, et al. (2023) Phoenix: democratizing chatgpt across languages. arXiv preprint arXiv:2304.10453. Cited by: SS1. * [8]P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei (2017) Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4299-4307. Cited by: SS1. * [9]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. Shane Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei (2022) Scaling instruction-finetuned language models. CoRRabs/2210.11416. Cited by: SS1. * [10]Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, L. Li, and Z. Sui (2023) A survey for in-context learning. CoRRabs/2301.00234. Cited by: SS1. * [11]N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. Wei Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui (2022) GLM: efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, pp. 5547-5569. Cited by: SS1. * [12]Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang (2022) GLM: general language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, pp. 320-335. Cited by: SS1. * [13]Z. Gu, X. Zhu, H. Ye, L. Zhang, Z. Xiong, Z. Li, Q. He, S. Jiang, H. Feng, and Y. Xiao (2023) Domain mastery benchmark: an ever-updating benchmark for evaluating holistic domain knowledge of large language model-a preliminary release. arXiv preprint arXiv:2304.11679. Cited by: SS1. * [14]D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021) Measuring massive multitask language understanding. In ICLR, Cited by: SS1. * [15]J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. abs/2203.15556. * Huang et al. (2023) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kritit Aggarwal, Zeven Chi, Johan Bjorck, Vishrav Chaudhry, Subhojit Som, Xia Song, and Furu Wei. 2023. Language is not all you need: Aligning perception with language models. _CoRR_, abs/2302.14045. * Khot et al. (2020) Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. QASC: A dataset for question answering via sentence composition. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pages 8082-8090. * Liu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Multi-task deep neural networks for natural language understanding. In _ACL (1)_, pages 4487-4496. Association for Computational Linguistics. * Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692. * Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeali, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning. _CoRR_, abs/2211.01786. * OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. _OpenAI_. * Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. _CoRR_, abs/2203.02155. * Paperno et al. (2016) Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In _ACL (1)_. The Association for Computer Linguistics. * Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_. * Rae et al. (2022) Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis & insights from training gopher. _CoRR_, abs/2112.11446. * Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, pages 140:1-140:67. * Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016_, pages 2383-2392. * Ren et al. (2023) Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, and Jun Yao. 2023. Pangu-\\(\\Sigma\\): Towards trillion parameter language model with sparse heterogeneous computing. _CoRR_, abs/2303.10845. * Sanh et al. (2016) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma Sharma, Eliza Szczczechla, Takewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-JianJiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheeseth Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask prompted training enables zero-shot task generalization. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net. * Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamachi, Thomas Wang, Benoit Sagot, Niklas Muenningoff, Albert Villanova del Moral, Olatuni Rruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzx, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. _CoRR_, abs/2211.05100. * Srivastava et al. (2021) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santiilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _CoRR_, abs/2206.04615. * Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020. Learning to summarize from human feedback. _CoRR_, abs/2009.01325. * Sun et al. (2021) Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. _CoRR_, abs/2107.02137. * Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca). * Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. _CoRR_. * Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In _NeurIPS_, pages 3261-3275. * Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of the Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018_, pages 353-355. Association for Computational Linguistics. * Wang et al. (2021) Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng, Ge Li, Wen Gao, and Haifeng Wang. 2021. ERNIE 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation. _CoRR_, abs/2112.12731. * Wang et al. (2022a) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a. Self-instruct: Aligning language model with self generated instructions. _CoRR_, abs/2212.10560. * Wang et al. (2021) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuzenia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravexheaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Eminates, December 7-11, 2022_, pages 5085-5109. * Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net. * Wu et al. (2021) Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, et al. 2021. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. _arXiv preprint arXiv:2110.04725_. * Xie et al. (2022) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. * Xu et al. (2020) Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. 2020. CLUE: A chinese language understanding evaluation benchmark. In _COLING_, pages 4762-4772. International Committee on Computational Linguistics. * Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021_, pages 483-498. * Yang et al. (2023) Yan Gong Yiping Peng Qiang Niu Baochang Ma Yunjie Ji, Yong Deng and Xiangang Li. 2023. Belle: Be everyone's large language model engine. [https://github.com/LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE). * Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062. * Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: an open bilingual pre-trained model. abs/2210.02414. * Zeng (2023) Hui Zeng. 2023. Measuring massive multitask chinese understanding. _arXiv preprint arXiv:2304.12986_. * Zeng et al. (2021) Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. 2021. Pangu-\\(\\alpha\\): Large-scale autoregressive pretrained chinese language models with auto-parallel computation. _CoRR_, abs/2104.12369. * Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihalyov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. _CoRR_, abs/2205.01068. * Zhang et al. (2021) Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, and Maosong Sun. 2021. CPM-2: large-scale cost-effective pre-trained language models. _CoRR_, abs/2106.10715. * Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. _arXiv preprint arXiv:2303.18223_. * Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_. * Zhou et al. (2023) Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, and Lichao Sun. 2023. A comprehensive survey on pretrained foundation models: A history from BERT to chatgpt. _CoRR_, abs/2302.09419. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015_, pages 19-27. IEEE Computer Society. \\begin{tabular}{l c c} \\hline **Tasks** & **Subjects** & **Education System** \\\\ \\hline Chinese & Arts \\& Humanities & Primary school \\\\ Math & Natural Sciences & Primary school \\\\ \\hline Chinese & Arts \\& Humanities & Junior high school \\\\ History & Arts \\& Humanities & Junior high school \\\\ Politics & Social Sciences & Junior high school \\\\ Math & Natural Sciences & Junior high school \\\\ Physics & Natural Sciences & Junior high school \\\\ Biology & Natural Sciences & Junior high school \\\\ Chemistry & Natural Sciences & Junior high school \\\\ Geography & Natural Sciences & Junior high school \\\\ \\hline Chinese & Arts \\& Humanities & High school \\\\ History & Arts \\& Humanities & High school \\\\ Politics & Social Sciences & High school \\\\ Math & Natural Sciences & High school \\\\ Physics & Natural Sciences & High school \\\\ Biology & Natural Sciences & High school \\\\ Chemistry & Natural Sciences & High school \\\\ Geography & Natural Sciences & High school \\\\ \\hline Modern History & Arts \\& Humanities & College \\\\ History Foundation & Arts \\& Humanities & College \\\\ Modern World History & Arts \\& Humanities & College \\\\ Chinese Constitutional Law & Social Sciences & College \\\\ History of Chinese Education & Social Sciences & College \\\\ History of the Chinese Legal System & Social Sciences & College \\\\ Developmental and Educational Psychology & Social Sciences & College \\\\ History of Foreign Education & Social Sciences & College \\\\ Experimental Psychology & Social Sciences & College \\\\ Introduction to Psychology & Social Sciences & College \\\\ Moral Cultivation & Social Sciences & College \\\\ Psychology of Teaching & Social Sciences & College \\\\ Principles of Pedagogy & Social Sciences & College \\\\ Educational Research Methods & Social Sciences & College \\\\ Current Affairs and Politics & Social Sciences & College \\\\ Introduction to Mao Tsetung Thoughts & Social Sciences & College \\\\ Civil Law & Social Sciences & College \\\\ Jurisprudence & Social Sciences & College \\\\ Sociology & Social Sciences & College \\\\ Basic Principle of Marxism & Social Sciences & College \\\\ Criminal Jurisprudence & Social Sciences & College \\\\ Outline of Chinese Modern History & Social Sciences & College \\\\ Humanistic Medicine & Natural Sciences & College \\\\ Internal Medicine & Natural Sciences & College \\\\ Animal Physiology & Natural Sciences & College \\\\ Surgical Sciences & Natural Sciences & College \\\\ Operating Systems & Natural Sciences & College \\\\ Data Structures & Natural Sciences & College \\\\ Probability Theory & Natural Sciences & College \\\\ Biochemistry & Natural Sciences & College \\\\ Physiology & Natural Sciences & College \\\\ \\hline \\end{tabular}"
    },
    {
      "title": "Appendix A All Subjects",
      "text": "See Table 8 for all 71 tasks. \\begin{table} \\begin{tabular}{l c c} Principles of Computer Composition & Natural Sciences & College \\\\ Computer Networks & Natural Sciences & College \\\\ Advanced Mathematics & Natural Sciences & College \\\\ Linear Algebra & Natural Sciences & College \\\\ Stomatology & Natural Sciences & College \\\\ Anthropotomy & Natural Sciences & College \\\\ Pharmacology & Natural Sciences & College \\\\ Immunology & Natural Sciences & College \\\\ Management & Natural Sciences & College \\\\ Economics & Natural Sciences & College \\\\ Film & Arts \\& Humanities & Other \\\\ Music & Arts \\& Humanities & Other \\\\ Dance & Arts \\& Humanities & Other \\\\ Fine Arts & Arts \\& Humanities & Other \\\\ Computer Fundamentals & Natural Sciences & Other \\\\ Computer Programming Language & Natural Sciences & Other \\\\ Chinese Medicine & Other & Other \\\\ Ancient Chinese Language & Other & Other \\\\ Novels & Other & Other \\\\ Religion & Other & Other \\\\ Chinese Civil Service Examination & Other & Other \\\\ \\hline \\end{tabular} \\end{table} Table 8: Summary of all 71 tasks."
    }
  ]
}