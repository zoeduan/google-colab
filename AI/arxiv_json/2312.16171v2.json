{
  "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
  "authors": [
    "Mahmoud Sondos",
    "Bsharat",
    "Aidar Myrzakhan",
    "Zhiqiang Shen"
  ],
  "abstract": "\n This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at  https://github.com/VILA-Lab/ATLAS . \n",
  "references": [
    {
      "id": null,
      "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
      "authors": [
        "Mahmoud Sondos",
        "Bsharat",
        "Aidar Myrzakhan",
        "Zhiqiang Shen"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Ask me anything: A simple strategy for prompting language models",
      "authors": [
        "Simran Arora",
        "Avanika Narayan",
        "Mayee F Chen",
        "Laurel Orr",
        "Neel Guha",
        "Kush Bhatia",
        "Ines Chami",
        "Frederic Sala",
        "Christopher Ré"
      ],
      "year": "2022",
      "venue": "Ask me anything: A simple strategy for prompting language models",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "",
      "authors": [
        "Jordan Hoffmann",
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Elena Buchatskaya",
        "Trevor Cai",
        "Eliza Rutherford",
        "Diego De Las",
        "Lisa Anne Casas",
        "Johannes Hendricks",
        "Aidan Welbl",
        "Tom Clark",
        "Eric Hennigan",
        "Katie Noland",
        "George Millican",
        "Bogdan Van Den Driessche",
        "Aurelia Damoc",
        "Simon Guy",
        "Karen Osindero",
        "Erich Simonyan",
        "Jack W Elsen",
        "Oriol Rae",
        "Laurent Vinyals",
        "Sifre"
      ],
      "year": "2022",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Mathematical reasoning using large language models",
      "authors": [
        "Shima Imani",
        "Liang Du",
        "Harsh Shrivastava",
        "Mathprompter"
      ],
      "year": "2023",
      "venue": "Mathematical reasoning using large language models",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Arthur Mensch",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Lample",
        "Lucile Saulnier",
        "Renard Lélio",
        "Marie-Anne Lavaud",
        "Pierre Lachaux",
        "Teven Stock",
        "Thibaut Le Scao",
        "Thomas Lavril",
        "Timothée Wang",
        "William El Lacroix",
        "Sayed"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Evaluating open-domain question answering in the era of large language models",
      "authors": [
        "Ehsan Kamalloo",
        "Nouha Dziri",
        "L A Charles",
        "Davood Clarke",
        "Rafiei"
      ],
      "year": "2023",
      "venue": "Evaluating open-domain question answering in the era of large language models",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Can language models learn from explanations in context?",
      "authors": [
        "Andrew Lampinen",
        "Ishita Dasgupta",
        "Stephanie Chan",
        "Kory Mathewson",
        "Mh Tessler",
        "Antonia Creswell",
        "James Mcclelland",
        "Jane Wang",
        "Felix Hill"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Starcoder: may the source be with you! arXiv preprint",
      "authors": [
        "Raymond Li",
        "Loubna Ben Allal",
        "Yangtian Zi",
        "Niklas Muennighoff",
        "Denis Kocetkov",
        "Chenghao Mou",
        "Marc Marone",
        "Christopher Akiki",
        "Jia Li",
        "Jenny Chim"
      ],
      "year": "2023",
      "venue": "Starcoder: may the source be with you! arXiv preprint",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Alpacaeval: An automatic evaluator of instructionfollowing models",
      "authors": [
        "Xuechen Li",
        "Tianyi Zhang",
        "Yann Dubois",
        "Rohan Taori",
        "Ishaan Gulrajani",
        "Carlos Guestrin",
        "Percy Liang",
        "Tatsunori B Hashimoto"
      ],
      "year": "",
      "venue": "Alpacaeval: An automatic evaluator of instructionfollowing models",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Competition-level code generation with alphacode",
      "authors": [
        "Yujia Li",
        "David Choi",
        "Junyoung Chung",
        "Nate Kushman",
        "Julian Schrittwieser",
        "Rémi Leblond",
        "Tom Eccles",
        "James Keeling",
        "Felix Gimeno",
        "Agustin Dal Lago"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Guiding large language models via directional stimulus prompting",
      "authors": [
        "Zekun Li",
        "Baolin Peng",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao",
        "Xifeng Yan"
      ],
      "year": "2023",
      "venue": "Guiding large language models via directional stimulus prompting",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "",
      "authors": [
        ": Openai",
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman",
        "Shyamal Anadkat",
        "Red Avila",
        "Igor Babuschkin",
        "Suchir Balaji",
        "Valerie Balcom",
        "Paul Baltescu",
        "Haiming Bao",
        "Mo Bavarian",
        "Jeff Belgum",
        "Irwan Bello",
        "Jake Berdine",
        "Gabriel Bernadett-Shapiro",
        "Christopher Berner",
        "Lenny Bogdonoff",
        "Oleg Boiko",
        "Madelaine Boyd",
        "Anna-Luisa Brakman",
        "Greg Brockman",
        "Tim Brooks",
        "Miles Brundage",
        "Kevin Button",
        "Trevor Cai",
        "Rosie Campbell",
        "Andrew Cann",
        "Brittany Carey",
        "Chelsea Carlson",
        "Rory Carmichael",
        "Brooke Chan",
        "Che Chang",
        "Fotis Chantzis",
        "Derek Chen",
        "Sully Chen",
        "Ruby Chen",
        "Jason Chen",
        "Mark Chen",
        "Ben Chess",
        "Chester Cho",
        "Casey Chu",
        "Hyung Won Chung",
        "Dave Cummings",
        "Jeremiah Currier",
        "Leo Gao",
        "Elie Georges",
        "Christian Gibson",
        "Vik Goel",
        "Tarun Gogineni",
        "Gabriel Goh",
        "Rapha Gontijo-Lopes",
        "Jonathan Gordon",
        "Morgan Grafstein",
        "Scott Gray",
        "Ryan Greene",
        "Joshua Gross",
        "Shane Shixiang",
        "Yufei Gu",
        "Chris Guo",
        "Jesse Hallacy",
        "Jeff Han",
        "Yuchen Harris",
        "Mike He",
        "Johannes Heaton",
        "Chris Heidecke",
        "Alan Hesse",
        "Wade Hickey",
        "Peter Hickey",
        "Brandon Hoeschele",
        "Kenny Houghton",
        "Shengli Hsu",
        "Xin Hu",
        "Joost Hu",
        "Shantanu Huizinga",
        "Shawn Jain",
        "Joanne Jain",
        "Angela Jang",
        "Roger Jiang",
        "Haozhun Jiang",
        "Denny Jin",
        "Shino Jin",
        "Billie Jomoto",
        "Heewoo Jonn",
        "Tomer Jun",
        "Łukasz Kaftan",
        "Ali Kaiser",
        "Ingmar Kamali",
        "Nitish Kanitscheider",
        "Tabarak Shirish Keskar",
        "Logan Khan",
        "Jong Wook Kilpatrick",
        "Christina Kim",
        "Yongjik Kim",
        "Hendrik Kim",
        "Jamie Kirchner",
        "Matt Kiros",
        "Daniel Knight",
        "Łukasz Kokotajlo",
        "Andrew Kondraciuk",
        "Aris Kondrich",
        "Kyle Konstantinidis",
        "Gretchen Kosic",
        "Vishal Krueger",
        "Michael Kuo",
        "Ikai Lampe",
        "Teddy Lan",
        "Jan Lee",
        "Jade Leike",
        "Daniel Leung",
        "Levy",
        "Ming Chak",
        "Rachel Li",
        "Molly Lim",
        "Stephanie Lin",
        "Mateusz Lin",
        "Theresa Litwin",
        "Ryan Lopez",
        "Patricia Lowe",
        "Anna Lue",
        "Kim Makanju",
        "Sam Malfacini",
        "Todor Manning",
        "Yaniv Markov",
        "Bianca Markovski",
        "Katie Martin",
        "Andrew Mayer",
        "Bob Mayne",
        "Scott Mayer Mcgrew",
        "Christine Mckinney",
        "Paul Mcleavey",
        "Jake Mcmillan",
        "David Mcneil",
        "Aalok Medina",
        "Jacob Mehta",
        "Luke Menick",
        "Andrey Metz",
        "Pamela Mishchenko",
        "Vinnie Mishkin",
        "Evan Monaco",
        "Daniel Morikawa",
        "Tong Mossing",
        "Mira Mu",
        "Oleg Murati",
        "David Murk",
        "Ashvin Mély",
        "Reiichiro Nair",
        "Rajeev Nakano",
        "Arvind Nayak",
        "Richard Neelakantan",
        "Hyeonwoo Ngo",
        "Long Noh",
        "Ouyang",
        "O' Cullen",
        "Jakub Keefe",
        "Alex Pachocki",
        "Joe Paino",
        "Ashley Palermo",
        "Giambattista Pantuliano",
        "Joel Parascandolo",
        "Emy Parish",
        "Alex Parparita",
        "Mikhail Passos",
        "Andrew Pavlov",
        "Adam Peng",
        "; Perelman",
        "Wei",
        "Akila Cj Weinmann",
        "Peter Welihinda",
        "Jiayi Welinder",
        "Lilian Weng",
        "Matt Weng",
        "Dave Wiethoff",
        "Clemens Willner",
        "Samuel Winter",
        "Hannah Wolrich",
        "Lauren Wong",
        "Sherwin Workman",
        "Jeff Wu",
        "Michael Wu",
        "Kai Wu",
        "Tao Xiao",
        "Sarah Xu",
        "Kevin Yoo",
        "Qiming Yu",
        "Wojciech Yuan",
        "Rowan Zaremba",
        "Chong Zellers",
        "Marvin Zhang",
        "Shengjia Zhang",
        "Tianhao Zhao",
        "Juntang Zheng",
        "William Zhuang",
        "Barret Zhuk",
        "Zoph"
      ],
      "year": "2023",
      "venue": "Felipe Petroski Such",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Plum: Prompt learning using metaheuristic",
      "authors": [
        "Rui Pan",
        "Shuo Xing",
        "Shizhe Diao",
        "Xiang Liu",
        "Kashun Shum",
        "Jipeng Zhang",
        "Tong Zhang"
      ],
      "year": "2023",
      "venue": "Plum: Prompt learning using metaheuristic",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan",
        "Tim Salimans",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "",
      "authors": [
        "Jack W Rae",
        "Sebastian Borgeaud",
        "Trevor Cai",
        "Katie Millican",
        "Jordan Hoffmann",
        "H Francis Song",
        "John Aslanides",
        "Sarah Henderson",
        "Roman Ring",
        "Susannah Young",
        "Eliza Rutherford",
        "Tom Hennigan",
        "Jacob Menick",
        "Albin Cassirer",
        "Richard Powell",
        "George Van Den Driessche",
        "Lisa Anne Hendricks",
        "Maribeth Rauh",
        "Po-Sen Huang",
        "Amelia Glaese",
        "Johannes Welbl",
        "Sumanth Dathathri",
        "Saffron Huang",
        "Jonathan Uesato",
        "John Mellor",
        "Irina Higgins",
        "Antonia Creswell",
        "Nat Mcaleese",
        "Amy Wu",
        "Erich Elsen",
        "M Siddhant",
        "Elena Jayakumar",
        "David Buchatskaya",
        "Esme Budden",
        "Karen Sutherland",
        "Michela Simonyan",
        "Laurent Paganini",
        "Lena Sifre",
        "Martens",
        "Lorraine Xiang",
        "Adhiguna Li",
        "Aida Kuncoro",
        "Elena Nematzadeh",
        "Domenic Gribovskaya",
        "Angeliki Donato",
        "Arthur Lazaridou",
        "Jean-Baptiste Mensch",
        "Maria Lespiau",
        "Nikolai Tsimpoukelli",
        "Doug Grigorev",
        "Thibault Fritz",
        "Mantas Sottiaux",
        "Toby Pajarskas",
        "Zhitao Pohlen",
        "Daniel Gong",
        "Cyprien Toyama",
        "Yujia De Masson D'autume",
        "Tayfun Li",
        "Vladimir Terzi",
        "Igor Mikulik",
        "Aidan Babuschkin",
        "Diego Clark",
        "De Las",
        "Aurelia Casas",
        "Chris Guy",
        "James Jones",
        "Matthew J Bradbury",
        "Blake A Johnson",
        "Laura Hechtman",
        "Iason Weidinger",
        "William Gabriel",
        "Edward Isaac",
        "Simon Lockhart",
        "Laura Osindero",
        "Chris Rimell",
        "Oriol Dyer",
        "Kareem Vinyals",
        "Jeff Ayoub",
        "Lorrayne Stanway",
        "Bennett"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J Liu"
      ],
      "year": "2019",
      "venue": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Atlas: A llm inquiry principle benchmark",
      "authors": [
        "Zhiqiang Shen",
        "Sondos",
        "Mahmoud Bsharat",
        "Aidar Myrzakhan"
      ],
      "year": "2024",
      "venue": "Atlas: A llm inquiry principle benchmark",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "authors": [
        "Taylor Shin",
        "Yasaman Razeghi",
        "Robert L Logan",
        "Eric Wallace",
        "Sameer Singh"
      ],
      "year": "2020",
      "venue": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Yonghui Wu",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew M Dai",
        "Anja Hauth",
        "Katie Millican",
        "David Silver",
        "Slav Petrov",
        "Melvin Johnson",
        "Ioannis Antonoglou",
        "Julian Schrittwieser",
        "Amelia Glaese",
        "Jilin Chen",
        "Emily Pitler",
        "Timothy Lillicrap",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "James Molloy",
        "Michael Isard",
        "Paul R Barham",
        "Tom Hennigan",
        "Benjamin Lee",
        "Fabio Viola",
        "Malcolm Reynolds",
        "Yuanzhong Xu",
        "Ryan Doherty",
        "Eli Collins",
        "Clemens Meyer",
        "Eliza Rutherford",
        "Erica Moreira",
        "Kareem Ayoub",
        "Megha Goel",
        "George Tucker",
        "Enrique Piqueras",
        "Maxim Krikun",
        "Iain Barr",
        "Nikolay Savinov",
        "Ivo Danihelka",
        "Becca Roelofs",
        "Anaïs White",
        "Anders Andreassen",
        "Lakshman Tamara Von Glehn",
        "Mehran Yagati",
        "Lucas Kazemi",
        "Misha Gonzalez",
        "Jakub Khalman",
        "Alexandre Sygnowski",
        "Charlotte Frechette",
        "Laura Smith",
        "Lev Culp",
        "Yi Proleev",
        "Xi Luan",
        "James Chen",
        "Nathan Lottes",
        "Federico Schucher",
        "Alban Lebron",
        "Natalie Rrustemi",
        "Phil Clay",
        "Tomas Crone",
        "Jeffrey Kocisky",
        "Bartek Zhao",
        "Dian Perz",
        "Heidi Yu",
        "Adam Howard",
        "Jack W Bloniarz",
        "Han Rae",
        "Laurent Lu",
        "Marcello Sifre",
        "Fred Maggioni",
        "Dan Alcober",
        "Megan Garrette",
        "Shantanu Barnes",
        "Jacob Thakoor",
        "Gabriel Austin",
        "William Barth-Maron",
        "Rishabh Wong",
        "Rahma Joshi",
        "Deeni Chaabouni",
        "Arun Fatiha",
        "Ruibo Ahuja",
        "Yunxuan Liu",
        "Sarah Li",
        "Jeremy Cogan",
        "Chao Chen",
        "Chenjie Jia",
        "Qiao Gu",
        "Jordan Zhang",
        "Ale Jakse Grimstad",
        "Martin Hartman",
        "Gaurav Chadwick",
        "Xavier Singh Tomar",
        "Evan Garcia",
        "Emanuel Senter",
        "Thanumalayan Taropa",
        "Jacob Sankaranarayana Pillai",
        "Michael Devlin",
        "Diego Laskin",
        "De Las",
        "Dasha Casas",
        "Connie Valter",
        "Lorenzo Tao",
        "Adrià Blanco",
        "David Puigdomènech Badia",
        "Mianna Reitter",
        "Jenny Chen",
        "Clara Brennan",
        "Sergey Rivera",
        "Shariq Brin",
        "Gabriela Iqbal",
        "Jane Surita",
        "Abhi Labanowski",
        "Stephanie Rao",
        "Emilio Winkler",
        "Yiming Parisotto",
        "Kate Gu",
        "Yujing Olszewska",
        "Ravi Zhang",
        "Antoine Addanki",
        "Annie Miech",
        "Laurent El Louis",
        "Denis Shafey",
        "Geoff Teplyashin",
        "Elliot Brown",
        "Nithya Catt",
        "Jan Attaluri",
        "Jackie Balaguer",
        "Pidong Xiang",
        "Zoe Wang",
        "Anton Ashwood",
        "Albert Briukhov",
        "Sanjay Webson",
        "Smit Ganapathy",
        "Ajay Sanghavi",
        "Ming-Wei Kannan",
        "Axel Chang",
        "Josip Stjerngren",
        "Yuting Djolonga",
        "Ankur Sun",
        "Matthew Bapna",
        "Pedram Aitchison",
        "Henryk Pejman",
        "Tianhe Michalewski",
        "Cindy Yu",
        "Juliette Wang",
        "Junwhan Love",
        "Dawn Ahn",
        "Kehang Bloxwich",
        "Peter Han",
        "Thibault Humphreys",
        "James Sellam",
        "Varun Bradbury",
        "Sina Godbole",
        "Bogdan Samangooei",
        "Alex Damoc",
        "Kaskasoli",
        "M R Sébastien",
        "Vijay Arnold",
        "Shubham Vasudevan",
        "Jason Agrawal",
        "Dmitry Riesa",
        "Richard Lepikhin",
        "Srivatsan Tanburn",
        "Hyeontaek Srinivasan",
        "Sarah Lim",
        "Pranav Hodkinson",
        "Johan Shyam",
        "Steven Ferret",
        "Ankush Hand",
        "Tom Le Garg",
        "Jian Paine",
        "Yujia Li",
        "Minh Li",
        "Alexander Giang",
        "Zaheer Neitz",
        "Sarah Abbas",
        "Machel York",
        "Elizabeth Reid",
        "Aakanksha Cole",
        "Dipanjan Chowdhery",
        "Dominika Das",
        "Vitaly Rogozińska",
        "Pablo Nikolaev",
        "Zachary Sprechmann",
        "Lukas Nado",
        "Nick Zilka",
        "Will Fernando",
        "Behnam Hawkins",
        "Solomon Neyshabur",
        "Adrian Kim",
        "Priyanka Hutter",
        "Alex Agrawal",
        "George Castro-Ros",
        "Tao Van Den Driessche",
        "Fan Wang",
        "Yang",
        "Paul Shuo Yiin Chang",
        "Ross Komarek",
        "Mario Mcilroy",
        "Guodong Lučić",
        "Wael Zhang",
        "Michael Farhan",
        "Paul Sharman",
        "Paul Natsev",
        "Yong Michel",
        "Yamini Cheng",
        "Siyuan Bansal",
        "Kris Qiao",
        "Siamak Cao",
        "Christina Shakeri",
        "Justin Butterfield",
        "Paul Chung",
        "Shivani Kishan Rubenstein",
        "Arthur Agrawal",
        "Kedar Mensch",
        "Karel Soparkar",
        "Timothy Lenc",
        "Aedan Chung",
        "Loren Pope",
        "Jackie Maggiore",
        "Priya Kay",
        "Shibo Jhakra",
        "Joshua Wang",
        "Mary Maynez",
        "Taylor Phuong",
        "Andrea Tobin",
        "Maja Tacchetti",
        "Kevin Trebacz",
        "Yash Robinson",
        "Sebastian Katariya",
        "Paige Riedel",
        "Kefan Bailey",
        "Nimesh Xiao",
        "Lora Ghelani",
        "Ambrose Aroyo",
        "Neil Slone",
        "Xuehan Houlsby",
        "Zhen Xiong",
        "Elena Yang",
        "Jonas Gribovskaya",
        "Mateo Adler",
        "Lisa Wirth",
        "Music Lee",
        "Thais Li",
        "Jay Kagohara",
        "Sophie Pavagadhi",
        "Anna Bridgers",
        "Sanjay Bortsova",
        "Zafarali Ghemawat",
        "Tianqi Ahmed",
        "Richard Liu",
        "Vijay Powell",
        "Mariko Bolina",
        "Polina Iinuma",
        "James Zablotskaia",
        "Besley",
        "Da-Woon",
        "Timothy Chung",
        "Ramona Dozat",
        "Xiance Comanescu",
        "Jeremy Si",
        "Guolong Greer",
        "Martin Su",
        "Raphaël Polacek",
        "Simon Lopez Kaufman",
        "Hexiang Tokumine",
        "Elena Hu",
        "Yingjie Buchatskaya",
        "Mohamed Miao",
        "Aditya Elhawaty",
        "Nenad Siddhant",
        "Jinwei Tomasev",
        "Christina Xing",
        "Helen Greer",
        "Shereen Miller",
        "Aurko Ashraf",
        "Zizhao Roy",
        "Ada Zhang",
        "Angelos Ma",
        "Milos Filos",
        "Rory Besta",
        "Ted Blevins",
        "Chih-Kuan Klimenko",
        "Soravit Yeh",
        "Jiaqi Changpinyo",
        "Oscar Mu",
        "Mantas Chang",
        "Carrie Pajarskas",
        "Vered Muir",
        "Charline Le Cohen",
        "Krishna Lan",
        "Amit Haridasan",
        "Steven Marathe",
        "Sholto Hansen",
        "Rajkumar Douglas",
        "Mingqiu Samuel",
        "Sophia Wang",
        "Chang Austin",
        "Jiepu Lan",
        "Justin Jiang",
        "Jaime Alonso Chiu",
        "Lars Lowe Lorenzo",
        "Sébastien Sjösund",
        "Zach Cevey",
        "Thi Gleicher",
        "Anudhyan Avrahami",
        "Hansa Boral",
        "Vittorio Srinivasan",
        "Rhys Selo",
        "Konstantinos May",
        "Léonard Aisopos",
        "Hussenot",
        "Baldini Livio",
        "Kate Soares",
        "Michael B Baumli",
        "Adrià Chang",
        "Ben Recasens",
        "Alexander Caine",
        "Filip Pritzel",
        "Fabio Pavetic",
        "Anita Pardo",
        "Justin Gergely",
        "Vinay Frye",
        "Dan Ramasesh",
        "Kartikeya Horgan",
        "Nora Badola",
        "Subhrajit Kassner",
        "Ethan Roy",
        "Víctor Dyer",
        "Alex Campos",
        "Yunhao Tomala",
        "Dalia El Tang",
        "Elspeth Badawy",
        "Basil White",
        "Oran Mustafa",
        "Abhishek Lang",
        "Sharad Jindal",
        "Zhitao Vikram",
        "Sergi Gong",
        "Ross Caelles",
        "Gregory Hemsley",
        "Fangxiaoyu Thornton",
        "Wojciech Feng",
        "Ce Stokowiec",
        "Phoebe Zheng",
        "C Thacker",
        "Zhishuai ¸aglar Ünlü",
        "Mohammad Zhang",
        "James Saleh",
        "Max Svensson",
        "Piyush Bileschi",
        "Ankesh Patil",
        "Roman Anand",
        "Katerina Ring",
        "Arpi Tsihlas",
        "Marco Vezer",
        "Toby Selvi",
        "Mikel Shevlane",
        "Tom Rodriguez",
        "Samira Kwiatkowski",
        "Keran Daruki",
        "Allan Rong",
        "Nicholas Dafoe",
        "Keren Fitzgerald",
        "Mina Gu-Lemberg",
        "Lisa Anne Khan",
        "Marie Hendricks",
        "Vladimir Pellat",
        "James Feinberg",
        "Tara Cobon-Kerr",
        "Maribeth Sainath",
        "Sayed Rauh",
        "Richard Hadi Hashemi",
        "Yana Ives",
        "Yaguang Hasson",
        "Eric Li",
        "Yuan Noland",
        "Nathan Cao",
        "Le Byrd",
        "Qingze Hou",
        "Thibault Wang",
        "Michela Sottiaux",
        "Jean-Baptiste Paganini",
        "Alexandre Lespiau",
        "Samer Moufarek",
        "Kaushik Hassan",
        "Joost Shivakumar",
        "Amol Van Amersfoort",
        "Pratik Mandhane",
        "Anirudh Joshi",
        "Matthew Goyal",
        "Andrew Tung",
        "Hannah Brock",
        "Vedant Sheahan",
        "Cheng Misra",
        "Nemanja Li",
        "Mostafa Rakićević",
        "Fangyu Dehghani",
        "Sid Liu",
        "Junhyuk Mittal",
        "Seb Oh",
        "Eren Noury",
        "Fantine Sezener",
        "Matthew Huot",
        "Nicola De Lamm",
        "Charlie Cao",
        "Gamaleldin Chen",
        "Ed Elsayed",
        "Mahdis Chi",
        "Ian Mahdieh",
        "Nan Tenney",
        "Ivan Hua",
        "Patrick Petrychenko",
        "Dylan Kane",
        "Rishub Scandinaro",
        "Jonathan Jain",
        "Romina Uesato",
        "Adam Datta",
        "Oskar Sadovsky",
        "Dominik Bunyan",
        "Shimu Rabiej",
        "John Wu",
        "Gautam Zhang",
        "Edouard Vasudevan",
        "Mahmoud Leurent",
        "Ionut Alnahlawi",
        "Nan Georgescu",
        "Ivy Wei",
        "Betty Zheng",
        "Pam G Chan",
        "Piotr Rabinovitch",
        "Ye Stanczyk",
        "David Zhang",
        "Subhajit Steiner",
        "Michael Naskar",
        "Matthew Azzam",
        "Adam Johnson",
        "Chung-Cheng Paszke",
        "Jaume Chiu",
        "Afroz Sanchez Elias",
        "Faizan Mohiuddin",
        "Jin Muhammad",
        "Andrew Miao",
        "Nino Lee",
        "Sahitya Vieillard",
        "Jane Potluri",
        "Elnaz Park",
        "Jiageng Davoodi",
        "Jeff Zhang",
        "Drew Stanway",
        "Abhijit Garmon",
        "Zhe Karmarkar",
        "Jong Dong",
        "Aviral Lee",
        "Luowei Kumar",
        "Jonathan Zhou",
        "William Evens",
        "Zhe Isaac",
        "Johnson Chen",
        "Anselm Jia",
        "Zhenkai Levskaya",
        "Chris Zhu",
        "Peter Gorgolewski",
        "Yu Grabowski",
        "Alberto Mao",
        "Kaisheng Magni",
        "Javier Yao",
        "Norman Snaider",
        "Paul Casagrande",
        "Evan Suganthan",
        "Geoffrey Palmer",
        "Edward Irving",
        "Manaal Loper",
        "Isha Faruqui",
        "Nanxin Arkatkar",
        "Izhak Chen",
        "Michael Shafran",
        "Alfonso Fink",
        "Irene Castaño",
        "Wooyeol Giannoumis",
        "Mikołaj Kim",
        "Ashwin Rybiński",
        "Jennifer Sreevatsa",
        "David Prendki",
        "Adrian Soergel",
        "Willi Goedeckemeyer",
        "Mohsen Gierke",
        "Meenu Jafari",
        "Jeremy Gaba",
        "Diana Gage Wiesner",
        "Yawen Wright",
        "Harsha Wei",
        "Yana Vashisht",
        "Jay Kulizhskaya",
        "Maigo Hoover",
        "Lu Le",
        "Chimezie Li",
        "Lu Iwuanyanwu",
        "Kevin Liu",
        "Andrey Ramirez",
        "Albert Khorlin",
        "Cui",
        "Lin Tian",
        "Marin Georgiev",
        "Marcus Wu",
        "Ricardo Aguilar",
        "Keith Pallo",
        "Abhishek Chakladar",
        "Alena Repina",
        "Xihui Wu",
        "Tom Van Der Weide",
        "Priya Ponnapalli",
        "Caroline Kaplan",
        "Jiri Simsa",
        "Shuangfeng Li",
        "Olivier Dousse",
        "Fan Yang",
        "Jeff Piper",
        "Nathan Ie",
        "Minnie Lui",
        "Rama Pasumarthi",
        "Nathan Lintz",
        "Anitha Vijayakumar",
        "Lam Nguyen Thiet",
        "Daniel Andor",
        "Pedro Valenzuela",
        "Cosmin Paduraru",
        "Daiyi Peng",
        "Katherine Lee",
        "Shuyuan Zhang",
        "Somer Greene",
        "Dung Duc",
        "Paula Nguyen",
        "Sarmishta Kurylowicz",
        "Sebastian Velury",
        "Cassidy Krause",
        "Lucas Hardin",
        "Lili Dixon",
        "Kiam Janzer",
        "Ziqiang Choo",
        "Biao Feng",
        "Achintya Zhang",
        "Tejasi Singhal",
        "Mingyang Latkar",
        "Quoc Zhang",
        "Elena Le",
        "Dayou Allica Abellan",
        "Dan Du",
        "Natasha Mckinnon",
        "Tolga Antropova",
        "Orgad Bolukbasi",
        "David Keller",
        "Daniel Reid",
        "Maria Finchelstein",
        "Remi Abi Raad",
        "Peter Crocker",
        "Robert Hawkins",
        "Colin Dadashi",
        "Sid Gaffney",
        "Ken Lall",
        "Egor Franko",
        "Anna Filonov",
        "Rémi Bulanova",
        "Vikas Leblond",
        "Shirley Yadav",
        "Harry Chung",
        "Luis C Askham",
        "Kelvin Cobo",
        "Felix Xu",
        "Jun Fischer",
        "Christina Xu",
        "Chris Sorokin",
        "Chu-Cheng Alberti",
        "Colin Lin",
        "Hao Evans",
        "Alek Zhou",
        "Hannah Dimitriev",
        "Dylan Forbes",
        "Zora Banarse",
        "Jeremiah Tung",
        "Mark Liu",
        "Jing Omernick",
        "Sabaer Li",
        "John Fatehi",
        "Omar Wieting",
        "Benigno Ajmeri",
        "Tao Uria",
        "Yeongil Zhu",
        "Laura Ko",
        "Amélie Knight",
        "Ning Héliou",
        "Shane Niu",
        "Chenxi Gu",
        "Dustin Pang",
        "Yeqing Tran",
        "Nir Li",
        "Ariel Levine",
        "Norbert Stolovich",
        "Rebeca Kalb",
        "Sonam Santamaria-Fernandez",
        "Wenny Goenka",
        "Robin Yustalim",
        "Ali Strudel",
        "Balaji Elqursh",
        "Charlie Lakshminarayanan",
        "Shyam Deck",
        "Hyo Upadhyay",
        "Mike Lee",
        "Zonglin Dusenberry",
        "Xuezhi Li",
        "Kyle Wang",
        "Raphael Levin",
        "Dan Hoffmann",
        "Olivier Holtmann-Rice",
        "Summer Bachem",
        "Sho Yue",
        "Eric Arora",
        "Daniil Malmi",
        "Qijun Mirylenka",
        "Christy Tan",
        "Soheil Koh",
        "Siim Hassas Yeganeh",
        "Steven Põder",
        "Francesco Zheng",
        "Mukarram Pongetti",
        "Yanhua Tariq",
        "Lucian Sun",
        "Mojtaba Ionita",
        "Pouya Seyedhosseini",
        "Ragha Tafti",
        "Zhiyu Kotikalapudi",
        "Anmol Liu",
        "Jasmine Gulati",
        "Xinyu Liu",
        "Bart Ye",
        "Lily Chrzaszcz",
        "Nikhil Wang",
        "Tianrun Sethi",
        "Ben Li",
        "Shreya Brown",
        "Wei Singh",
        "Aaron Fan",
        "Joe Parisi",
        "Chenkai Stanton",
        "Vinod Kuang",
        "Christopher A Koverkathu",
        "Yunjie Choquette-Choo",
        "Li",
        "Abe Lu",
        "Yi Ittycheriah",
        "Yao Sun",
        "Stephan Zhao",
        "Pandu Lee",
        "Doug Nayak",
        "Manish Reddy Fritz",
        "John Vuyyuru",
        "Nidhi Aslanides",
        "Martin Vyas",
        "Xiao Wicke",
        "James Ma ; Cate",
        "Keyvan Manyika",
        "Yelin Amiri",
        "Xi Kim",
        "Kai Xiong",
        "Florian Kang",
        "Nilesh Luisier",
        "David Tripuraneni",
        "Mandy Madras",
        "Austin Guo",
        "Oliver Waters",
        "Joshua Wang",
        "Jason Ainslie",
        "Han Baldridge",
        "Garima Zhang",
        "Jakob Pruthi",
        "Feng Bauer",
        "Riham Yang",
        "Jason Mansour",
        "Yang Gelman",
        "George Xu",
        "Ji Polovets",
        "Honglong Liu",
        "Warren Cai",
        "Xianghai Chen",
        "Emily Sheng",
        "Sherjil Xue",
        "Adams Ozair",
        "Christof Yu",
        "Xiaowei Angermueller",
        "Weiren Li",
        "Julia Wang",
        "Emmanouil Wiesinger",
        "Yuan Koukoumidis",
        "Anand Tian",
        "Madhu Iyer",
        "Mark Gurumurthy",
        "Parashar Goldenson",
        "Shah",
        "Hongkun Blake",
        "Anthony Yu",
        "Jennimaria Urbanowicz",
        "Chrisantha Palomaki",
        "Fernando ; Dinghua",
        "Ginger Li",
        "Blake Perng",
        "Parker Hechtman",
        "Milad Schuh",
        "Mia Nasr",
        "Kieran Chen",
        "Vladimir Milan",
        "Trevor Mikulik",
        "Juliana Strohman",
        "Franco"
      ],
      "year": "2023",
      "venue": "Andreas Fidjeland, Salvatore Scellato",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra",
        "Igor Molybog",
        "Yixin Nie",
        "Andrew Poulton",
        "Jeremy Reizenstein",
        "Rashi Rungta",
        "Kalyan Saladi",
        "Alan Schelten",
        "Ruan Silva"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting elicits reasoning in large language models",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "Jules White",
        "Quchen Fu",
        "Sam Hays",
        "Michael Sandborn",
        "Carlos Olea",
        "Henry Gilbert",
        "Ashraf Elnashar",
        "Jesse Spencer-Smith",
        "Douglas C Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Least-to-most prompting enables complex reasoning in large language models",
      "authors": [
        "Denny Zhou",
        "Nathanael Schärli",
        "Le Hou",
        "Jason Wei",
        "Nathan Scales",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Claire Cui",
        "Olivier Bousquet",
        "Quoc Le",
        "Ed Chi"
      ],
      "year": "2023",
      "venue": "Least-to-most prompting enables complex reasoning in large language models",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Principled Instructions Are All You Need For Questioning Llama-1/2, Gpt-3.5/4",
      "text": "Sondos Mahmoud Bsharat\\({}^{*}\\), Aidar Myrzakhan\\({}^{*}\\), Zhiqiang Shen\\({}^{*}\\) \\({}^{*}\\)joint first author & equal contribution VILA Lab, Mohamed bin Zayed University of AI"
    },
    {
      "title": "Abstract",
      "text": "This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at [https://github.com/VILA-Lab/ATLAS](https://github.com/VILA-Lab/ATLAS)."
    },
    {
      "title": "1 Introduction",
      "text": "\\(\\zeta\\in\\zeta\\) _Prompt engineering is the art of communicating with a generative large language model._ \\(9\\) \\(9\\) Large language models (LLMs) like ChatGPT [13] have shown impressive abilities in various domains and tasks, such as answering questions [7], mathematical reasoning [5], code generating [11, 9], etc. However, their application and usage, especially on designing the optimal instructions or prompts, can sometimes be unclear to the common users. In this work, we aim to reveal these mysteries for developers or general users when inquiring and interacting with LLMs, and further enhance the quality of the responses from the pretrained LLMs by simply curating better prompts. Given that directly fine-tuning LLMs for particular tasks tends to be impractical or unattainable for the majority of users and developers due to inefficiency, the research community has turned its attention to the optimization of prompts. The technique of prompt engineering, which entails the crafting of precise, task-specific instructions innatural language, either manually or through automated means, and the careful selection of representative examples for inclusion in the prompt, has become a central area of investigation for LLMs. Despite these dedicated efforts, the task of reliably guiding LLMs to produce specific responses and making full use of the capability of pretrained LLMs continues to pose a considerable challenge. In this work, we present comprehensive principled instructions to improve the quality of prompts for LLMs. Specifically, we investigate a wide range of behaviors when feeding into different types and formulations of prompts, such as integrating the intended audience in the prompt, e.g., add \"_the audience is an expert in the field_\", or \"_the audience is the 5-year-old child_\", as well as other multiple aspects of the characteristics of LLMs. Our findings indicate that larger models possess a considerable capacity for simulation. The more precise the task or directive provided, the more effectively the model performs, aligning its responses more closely with our expectations. This suggests that LLMs do not merely memorize training data but are capable of adapting this information to suit varying prompts, even when the core inquiries remain constant. Therefore, it proves beneficial to assign a specific role to LLMs as a means to elicit outputs that better match our intended results. Figure 1: Illustration example of prompts and corresponding responses before and after applying principles. Left is the original promotes and their responses from GPT-4, right is the principled prompts and the associated responses. Principles 5 and 6 are utilized. We elaborate the principled instructions for LLM prompting, provide further motivation, and detail several specific designing principles in Section 3. In Section 4 we show experimentally that the proposed principles can produce higher quality, more concise, factual and less complicated or intricate responses than standard prompts for LLMs. Specifically, with the manually-designed ATLAS benchmark, which includes multiple questions for each principle, the specialized prompts we introduced have enhanced both the quality and accuracy of the LLM responses by an average of 57.7% and 36.4%, respectively, when applied to GPT-4. Furthermore, the improvements are more pronounced with the increase in model size, for example, the performance gains when moving from LLaMA-2-7B to GPT-4 exceed 20%."
    },
    {
      "title": "2 Related Work",
      "text": "**Large Language Models.** The evolution of large language models (LLMs) has been pivotal in advancing natural language processing (NLP). This section reviews key developments in LLMs, providing a foundation for the current study. Beginning with Google's BERT [3] revolutionized context understanding through its bidirectional training approach, while T5 [18] further advanced the field by unifying various NLP tasks into a single framework. Concurrently, GPT-1 [15] introduced a pioneering model leveraging transformer architectures for unsupervised learning. This was followed by its successor, GPT-2 [16] which significantly expanded its parameter count to 1.5 billion, demonstrating remarkable capabilities in text generation. Then, GPT-3 [2] marked a substantial leap in scale and capability, boasting 175 billion parameters and showcasing proficiency across a wide range of language tasks. Regarding other recently proposed LLMs, Gopher [17], not only advanced language processing capabilities with its 280-billion parameter model but also brought ethical considerations to the forefront. Meta's LLaMA series [22, 23] highlighted the importance of efficiency, suggesting powerful performance with fewer resources, a concept also advocated by Chinchilla [4], which proposed that smaller, optimally trained models could achieve exceptional results. The latest in this series of innovations is Mistral [6] excels in efficiency and performance, outperforming larger models. The most recent milestones in this trajectory are OpenAI's GPT-4 [13] and Google's Gemini family [21]. They represent another significant advancement in the field with their enhanced understanding and generative capabilities, setting new benchmarks for the application of LLMs in various domains. **Prompting.** Prompting [20, 12, 25, 27, 14], as a distinct aspect of interacting with LLMs and its simplicity with no need to fine-tune the model, has evolved into a nuanced field of study, highlighting the intricate relationship between user inputs and LLM responses. Early explorations, such as those by [20], delved into how varying prompt designs could dramatically influence the performance and outputs of language models, marking the birth of _prompt engineering_. This area rapidly expanded, uncovering the critical role of prompts in few-shot and zero-shot learning scenarios, exemplified by [2] work with GPT-3, where strategically crafted prompts enabled the model to perform tasks with minimal prior examples. Beyond mere task instruction, recent studies have shifted towards understanding the semantic and contextual nuances in prompts, examining how subtle changes can lead to significantly different responses from the LLM. _Ask-Me-Anything_[1] prompting introduced focusing on using multiple imperfect prompts and aggregating them to improve model performance, particularly in question-answering formats. Another one, _Chain-of-Thought_ method [24], where the model generates a series of intermediate reasoning steps to improve performance on complex tasks. Also, _least-to-most prompting_[27] a novel strategy to break down complex problems into simpler subproblems, significantly enhancing the model's capability to tackle more challenging problems than those presented in the prompts. The effectiveness of explanation was explored [8], finding that explanations can enhance LLM's learning capabilities on complex tasks. Furthermore, a catalog of prompt engineering techniques was examined with ChatGPT [25], emphasizing the importance of prompt engineering in enhancing LLM applications in software development and education. It also highlighted that effective prompt design is crucial in improving LLM performance, particularly in coding practices and learning experiences. Lastly, _Directional Stimulus Prompting_[12] presents a novel framework that uses a tunable policy model to generate auxiliary prompts, guiding LLMs towards specific desired outcomes. This diversity in prompting strategies underscores the rapidly evolving landscape of LLMs, offering multiple directions to harness their capabilities more effectively."
    },
    {
      "title": "3 Principles",
      "text": ""
    },
    {
      "title": "Motivation",
      "text": "Since the quality of the responses generated by a pretrained and aligned LLM is directly relevant to the quality of the prompts or instructions provided by the users, it is essential to craft prompts that the LLM can comprehend and respond to effectively. The prompts delivered to an LLM serve as a way to program the interaction between a user and the LLM, enhancing its ability to address a diverse range of tasks. The primary focus of this work is on the methodology of crafting and customizing prompts to enhance output quality. This necessitates a comprehensive grasp of the functioning and behaviors of LLMs, their underlying mechanisms, and the principles governing their responses. In this work, we achieve this goal through elaborating 26 principles for comprehensive prompts in different scenarios and circumstances, examples are shown in Fig. 1."
    },
    {
      "title": "Overview",
      "text": "The overview of principles is presented in Table 1. According to their unique nature, we group them into five categories as in Table 2: (1) Prompt Structure and Clarity, e.g., _integrate the intended audience in the prompt such as the audience is an expert in the field_; (2) Specificity and Information, e.g., _Add to your prompt the following phrase \"Ensure that your answer is unbiased and does not rely on stereotypes.\"_; (3) User Interaction and Engagement, e.g., _Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output \"From now on, I would like you to ask me questions to...\"_. (4) Content and Language Style, e.g., _No need to be polite with LLM so there is no need to add phrases like \"please\", \"if you don't mind\", \"thank you\", \"I would like to\", etc., and get straight to the point_; (5) Complex Tasks and Coding Prompts, e.g., _Break down complex tasks into a sequence of simpler prompts in an interactive conversation._ \\begin{table} \\begin{tabular}{|c|l|l|} \\hline **\\#Principle** & \\multicolumn{2}{c|}{**Prompt Principle for Instructions**} \\\\ \\hline 1 & \\begin{tabular}{l} If you prefer more concise answers, no need to be polite with LLM so there is no need to add phrases like \\\\ “please”, “if you don’t mind”, “thank you”, “I would like to”, etc., and get straight to the point. \\\\ \\end{tabular} \\\\ \\hline 2 & \\begin{tabular}{l} Integrate the intended audience in the prompt, e.g., the audience is an expert in the field. \\\\ \\end{tabular} \\\\ \\hline 3 & \\begin{tabular}{l} Break down complex tasks into a sequence of simpler prompts in an interactive conversation. \\\\ \\end{tabular} \\\\ \\hline 4 & \\begin{tabular}{l} Employ affirmative directives such as ‘\\(do\\),’ while steering clear of negative language like ‘\\(don\\)’\\(\\tau\\). \\\\ \\end{tabular} \\\\ \\hline 5 & \\begin{tabular}{l} When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the \\\\ following prompts: \\\\ \\end{tabular} \\\\ \\end{tabular} \\end{table} Table 1: Overview of 26 randomly ordered prompt principles. \\begin{table} \\begin{tabular}{|p{142.3pt}|p{142.3pt}|p{142.3pt}|} \\hline **Category** & **Principles** & **\\#Principle** \\\\ \\hline & Integrate the intended audience in the prompt. & 2 \\\\ & Employ affirmative directives such as ‘do’ while steering clear of negative language like ‘don’t’. & 4 \\\\ & Use Leading words like writing “think step by step.” & 12 \\\\ Prompt Structure and Clarity & Use output primers, which involve concluding your prompt with the beginning of the desired output. & 20 \\\\ & by ending your prompt with the start of the anticipated response. & 17 \\\\ & Use Delimiters. & 8 \\\\ & When formatting your prompt, start with ‘##Instruction###’, followed by either ‘##Example###” or ‘##Question##’ if relevant. Subsequently, present your content. Use one or more line breaks to separate instructions, examples, questions, context, and input data. & 8 \\\\ \\hline & Implement example-driven prompting (Use few-shot prompting). & 7 \\\\ & When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: & 7 \\\\ & \\(\\circ\\) Explain [insert specific topic] in simple terms. & 5 \\\\ & \\(\\circ\\) Explain to me like I’m 11 years old. & 5 \\\\ & \\(\\circ\\) Explain to me as if I’m a begininer in [ field ]. & 5 \\\\ & \\(\\circ\\) “Write the [essay/text/paragraph] using simple English like you’re explaining something to a 5-year-old.” & 13 \\\\ & Add to your prompt the following phrase “Ensure that your answer is unbiased and avoids relying on stereotypes.” & 14 \\\\ & To write any text intended to be similar to a provided sample, include specific instructions: & 26 \\\\ Specificity and Information & \\(\\circ\\) “Use the same language based on the provided paragraph |/title/text/essay/answer|.” & 27 \\\\ & When you want to initiate or continue a text using specific works, phrases, or sentences, utilize the provided prompt structure: & 5 \\\\ & \\(\\circ\\) I’m providing you with the beginning [song lyrics/story/paragraph/essay...]: [Insert lyrics/words/sentence]. & 28 \\\\ & Finish it based on the words provided. Keep the flow consistent. & 29 \\\\ & Clearly state the model’s requirements that the model must follow in order to produce content, in form of the keywords, regulations, hint, or instructions. & 29 \\\\ & To inquire about a specific topic or idea and test your understanding g, you can use the following phrase [16]: & 16 \\\\ & \\(\\circ\\) “Teach me the [Any theorem/topic/rule name] and include a test at the end, and let me know if my answers are correct after I respond, without providing the answers beforehand.” & 17 \\\\ & To write an essay/text/paragraph/article or any type of text that should be detailed: & 20 \\\\ & \\(\\circ\\) “Write a detailed [essay/text/paragraph] for me on [topic] in detail by adding all the information necessary.” & 21 \\\\ & Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information to provide the needed output & 21 \\\\ User Interaction and Engagement & \\(\\circ\\) “From now on, I would like you to ask me questions to...” & 22 \\\\ & To write an essay /text / paragraph/ article or any type of text that should be detailed: “Write a detailed [essay/text/-paragraph] for me on [topic] in detail by adding all the necessary information.” & 23 \\\\ & To correct/change specific text without changing its style: “Try to revise every paragraph sent by users. You should only improve the user’s grammar and vocabulary and make sure it sounds natural. You should maintain the original writing style, ensuring that a formal paragraph remains formal.” & 24 \\\\ & Incorporate the following phrases: “Your task is” and “You MUST.” & 25 \\\\ & Incorporate the following phrases: “You will be penalized.” & 26 \\\\ Content and Language Style & Assign a role to the language model. & 27 \\\\ & Use the phrase “Answer a question given in natural language form” in your prompts. & 28 \\\\ & No need to be polite with LLM so there is no need to add phrases like “please”, “if you don’t minf”, “thank you”, “I would like to”, etc., and get straight to the point. & 29 \\\\ & Repeat a specific word or phrase multiple times within a prompt. & 29 \\\\ & Add “Tm going to tip 5xxx for a better solution!” & 29 \\\\ \\hline & Break down complex tasks into a sequence of simpler prompts in an interactive conversation. & 29 \\\\ & When you have a complex coding prompt that may be in different files: & 28 \\\\ Complex Tasks and Coding Prompts & \\(\\circ\\) “From now and on whenever you generate code that spans more than one file, generate a [programming language ] script that can be run to automatically create the specified files or make changes to existing files to insert the generated code. [your question].” & 29 \\\\ & Combine Chain-of-thought (Cot) with few-shot prompts. & 20 \\\\ \\hline \\end{tabular} \\end{table} Table 2: Prompt principle categories."
    },
    {
      "title": "Design Principles",
      "text": "In this study, a number of guiding principles are established for formulating prompts and instructions to elicit high-quality responses from pre-trained large language models: **Conciseness and Clarity:** Generally, overly verbose or ambiguous prompts can confuse the model or lead to irrelevant responses. Thus, the prompt should be concise, avoiding unnecessary information that does not contribute to the task while being specific enough to guide the model. This is the basic principle guidance for prompt engineering. **Contextual Relevance:** The prompt must provide relevant context that helps the model understand the background and domain of the task. Including keywords, domain-specific terminology, or situational descriptions can anchor the model's responses in the correct context. We highlight this design philosophy in our presented principles. **Task Alignment:** The prompt should be closely aligned with the task at hand, using language and structure that clearly indicate the nature of the task to the model. This may involve phrasing the prompt as a question, a command, or a fill-in-the-blank statement that fits the task's expected input and output format. **Example Demonstrations:** For more complex tasks, including examples within the prompt can demonstrate the desired format or type of response. This often involves showing input-output pairs, especially in \"few-shot\" or \"zero-shot\" learning scenarios. **Avoiding Bias:** Prompts should be designed to minimize the activation of biases inherent in the model due to its training data. Use neutral language and be mindful of potential ethical implications, especially for sensitive topics. **Incremental Prompting:** For tasks that require a sequence of steps, prompts can be structured to guide the model through the process incrementally. Break down the task into a series of prompts that build upon each other, guiding the model step-by-step. Also, prompts should be adjustable based on the performance of the model and iterative feedback, i.e., it needs to be well prepared to refine the prompt based on initial outputs and model behaviors. Moreover, prompts should be adjustable based on the performance and response of the model, and iterative human feedback and preference. Finally, more advanced prompts may incorporate programming-like logic to achieve complex tasks. For instance, use of conditional statements, logical operators, or even pseudo-code within the prompt to guide the model's reasoning process. The design of prompts is an evolving field, especially as LLMs become more sophisticated. As researchers continue to explore the limits of what can be achieved through prompt engineering, these principles will likely be refined and expanded."
    },
    {
      "title": "4 Experiments",
      "text": ""
    },
    {
      "title": "Setup And Implementation Details",
      "text": "All our evaluation is performed on ATLAS [19], a manually crafted benchmark for principled prompt evaluation. It contains a standard subset featuring questions across various domains, along with a challenging subset dedicated to reasoning and other complex tasks. In our evaluation, we utilize a single response for each question. For each principle and the challenging subset, it contains 20 human-selected questions with and without the principled prompts. Similar to [10, 26], we compare each pairof responses from the same instructions with and without principles, and evaluate the various scales of LLM outputs by human evaluation."
    },
    {
      "title": "Models And Metrics",
      "text": "We use instruction finetuned LLaMA-1-{7, 13}, LLaMA-2-{7, 13}, off-the-shelf LLaMA-2-70B-chat, GPT-3.5 (ChatGPT) and GPT-4 as our base models. We group these models into different scales: small-scale (7B models), medium-scale (13B) and large-scale (70B, GPT-3.5/4). We evaluate these models in two settings: **Boosting** and **Correctness**. They are employed together to provide a comprehensive understanding of a model's performance. For correctness, we specifically utilize complex reasoning tasks to accurately gauge the precision of the models' outputs, contrasting with our evaluation for boosting, where simpler tasks are employed to effectively measure quality improvements. This distinction ensures a better reflection of the true capabilities for different scales of models and the effect of the principles for prompts. Since we use questions that typically involve complex reasoning tasks for correctness, some principles are not applicable including principles 14, 15, 21, 22, 23. For instance, \"_Suppose \\(a\\) and \\(b\\) are positive real numbers with \\(a>b\\) and \\(ab=8\\). Find the minimum value of \\(\\frac{a^{2}+b^{2}}{a-b}\\)._\" * **Boosting.** The result of _boosting_ refers to the percentage increase in response quality across a set of questions when the proposed principles are applied. We assess the enhancement in the quality of responses from different LLMs via human evaluation after applying the outlined prompt principles. The original, unmodified prompts act as a baseline for measuring this enhancement. Demonstrating _boosting_ confirms that a model's performance has improved due to the use of structured, principled instructions, as shown in Fig. 2. Figure 2: Boosting example of LLM response after using the principle 13 on prompts. * **Correctness.** The concept of _correctness_ refers to the precision of the model's outputs or responses, ensuring they are accurate, relevant, and devoid of errors. We consider both absolute and relative correctness accuracy. Human evaluators are utilized to gauge this aspect, which is crucial for verifying the model's accuracy. Correctness is a testament to the model's ability to generate outputs that align with the expected standards of accuracy, as shown in Fig. 3."
    },
    {
      "title": "Results",
      "text": ""
    },
    {
      "title": "4.3.1 Results On Small, Medium And Large-Scale Llms",
      "text": "**Boosting.** The results of improvement after employing the introduced principles are shown in Fig. 4. Generally, all principles can bring a significant improvement on the three scales of LLMs. In the cases of principles 2, 5, 15, 16, 25 and 26, the large-scale models get the most improvement by the principled prompts. Particularly, for principle 14, as shown in Fig. 4, it has improved all questions it is applied to. **Correctness.** (1) Absolute accuracy: we examine the absolute performance when employing the principles on various scales of models. Generally, these models achieve 20%\\(\\sim\\)40% accuracy on the averaged performance, as shown in Fig. 5. In particular, for small and medium scale models, the accuracy can basically reach between 10% and 40%, and for large models, the accuracy can reach more than 40%. (2) Relative accuracy: Figure 3: Correctness improvement example of LLM response after using the introduced principle 7 on prompts. Fig. 6 illustrates that applying the principles generally leads to a performance increase of over 10% across different models on average. For larger models, this enhancement can surpass 20%."
    },
    {
      "title": "4.3.2 Results On Individual Llms",
      "text": "**Boosting.** Fig. 7 illustrates the improvement of response quality on individual model and principle after using the revised prompts. On average, there is a stable 50% improvement across different LLMs. Fig. 10 further provides the detailed results of improvement for each principle with different LLMs. **Correctness.** Fig. 8 illustrates the absolute correctness accuracy and Fig. 9 shows the relative enhancements in accuracy across different sizes of LLMs. From LLaMA-2-13B, Figure 4: Boosting of LLM response quality after employing the introduced principles on prompts. _small-scale_ indicates the 7B models, _medium-scale_ indicates the 13B models and _large-scale_ indicates the 70B and GPT-3.5/4 models. Figure 5: Absolute correctness of LLM response quality after employing the introduced principles on prompts. _small-scale_ indicates the 7B models, _medium-scale_ indicates the 13B models and _large-scale_ indicates the 70B and GPT-3.5/4 models. [MISSING_PAGE_FAIL:11] Figure 8: Absolute correctness score on the ATLAS dataset. Figure 7: Boosting score across various LLMs on the ATLAS dataset. Figure 10: Illustration of heatmap for LLMs boosting percentages. Figure 9: Relative correctness improvement score on the ATLAS dataset. Figure 11: Illustration of heatmap for absolute correctness percentages. Figure 12: Illustration of heatmap for relative correctness improvement percentages. Figure 13: Correctness improvement on small-scale LLaMA-2-7B model after using the introduced principle on prompts. **Human (without principle)** Luna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag? **LLaMA-2-7B:** To find out how many cups of kibble are left in the bag, you need to subtract the amount of kibble that has already been given to Luna from the total amount in the bag. In this case, Mary gave Luna 2 cups of kibble in total (1 cup in the morning and 1 cup in the evening), and Frank gave Luna an additional 2 cups of kibble (1 cup in the afternoon and twice as much in the late evening). So, the total amount of kibble given to Luna was 4 cups. The 12-cup bag of kibble was meant to last Luna for how many days? Let's say the 12-cup bag was meant to last Luna for n days. Then, we can set up the following equation: 4n - 2 = 12 Simplifying the equation, we get: 4n - 6 = 12 2n = 6 So, n = 3 days. Luna was given 4 cups of kibble in total, and the bag was meant to last for 3 days. After 3 days, Mary will find 12 cups of kibble remaining in the bag. **Human (with principle)** Luna, the poodle, is supposed to eat 2 cups of kibble every day. But Luna's master, Mary, and her husband, Frank, sometimes feed Luna too much kibble. One day, starting with a new, 12-cup bag of kibble, Mary gave Luna 1 cup of kibble in the morning and 1 cup of kibble in the evening, But on the same day, Frank also gave Luna 1 cup of kibble in the afternoon and twice as much in the late evening as he had given Luna in the afternoon. The next morning, how many cups of kibble will Mary find remaining in the bag? **Many find remaining in the bag?** **hint: Calculate the remaining cups of kibble will Mary find in the bag** **LLaMA-2-7B:** To find the remaining cups of kibble in the bag, we need to calculate the total amount of kibble that Luna actually received, and then subtract the amount that Mary gave her. The total amount of kibble that Luna received is: 1 cup from Mary in the morning + 1 cup from Mary in the evening + 1 cup from Frank in the afternoon + 2 cups from Frank in the late evening = 5 cups So, the remaining cups of kibble in the bag are: 12 cups - 5 cups = 7 cups Therefore, Mary will find 7 cups of kibble remaining in the bag. Figure 14: Correctness improvement on small-scale LLaMA-2-7B model after using the introduced principle on prompts. [MISSING_PAGE_EMPTY:17] **LLaMA-2-13B:**: _Mary gave Luna 1 cup of kibble in the afternoon and 2 times 1 cup in the late evening, which is a total of 3 cups. So, a total of 5 cups of kibble were given to Luna that day. If there was a 12-cup bag of kibble, then there are 7 cups remaining in the bag the next morning. Figure 16: Correctness improvement on medium-scale LLaMA-2-13B model after using the introduced principle on prompts. Limitations and Discussion While the proposed 26 principles are designed to improve and enhance the quality of responses of LLMs across a diverse array of queries, the effectiveness of these principles may diminish when dealing with questions that are very complex or highly specialized. This limitation can mainly depend on the reasoning capabilities and training of each model. To address these variations, we have tested the principles across different scales to measure their effectiveness comprehensively. Despite our efforts in evaluating these principles on seven distinct language models, it is crucial to acknowledge that models with architectures different from those tested might respond in different ways to these principles. Additionally, our assessment of improvement and correctness percentages was based on a limited selection of questions. Expanding the question set in future research could yield more generalized findings and offer deeper insights into the applicability of each principle. Furthermore, the criteria and results may vary across various personnel assessments on the model responses."
    },
    {
      "title": "References",
      "text": "* [1] Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Re. Ask me anything: A simple strategy for prompting language models, 2022. * [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. * [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _CoRR_, abs/1810.04805, 2018. * [4] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. * [5] Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_, 2023. * [6] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. * [7] Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei. Evaluating open-domain question answering in the era of large language models. _arXiv preprint arXiv:2305.06984_, 2023. * [8] Andrew Lampinen, Ishita Dasgupta, Stephanie Chan, Kory Mathewson, Mh Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 537-563, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics. * [9] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_, 2023. * [10] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023. * [11] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. _Science_, 378(6624):1092-1097, 2022. * [12] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding large language models via directional stimulus prompting. _arXiv preprint arXiv:2302.11520_, 2023. * [13] OpenAI, ;., Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell,Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremia Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atry Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhan Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Amy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023. * [14] Rui Pan, Shuo Xing, Shizhe Diao, Xiang Liu, Kashun Shum, Jipeng Zhang, and Tong Zhang. Plum: Prompt learning using metaheuristic. _arXiv preprint arXiv:2311.08364_, 2023. * [15] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. * [16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019. * [17] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Dudden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. _CoRR_, abs/2112.11446, 2021. * [18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _CoRR_, abs/1910.10683, 2019. * [19] Zhiqiang Shen, Sondos Mahmoud Bsharat, and Aidar Myrzakhan. Atlas: A llvm inquiry principle benchmark. _Preprint_, 2024. * [20] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020. * [21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anais White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adria Puigdomenech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Sijerngren, Josip Djolonga, YutingSun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sebastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozinska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newman, Dawei Jia, Miliadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gimenez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yin Chang, Paul Komarek, Ross McIlroy, Mario Lucic, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Inuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphael Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjosund, Sebastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Leonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adria Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Victor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Caglar Unlu, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed HadiHashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakicevic, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Poluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Castano, Irene Giannoumis, Wooyeol Kim, Mikolaj Rybinski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan le, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Cho, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, Remi Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanniya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, RishikaSinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Amelie Heliou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim Poder, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Riviere, Alanna Walton, Clement Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucinska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A family of highly capable multimodal models, 2023. * [22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023. * [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. * [24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. * [25] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt, 2023. * [26] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023. * [27] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023."
    }
  ]
}