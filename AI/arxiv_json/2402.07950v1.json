{
  "title": "SENTINELS OF THE STREAM: UNLEASHING LARGE LANGUAGE MODELS FOR DYNAMIC PACKET CLASSIFICATION IN SOFTWARE DEFINED NETWORKS -POSITION PAPER",
  "authors": [
    "Shariq Murtuza"
  ],
  "abstract": "\n With the release of OpenAI's ChatGPT, the field of large language models (LLM) saw an increase of academic interest in GPT based chat assistants. In the next few months multiple accesible large language models were released that included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models. These models are available openly for a wide array of purposes with a wide spectrum of licenses. These LLMs have found their use in a different number of fields like code development, SQL generation etc. In this work we propose our plan to explore the applicability of large language model in the domain of network security. We plan to create Sentinel, a LLM, to analyse network packet contents and pass a judgment on it's threat level. This work is a preliminary report that will lay our plan for our future endeavors. 1    \n",
  "references": [
    {
      "id": null,
      "title": "SENTINELS OF THE STREAM: UNLEASHING LARGE LANGUAGE MODELS FOR DYNAMIC PACKET CLASSIFICATION IN SOFTWARE DEFINED NETWORKS -POSITION PAPER",
      "authors": [
        "Shariq Murtuza"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "A commentary of GPT-3 in MIT Technology Review 2021",
      "authors": [
        "Min Zhang",
        "Juntao Li"
      ],
      "year": "2021",
      "venue": "Fundamental Research",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
      "authors": [
        "Zhiqiang Hu",
        "Yihuai Lan",
        "Lei Wang",
        "Wanyu Xu",
        "Ee-Peng Lim",
        "Roy",
        "Ka-Wei Lee",
        "Lidong Bing",
        "Soujanya Poria"
      ],
      "year": "2023",
      "venue": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Chen Hao"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Chen Hao"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Intelligent Systems and Technology",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "The Potential of Visual ChatGPT for Remote Sensing",
      "authors": [
        "Lucas Osco",
        "Eduardo Prado",
        "Wesley Lopes De Lemos",
        "Ana Nunes Gonçalves",
        "Paula Marques Ramos",
        "José Marcato",
        "Junior"
      ],
      "year": "2023",
      "venue": "Remote Sensing",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Is GPT-3 text indistinguishable from human text? SCARECROW: A framework for scrutinizing machine text",
      "authors": [
        "Yao Dou",
        "Maxwell Forbes",
        "Rik Koncel-Kedziorski",
        "Noah A Smith",
        "Yejin Choi"
      ],
      "year": "2021",
      "venue": "Is GPT-3 text indistinguishable from human text? SCARECROW: A framework for scrutinizing machine text",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Internet protocol-DARPA internet program protocol specification, RFC 791",
      "authors": [
        "Jon Postel"
      ],
      "year": "1981",
      "venue": "Internet protocol-DARPA internet program protocol specification, RFC 791",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "RFC 9293: Transmission Control Protocol (TCP)",
      "authors": [
        "W Eddy"
      ],
      "year": "2022",
      "venue": "RFC 9293: Transmission Control Protocol (TCP)",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Llama",
      "authors": [
        "Llama"
      ],
      "year": "",
      "venue": "Llama",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "LLaMA: Open and Efficient Foundation Language Models -Meta Research | Meta Research",
      "authors": [],
      "year": "2023",
      "venue": "Meta Research",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Introducing LLaMA: A foundational, 65-billion-parameter language model",
      "authors": [],
      "year": "",
      "venue": "Introducing LLaMA: A foundational, 65-billion-parameter language model",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Meta-Llama (Meta Llama 2)",
      "authors": [],
      "year": "2023",
      "venue": "Meta-Llama (Meta Llama 2)",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Mistral AI | Open-Weight Models",
      "authors": [
        "Mistral Ai"
      ],
      "year": "",
      "venue": "Mistral AI | Open-weight models",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Mistral 7B",
      "authors": [
        "Mistral Ai"
      ],
      "year": "2023",
      "venue": "Mistral 7B | Mistral AI | Open-weight models",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Open LLM Leaderboard -a Hugging Face Space by HuggingFaceH4",
      "authors": [],
      "year": "",
      "venue": "Open LLM Leaderboard -a Hugging Face Space by HuggingFaceH4",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "Open-Llm-Leaderboard (Open LLM Leaderboard)",
      "authors": [
        "Llm Open",
        "Leaderboard"
      ],
      "year": "2024",
      "venue": "Open-Llm-Leaderboard (Open LLM Leaderboard)",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Falcon LLM",
      "authors": [],
      "year": "2024",
      "venue": "Falcon LLM",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Tiiuae (Technology Innovation Institute)",
      "authors": [
        "Tiiuae"
      ],
      "year": "2023",
      "venue": "Tiiuae (Technology Innovation Institute)",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
      "authors": [
        "Guilherme Penedo",
        "Quentin Malartic",
        "Daniel Hesslow",
        "Ruxandra Cojocaru",
        "Alessandro Cappelli",
        "Hamza Alobeidli",
        "Baptiste Pannier",
        "Ebtesam Almazrouei",
        "Julien Launay"
      ],
      "year": "2023",
      "venue": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "Instruction tuning with gpt-4",
      "authors": [
        "Baolin Peng",
        "Chunyuan Li",
        "Pengcheng He",
        "Michel Galley",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Instruction tuning with gpt-4",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "Parameterefficient fine-tuning of large-scale pre-trained language models",
      "authors": [
        "Ning Ding",
        "Yujia Qin",
        "Guang Yang",
        "Fuchao Wei",
        "Zonghan Yang",
        "Yusheng Su",
        "Shengding Hu"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Large language models in medicine",
      "authors": [
        "Arun Thirunavukarasu",
        "Darren James",
        "Jeng Shu",
        "Kabilan Ting",
        "Laura Elangovan",
        "Ting Gutierrez",
        "Daniel Fang Tan",
        "Wei Shu",
        "Ting"
      ],
      "year": "2023",
      "venue": "Nature medicine",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "Shengyu Zhang",
        "Linfeng Dong",
        "Xiaoya Li",
        "Sen Zhang",
        "Xiaofei Sun",
        "Shuhe Wang",
        "Jiwei Li"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "GitHub -Rdpahalavan/PADEC: Thesis Research on Enhancing Network Intrusion Detection System (NIDS) Explainability Using Transformers",
      "authors": [
        "Pahalavan Rajkumar",
        "Dheivanayahi"
      ],
      "year": "",
      "venue": "GitHub",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Meta-Llama/Llama-2-7b • Hugging Face",
      "authors": [],
      "year": "",
      "venue": "Meta-Llama/Llama-2-7b • Hugging Face",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Tiiuae/Falcon-7b • Hugging Face",
      "authors": [],
      "year": "2023",
      "venue": "Tiiuae/Falcon-7b • Hugging Face",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "Mixtral of Experts",
      "authors": [
        "Mistral Ai"
      ],
      "year": "2023",
      "venue": "Mixtral of experts | Mistral AI | Open-weight models",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "Mixtral of experts",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Antoine Roux",
        "Arthur Mensch",
        "Blanche Savary",
        "Chris Bamford",
        "Devendra Singh Chaplot"
      ],
      "year": "2024",
      "venue": "Mixtral of experts",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Containernet 2.0: A rapid prototyping platform for hybrid service function chains",
      "authors": [
        "Manuel Peuster",
        "Johannes Kampmeyer",
        "Holger Karl"
      ],
      "year": "2018",
      "venue": "2018 4th IEEE Conference on Network Softwarization and Workshops (NetSoft)",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "Home | TCPDUMP & LIBPCAP",
      "authors": [
        "| Home",
        "Tcpdump",
        "Libpcap"
      ],
      "year": "",
      "venue": "Home | TCPDUMP & LIBPCAP",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "DDoS attack detection and mitigation using SDN: methods, practices, and solutions",
      "authors": [
        "Narmeen Bawany",
        "Jawwad A Zakaria",
        "Khaled Shamsi",
        "Salah"
      ],
      "year": "2017",
      "venue": "Arabian Journal for Science and Engineering",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Abstract",
      "text": "With the release of OpenAI's ChatGPT, the field of large language models (LLM) saw an increase of academic interest in GPT based chat assistants. In the next few months multiple accesible large language models were released that included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models. These models are available openly for a wide array of purposes with a wide spectrum of licenses. These LLMs have found their use in a different number of fields like code development, SQL generation etc. In this work we propose our plan to explore the applicability of large language model in the domain of network security. We plan to create Sentinel, a LLM, to analyse network packet contents and pass a judgment on it's threat level. This work is a preliminary report that will lay our plan for our future endeavors. 1 Footnote 1: Technical Report, work in progress. Large Language Models Network Security Distributed Denial of Service Attacks Traffic Classification"
    },
    {
      "title": "1 Introduction",
      "text": "With the launch of ChatGPT [1, 2] the field of Artificial Intelligence experienced a renewed interest from all walks of life. ChatGPT is a type of language model called a Large Language Model (LLM) [4]. In simple terms, a LLM is initially trained on data using semi supervised and self supervised learning, and then produce output by predicting the next token. LLMs have strong potential to perform a wide range of tasks [5, 6] specified in typical spoken language or technically, in natural language. These models can extensively understand and perform Natural Language Processing (NLP) tasks such as writing letters, invitation, apologies, participate in a dialogue, give solution to mathematical question and lot more. Even though the main expertise of these models is in language(pattern) based tasks, these models can also be used to produced highly specialised texts similar to humans [7] based on the given instruction. A network packet is an ordered collection of bits with each bit reserved for a specific flag representing a value. Each network packet is made up of a sequence of zeros and ones (bits) with each bit location earmarked for a specific purpose. The original IP protocol specification was give in RFC 791 in the year 1981 [8]. While the TCP protocol specification was given in RFC 9293 [9]. Over time these specifications have been updated over time but have provided a concrete foundation for network stack. As each bit location has a specific meaning, we can map it as a sentence of a hypothetical language that consists of bit stream with individual components like flags, port number, IP address analogous to words. Meta(earlier known as Facebook) launched Llama and Llama2 (two different iterations a single model) and associated weights to the research community in 2023 with different licenses [10; 11]. The model was named as LLaMA (Large Language Model Meta AI) [12; 13] and was released in February 2023, and the second model called LLaMA-2 [14], in July 2023. LLaMA was released in four different model sizes 7, 13, 33 and 65 billion parameters, while LLaMA-2 was released in three model sizes 7, 13, and 70 billion parameters. The next update in the public LLM domain came with the release of Mistral AI's Mistral 7B [15; 16] (in September, 2023) that outperformed all the available open models up to 13B parameters, at that time, as per the existing language and code benchmarks [17; 18] Soon after the Falcon family of models (7B, 40B, and 180B) were released by the Technology Innovation Institute (TII) [19; 21] in November, 2023 outperforming the Llama-2 models on different benchmarks. These models are also open and are available for personal and commercial usage under different licences [20]. In this work we present our plan to finetune and create a large language model named Sentinel, that can analyses network packets and identify malicious attack packets from a given network flow. We discuss the details in the next section."
    },
    {
      "title": "2 Creating Our Large Language Model - Sentinel",
      "text": "Finetuning is an approach often deployed in LLMs to increase their accuracy at the cost of shrinking their output domain [3; 22; 23]. For example, an LLM fine-tuned on medical science data will give better results compared to an LLM that has not been fine-tuned yet [24; 25]. However, finetuning also reduces the number of different domains an LLM can respond to. In layman's terms, finetuning involves adjusting the weights of an already existing pre-trained model to enhance its performance on specific types of problems. Our work is based on the work done in PADEC [26] and extending it to encompass multiple different attack scenarios. We plan to fine-tune three different models namely, Llama2-7B, Falcon-7B and Mistral MoE (mixture of experts) [30; 29] on our custom-generated SDN link flood attack dataset. Our dataset was generated with the Containment [31] simulator. We created multiple nodes, attackers, switches, and the controller and captured the network traffic. Traditional datasets were avoided due to their lack of alignment with the rapid advancements in network attacks. We are not creating a new LLM model from scratch, but instead we will be using existing open models and train them upon our dataset."
    },
    {
      "title": "Dataset Processing",
      "text": "To create the dataset, we simulated a multihost network with various nodes providing different services. Our dataset comprises captured network packets obtained using the TCPdump tool [32]. The traffic capture commenced after the attack was initiated and had stabilized over time. The attack packets comprises of different attack categories, including volumetric attacks, protocol attacks, and other vulnerability-based attacks [33]."
    },
    {
      "title": "Finetuning",
      "text": "Each packet is a collection of bits, with each bit corresponding to a specific defined purpose in the network packet. This makes it very similar to natural spoken language, and thus we can safely assume a network packet to be a piece of textual language, where the language consists of only two alphabets: zero and one. Just as a language sentence has a format, a group of semantically related words delimited by special punctuation marks (comma, full stop, question mark, etc.), the network packets also have a clearly defined format, with each numbered bit reserved for a specific purpose. Similar to how the next word in a sentence are related to the current word, inside a network packet's header, the next bit(s) are related to the current bit value. For example within the TCP header headers as shown in Fig. 1, the source port is always followed by the destination port. Similarly an IP packet (shown in Fig. 2 also follows specific rules and in no scenarios can we have a packet without an IP address and/or a port number. This compulsion makes a network packet highly similar to a language with grammar rules and a captured network packet equivalent to a written sentence. As each bit location has a specific meaning, we can map it as a sentence of a hypothetical language that consists of bit stream with individual components like flags, port number, IP address analogous to words. As we have proven, a captured network packet is no different from a written sentence, thus we can safely say that a large language model can be created to read, write and understand a language that has it's constituents sentences in the form of network packets. The implied similarity is shown in Fig 3 where it is evident that specific bits always refer to specific information in a packet. This language (consisting of network packets) would be simpler than spoken languages like English due to the Figure 1: TCP Packet format Figure 2: IP Packet format relative smaller possible vocabulary but in certain aspects it would be more complex since the different permutation and combinations of each flag in the header creates a new network packet variant."
    },
    {
      "title": "3 Conclusion",
      "text": "In this text we share our plans to create Sentinel, a fine tuned LLM to analyse network packets and take a decision regarding the amount of threat posed by the packets. We are starting with Llama2-7B then we will move towards Mistral AI's 7B model and shall finally study the feasibility of Mistral MoE model. We are using three very different models to gauge their efficacy in handling this task and to select the best among the candidates. Our selection of model is purely on their performance on various public leader-boards and their proven capabilities in performing given tasks."
    },
    {
      "title": "References",
      "text": "* [1]Achiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida et al. \"Gpt-4 technical report.\" arXiv preprint arXiv:2303.08774 (2023). * [2]Zhang, Min, and Juntao Li. \"A commentary of GPT-3 in MIT Technology Review 2021.\" Fundamental Research 1, no. 6 (2021): 831-833. * [3]Hu, Zhiqiang, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. \"LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.\" arXiv preprint arXiv:2304.01933 (2023). * [4]Chang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen et al. \"A survey on evaluation of large language models.\" ACM Transactions on Intelligent Systems and Technology (2023). * [5]Chang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen et al. \"A survey on evaluation of large language models.\" ACM Transactions on Intelligent Systems and Technology (2023). * [6]Osco, Lucas Prado, Eduardo Lopes de Lemos, Wesley Nunes Goncalves, Ana Paula Marques Ramos, and Jose Marcato Junior. \"The Potential of Visual ChatGPT for Remote Sensing.\" Remote Sensing 15, no. 13 (2023): 3232. * [7]Dou, Yao, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. \"Is GPT-3 text indistinguishable from human text? SCARECROW: A framework for scrutinizing machine text.\" arXiv preprint arXiv:2107.01294 (2021). * [8]Postel, Jon. \"Internet protocol--DARPA internet program protocol specification, RFC 791.\" (Internet protocol--DARPA internet program protocol specification) (1981). * [9]Eddy, W., ed. \"RFC 9293: Transmission Control Protocol (TCP).\" (2022). * [10]Llama. \"Llama,\" n.d. [https://lllama.meta.com/](https://lllama.meta.com/). Figure 3: Similarity of a sentence with an IP Packet * [11] Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023). * Meta Research | Meta Research,\" February 24, 2023. [https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/). * [13] Introducing LLaMA: A foundational, 65-billion-parameter language model. \"Introducing LLaMA: A Foundational, 65-Billion-Parameter Language Model,\" n.d. [https://ai.meta.com/blog/large-language-model-llama-meta-ai/](https://ai.meta.com/blog/large-language-model-llama-meta-ai/). * [14] meta-llama (Meta Llama 2). \"Meta-Llama (Meta Llama 2),\" December 27, 2023. [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama). * [15] AI, Mistral. \"Mistral AI | Open-Weight Models.\" Mistral AI | Open-weight models, n.d. /. * [16] AI, Mistral. \"Mistral 7B.\" Mistral 7B | Mistral AI | Open-weight models, September 27, 2023. [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/). * a Hugging Face Space by HuggingFaceH4. \"Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4,\" n.d. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). * [18] open-llm-leaderboard (Open LLM Leaderboard). \"Open-LLm-Leaderboard (Open LLM Leaderboard),\" February 6, 2024. [https://huggingface.co/open-llm-leaderboard](https://huggingface.co/open-llm-leaderboard). * [19][https://falconllm.tii.ae/](https://falconllm.tii.ae/). \"Falcon LLM.\" Accessed February 9, 2024. [https://falconllm.tii.ae/](https://falconllm.tii.ae/). * [20]tiuae (Technology Innovation Institute). \"Tiiuae (Technology Innovation Institute),\" September 6, 2023. [https://huggingface.co/tiuae](https://huggingface.co/tiuae). * [21] Penedo, Guilherme, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. \"The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only.\" arXiv preprint arXiv:2306.01116 (2023). * [22] Peng, Baolin, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. \"Instruction tuning with gpt-4.\" arXiv preprint arXiv:2304.03277 (2023). * [23] Ding, Ning, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu et al. \"Parameter-efficient fine-tuning of large-scale pre-trained language models.\" Nature Machine Intelligence 5, no. 3 (2023): 220-235. * [24] Thirunavukarasu, Arun James, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. \"Large language models in medicine.\" Nature medicine 29, no. 8 (2023): 1930-1940. * [25] Zhang, Shengyu, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li et al. \"Instruction tuning for large language models: A survey.\" arXiv preprint arXiv:2308.10792 (2023). * Rdpahalavan/PADEC: Thesis Research on Enhancing Network Intrusion Detection System (NIDS) Explainability Using Transformers.\" GitHub, n.d. [https://github.com/rdpahalavan/PADEC](https://github.com/rdpahalavan/PADEC). * Hugging Face. \"Meta-Llama/Llama-2.7b - Hugging Face,\" n.d. [https://huggingface.co/meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b). * Hugging Face. \"Tiiuae/Falcon-7b - Hugging Face,\" June 20, 2023. [https://huggingface.co/tiuae/falcon-7b](https://huggingface.co/tiuae/falcon-7b). * [29] AI, Mistral. \"Mistral of Experts.\" Mistral of Experts.\" Mistral of Experts | Mistral AI | Open-weight models, December 11, 2023. [https://mistral.ai/news/mistral-of-experts/](https://mistral.ai/news/mistral-of-experts/). * [30] Jiang, Albert Q., Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot et al. \"Mistral of experts.\" arXiv preprint arXiv:2401.04088 (2024). * [31] Peuster, Manuel, Johannes Kampmeyer, and Holger Karl. \"Container 2.0: A rapid prototyping platform for hybrid service function chains.\" In 2018 4th IEEE Conference on Network Softwarization and Workshops (NetSoft), pp. 335-337. IEEE, 2018. * [32] Home | TCPDUMP & LIBPCAP. \"Home | TCPDUMP & LIBPCAP,\" n.d. [https://www.tcpdump.org/](https://www.tcpdump.org/). * [33] Bawany, Narmeen Zakaria, Jawwad A. Shamsi, and Khaled Salah. \"DDoS attack detection and mitigation using SDN: methods, practices, and solutions.\" Arabian Journal for Science and Engineering 42 (2017): 425-441."
    }
  ]
}