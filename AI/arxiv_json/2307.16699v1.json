{
  "title": "Ontology engineering with Large Language Models",
  "authors": [
    "Patricia Mateiu",
    "Adrian Groza"
  ],
  "abstract": "\n We tackle the task of enriching ontologies by automatically translating natural language sentences into Description Logic. Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert Natural Language sentences into OWL Functional Syntax. We employ objective and concise examples to fine-tune the model regarding: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin. \n",
  "references": [
    {
      "id": null,
      "title": "Ontology engineering with Large Language Models",
      "authors": [
        "Patricia Mateiu",
        "Adrian Groza"
      ],
      "year": "2023",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Ontocom: A cost estimation model for ontology engineering",
      "authors": [
        "E Paslaru Bontas",
        "C Simperl",
        "Y Tempich",
        "Sure"
      ],
      "year": "2006",
      "venue": "5th Int. Semantic Web Conf., ISWC 2006",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "An analysis of ontology engineering methodologies: A literature review",
      "authors": [
        "R Iqbal",
        "M A A Murad",
        "A Mustapha",
        "N M Sharef"
      ],
      "year": "2013",
      "venue": "Research journal of applied sciences, engineering and technology",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Ontology engineering methodologies for the evolution of living and reused ontologies: status, trends, findings and recommendations",
      "authors": [
        "K I Kotis",
        "G A Vouros",
        "D Spiliotopoulos"
      ],
      "year": "2020",
      "venue": "The Knowledge Engineering Review",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "The OWL API: A Java api for OWL ontologies",
      "authors": [
        "M Horridge",
        "S Bechhofer"
      ],
      "year": "2011",
      "venue": "Semantic web",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Fred: From natural language text to RDF and OWL in one click",
      "authors": [
        "F Draicchio",
        "A Gangemi",
        "V Presutti",
        "A G Nuzzolese"
      ],
      "year": "2013",
      "venue": "The Semantic Web: ESWC 2013 Satellite Events",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Automatic ontology construction from text: a review from shallow to deep learning trend",
      "authors": [
        "F N Al-Aswadi",
        "H Y Chan",
        "K H Gan"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence Review",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Semantic web and generative pre-trained transformer (gpt)",
      "authors": [
        "V Božić"
      ],
      "year": "",
      "venue": "Semantic web and generative pre-trained transformer (gpt)",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Structured prompt interrogation and recursive extraction of semantics (spires): A method for populating knowledge bases using zero-shot learning",
      "authors": [
        "J H Caufield",
        "H Hegde",
        "V Emonet"
      ],
      "year": "2023",
      "venue": "Structured prompt interrogation and recursive extraction of semantics (spires): A method for populating knowledge bases using zero-shot learning",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Conceptual modeling and large language models: impressions from first experiments with chatgpt",
      "authors": [
        "H.-G Fill",
        "P Fettke",
        "J Köpke"
      ],
      "year": "2023",
      "venue": "Enterprise Modelling and Information Systems Architectures (EMISAJ)",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Graph GPT",
      "authors": [],
      "year": "",
      "venue": "Graph GPT",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "Synthetic ontologies: A hypothesis",
      "authors": [
        "A Bikeyev"
      ],
      "year": "2023",
      "venue": "Available at SSRN",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Plausible description logic programs for stream reasoning",
      "authors": [
        "A Groza",
        "I A Letia"
      ],
      "year": "2012",
      "venue": "Future Internet",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Interactive call graph generation for software projects",
      "authors": [
        "I.-A Császár",
        "R R Slavescu"
      ],
      "year": "2020",
      "venue": "2020 IEEE 16th International Conference on Intelligent Computer Communication and Processing",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Understanding cooking recipes' structure using grammars",
      "authors": [
        "A Ilies",
        "A N Marginean"
      ],
      "year": "2021",
      "venue": "2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Harnessing the power of large language models for natural language to first-order logic translation",
      "authors": [
        "Y Yang",
        "S Xiong",
        "A Payani",
        "E Shareghi",
        "F Fekri"
      ],
      "year": "2023",
      "venue": "Harnessing the power of large language models for natural language to first-order logic translation",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Ontology Engineering With Large Language Models",
      "text": "Patricia Mateiu\\({}^{*}\\) and Adrian Groza\\({}^{*}\\) \\({}^{*}\\)Department of Computer Science, Technical University of Cluj-Napoca, 400114 Cluj-Napoca, Romania {patriciamateiu33@gmail.com, adrian.groza@cs.utcluj.ro}"
    },
    {
      "title": "Abstract",
      "text": "We tackle the task of enriching ontologies by automatically translating natural language sentences into Description Logic. Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert Natural Language sentences into OWL Functional Syntax. We employ objective and concise examples to fine-tune the model regarding: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protege plugin."
    },
    {
      "title": "I Motivation",
      "text": "The technical challenges and costs associated with the development of ontologies are arguable the main causes for the partial failure of the Semantic Web. Aiming to facilitate the development of ontologies by industry, the economical aspects of ontology engineering have been subject to the Ontology Cost Model (ONTOCOM) [1]. Despite the existence of several ontology engineering methodologies (OEMs) (no less than 15 as identified by [2] in 2013), the domain of Semantic Web does not benefit from a mature and largely accepted methodology. More recently, [3] have analysed 9 OEMs, concluding that non-collaborative methodologies have a negative impact on theiveness, evolution, and reusability of the ontologies. We rely here on the current opportunities provided by Large Language Models (LLMs). We argue that LLMs have the potential to largely increase the efficiency of the ontology engineering. Since LLMs are best technology for language translation, our employ them to translate from natural language to description logic (DL). That is, given a description in natural language (definition, domain knowledge), the aim is to automatically obtain corresponding ontology in a formal language. We developed a tool able to enrich and populate an ontology with domain knowledge available in natural language. The tool relies on GPT-3 which we fine-tuned for the ontology engineering task. The solution is freely available and is provided as a plugin for the Protege editor."
    },
    {
      "title": "Ii Tuning The Model For Owl",
      "text": "The tool is constructed as a Protege plugin that supports the development of an ontology from scratch and also the enrichment of an existing ontology. Natural language sentences are translated into ontological elements and appended to the current ontology (Figure 1). The prompts are sent to a fine-tuned GPT-3 davinci model which returns the result into ontology axioms. Technically, these axioms are handled using the _owlapi_ Java library [4], and appended to the active ontology in the Protege editor. We developed a dataset of 150 pairs of prompts and their corresponding translations into OWL Functional Syntax. We seek to cover various cases and to incorporate a variety of domains, in favor of attaining an adaptable model. Tables I and II depict some of the prompts used in the training set. We used the following conventions: First, the underscore symbol is used for elements having multiple words, since it is the default naming convention used in Protege, e.g., the object property named _has_sibling_ or compound individual names such as _Wolfgang_Amadeus_Mozart_. Second, we settled the class and object property names in only lowercase letters, while the individual names can begin either with a lowercase or an uppercase letter. Third, for subclass relationships, to highlight the general aspect, several connection words were used. For example, the sentence _every man is a person_ creates a subclass relationship between the class _person_ and the class _man_. If these classes do not exist in the ontology, the declaration axioms will generate them. Otherwise, these axioms are ignored. The quantifier _every_ is used to emphasize that there is no instance where an individual that belongs to the class _man_ will not belong to the class _person_. Other connection words can be used for defining this type of relationship, e.g., _all lilies are flowers_, _cats are a type of animal_. Multiple subclass relationships can be used at the same time, as in the example _bugs_, _ants_, _ladybugs_, _flies are insects_. Fig. 1: Data flow for the presented approach [MISSING_PAGE_FAIL:2] class _girl_. The plugin is able to deliver the same ontology as in case of statements \\(s_{1}\\) and \\(s_{2}\\) (see Figure 4). The class is declared only once, while the second declaration of the same class is ignored. The trained model offers the users several options in transmitting the components they want to include in the ontology. Fourth, one can add object properties. Consider the statement \\(s_{4}\\): _Anna and Lana are each other's sisters_. The resulted property is attached to the object property taxonomy, and the assertions can be seen in the third panel of the left half in Figure 5, by clicking on each individual. Figure 5 presents the relation _Lana has sister Anna_ and the inverse relation. Fifth, one can add assertions about new individuals. Let \\(s_{5}\\): _Nola and Anna are each other's cousins_. In this case, _Nola_, who was not defined prior and has no class association, will be added as an individual, but separate than the ones grouped by class. The object property will be added for both individuals, just like in the previous step (see Figure 6)."
    },
    {
      "title": "_Prompt Engineering Strategies_",
      "text": "We run experiments with three strategies: zero-shot learning, few-shot learning and fine-tuning. The _Zero-shot learning_ strategy asks the language model to generate the output directly, with no presented examples. Several experiments were run with this strategy, using the GPT-3.5-turbo model. The results were, although not incorrect, not the expected ones. For example, using prompt _Translate 'Anna is a girl' into Functional Syntax_, the model returned _is(Anna, girl)_, which is not helpful since it is not in the form of an axiom. It rather represents the relationship _is_ between _Anna_ and _girl_, yet does not offer any information on the form or meaning of these words. The _few-shot learning_ strategy lets LLMs train for specific tasks from a few examples. To assess this strategy, we tested various prompts. The first trial was _Anna is a girl_, whose corresponding axioms are also included in the example. The outcome for this prompt was not the expected one and the solution was proven to be inconsistent, returning different results for the same prompt. In the first trial, besides the declarations and the class assertion for individual _Anna_ and class _girl_, the response includes other axioms, that are not logical for the given context. Namely, the last line is an object property assertion between an individual and a class, which is not possible. The second trial is not as faulty, it does not include incorrect axioms, but it includes extra axioms that are not needed. For example, the declaration for the class _person_ and the object property _married_to_, which are not in the prompt. Results such as this one would cause users to have an ontology that is too big, and only partially used, which, in fact, contradicts their preferences and intentions. The _fine-tuning_ strategy requires a dedicated dataset. A dataset with 150 prompt-result pairs and a validation set with 50 such pairs were used, with the same format and variation as those in the training data set, but with different cases and examples. The validation data set is used to determine the optimal combination of hyper-parameters that would have the best token accuracy. Regarding the data structure, several tests were done to determine the correct order of the completion result with respect to the prompt. One question was whether it was better to write the declarations and assertion or relationship axioms in the order that the words appear in the sentence or to write them in the order that Protege would save them in a Functional Syntax. Both options were considered in training with several combinations of hyper-parameters."
    },
    {
      "title": "Iv Related Work",
      "text": "Before the LLMs era tools like Fred [5] were used. Fred is a machine reader designed for the Semantic Web that can analyze natural language in 48 different languages and generates linked data in the OWL format. However, the resulting axioms require further processing before they can be effectively utilized for reasoning. In the recent years, there have been various approaches in learning ontologies from text data, by extracting the ontological terms and structuring them into one component [6]. For instance, Bozic [7] has analysed Fig. 4: Formalising multiple individuals belonging to the same collective class Fig. 5: Handling symmetric relations Fig. 6: Adding assertions about new individualsthe potential combining Semantic Web and GPT, as well as the related risks that might pose a threat. OntoGPT tool [8] extracts information from text, by using three strategies: SPIRES, HALO, and SPINDOCTOR. SPIRES applies a knowledge schema on the input text and returns an instance with multiple attribute-value relations, where the values are either data primitives or other instances, thus creating a linked scheme. HALO is a Few-Shot Learning approach, which solves tasks with limited number of examples for learning while using prior knowledge. Conceptual modelling using large language models have been experienced by [9]. ChatGPT was used to generate entity-relations diagrams (ER). The designed prompt starts with an explanation of ER diagrams. Then the prompt includes an example of ER diagram in JSON syntax. The last part of the prompt is the natural language description of the task. A second experiment has focused on business process diagrams. A subset of BPMN diagrams has been considered. The prompt describes the meta-model in NL (e.g. a task has exactly one predecessor and one successor) and an example in JSON format. The third experiment has targeted UML class diagrams, for which a Zero-Shot approach has been preferred. Even if large parts of the conceptual modelling was correct, modelling experience of a human expert was required to validate the model. GraphGPT [10] converts natural language into knowledge graphs. The application does not imply ontology population, it rather offers the users a view on how the data they submit might be connected. Bikeyev [11] has proposed an alternative of knowledge model engineering and knowledge graph generation as an automated approach that avoids the vagueness of Natural Language Processing. A bottom-up approach is combined to a LLM, namely GPT-3. The method uses two types of prompts, one to generate a hierarchy of elements and the other to determine possible relationships between them. In both cases, it is necessary that the prompts respect memory limitations, so that the prompt and the result can fit entirely in the given memory slot. After the initial hierarchy is constructed, each element can be used in another prompt to give a more detailed result, and this step can continue until the result is satisfactory. The advantage is that this approach can suit the individual preferences of each user, depending on how much detail they need in the ontology. GraphGPT can incorporate newly available data when updating, thus allowing a form of stream reasoning [12] when populating the knowledge graph. Csaszar and Slavescu have developed a tool to help software developers visualize the call graph of their code while editing it. Two graphs are automatically built from the source code: the import graph and the call graph. Instead of LLMs, the process of building the two graphs is based on using a query based architecture, commonly used by language servers [13] In a literature dominated by transformers, Ilies and Marginean have used grammars to provide a white box alternative [14]. Context free grammars and semantic roles are used to structure knowledge from texts related to cooking recipes. Lex and Yacc interleave with AllenNLP to compute a parse tree for a cooking recipe, where each group of words is labeled with an appropriate semantic role. The approach can be applied to other types of instruction manuals. Yang et al. [15] have developed LOGICLLAMA, a fine-tuned tool used for natural language to First-Order Logic translation, which can be also used for correcting FOL results generated by GPT-3.5 and is comparable to GPT-4. The MALLS dataset contains pairs NL-FOL generated by GPT-4 and is intended to be used for fine-tuning and testing the model. These pairs are resulted by repeatedly prompting GPT-4 using a pipeline which adjusts depending on the previous results. The LLOGICLLAMA is obtained from training and fine-tuning a model using the MALLS data set for two main tasks, generating translation from NL to FOL and correcting already generated translations by GPT-3.5. The first one uses natural language text as input and provides FOL output, while the second one uses a pair of NL and the resulted FOL translation returned by GPT-3.5 and provides a single output in FOL, representing the necessary adjustments or corrections."
    },
    {
      "title": "V Conclusion",
      "text": "The developed plugin shows hows language models (e.g. GPT) can be used in automating learning and populating ontologies, a process which is very time-consuming, complex and could be overwhelming in terms of decision making. The aim was to exploit the capabilities of pre-trained language models to obtain OWL axioms. Aware of the limitations and risks of Large Language Models, the tool aims to be a support tool that saves development time. It also reduces the interaction time with the domain expert, given that a description of the domain exists in natural language. Ongoing work regards quantitative evaluation and assessing the efficiency of ontology engineering with and without the tool."
    },
    {
      "title": "References",
      "text": "* [1] E. Paslam Bontas Simperl, C. Tempich, and Y. Sure, \"Ontocom: A cost estimation model for ontology engineering,\" in _5th Int. Semantic Web Conf., ISWC 2006, Athens, GA, USA, November 5-9_. Springer, 2006, pp. 625-639. * [2] R. Iqbal, M. A. A. Murad, A. Mustapha, N. M. Sharef _et al._, \"An analysis of ontology engineering methodologies: A literature review,\" _Research journal of applied sciences, engineering and technology_, vol. 6, no. 16, pp. 2993-3000, 2013. * [3] K. I. Kotsis, G. A. Vouros, and D. Spiliotopoulos, \"Ontology engineering methodologies for the evolution of living and reused ontologies: status, trends, findings and recommendations,\" _The Knowledge Engineering Review_, vol. 35, p. e4, 2020. * [4] M. Horridge and S. Bechhofer, \"The OWL API: A Java api for OWL ontologies,\" _Semantic web_, vol. 2, no. 1, pp. 11-21, 2011. * [5] F. Draicchio, A. Gangemi, V. Presutti, and A. G. Nuzzolese, \"Fred: From natural language text to RDF and OWL in one click,\" in _The Semantic Web: ESWC 2013 Satellite Events: Montpellier, France, May 26-30, 2013, Revised Selected Papers_. Springer, 2013, pp. 263-267. * [6] F. N. Al-Aswadi, H. Y. Chan, and K. H. Gan, \"Automatic ontology construction from text: a review from shallow to deep learning trend,\" _Artificial Intelligence Review_, vol. 53, pp. 3901-3928, 2020. * [7] V. Bozic, \"Semantic web and generative pre-trained transformer (gpt).\" * [8] J. H. Caufield, H. Hegde, V. Emonet _et al._, \"Structured prompt interrogation and recursive extraction of semantics (spires): A method for populating knowledge bases using zero-shot learning,\" _arXiv preprint arXiv:2304.02711_, 2023. * [9] H.-G. Fill, P. Fettke, and J. Kopke, \"Conceptual modeling and large language models: impressions from first experiments with chatgpt,\" _Enterprise Modelling and Information Systems Architectures (EMISAJ)_, vol. 18, pp. 1-15, 2023. * [10] \"Graph GPT.\" [Online]. Available: [https://graphgpt.vercel.app](https://graphgpt.vercel.app) * [11] A. Bikeyev, \"Synthetic ontologies: A hypothesis,\" _Available at SSRN 4373537_, 2023. * [12] A. Groza and I. A. Letia, \"Plausible description logic programs for stream reasoning,\" _Future Internet_, vol. 4, no. 4, pp. 865-881, 2012. * [13] I.-A. Csaszar and R. R. Slavescu, \"Interactive call graph generation for software projects,\" in _2020 IEEE 16th International Conference on Intelligent Computer Communication and Processing (ICCP)_. IEEE, 2020, pp. 51-58. * [14] A. Ilies and A. N. Marginean, \"Understanding cooking recipes' structure using grammars,\" in _2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing (ICCP)_. IEEE, 2021, pp. 327-334. * [15] Y. Yang, S. Xiong, A. Payani, E. Shareghi, and F. Fekri, \"Harnessing the power of large language models for natural language to first-order logic translation,\" _arXiv preprint arXiv:2305.15541_, 2023."
    }
  ]
}