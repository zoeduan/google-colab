{
  "title": "AI for Biomedicine in the Era of Large Language Models",
  "authors": [
    "Zhenyu Bi",
    "Sajib Acharjee Dip",
    "Daniel Hajialigol",
    "Sindhura Kommu",
    "Hanwen Liu",
    "Meng Lu",
    "Xuan Wang"
  ],
  "abstract": "\n The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences -biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this tutorial, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language models' challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation. \n",
  "references": [
    {
      "id": null,
      "title": "AI for Biomedicine in the Era of Large Language Models",
      "authors": [
        "Zhenyu Bi",
        "Sajib Acharjee Dip",
        "Daniel Hajialigol",
        "Sindhura Kommu",
        "Hanwen Liu",
        "Meng Lu",
        "Xuan Wang"
      ],
      "year": "2024",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Costas Bouyioukos, and Michalis Vazirgiannis. Prot2text: Multimodal protein's function generation with gnns and transformers",
      "authors": [
        "Michail Hadi Abdine",
        "Chatzianastasis"
      ],
      "year": "2023",
      "venue": "Costas Bouyioukos, and Michalis Vazirgiannis. Prot2text: Multimodal protein's function generation with gnns and transformers",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning",
      "authors": [
        "Manato Akiyama",
        "Yasubumi Sakakibara"
      ],
      "year": "",
      "venue": "NAR Genomics and Bioinformatics",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Biom-transformers: building large biomedical language models with bert, albert and electra",
      "authors": [
        "Sultan Alrowili",
        "K Vijay-Shanker"
      ],
      "year": "2021",
      "venue": "Proceedings of the 20th workshop on biomedical language processing",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Publicly available clinical bert embeddings. NAACL HLT 2019",
      "authors": [
        "Emily Alsentzer",
        "John R Murphy",
        "Willie Boag",
        "Wei-Hung Weng",
        "Di Jin",
        "Tristan Naumann",
        "Matthew Ba Wa Redmond",
        "Mcdermott"
      ],
      "year": "2019",
      "venue": "Publicly available clinical bert embeddings. NAACL HLT 2019",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "Construction of the literature graph in semantic scholar",
      "authors": [
        "Waleed Ammar",
        "Dirk Groeneveld",
        "Chandra Bhagavatula",
        "Iz Beltagy",
        "Miles Crawford",
        "Doug Downey",
        "Jason Dunkelberger",
        "Ahmed Elgohary",
        "Sergey Feldman",
        "Vu Ha",
        "Rodney Kinney",
        "Sebastian Kohlmeier",
        "Kyle Lo",
        "Tyler Murray",
        "Hsu-Han",
        "Matthew Ooi",
        "Joanna Peters",
        "Sam Power",
        "Lucy Lu Skjonsberg",
        "Chris Wang",
        "Zheng Wilhelm",
        "Madeleine Yuan",
        "Oren Van Zuylen",
        "Etzioni"
      ],
      "year": "2018",
      "venue": "Construction of the literature graph in semantic scholar",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Palm 2 technical report",
      "authors": [
        "Rohan Anil",
        "Andrew M Dai",
        "Orhan Firat",
        "Melvin Johnson",
        "Dmitry Lepikhin",
        "Alexandre Passos",
        "Siamak Shakeri",
        "Emanuel Taropa",
        "Paige Bailey",
        "Zhifeng Chen"
      ],
      "year": "2023",
      "venue": "Palm 2 technical report",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Speech synthesis from neural decoding of spoken sentences",
      "authors": [
        "Josh Gopala K Anumanchipalli",
        "Edward F Chartier",
        "Chang"
      ],
      "year": "2019",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "Convolutional neural networks for classification of alignments of non-coding rna sequences",
      "authors": [
        "Genta Aoki",
        "Yasubumi Sakakibara"
      ],
      "year": "2018",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Gene ontology: tool for the unification of biology",
      "authors": [
        "Michael Ashburner",
        "Catherine A Ball",
        "Judith A Blake",
        "David Botstein",
        "Heather Butler",
        "J Michael Cherry",
        "Allan P Davis",
        "Kara Dolinski",
        "Selina S Dwight",
        "Janan T Eppig"
      ],
      "year": "2000",
      "venue": "Nature genetics",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Effective gene expression prediction from sequence by integrating long-range interactions",
      "authors": [
        "Å½iga Avsec",
        "Vikram Agarwal",
        "Daniel Visentin",
        "Joseph R Ledsam",
        "Agnieszka Grabska-Barwinska",
        "Kyle R Taylor",
        "Yannis Assael",
        "John Jumper",
        "Pushmeet Kohli",
        "David R Kelley"
      ],
      "year": "2021",
      "venue": "Nature Methods",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "The swiss-prot protein sequence database and its supplement trembl in 2000",
      "authors": [
        "Amos Bairoch",
        "Rolf Apweiler"
      ],
      "year": "2000",
      "venue": "Nucleic acids research",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Scibert: A pretrained language model for scientific text",
      "authors": [
        "Iz Beltagy",
        "Kyle Lo",
        "Arman Cohan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Dna language models are powerful predictors of genome-wide variant effects",
      "authors": [
        "Gonzalo Benegas",
        "Sanjit Singh Batra",
        "Yun S Song"
      ],
      "year": "2023",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "Learning the protein language: Evolution, structure, and function",
      "authors": [
        "Tristan Bepler",
        "Bonnie Berger"
      ],
      "year": "2021",
      "venue": "Cell systems",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Uniprotkb/swiss-prot: the manually annotated section of the uniprot knowledgebase",
      "authors": [
        "Emmanuel Boutet",
        "Damien Lieberherr",
        "Michael Tognolli",
        "Michel Schneider",
        "Amos Bairoch"
      ],
      "year": "2007",
      "venue": "Plant bioinformatics: methods and protocols",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Proteinbert: a universal deep-learning model of protein sequence and function",
      "authors": [
        "Nadav Brandes",
        "Dan Ofer",
        "Yam Peleg",
        "Nadav Rappoport",
        "Michal Linial"
      ],
      "year": "2022",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared D Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein",
      "authors": [
        "Bo Chen",
        "Xingyi Cheng",
        "Pan Li",
        "Yangli-Ao Geng",
        "Jing Gong",
        "Shen Li",
        "Zhilei Bei",
        "Xu Tan",
        "Boyan Wang",
        "Xin Zeng"
      ],
      "year": "2024",
      "venue": "xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Interpretable rna foundation model from unannotated data for highly accurate rna structure and function predictions",
      "authors": [
        "Jiayang Chen",
        "Zhihang Hu",
        "Siqi Sun",
        "Qingxiong Tan",
        "Yixuan Wang",
        "Qinze Yu",
        "Licheng Zong",
        "Liang Hong",
        "Jin Xiao",
        "Tao Shen"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Self-supervised learning on millions of pre-mrna sequences improves sequence-based rna splicing prediction",
      "authors": [
        "Ken Chen",
        "Yue Zhou",
        "Maolin Ding",
        "Yu Wang",
        "Zhixiang Ren",
        "Yuedong Yang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "doi": ""
    },
    {
      "id": "b21",
      "title": "scgpt: toward building a foundation model for single-cell multi-omics using generative ai",
      "authors": [
        "Haotian Cui",
        "Chloe Wang",
        "Hassaan Maan",
        "Kuan Pang",
        "Fengning Luo",
        "Nan Duan",
        "Bo Wang"
      ],
      "year": "2024",
      "venue": "Nature Methods",
      "doi": ""
    },
    {
      "id": "b22",
      "title": "The nucleotide transformer: Building and evaluating robust foundation models for human genomics",
      "authors": [
        "Hugo Dalla-Torre",
        "Liam Gonzalez",
        "Javier Mendoza-Revilla",
        "Nicolas Lopez Carranza",
        "Adam Henryk Grzywaczewski",
        "Francesco Oteri",
        "Christian Dallago",
        "Evan Trop",
        "Hassan Sirelkhatim",
        "Guillaume Richard"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b23",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "doi": ""
    },
    {
      "id": "b24",
      "title": "Discrete encoding of eeg waves for eeg to text translation",
      "authors": [
        "Yiqun Duan",
        "Charles Zhou",
        "Zhen Wang",
        "Yu-Kai Wang",
        "Chin-Teng Lin",
        "Dewave"
      ],
      "year": "2023",
      "venue": "Thirtyseventh Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b25",
      "title": "An integrated encyclopedia of dna elements in the human genome",
      "authors": [
        "Ian Dunham",
        "Anshul Kundaje",
        "Shelley F Aldred",
        "Patrick J Collins",
        "Carrie A Davis",
        "Francis Doyle",
        "Charles B Epstein",
        "Seth Frietze",
        "Jennifer Harrow",
        "Rajinder Kaul"
      ],
      "year": "2012",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b26",
      "title": "Protgpt2 is a deep unsupervised language model for protein design",
      "authors": [
        "Noelia Ferruz",
        "Steffen Schmidt",
        "Birte HÃ¶cker"
      ],
      "year": "2022",
      "venue": "Nature communications",
      "doi": ""
    },
    {
      "id": "b27",
      "title": "Gena-lm: A family of open-source foundational models for long dna sequences",
      "authors": [
        "Veniamin Fishman",
        "Yuri Kuratov",
        "Maxim Petrov",
        "Aleksei Shmelev",
        "Denis Shepelin",
        "Nikolay Chekanov",
        "Olga Kardymon",
        "Mikhail Burtsev"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b28",
      "title": "The pile: An 800gb dataset of diverse text for language modeling",
      "authors": [
        "Leo Gao",
        "Stella Biderman",
        "Sid Black",
        "Laurence Golding",
        "Travis Hoppe",
        "Charles Foster",
        "Jason Phang",
        "Horace He",
        "Anish Thite",
        "Noa Nabeshima"
      ],
      "year": "2020",
      "venue": "The pile: An 800gb dataset of diverse text for language modeling",
      "doi": ""
    },
    {
      "id": "b29",
      "title": "A pre-training and self-training approach for biomedical named entity recognition",
      "authors": [
        "Shang Gao",
        "Olivera Kotevska",
        "Alexandre Sorokine",
        "James Blair",
        "Christian"
      ],
      "year": "2021",
      "venue": "PloS one",
      "doi": ""
    },
    {
      "id": "b30",
      "title": "Contemporary progress and opportunities in RNA-targeted drug discovery",
      "authors": [
        "Amanda L Garner"
      ],
      "year": "2023",
      "venue": "ACS Med. Chem. Lett",
      "doi": ""
    },
    {
      "id": "b31",
      "title": "A large and rich eeg dataset for modeling human visual object recognition",
      "authors": [
        "Alessandro T Gifford",
        "Kshitij Dwivedi",
        "Gemma Roig",
        "Radoslaw M Cichy"
      ],
      "year": "2022",
      "venue": "NeuroImage",
      "doi": ""
    },
    {
      "id": "b32",
      "title": "Domain-specific language model pretraining for biomedical natural language processing",
      "authors": [
        "Yu Gu",
        "Robert Tinn",
        "Hao Cheng",
        "Michael Lucas",
        "Naoto Usuyama",
        "Xiaodong Liu",
        "Tristan Naumann",
        "Jianfeng Gao",
        "Hoifung Poon"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Computing for Healthcare (HEALTH)",
      "doi": ""
    },
    {
      "id": "b33",
      "title": "Large scale foundation model on single-cell transcriptomics",
      "authors": [
        "Minsheng Hao",
        "Jing Gong",
        "Xin Zeng",
        "Chiming Liu",
        "Yucheng Guo",
        "Xingyi Cheng",
        "Taifeng Wang",
        "Jianzhu Ma",
        "Le Song",
        "Xuegong Zhang"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b34",
      "title": "Brain-to-text: decoding spoken phrases from phone representations in the brain",
      "authors": [
        "Christian Herff",
        "Dominic Heger",
        "Adriana De Pesters",
        "Dominic Telaar",
        "Peter Brunner",
        "Gerwin Schalk",
        "Tanja Schultz"
      ],
      "year": "2015",
      "venue": "Frontiers in neuroscience",
      "doi": ""
    },
    {
      "id": "b35",
      "title": "Graphclust: alignment-free structural clustering of local rna secondary structures",
      "authors": [
        "Steffen Heyne",
        "Fabrizio Costa",
        "Dominic Rose",
        "Rolf Backofen"
      ],
      "year": "2012",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b36",
      "title": "Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading",
      "authors": [
        "Nora Hollenstein",
        "Jonathan Rotsztejn",
        "Marius Troendle",
        "Andreas Pedroni",
        "Ce Zhang",
        "Nicolas Langer"
      ],
      "year": "2018",
      "venue": "Scientific data",
      "doi": ""
    },
    {
      "id": "b37",
      "title": "Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome",
      "authors": [
        "Yanrong Ji",
        "Zhihan Zhou",
        "Han Liu",
        "Ramana V Davuluri"
      ],
      "year": "2021",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b38",
      "title": "Large brain model for learning generic representations with tremendous eeg data in bci",
      "authors": [
        "Wei-Bang Jiang",
        "Li-Ming Zhao",
        "Lu Bao-Liang"
      ],
      "year": "2024",
      "venue": "The Thriteenth International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b39",
      "title": "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
      "authors": [
        "Di Jin",
        "Eileen Pan",
        "Nassim Oufattole",
        "Wei-Hung Weng",
        "Hanyi Fang",
        "Peter Szolovits"
      ],
      "year": "2020",
      "venue": "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
      "doi": ""
    },
    {
      "id": "b40",
      "title": "Pubmedqa: A dataset for biomedical research question answering",
      "authors": [
        "Qiao Jin",
        "Bhuwan Dhingra",
        "Zhengping Liu",
        "William Cohen",
        "Xinghua Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": ""
    },
    {
      "id": "b41",
      "title": "The mimic code repository: enabling reproducibility in critical care research",
      "authors": [
        "E W Alistair",
        "David J Johnson",
        "Leo A Stone",
        "Tom J Celi",
        "Pollard"
      ],
      "year": "2018",
      "venue": "Journal of the American Medical Informatics Association",
      "doi": ""
    },
    {
      "id": "b42",
      "title": "",
      "authors": [
        "E W Alistair",
        "Tom J Johnson",
        "Lu Pollard",
        "Li-Wei H Shen",
        "Mengling Lehman",
        "Mohammad Feng",
        "Benjamin Ghassemi",
        "Peter Moody",
        "Leo Szolovits",
        "Roger G Anthony Celi",
        "Mark"
      ],
      "year": "2016",
      "venue": "Mimic-iii, a freely accessible critical care database. Scientific data",
      "doi": ""
    },
    {
      "id": "b43",
      "title": "Motor activity recognition using eeg data and ensemble of stacked BLSTM-LSTM network and transformer model",
      "authors": [
        "Pallavi Kaushik",
        "Ilina Tripathi",
        "Partha Pratim Roy"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023",
      "doi": ""
    },
    {
      "id": "b44",
      "title": "Cross-species regulatory sequence activity prediction",
      "authors": [
        "David R Kelley"
      ],
      "year": "2020",
      "venue": "PLoS computational biology",
      "doi": ""
    },
    {
      "id": "b45",
      "title": "Konstantinos Bougiatiotis, and Georgios Paliouras. Bioasq-qa: A manually curated corpus for biomedical question answering",
      "authors": [
        "Anastasia Krithara",
        "Anastasios Nentidis"
      ],
      "year": "2023",
      "venue": "Scientific Data",
      "doi": ""
    },
    {
      "id": "b46",
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "authors": [
        "Taku Kudo",
        "John Richardson"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": ""
    },
    {
      "id": "b47",
      "title": "ClinVar: improvements to accessing data",
      "authors": [
        "Melissa J Landrum",
        "Shanmuga Chitipiralla",
        "Garth R Brown",
        "Chao Chen",
        "Baoshan Gu",
        "Jennifer Hart",
        "Douglas Hoffman",
        "Wonhee Jang",
        "Kuljeet Kaur",
        "Chunlei Liu",
        "Vitaly Lyoshin",
        "Zenith Maddipatla",
        "Rama Maiti",
        "Joseph Mitchell",
        "O' Nuala",
        "George R Leary",
        "Wenyao Riley",
        "George Shi",
        "Valerie Zhou",
        "Donna Schneider",
        "Bradley Maglott",
        "Brandi L Holmes",
        "Kattman"
      ],
      "year": "",
      "venue": "Nucleic Acids Research",
      "doi": ""
    },
    {
      "id": "b48",
      "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho",
        "So",
        "Jaewoo Kang"
      ],
      "year": "2020",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b49",
      "title": "Towards voice reconstruction from EEG during imagined speech",
      "authors": [
        "Young-Eun Lee",
        "Seo-Hyun Lee",
        "Sang-Ho Kim",
        "Seong-Whan Lee"
      ],
      "year": "2023",
      "venue": "Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023",
      "doi": ""
    },
    {
      "id": "b50",
      "title": "",
      "authors": [
        "Rasko Leinonen",
        "Federico Garcia Diez",
        "David Binns",
        "Wolfgang Fleischmann",
        "Rodrigo Lopez",
        "Rolf Apweiler"
      ],
      "year": "2004",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b51",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Ves Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Denoising sequence-to-sequence pre-training for natural language generation",
      "doi": ""
    },
    {
      "id": "b52",
      "title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction",
      "authors": [
        "Jiao Li",
        "Yueping Sun",
        "Robin J Johnson",
        "Daniela Sciaky",
        "Chih-Hsuan Wei",
        "Robert Leaman",
        "Allan Peter Davis",
        "Carolyn J Mattingly",
        "Thomas C Wiegers",
        "Zhiyong Lu"
      ],
      "year": "2016",
      "venue": "Database",
      "doi": ""
    },
    {
      "id": "b53",
      "title": "Codonbert: Large language models for mrna design and optimization",
      "authors": [
        "Sizhen Li",
        "Saeed Moayedpour",
        "Ruijiang Li",
        "Michael Bailey",
        "Saleh Riahi",
        "Lorenzo Kogler-Anele",
        "Milad Miladi",
        "Jacob Miner",
        "Dinghai Zheng",
        "Jun Wang",
        "Akshay Balsubramani",
        "Khang Tran",
        "Minnie Zacharia",
        "Monica Wu",
        "Xiaobo Gu",
        "Ryan Clinton",
        "Carla Asquith",
        "Joseph Skaleski",
        "Lianne Boeglin",
        "Sudha Chivukula",
        "Anusha Dias",
        "Fernando Ulloa Montoya",
        "Vikram Agarwal",
        "Ziv Bar-Joseph",
        "Sven Jager"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b54",
      "title": "Evolutionaryscale prediction of atomic-level protein structure with a language model",
      "authors": [
        "Zeming Lin",
        "Halil Akin",
        "Roshan Rao",
        "Brian Hie",
        "Zhongkai Zhu",
        "Wenting Lu",
        "Nikita Smetanin",
        "Robert Verkuil",
        "Ori Kabeli",
        "Yaniv Shmueli"
      ],
      "year": "2023",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b55",
      "title": "Compositional zero-shot domain transfer with text-to-text models",
      "authors": [
        "Fangyu Liu",
        "Qianchu Liu",
        "Shruthi Bannur",
        "Fernando PÃ©rez-GarcÃ­a",
        "Naoto Usuyama",
        "Sheng Zhang",
        "Tristan Naumann",
        "Aditya Nori",
        "Hoifung Poon",
        "Javier Alvarez-Valle"
      ],
      "year": "2023",
      "venue": "Compositional zero-shot domain transfer with text-to-text models",
      "doi": ""
    },
    {
      "id": "b56",
      "title": "BioRED: a rich biomedical relation extraction dataset",
      "authors": [
        "Ling Luo",
        "Po-Ting Lai",
        "Chih-Hsuan Wei",
        "Cecilia N Arighi",
        "Zhiyong Lu"
      ],
      "year": "",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b57",
      "title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
      "authors": [
        "Renqian Luo",
        "Liai Sun",
        "Yingce Xia",
        "Tao Qin",
        "Sheng Zhang",
        "Hoifung Poon",
        "Tie-Yan Liu"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics",
      "doi": ""
    },
    {
      "id": "b58",
      "title": "Large language models generate functional protein sequences across diverse families",
      "authors": [
        "Ali Madani",
        "Ben Krause",
        "Eric R Greene",
        "Subu Subramanian",
        "Benjamin P Mohr",
        "James M Holton",
        "Jose Luis Olmos",
        "Caiming Xiong",
        "Zachary Z Sun",
        "Richard Socher"
      ],
      "year": "2023",
      "venue": "Nature Biotechnology",
      "doi": ""
    },
    {
      "id": "b59",
      "title": "Machine translation of cortical activity to text with an encoder-decoder framework",
      "authors": [
        "David A Joseph G Makin",
        "Edward F Moses",
        "Chang"
      ],
      "year": "2020",
      "venue": "Nature neuroscience",
      "doi": ""
    },
    {
      "id": "b60",
      "title": "Neurogan: image reconstruction from EEG signals via an attention-based GAN",
      "authors": [
        "Rahul Mishra",
        "Krishan Sharma",
        "Ranjeet Ranjan Jha",
        "Arnav Bhavsar"
      ],
      "year": "2023",
      "venue": "Neural Comput. Appl",
      "doi": ""
    },
    {
      "id": "b61",
      "title": "Neuroprosthesis for decoding speech in a paralyzed person with anarthria",
      "authors": [
        "Sean L David A Moses",
        "Jessie R Metzger",
        "Gopala K Liu",
        "Anumanchipalli",
        "Joseph G Makin",
        "F Pengfei",
        "Josh Sun",
        "Maximilian E Chartier",
        "Patricia M Dougherty",
        "Gary M Liu",
        "Abrams"
      ],
      "year": "2021",
      "venue": "New England Journal of Medicine",
      "doi": ""
    },
    {
      "id": "b62",
      "title": "Long-range genomic sequence modeling at single nucleotide resolution",
      "authors": [
        "Eric Nguyen",
        "Michael Poli",
        "Marjan Faizi",
        "Armin Thomas",
        "Callum Birch-Sykes",
        "Michael Wornow",
        "Aman Patel",
        "Clayton Rabideau",
        "Stefano Massaroli",
        "Yoshua Bengio"
      ],
      "year": "2023",
      "venue": "Long-range genomic sequence modeling at single nucleotide resolution",
      "doi": ""
    },
    {
      "id": "b63",
      "title": "Thinking out loud, an open-access eeg-based bci dataset for inner speech recognition",
      "authors": [
        "NicolÃ¡s Nieto",
        "Victoria Peterson",
        "Leonardo Hugo",
        "Juan Esteban Rufiner",
        "Ruben Kamienkowski",
        "Spies"
      ],
      "year": "2022",
      "venue": "Scientific Data",
      "doi": ""
    },
    {
      "id": "b64",
      "title": "Progen2: exploring the boundaries of protein language models",
      "authors": [
        "Erik Nijkamp",
        "Jeffrey A Ruffolo",
        "Eli N Weinstein",
        "Nikhil Naik",
        "Ali Madani"
      ],
      "year": "2023",
      "venue": "Cell systems",
      "doi": ""
    },
    {
      "id": "b65",
      "title": "The complete sequence of a human genome",
      "authors": [
        "Sergey Nurk",
        "Sergey Koren",
        "Arang Rhie",
        "Mikko Rautiainen",
        "Andrey V Bzikadze",
        "Alla Mikheenko",
        "Mitchell R Vollger",
        "Nicolas Altemose",
        "Lev Uralsky",
        "Ariel Gershman"
      ],
      "year": "2022",
      "venue": "Science",
      "doi": ""
    },
    {
      "id": "b66",
      "title": "Decoding covert speech from eeg-a comprehensive review",
      "authors": [
        "Jerrin Thomas",
        "Panachakel",
        "Angarai Ganesan Ramakrishnan"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience",
      "doi": ""
    },
    {
      "id": "b67",
      "title": "A study of generative large language model for medical research and healthcare",
      "authors": [
        "Cheng Peng",
        "Xi Yang",
        "Aokun Chen",
        "Kaleb E Smith",
        "Nima Pournejatian",
        "Anthony B Costa",
        "Cheryl Martin",
        "Mona G Flores",
        "Ying Zhang",
        "Tanja Magoc"
      ],
      "year": "2023",
      "venue": "A study of generative large language model for medical research and healthcare",
      "doi": ""
    },
    {
      "id": "b68",
      "title": "Erol Bahadroglu, Alec Peltekian, and GrÃ©goire Altan-Bonnet. Scifive: a text-to-text transformer model for biomedical literature",
      "authors": [
        "James T Long N Phan",
        "Hieu Anibal",
        "Shaurya Tran",
        "Chanana"
      ],
      "year": "2021",
      "venue": "Erol Bahadroglu, Alec Peltekian, and GrÃ©goire Altan-Bonnet. Scifive: a text-to-text transformer model for biomedical literature",
      "doi": ""
    },
    {
      "id": "b69",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog",
      "doi": ""
    },
    {
      "id": "b70",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": ""
    },
    {
      "id": "b71",
      "title": "Ai in health and medicine",
      "authors": [
        "Pranav Rajpurkar",
        "Ewen Chen",
        "Oishi Banerjee",
        "Eric J Topol"
      ],
      "year": "2022",
      "venue": "Nat Med",
      "doi": ""
    },
    {
      "id": "b72",
      "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "authors": [
        "Alexander Rives",
        "Joshua Meier",
        "Tom Sercu",
        "Siddharth Goyal",
        "Zeming Lin",
        "Jason Liu",
        "Demi Guo",
        "Myle Ott",
        "C Lawrence Zitnick",
        "Jerry Ma"
      ],
      "year": "2021",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": ""
    },
    {
      "id": "b73",
      "title": "Fast and accurate clustering of noncoding rnas using ensembles of sequence alignments and secondary structures",
      "authors": [
        "Yutaka Saito",
        "Kengo Sato",
        "Yasubumi Sakakibara"
      ],
      "year": "2011",
      "venue": "BMC bioinformatics",
      "doi": ""
    },
    {
      "id": "b74",
      "title": "In-domain contextaware token embeddings improve biomedical named entity recognition",
      "authors": [
        "Golnar Sheikhshab",
        "InanÃ§ Birol",
        "Anoop Sarkar"
      ],
      "year": "2018",
      "venue": "Louhi@EMNLP",
      "doi": ""
    },
    {
      "id": "b75",
      "title": "Biomegatron: Larger biomedical domain language model",
      "authors": [
        "Hoo-Chang Shin",
        "Yang Zhang",
        "Evelina Bakhturina",
        "Raul Puri",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Raghav Mani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": ""
    },
    {
      "id": "b76",
      "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "authors": [
        "Mohammad Shoeybi",
        "Mostofa Patwary",
        "Raul Puri",
        "Patrick Legresley",
        "Jared Casper",
        "Bryan Catanzaro"
      ],
      "year": "2019",
      "venue": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
      "doi": ""
    },
    {
      "id": "b77",
      "title": "EEG2IMAGE: image reconstruction from EEG brain signals",
      "authors": [
        "Prajwal Singh",
        "Pankaj Pandey",
        "Krishna P Miyapuram",
        "Shanmuganathan Raman"
      ],
      "year": "2023",
      "venue": "EEG2IMAGE: image reconstruction from EEG brain signals",
      "doi": ""
    },
    {
      "id": "b78",
      "title": "Large language models encode clinical knowledge",
      "authors": [
        "Karan Singhal",
        "Shekoofeh Azizi",
        "Tao Tu",
        "Sara Mahdavi",
        "Jason Wei",
        "Hyung Won Chung",
        "Nathan Scales",
        "Ajay Tanwani",
        "Heather Cole-Lewis",
        "Stephen Pfohl"
      ],
      "year": "2022",
      "venue": "Large language models encode clinical knowledge",
      "doi": ""
    },
    {
      "id": "b79",
      "title": "Towards expert-level medical question answering with large language models",
      "authors": [
        "Karan Singhal",
        "Tao Tu",
        "Juraj Gottweis",
        "Rory Sayres",
        "Ellery Wulczyn",
        "Le Hou",
        "Kevin Clark",
        "Stephen Pfohl",
        "Heather Cole-Lewis",
        "Darlene Neal"
      ],
      "year": "2023",
      "venue": "Towards expert-level medical question answering with large language models",
      "doi": ""
    },
    {
      "id": "b80",
      "title": "Transfer learning in electronic health records through clinical concept embedding",
      "authors": [
        "Jose Roberto",
        "Ayala Solares",
        "Yajie Zhu",
        "Abdelaali Hassaine",
        "Shishir Rao",
        "Yikuan Li",
        "Mohammad Mamouei",
        "Dexter Canoy",
        "Kazem Rahimi",
        "Gholamreza Salimi-Khorshidi"
      ],
      "year": "2021",
      "venue": "Transfer learning in electronic health records through clinical concept embedding",
      "doi": ""
    },
    {
      "id": "b81",
      "title": "Decoding natural images from EEG for object recognition",
      "authors": [
        "Yonghao Song",
        "Bingchuan Liu",
        "Xiang Li",
        "Nanlin Shi",
        "Yijun Wang",
        "Xiaorong Gao"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b82",
      "title": "Towards sentence-level brain decoding with distributed representations",
      "authors": [
        "Jingyuan Sun",
        "Shaonan Wang",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b83",
      "title": "Uniref: comprehensive and non-redundant uniprot reference clusters",
      "authors": [
        "Hongzhan Baris E Suzek",
        "Peter Huang",
        "Raja Mcgarvey",
        "Cathy H Mazumder",
        "Wu"
      ],
      "year": "2007",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b84",
      "title": "Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches",
      "authors": [
        "Yuqi Baris E Suzek",
        "Hongzhan Wang",
        "Peter B Huang",
        "Cathy H Mcgarvey",
        "Uniprot Wu",
        "Consortium"
      ],
      "year": "2015",
      "venue": "Bioinformatics",
      "doi": ""
    },
    {
      "id": "b85",
      "title": "Semantic reconstruction of continuous language from non-invasive brain recordings",
      "authors": [
        "Jerry Tang",
        "Amanda Lebel",
        "Shailee Jain",
        "Alexander G Huth"
      ],
      "year": "2023",
      "venue": "Nature Neuroscience",
      "doi": ""
    },
    {
      "id": "b86",
      "title": "Medagents: Large language models as collaborators for zero-shot medical reasoning",
      "authors": [
        "Xiangru Tang",
        "Anni Zou",
        "Zhuosheng Zhang",
        "Ziming Li",
        "Yilun Zhao",
        "Xingyao Zhang",
        "Arman Cohan",
        "Mark Gerstein"
      ],
      "year": "2024",
      "venue": "Medagents: Large language models as collaborators for zero-shot medical reasoning",
      "doi": ""
    },
    {
      "id": "b87",
      "title": "Galactica: A large language model for science",
      "authors": [
        "Ross Taylor",
        "Marcin Kardas",
        "Guillem Cucurull",
        "Thomas Scialom",
        "Anthony Hartshorn",
        "Elvis Saravia",
        "Andrew Poulton",
        "Viktor Kerkez",
        "Robert Stojnic"
      ],
      "year": "2022",
      "venue": "Galactica: A large language model for science",
      "doi": ""
    },
    {
      "id": "b88",
      "title": "Transfer learning enables predictions in network biology",
      "authors": [
        "Christina V Theodoris",
        "Ling Xiao",
        "Anant Chopra",
        "Zeina R Al Mark D Chaffin",
        "Matthew C Sayed",
        "Helene Hill",
        "Elizabeth M Mantineo",
        "Zexian Brydon",
        "X Zeng",
        "Shirley Liu"
      ],
      "year": "2023",
      "venue": "Nature",
      "doi": ""
    },
    {
      "id": "b89",
      "title": "Large language models in medicine",
      "authors": [
        "J Ajan",
        "Thirunavukarasu",
        "S J Daniel",
        "Karthikeyan Ting",
        "Livia Elangovan",
        "Tien F Gutierrez",
        "Tan",
        "S W Daniel",
        "Ting"
      ],
      "year": "2023",
      "venue": "Nat Med",
      "doi": ""
    },
    {
      "id": "b90",
      "title": "Pre-trained language models in biomedical domain: A systematic survey",
      "authors": [
        "Benyou Wang",
        "Qianqian Xie",
        "Jiahuan Pei",
        "Zhihong Chen",
        "Prayag Tiwari",
        "Zhao Li",
        "Jie Fu"
      ],
      "year": "2021",
      "venue": "Pre-trained language models in biomedical domain: A systematic survey",
      "doi": ""
    },
    {
      "id": "b91",
      "title": "Brainbert: Self-supervised representation learning for intracranial recordings",
      "authors": [
        "Christopher Wang",
        "Vighnesh Subramaniam",
        "Uri Adam",
        "Gabriel Yaari",
        "Boris Kreiman",
        "Ignacio Katz",
        "Andrei Cases",
        "Barbu"
      ],
      "year": "2022",
      "venue": "The Eleventh International Conference on Learning Representations",
      "doi": ""
    },
    {
      "id": "b92",
      "title": "Chatcad: Interactive computer-aided diagnosis on medical image using large language models",
      "authors": [
        "Sheng Wang",
        "Zihao Zhao",
        "Xi Ouyang",
        "Qian Wang",
        "Dinggang Shen"
      ],
      "year": "2023",
      "venue": "Chatcad: Interactive computer-aided diagnosis on medical image using large language models",
      "doi": ""
    },
    {
      "id": "b93",
      "title": "Reactclass: Cross-modal supervision for subword-guided reactant entity classification",
      "authors": [
        "Xuan Wang",
        "Vivian Hu",
        "Minhao Jiang",
        "Yu Zhang",
        "Jinfeng Xiao",
        "Danielle Cherrice Loving",
        "Heng Ji",
        "Martin Burke",
        "Jiawei Han"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": ""
    },
    {
      "id": "b94",
      "title": "Chemner: fine-grained chemistry named entity recognition with ontologyguided distant supervision",
      "authors": [
        "Xuan Wang",
        "Vivian Hu",
        "Xiangchen Song",
        "Shweta Garg",
        "Jinfeng Xiao",
        "Jiawei Han"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": ""
    },
    {
      "id": "b95",
      "title": "Open vocabulary electroencephalography-totext decoding and zero-shot sentiment classification",
      "authors": [
        "Zhenhailong Wang",
        "Heng Ji"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": ""
    },
    {
      "id": "b96",
      "title": "Can gpt-4v(ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "authors": [
        "Chaoyi Wu",
        "Jiayu Lei",
        "Qiaoyu Zheng",
        "Weike Zhao",
        "Weixiong Lin",
        "Xiaoman Zhang",
        "Xiao Zhou",
        "Ziheng Zhao",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "year": "2023",
      "venue": "Can gpt-4v(ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis",
      "doi": ""
    },
    {
      "id": "b97",
      "title": "Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data",
      "authors": [
        "Chaoyi Wu",
        "Xiaoman Zhang",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
      ],
      "year": "2023",
      "venue": "Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data",
      "doi": ""
    },
    {
      "id": "b98",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Yonghui Wu",
        "Mike Schuster",
        "Zhifeng Chen",
        "V Quoc",
        "Mohammad Le",
        "Wolfgang Norouzi",
        "Maxim Macherey",
        "Yuan Krikun",
        "Qin Cao",
        "Klaus Gao",
        "Macherey"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "doi": ""
    },
    {
      "id": "b99",
      "title": "The rise and potential of large language model based agents: A survey",
      "authors": [
        "Zhiheng Xi",
        "Wenxiang Chen",
        "Xin Guo",
        "Wei He",
        "Yiwen Ding",
        "Boyang Hong",
        "Ming Zhang",
        "Junzhe Wang",
        "Senjie Jin",
        "Enyu Zhou",
        "Rui Zheng",
        "Xiaoran Fan",
        "Xiao Wang",
        "Limao Xiong",
        "Yuhao Zhou",
        "Weiran Wang",
        "Changhao Jiang",
        "Yicheng Zou",
        "Xiangyang Liu",
        "Zhangyue Yin",
        "Shihan Dou",
        "Rongxiang Weng",
        "Wensen Cheng",
        "Qi Zhang",
        "Wenjuan Qin",
        "Yongyan Zheng",
        "Xipeng Qiu",
        "Xuanjing Huang",
        "Tao Gui"
      ],
      "year": "2023",
      "venue": "The rise and potential of large language model based agents: A survey",
      "doi": ""
    },
    {
      "id": "b100",
      "title": "Protst: Multimodality learning of protein sequences and biomedical texts",
      "authors": [
        "Minghao Xu",
        "Xinyu Yuan",
        "Santiago Miret",
        "Jian Tang"
      ],
      "year": "2023",
      "venue": "Protst: Multimodality learning of protein sequences and biomedical texts",
      "doi": ""
    },
    {
      "id": "b101",
      "title": "Prediction of rna-protein interactions using a nucleotide language model",
      "authors": [
        "Keisuke Yamada",
        "Michiaki Hamada"
      ],
      "year": "2022",
      "venue": "Bioinformatics Advances",
      "doi": ""
    },
    {
      "id": "b102",
      "title": "scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data",
      "authors": [
        "Fan Yang",
        "Wenchuan Wang",
        "Fang Wang",
        "Yuan Fang",
        "Duyu Tang",
        "Junzhou Huang",
        "Hui Lu",
        "Jianhua Yao"
      ],
      "year": "2022",
      "venue": "Nature Machine Intelligence",
      "doi": ""
    },
    {
      "id": "b103",
      "title": "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "authors": [
        "Jingfeng Yang",
        "Hongye Jin",
        "Ruixiang Tang",
        "Xiaotian Han",
        "Qizhang Feng",
        "Haoming Jiang",
        "Bing Yin",
        "Xia Hu"
      ],
      "year": "2023",
      "venue": "Harnessing the power of llms in practice: A survey on chatgpt and beyond",
      "doi": ""
    },
    {
      "id": "b104",
      "title": "A large language model for electronic health records",
      "authors": [
        "Xi Yang",
        "Aokun Chen",
        "Nima Pournejatian",
        "Chang Hoo",
        "Kaleb E Shin",
        "Christopher Smith",
        "Colin Parisien",
        "Cheryl Compas",
        "Anthony B Martin",
        "Mona G Costa",
        "Flores"
      ],
      "year": "2022",
      "venue": "NPJ digital medicine",
      "doi": ""
    },
    {
      "id": "b105",
      "title": "Linkbert: Pretraining language models with document links",
      "authors": [
        "Michihiro Yasunaga",
        "Jure Leskovec",
        "Percy Liang"
      ],
      "year": "",
      "venue": "Association for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b106",
      "title": "Learning topology-agnostic eeg representations with geometry-aware modeling",
      "authors": [
        "Ke Yi",
        "Yansen Wang",
        "Kan Ren",
        "Dongsheng Li"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b107",
      "title": "Artificial intelligence for science in quantum, atomistic, and continuum systems",
      "authors": [
        "Xuan Zhang",
        "Limei Wang",
        "Jacob Helwig",
        "Youzhi Luo",
        "Cong Fu",
        "Yaochen Xie",
        "Meng Liu",
        "Yuchao Lin",
        "Zhao Xu",
        "Keqiang Yan"
      ],
      "year": "2023",
      "venue": "Artificial intelligence for science in quantum, atomistic, and continuum systems",
      "doi": ""
    },
    {
      "id": "b108",
      "title": "Multiple sequencealignment-based rna language model and its application to structural inference",
      "authors": [
        "Yikun Zhang",
        "Mei Lang",
        "Jiuhong Jiang",
        "Zhiqiang Gao",
        "Fan Xu",
        "Thomas Litfin",
        "Ke Chen",
        "Jaswinder Singh",
        "Xiansong Huang",
        "Guoli Song"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b109",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics",
      "doi": ""
    },
    {
      "id": "b110",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development",
      "doi": ""
    },
    {
      "id": "b111",
      "title": "Structure-informed language models are protein designers",
      "authors": [
        "Zaixiang Zheng",
        "Yifan Deng",
        "Dongyu Xue",
        "Yi Zhou",
        "Fei Ye",
        "Quanquan Gu"
      ],
      "year": "2023",
      "venue": "bioRxiv",
      "doi": ""
    },
    {
      "id": "b112",
      "title": "Reactie: Enhancing chemical reaction extraction with weak supervision",
      "authors": [
        "Ming Zhong",
        "Siru Ouyang",
        "Minhao Jiang",
        "Vivian Hu",
        "Yizhu Jiao",
        "Xuan Wang",
        "Jiawei Han"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": ""
    },
    {
      "id": "b113",
      "title": "Predicting effects of noncoding variants with deep learning-based sequence model",
      "authors": [
        "Jian Zhou",
        "Olga G Troyanskaya"
      ],
      "year": "2015",
      "venue": "Nat. Methods",
      "doi": ""
    },
    {
      "id": "b114",
      "title": "Clinical concept extraction with contextual word embedding",
      "authors": [
        "Henghui Zhu",
        "Ioannis Ch",
        "Amir M Paschalidis",
        "Tahmasebi"
      ],
      "year": "2018",
      "venue": "Clinical concept extraction with contextual word embedding",
      "doi": ""
    },
    {
      "id": "b115",
      "title": "Genome-scale language models reveal sars-cov-2 evolutionary dynamics",
      "authors": [
        "Maxim Zvyagin",
        "Alexander Brace",
        "Kyle Hippe",
        "Yuntian Deng",
        "Bin Zhang",
        "Cindy Orozco Bohorquez",
        "Austin Clyde",
        "Bharat Kale",
        "Danilo Perez-Rivera",
        "Heng Ma"
      ],
      "year": "2022",
      "venue": "bioRxiv",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Rna Sequences",
      "text": "Ribonucleic acid (RNA) is another critical component of molecular biology, playing various essential roles in fundamental biological processes. In recent years, RNA has emerged as an intriguing target for drug development (Kumar et al., 2018), underscoring the importance of enhancing our understanding of its structures and functions. We embrace the innovative contributions highlighted in foundation models such as RNABERT (Kumar et al., 2018), RNA-FM (Kumar et al., 2018), and RNA-MSM (Kumar et al., 2018), which are pivotal in elucidating the intricate mechanisms governing biological processes mediated by RNA. These advancements empower us to decipher the complex orchestration of biological processes by RNA molecules. RNABERTRNABERT (Kumar et al., 2018) proposes an effective method for embedding RNA bases by applying the pre-training approach of BERT to non-coding RNA (ncRNA). The research aims to develop a robust RNA base embedding technique for tasks like RNA structural alignment and clustering using deep representation learning. By leveraging pre-training algorithms, the researchers aim to create semantically rich representations of RNA bases to improve the accuracy of structural alignment and clustering tasks. This method of informative base embedding incorporates contextual information and secondary structure details of RNA sequences, resulting in superior accuracies compared to existing methods such as GraphClust (Kumar et al., 2019), EnsembleClust (Kumar et al., 2019), and CNNclust (Cheng et al., 2019). The approach combines informative base embedding with a simple Needleman-Wunsch alignment algorithm to calculate structural alignments with improved time complexity, making it a significant advancement in RNA sequence analysis. RNA-FMRNA-FM(Kumar et al., 2018) is a novel interpretable computational model proposed to predict RNA structure and function accurately using unannotated data. It leverages self-supervised learning on 23 million non-coding RNA sequences to infer sequential and evolutionary information without labels. It has shown effectiveness in predicting secondary/3D structures, SARS-CoV-2 genome structure, protein-RNA binding preferences, and gene expression regulation. Despite being trained on unlabelled data, RNA-FM significantly improves RNA structural and functional modeling results, making it a foundational model for the field. RNA-MsmRNA-MSM(Kumar et al., 2018) is an RNA language model that leverages homologous RNA sequences to interpret and predict RNA structures more effectively than previous methods. It is distinguished by its use of unsupervised learning on multiple sequence alignments (MSAs) derived from RNAcmap, a tool that outperforms traditional methods like Rfam in identifying homologous sequences due to its comprehensive and automatic pipeline. This approach allows RNA-MSM to capture evolutionary information and structural nuances of RNA sequences, which are less conserved and thus more challenging to analyze compared to proteins. The model generates two-dimensional attention maps and one-dimensional embeddings that contain rich structural information. These outputs can be directly correlated with RNA structural features, such as 2D base pairing probabilities and 1D solvent accessibilities, with high accuracy. The study demonstrates that RNA-MSM significantly outperforms existing techniques, including SPOT-RNA2 and RNA-nap2, in predicting these structural features after fine-tuning. This research underscores the potential of RNA-MSM to revolutionize RNA structure prediction and functional analysis by incorporating evolutionary and structural insights from homologous sequences. ApplicationsRNA language models have a wide range of applications in RNA research and biotechnology. These models utilize evolutionary information from homologous sequences to accurately interpret RNA sequences, making them valuable in tasks such as RNA structure (Kumar et al., 2018), function (Kumar et al., 2018), and RNA-protein interaction prediction (Kumar et al., 2018). They can predict crucial aspects of RNA structures, such as 2D base pairing probabilities and 1D solvent accessibilities (Kumar et al., 2018), which are essential for understanding RNA function and cellular interactions. Additionally, RNA language models facilitate the design of RNA-based therapeutics (Kumar et al., 2019). Understanding RNA structures helps identify novel drug targets and design RNA-based drugs, speeding up drug discovery processes. These pre-trained models can be fine-tuned for various downstream tasks related to RNA structure and function, including RNA 2D/3D structure prediction (Kumar et al., 2018), RNA structural alignment and family clustering (Kumar et al., 2018), and RNA splice site prediction (Kumar et al., 2018) from RNA sequences."
    },
    {
      "title": "Protein Sequences",
      "text": "Venturing into the complex realm of proteins, we embrace luminous works such as (Kumar et al., 2018), (Kumar et al., 2018), (Kumar et al., 2018; Kumar et al., 2018), (Kumar et al., 2018), and (Kumar et al., 2018). These endeavors illuminate the path to unraveling the intricate choreography of molecular functions and interactions. The protein LLMs have a wide application in functional protein generation (Kumar et al., 2018) and protein structure prediction (Kumar et al., 2018). Protein language models can be broadly classified into three main categories based on their functionalities: 1) structure and function understanding: interpreting protein sequences to reveal their structures and functions through evolutionary data or textual enhancements, 2) protein sequence generation: creating novel protein sequences using generative modeling for unexplored spaces or precise design, and 3) multi-modal integration for enhanced understanding and generation: combining various data types to generate protein functions or achieve a unified understanding and generation of protein sequences. _Structure and Function Understanding models._ These models are primarily concerned with interpreting existing protein sequences to deduce their structures, functions, or both, using vast datasets of known proteins. ProteinBERT (Krishnan et al., 2017) is a deep-learning model designed for understanding protein functions and sequences. It combines language modeling with Gene Ontology (GO) (Birn et al., 2017) annotation predictions in a self-supervised training approach. Trained on over 106 million proteins from the UniProtKB/UniRef90 database (Krishnan et al., 2017; Krishnan et al., 2018), it efficiently captures detailed and broad protein features using a transformer-like architecture. ESM models are noted for their ability to capture deep evolutionary signals and predict protein structures with high accuracy, contributing significantly to structural biology. ESM-1b (Krishnan et al., 2017), utilizing a Transformer architecture, harnesses unsupervised learning from 250 million protein sequences to predict biological properties. It offers a versatile and comprehensive approach to protein analysis, with applications ranging from structure prediction to functional annotation, by capturing deep evolutionary signals across various biological research tasks. ESM-2 (Krishnan et al., 2017) significantly advances over ESM-1b by utilizing a scaled-up model with 15 billion parameters (compared to 650 million) and a richer UniRef90 dataset to predict atomic-level protein structures directly from sequences. This enhancement not only allows for high-resolution structure predictions and the construction of the ESM Metagenomic Atlas but also achieves a notable speed improvement and wider applicability in structural genomics and protein design, thus enhancing diversity and accuracy in structural predictions. ProtST (Krishnan et al., 2017) enhances protein language models by merging protein sequences with biomedical texts from the ProtDescribe dataset, sourced from Swiss-Prot (Krishnan et al., 2017), to provide a comprehensive understanding of protein functions and properties. This multi-modality learning not only addresses gaps in sequence-only models but also enhances protein representations for a variety of applications, including function annotation and localization prediction. xTrimoPGLM (Krishnan et al., 2017) revolutionizes protein sequence analysis and generation by merging autoencoding and autoregressive objectives within a massive 100 billion parameter framework, trained on about 940 million unique protein sequences. _Sequence Generation based models._ In the quest to generate novel protein sequences with the potential to fold into functional proteins, several key players have emerged, notably ProGen (Krishnan et al., 2017), ProGen2 (Krishnan et al., 2017), ProtoGPT2 (Krishnan et al., 2017), and LM-Design (Krishnan et al., 2017). ProGen starts the journey by training on a vast array of 280 million protein sequences, using special tags to make proteins with specific functions (Krishnan et al., 2017). ProGen2 takes this further by using a much larger model to capture more complex protein patterns and making it possible to predict protein fitness without extra steps (Krishnan et al., 2017). ProtGPT2, inspired by language models, generates proteins that could exist in nature, trained on 50 million sequences for wide exploration. While ProGen series focus on making diverse and functional proteins, ProtGPT2 pushes the limits of designing completely new proteins, showing the power of deep learning in understanding and creating life's building blocks (Krishnan et al., 2017). LM-Design innovates in protein design by infusing language models with structural adapters, enhancing the generation of sequences. This addresses both the creativity seen in ProtGPT2 and the functional specificity of the ProGen series with an added focus on structural viability (Krishnan et al., 2017). _Multi-modal Integration models._ ProtST (Krishnan et al., 2017), Prot2Text (Krishnan et al., 2017), and xTrimoPGLM (Krishnan et al., 2017) exemplify the fusion of sequence, structure, and textual data to redefine protein function understanding and design. ProtST integrates dual language models, specifically the ESM series and ProtBert for protein sequences, alongside PubmedBERT (Krishnan et al., 2017) for biomedical texts, to synergize textual descriptions with sequence data (Krishnan et al., 2017). Prot2Text utilizes Graph Neural Networks and Transformers to generate descriptive texts of protein functions, integrating structural and sequence data with textual annotations. This approach not only enhances function annotation and drug discovery efforts but also enriches databases with contextually rich protein descriptions, setting a new standard for how protein functions are predicted and understood (Krishnan et al., 2017). xTrimoPGLM, while its primary category is understanding and generation, its methodological approach of combining masked language modeling (MLM) and general language modeling (GLM) objectives allows it to handle multimodal data for enhanced protein sequence analysis (Krishnan et al., 2017). _Applications_: In the rapidly evolving field of computational biology, all these protein sequence based large language methods (Krishnan et al., 2017; Krishnan et al., 2017; Krishnan et al., 2017; Krishnan et al., 2017; Krishnan et al., 2017) are pushing the boundaries of how we understand, design, and utilize proteins. The application of these models spans from detailed protein function annotation (Krishnan et al., 2017) and structure prediction (Krishnan et al., 2017; Krishnan et al., 2017) to the high-throughput design of novel proteins with specific functionalities (Krishnan et al., 2017; Krishnan et al., 2017; Krishnan et al., 2017), significantly impacting drug discovery, synthetic biology, and therapeutic development (Krishnan et al., 2017; Krishnan et al., 2017). Looking forward, the integration of even more diverse datasets, the refinement of models to enhance accuracy and efficiency, and the exploration of uncharted areas of the protein universe remain pivotal directions."
    },
    {
      "title": "2.2.4. **Multi-Omics Sequencing Data**",
      "text": "Within these domains, the transformative capabilities of LLMs manifest in high-impact downstream applications. From predicting molecular structures to forecasting molecule interactions, and from unraveling molecule functions to drawing poignant associations with disease progression processes, LLMs guides us towards a deeper comprehension of life's building blocks. In multi-omic sequencing, models such as scBERT(Krishnan et al., 2017), Generformer(Krishnan et al., 2017), scFoundation(Krishnan et al., 2017) and scGPT(Krishnan et al., 2017) use advanced pre-trained language models to deeply analyze cellular biology and genetics, showcasing the fusion of cutting-edge computation with biological understanding. _scBERT_. scBERT (Krishnan et al., 2017), a transformer-based deep neural network model, leverages extensive unlabeled single-cell RNA-seq data for pre-training, addressing the challenges of cell type annotation by capturing latent gene-gene interactions and overcoming limitations such as batch effects and the absence of curated marker genes. Through self-supervised learning and supervised fine-tuning, scBERT significantly enhances cell type annotation accuracy, robustness against batch effects, and interpretability over existing methods. Its versatility extends to discovering novel cell types and understanding complex gene expression patterns, offering a comprehensive tool for single-cell omics analysis. _Generformer._ Generformer (Krishnan et al., 2017), pre-trained on about 30 million single-cell transcriptomes, employs a context-aware, attention-based framework for versatile applications in network biology, including disease modeling and therapeutic target identification,especially in data-scarce areas. It differentiates itself with a self-supervised learning strategy for fundamental insights into network dynamics. This emphasizes its utility in a broader range of downstream tasks such as chromatin dynamics predictions. In comparison, scBERT, focusing on cell type annotation, demonstrates its strength in handling batch effects and discovering novel cell types through its transformer architecture. Both models offer significant advancements over traditional methods, with Genetormer offering a broader application scope and scBERT excelling in cell type specificity and interpretability. scFoundation.scFoundation (Srivastava et al., 2017) redefines the landscape of single-cell analysis with its large-scale pre-trained model, trained on an unprecedented dataset of over 50 million human single-cell transcriptomics. Through its unique asymmetric architecture and the introduction of a read-depth-aware pre-training task, the model efficiently addresses the complexities of single-cell RNA-seq data, including its sparsity and high dimensionality. Distinguished by its vast scale of trainable parameters and extensive data coverage, scFoundation delivers superior performance across a spectrum of downstream applications, from enhancing gene expression to predicting drug responses. scGPT.scGPT (Krishnan et al., 2017), trained on 33 million cells, significantly advances cell biology and genetics research with its large-scale data analysis. It differentiates itself by applying generative pre-training on extensive multi-omics data for a comprehensive analysis. Using a self-attention transformer, scGPT excels in analyzing complex cell data, enabling it to perform exceptionally well in identifying cell types, predicting genetic changes, and integrating multi-omics data. Its ability to derive insights into gene-gene interactions and demonstrate a scaling effect of improved performance with larger datasets highlights its potential for single-cell omics research. ApplicationsThe integration of advanced computational models like scBERT (Krishnan et al., 2017), scFoundation (Srivastava et al., 2017), scGPT (Krishnan et al., 2017), and Genetormer (Genes et al., 2017) into single-cell and multi-omics analysis heralds a new era in precision biology. These models, each with its unique approach ranging from cell type annotation(Krishnan et al., 2017; Krishnan et al., 2017) to the elucidation of complex gene networks, offer comprehensive tools for dissecting the intricacies of cellular functions. Future research directions anticipate the convergence of these models into a cohesive analytical framework, enhancing our capability to predict disease mechanisms and responses to treatment based on sophisticated cell-level predictions (Srivastava et al., 2017; Genes et al., 2017). As the pool of omics data expands, these models will continue to evolve, driving forward the discovery of novel biological mechanisms and therapeutic targets (Genes et al., 2017)."
    },
    {
      "title": "Llms On Brain Signals",
      "text": "Last, we delve into the fascinating realm of applying LLMs to brain signals. In this section, we focus on two topics: brain LLM building and downstream applications in brain-to-text translation. We start with introducing various pre-training techniques for brain EEG signal representation learning, including BrainBERT (self-supervised representation learning for intracranial recordings) (Krishnan et al., 2017), MMM (topology-agnostic EEG representation learning) (Krishnan et al., 2017), and LaBraM (generic representation learning for tremendous EEG data from various sources) (Genes et al., 2017). BrainBERT.BrainBERT (Krishnan et al., 2017) is a Transformer-based model designed for decoding intracranial field potential recordings. Pretrained unsupervised on large datasets of neural activities recorded while subjects watched movies, BrainBERT facilitates significant improvements in decoding neural signals, addressing the challenges posed by the scarcity and variability of neural data. It excels in generalizing across different subjects and electrode locations, a notable advantage over traditional methods. Employing techniques like super-resolution spectrograms and contextual representation, BrainBERT outperforms existing models, offering enhanced accuracy with less data. BrainBERT demonstrates superior performance across various neural decoding tasks, establishing a new benchmark in the field compared to standard linear decoders and deeper neural networks. The model not only improves brain-computer interface technologies but also enables novel analyses, such as exploring the intrinsic dimensionality of brain computations. Mmm.MMM (Krishnan et al., 2017) is a novel framework for EEG pre-training, MMM achieves this by mapping different EEG channel selections onto a unified topology and employing strategies such as Multi-dimensional Positional Encoding, Multi-level Channel Hierarchy, and Multi-stage Pre-training. This approach facilitates topology-agnostic EEG representation learning, enhancing cross-dataset generalizability. Evaluations on SEED (Krishnan et al., 2017) and SEED-IV (Krishnan et al., 2017) datasets demonstrate MMM's superior performance in emotion recognition tasks, outperforming traditional and deep learning baselines. Specifically, MMM surpasses previous state-of-the-art methods in subject-dependent classification, showcasing its robust transferability across different EEG datasets and sensor configurations. The results validate MMM's effectiveness in leveraging larger datasets for extracting more generalized representations. LaBraM.LaBraM (Genes et al., 2017) is a novel Large Brain Model, which marks a significant advancement in EEG-based deep learning through its pre-training on over 2500 hours of diverse EEG data. LaBraM overcomes the limitations of EEG data variability and volume by transforming EEG signals into unified channel patches and employing vector-quantized neural spectrum prediction for efficient learning. The model's architecture enables the effective learning of EEG signal representations, addressing both temporal and spatial features. LaBraM is evaluated on various downstream tasks such as abnormal detection, event type classification, emotion recognition, and gait prediction. In these evaluations, LaBraM consistently outperforms state-of-the-art methods across multiple metrics, underscoring its Figure 3. Overview of models, downstream tasks and challenges for brain signal LLMs. superior ability to generalize from large-scale EEG data. Moreover, the model exhibits scalability with increasing dataset sizes, suggesting potential for further performance improvements with even larger datasets. This research not only sets new benchmarks for EEG-based analyses but also opens up new avenues for deep learning applications in neuroscience and medical diagnostics. _Applications:_ We further discuss a broad range of brain LLM applications in brain-computer interfaces (BCIs). _Brain-to-Text Translation:_ Previous research on converting brain signals to text, as described in papers[7, 35, 60, 62, 64, 67, 83], has shown notable success with small and restricted vocabularies. However, these studies have struggled to achieve similar accuracy levels with larger and unrestricted vocabularies. Building upon this foundation, we further introduce an exciting topic of open-vocabulary brain-to-text translation [96], including recent work DeWave (discrete encoding for EEG-to-text translation) [25] and continuous language reconstruction from fMRI images [86]. Starting with EEG2TEXT [96], this paper presents a method of open vocabulary EEG-To-Text decoding and zero-shot sentence sentiment classification. Tested on the ZuCo dataset [37], the author utilizes pre-trained language models like BART [52], achieving significant advancements with a 40.1% BLEU-1 score for EEG-To-Text decoding and a 55.6% F1 score for zero-shot EEG-based ternary sentiment classification, which notably surpass the supervised baselines. Further advancing the field, DeWave [25] is a novel framework for converting EEG signals into text by leveraging a discrete codex encoding. DeWave utilizes a quantized variational encoder to transform EEG waves into discrete representations, aligning them with pre-trained language models for enhanced translation accuracy. The result surpassing previous baselines including EEG2TEXT. In a parallel vein, the paper [86] explores a non-invasive brain-computer interface decoding continuous natural language from fMRI brain recordings. Employing a unique approach, the model addresses fMRI's low temporal resolution by generating candidate word sequences, scoring them against brain responses to identify the most likely stimuli being heard or imagined. _Brain-to-Image Translation:_ Image Translation is also a mainstream Downstream Tasks of brain signals. Some recent interesting work includes nature image decoding [82] NeuroGAN [61] and EEG2IMAGE [78]. Specifically, nature image decoding [82] introduces a self-supervised framework, which leverages a large and diverse EEG-image dataset [32]. This dataset is used in conjunction with a novel approach that applies contrastive learning to align features extracted from image stimuli and corresponding EEG responses, significantly advancing the field of non-invasive brain-computer interfaces. The framework employs self-attention and graph attention modules within the EEG encoder to enhance spatial feature extraction, reflecting the spatial dynamics of brain activity related to object recognition. NeuroGAN [61] introduces a sophisticated method for transforming EEG signals into visual images, leveraging a specialized architecture to enhance image synthesis from brain activity data. Central to NeuroGAN is a cross-modality encoder-decoder structure, which effectively compresses EEG features into a latent space and reconstructs corresponding images, focusing on capturing the complex relationship between neural signals and visual stimuli. The method also incorporates a perceptual loss function, utilizing a pre-trained image classifier to measure the perceptual similarity between generated and real images. Similarly, EEG2IMAGE [78] introduces an innovative framework for synthesizing images from EEG signals. The framework uses small EEG datasets to learn features via a contrastive learning approach and synthesizes images using a modified conditional Generative Adversarial Network. Specifically, the framework employs semi-hard triplet loss for feature extraction from EEG signals, ensuring that signals from similar images are closer to the learned feature space, leading to more accurate image reconstruction. _Other Brain EEG Applications:_ In addition to text translation and image translation, brain signal decoding also performs well in other downstream tasks. In the field of voice decoding, NeuroTalk [50] presents a novel model, for transforming non-invasive EEG signals of imagined speech into audible voice outputs. The model combines multi-receptive residual modules with recurrent neural networks to process brain signals effectively. Notably, the model could generate words not included in the training dataset, indicating its potential for broader vocabulary coverage. In the motion decoding domain, Motor Recognition [44] explores using EEG data for recognizing motor activities. The research employs an innovative ensemble approach combining stacked bidirectional long short-term memory (BiLSTM) with long short-term memory (LSTM) networks alongside a newly proposed EEG-transformer network to classify 17 different everyday motor activities. Their ensemble achieves a classification accuracy of 98.5%, which has a substantial advancement over existing state-of-the-art methodologies."
    },
    {
      "title": "3. Conclusions And Future Directions",
      "text": "As a conclusion, this is a comprehensive survey of large language models for biomedicine, focusing on three pivotal data types: 1) textual data, 2) biological sequences, and 3) brain signals. There are also significant new challenges that come with using AI in biomedical research. One big challenge is making sure that the biomedical insights enhanced by AI are reliable and trustworthy, including model explainability and interpretability, model robustness to adversarial attacks, model bias towards different populations, and data privacy issues. Another challenge is the personalization of LLMs, which means adjusting LLMs to fit the specific needs of different personalized data. For example, there is a large individual variance in brain signals when different people are thinking of the same word under the same context. Instead of using one LLM to fit everyone, can we construct personalized LLMs based on different brain patterns for different people? The last challenge is the multi-modality data. Since biomedical information can be very varied, we learn how to handle different types of data in a skillful and effective way. For example, Google has announced Med-PalM-2 [80] that integrates image, text, and genome data in the electronic health record, declaring an expert-level ability for medical question answering. Can we develop more effective and efficient methods to integrate multi-modal and multi-omic LLMs into one powerful unified LLM? This survey paper serves as a foundation step to bridge the gap and encourages both theoretical researchers and practitioners in LLMs to look into real-world scientific applications."
    },
    {
      "title": "Acknowledgement",
      "text": "This work is sponsored by the Commonwealth Cyber Initiative, Children's National Hospital, Fralin Biomedical Research Institute (Virginia Tech), Sanghani Center for AI and Data Analytics (Virginia Tech), Virginia Tech Innovation Campus, and a generous gift from the Amazon + Virginia Tech Center for Efficient and Robust Machine Learning."
    },
    {
      "title": "References",
      "text": "* [1]H. Abidine, M. Chatzinastasas, C. Bouyjoukos, and M. Vazirgiannis (2023) Protzet: multimodal protein's function generation with guns and transformers. arXiv preprint arXiv:2307.14367. Cited by: SS1. * [2]M. Akiyama and N. Sakakibara (2022-02) Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning. NAR Genomics and Bioinformatics4 (4), pp. 1920-1922. Cited by: SS1. * [3]S. Alrowli and K. Vijay-Shanker (2021) BIOM-transformers: building large biomedical image models with bert, absent and electra. In Proceedings of the 20th workshop on biomedical language processing, pp. 221-227. Cited by: SS1. * [4]E. Alsentre, J. Murphy, W. Boag, W. Weng, D. Jin, T. Naumann, W. Redmond, and M. B. McDermott (2019) Publicly available clinical bent embeddings. NAACL HLT 2019, pp. 72-9. Cited by: SS1. * [5]W. Ammmair, D. Kreeveld, C. Bhagavatula, I. Beltagy, M. Crawford, D. Downey, J. Dunkelberger, A. Elgayry, S. Feldman, V. Ha, R. Kinney, S. Kohlmeier, K. Lo, T. Murray, H. Ooi, M. Peters, J. Power, S. Schjonsberg, L. Luw, C. Wilhelm, Z. Yuan, M. Van Zijberan, and O. Etzioni (2018-01) Construction of the literature graph in semantic scholar. pp. 84-91. Cited by: SS1. * [6]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepablin, A. Panstoy, S. Shakeri, E. Tarpa, J. Bailey, Z. Chen, et al. Palm (2023) PLAN 2 technical report. arXiv preprint arXiv:2305.104023. Cited by: SS1. * [7]G. Aoki and Y. S. Sakakibara (2018) Convolutional neural networks for classification of alignments of non-coding rna sequences. Bioinformatics34 (13), pp. 237-244. Cited by: SS1. * [8]M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. Cherny, A. P. Davis, K. Dolissin, S. D'Night, J. T. Eppig, et al. (2000) Genet ontology: tool for the unification of biology. Nature genetics25 (1), pp. 25-29. Cited by: SS1. * [9]Z. Atsev, Y. Agarwal, D. Visentin, J. R. Ledsam, A. Grahske-Barwinska, K. R. Taylor, Y. Assael, J. Jumper, P. Kohli, and D. R. Kelley (2021) Effective gene expression prediction from sequence by integrating long-range interactions. Nature Methods18 (10), pp. 1196-1203. Cited by: SS1. * [10]A. Bianchol and R. Appweiler (2000) The swiss-prot protein sequence database and its supplement termell in 2000. Nucleic acids research2 (14), pp. 45-48. Cited by: SS1. * [11]I. Beltagy, K. Lo, and A. Cohan (2019) Scibert: a pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3615-3620. Cited by: SS1. * [12]G. Benegas, S. Sharma, and Y. S. Song (2020) Dual language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences120 (44), pp. e231129120. Cited by: SS1. * [13]T. Belp and B. Berger (2021) Learning the protein language: evolution, structure, and function. Cell systems2 (16), pp. 654-669. Cited by: SS1. * [14]E. Boustet, D. Lieberber, M. Tognolli, M. Schneider, and A. Bianchot (2007) Uniprtikh/swiss-prot: the imannely annotated section of the uniprtikh model/package. In Plant bioinformatics: methods and protocols, pp. 89-112. Cited by: SS1. * [15]N. Brandes, D. Ger, Y. Pezs, N. Bappport, and M. Linial (2022) ProteinNet: a universal deep-learning model of protein sequence and function. Bioinformatics38 (8), pp. 2102-2110. Cited by: SS1. * [16]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1837-1901. Cited by: SS1. * [17]B. Chen, X. Cheng, P. Li, J. Gong, J. Gong, S. Li, Z. Bei, X. Tan, B. Wang, X. Zeng, et al. (2024) strimepfm: unified 100b-scale pre-trained transformer for deciphering the language of protein. arXiv preprint arXiv:2401.06199. Cited by: SS1. * [18]J. Chen, Z. Hu, S. Sun, Q. Tan, Y. Wang, Q. Yu, L. Z. Hong, J. Xiao, T. Shen, et al. (2022) Interpretable rna foundation model from unannotated data for highly accurate rna structure and function predictions. BioRxiv, pp. 2022-08. Cited by: SS1. * [19]K. Chen, Y. Zhou, M. Ding, Y. Wang, Z. Ren, and Y. Yang (2023) Self-supervised learning on millions of pre-mma sequences improves sequence-based rna splicing prediction. BioRxiv. Cited by: SS1. * [20]H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al. (2022) Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11146. Cited by: SS1. * [21]H. Cui, C. Wang, H. Maan, K. Pang, F. Luo, N. Duan, and B. Wang (2024) Segt: toward building foundation model for single-cell multi-omics using generative ai. Nature Methods, pp. 1-11. Cited by: SS1. * [22]H. Dalla-Torre, L. Gonzalez, J. Mendez-Ruilla, N. Lopez Carranza, A. Henry Gravesweik, F. Oteri, C. Dalalgo, E. Trop, H. Sirelkhtam, G. Richard, et al. (2023) The nucleotide transformer: building and evaluating robust foundation models for human genomics. bioRxiv, pp. 203-201. Cited by: SS1. * [23]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.084058. Cited by: SS1. * [24]Y. Duan, C. Zhou, Z. Wang, Y. Wang, and C. Lin (2023) Dewaev: discrete encoding of gene sources for gene to text translation. In Thirty-seventh Conference on Neural Information Processing Systems, Cited by: SS1. * [25]I. Dunham, A. Kundaje, S. J. Ahlred, P. J. Collins, C. A. Davis, F. Doyle, C. B. Epstein, S. Fruejennefarrro, R. Kaul, and et al. (2012-05) An integrated encyclopedia of dna elements in the human genome. Nature489 (7414), pp. 57-74. Cited by: SS1. * [26]N. Ferruite, S. Schmidt, and H. Hocker (2022) ProtGPU2 is a deep unsupervised language model for protein design. Nature communications13 (1), pp. 4348-4356. Cited by: SS1. * [27]V. Fishman, Y. Kuratov, M. Petrov, A. Shmelev, D. Shepelin, N. Chekanov, O. Kardymon, and M. Burtsev (2023) Gen-lm: a family of open-source foundational models for long dna sequences. bioRxiv, pp. 2023-2036. Cited by: SS1. * [28]L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phong, H. He, A. Thite, N. Nabeshima, et al. (2020) The pile: an 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2201.00027. Cited by: SS1. * [29]S. Gao, O. Kotewska, A. Sorokine, and J. Blair Christian (2021) A pre-training and self-training approach for biomedical named entity recognition. PLoS one16 (2), pp. e20240513. Cited by: SS1. * [30]A. L. Garner (2022) Contemporary progress and opportunities in RNA-targeted drug discovery. ACS Med. Chem.14 (3), pp. 1-259. Cited by: SS1. * [31]A. T. Gifford, K. Dwivedi, G. Roig, and R. M. Cichy (2022) A large and rich eg dataset for modeling human visual object recognition. NeuroImage264, pp. 119754. Cited by: SS1. * [32]Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Uusyama, X. Liu, T. Naumann, J. Gao, and H. Poom (2021) Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (Health)3 (1), pp. 1-23. Cited by: SS1. * [33]M. H. Hago, J. G., X. Zeng, C. Liu, Y. Guo, X. Wang, J. Wang, J. Ma, L. Song, and X. Zhang (2023) Large scale foundation model on single-cell transcriptomics. bioRxiv, pp. 2023-05. Cited by: SS1. * [34]C. Herff, D. Heger, A. De Pesters, D. Teatar, P. Brunner, G. Schalk, and T. Schultz (2015) Brain-to-text decoding po2 phrases from phone representations in the brain. In neuroscience, pp. 2917. Cited by: SS1. * [35]S. Heyne, F. Costa, D. Rose, and R. Backofen (2012) Graphclust: alignment-free structural clustering of local rna secondary structures. Bioinformatics28 (12), pp. 22124-2132. Cited by: SS1. * [36]N. Holenstein, J. Botstein, M. Troemdle, and A. Pedroni (2018) Cezhang, and Nicolas Langer. Zuco (2018) a simultaneous eeg and eye-tracking resource for natural sentence reading. Scientific data5 (1), pp. 1-13. Cited by: SS1. * [37]Y. Ji, Z. Zhou, H. Liu, and R. V. Duvahari (2021) Dusper-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics37 (15), pp. 2112-2120. Cited by: SS1. * [38]W. Jiang, L. Zhao, J. Zhao, and L. Zhou (2021) Large brain model for learning generic representations with tremendous eeg data in bci. In The Thirteenth International Conference on Learning Representations, Cited by: SS1. * [39]D. Jin, E. Pan, N. Oortlude, W. Weng, H. Fang, and P. Szolovits (2020) What disease does this patient haw? a large-scale open domain question answering dataset from medical exams. arXiv preprint arXiv:2009.13081. Cited by: SS1. * [40]Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu (2019) Pubmedg: a dataset for biomedical research question answering. In Proceedings of the 9th International Joint Conference on Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMAmerican Medical Informatics Association_, 25(1):32-39, 2018. * Johnson et al. [2016] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Glasserman, Benjamin Moody, Peter Smolivis, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016. * Kuahik et al. [2023] Pallavu Kuahik, Ilina Tripathi, and Partha Primin. _Hoy_. Motor activity recognition using egg data and ensemble of stacked BSTM-STM network and transformer model. In _IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-6, 2023_, pages 1-5. IEEE, 2023. * Kelley [2020] David R Kelley. Cross-species regulatory sequence activity prediction. _PLoS computational biology_, 16(7):e1008050, 2020. * Krikhara et al. [2023] Anastasia Krikhara, Anastasios Nentidis, Konstantinos Bogiotis, and Georgios Palvarois. Bioas: A manually curated corpus for biomedical question answering. _Scientific Data_, 10(1):170, 2023. * Kato and Richardson [2018] Taku Kato and John Richardson. SentencePerec: A simple and language independent subword tokenizer and dedelvecizer for neural text processing. In Eduardo Blanco and Wei Lu, editors, _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. * Landrum et al. [2019] Melissa J Landrum, Shawnang Chitigrilla, Garth B Brown, Chao Chen, Baoshan Gu, Jennifer Hart, Douglas Hoffman, Wonhue Jang, Kailerwal, Chunlei Liu, Vitaly Lyoshin, Zenith Maddisdispla, Rama Mati, Joseph Mitchell, Naola V O'Leary, George R Riley, Wenyao Shi, George Zhou, Valerie Scheineker, DonagMot, J Bradley Holmes, and Brandt I. Katlerman. Chivar: Improvements to accessing data. _Nucleic Acids Research_, 48(1):D1835-D1841, 11 2019. * Lee et al. [2020] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kangboer. A pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240, 2020. * Lee et al. [2022] Young-Eun Lee, Seo-Hyun Lee, Sang-Ho Kim, and Seong-Whan Lee. Towards voice reconstruction from EEG during imagined speech. In Brian Williams, Yiling Chen, and Jennifer Neville, editors, _Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2022, Thirtiphy Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, Thirtiphy Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-4, 2022_, pages 6030-6038. AAAI Press, 2023. * Leinonon et al. [2017] Rasko Leinonon, Federico Garcia Diez, David Binnis, Wolfgang Fleischmann, Rodrigo Lopez, and Rolf Apweiler. Uniprot archive. _Bioinformatics_, 20(17):3236-3237, 2004. * Lewis et al. [2019] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahaman Mohamed, Omer Levy, Vesg Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019. * Li et al. [2016] Jiao Li, Yueping Sun, Robin J Johnson, Daniela Scialky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Liu. BioCleactive V CDR task corpus: a resource for chemical disease relation extraction. _Database_, 2016:dehows60, 05 2016. * Knaible et al. [2021] Silen Knaible, Iseh Mosejdeborbui, Rujiang Li, Michael Bailey, Saleh Riazhi, Lorenzo Kogler-Anchee, Mald Majdoli, Jacob Minter, Dinghai Zheng, Jun Wang, Akshay Bashasubramani, Khang Tran, Minnie Zacharia, Monica Wu, Xiaobo Gu, Ryan Clinton, Carta Asquith, Joseph Skelasci, Lizanne Degolin, Sushchuyukula, Anshua Dias, Fernando Ullio Montopova, Vikram Agarwal, Zwar Josejosch, and Sen Jager. Colonbbert: Large language models for mrm design and optimization. _bioRxiv_, 2021. * Lin et al. [2023] Zeming Lin, Halil Akin, Roshan Rao, Brian Hee, Zhongkai Zhu, Wenting Lu, Nikita Santenian, Robert Vestzull, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023. * Liu et al. [2023] Fangyu Liu, Qianchu Liu, Shruthi Banur, Fernando Perez-Garcia, Naoto Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hofung Poon, Javier Alvarez-Valle, et al. Compositional zero-shot domain transfer with text-to-text models. _arXiv preprint arXiv:2303.13386_, 2023. * Luo et al. [2022] Ling Luo, P-Ying Lai, Chi-Hsuan Wei, Cecilia N Arighi, and Zhiyong Lu. BioRED: a rich biomedical relation extraction dataset. _Biergings in Bioinformatics_, 23(5):Jabae228, 07 2022. * Lu et al. [2022] Renning Luo, Iain Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hofling Poon, and Tix-Yan Liu. Biogt: generative pre-trained transformer for biomedical text generation and mining. _Briefing in Bioinformatics_, 23(6):bhao402, 2022. * Madani et al. [2023] Al Madani, Ben Krause, Eric G Greene, Sub Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos Jr, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. Large language models generate functional protein sequences across diverse families. _Nature Biotechnology_, pages 1-8, 2023. * Makin et al. [2020] Joseph G Makin, David A Moses, and Edward F Chang. Machine translation of cortical activity to text with an encoder-decoder framework. _Nature neuroscience_, 23(4):575-582, 2020. * Mishra et al. [2023] Rahul Mishra, Krishnan Sharma, Ranjeet Ranjan Jha, and Arnav Bhavsar. Neurogram: image reconstruction from EEG signals via an attention-based GAN. _Neural Comput. Appl._, 35(12):9181-9192, 2023. * Mi et al. [2021] David A Moses, Sean L Mettger, Jessie R Lin, Gopala K Anumanchimpalli, Joseph G Makin, Pengfei F Sun, Josh Chartier, Maximilian E Dougherty, Patricia M Liu, Gary M Ahrams, et al. Neuropsthesis for decoding speech in a paralleged person with anthria. _New England Journal of Medicine_, 385(3):2177, 2021. * Nguyen et al. [2023] Eric Nguyen, Michael Poli, Marian Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Anam Patel, Clayton Rabidenz, Stefano Massordi, Yoshua Bengio, et al. Hyendama: Long-range genomic sequence modeling at single nucleotide resolution. _arXiv preprint arXiv:2306.15740_, 2023. * Nieto et al. [2022] Nicolas Nieto, Victoria Peterson, Hugo Leonardo Ruhner, Juan Esteban Kamienkowski, and Ruben Spies. Thinking out loud, an open-access egg-based bicl dataset for inner speech recognition. _Scientific Data_, 9(1):52, 2022. * Nylam et al. [2022] Erik Nylam, Jeffrey A Ruffolo, Eli N Wentisen, Nik Nikit, and Ali Madani. Progeni: exploring the boundaries of protein language models. _Cell systems_, 14(11):968-978, 2023. * Nurt et al. [2022] Sergey Nurt, Sergey Koren, Arang Rike, Micko Rautiainen, Andrey V Erikadze, Ali Makinekova, Mohell R Volliger, Nicolas Allemose, Lev Uralsky, Ariel Gershman, and et al. The complete sequence of a human genome. _Science_, 376(6588):44-53, April 2022. * Panachakhel and Ramakrishnan [2021] Jerim Thomas Panachakhel and Angrarai Ganesan Ramakrishnan. Decoding covert speech from egg-a comprehensive review. _Frontiers in Neuroscience_, 15:392, 2021. * Peng et al. [2023] Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Mago, et al. A study of generative large language model for medical research and healthcare. _arXiv preprint arXiv:2305.13523_, 2023. * Phan et al. [2021] Long N Phan, James T Anibal, Hieu Tran, Shawnya Channa, Erol Bahadroglu, Alaee Pellekian, and Gregorie Alita-Bonnet. Schifre: a text-to-text transformer model for biomedical literalure. _arXiv preprint arXiv:2106.03598_, 2021. * Radford et al. [1989] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 16(8):9, 2019. * Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020. * Rajpurakar et al. [2022] Pranav Rajpurakar, Ewen Chen, Oishi Banerjee, and Eric J Topol. Ai in health and medicine. _Nat Med_, 28(1):31-38, 2022. * Rives et al. [2021] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zining, Jason Liu, Demi Guo, Myle Ott, L Casurence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _Proceedings of the National Academy of Sciences_, 118(15):e2016239118, 2021. * Saito et al. [2011] Yutaka Saito, Kengo Sato, and Yarububumi Sakakibara. Fast and accurate clustering of noncoding rmas using ensembles of sequence alignments and secondary structures. _BMC bioinformatics_, 12:1-8, 2011. * Sheikhshtab et al. [2018] Golnar Sheikhshtab, Inna Bird, and Anoop Sarkar. In-domain context-aware token embeddings improve biomedical named entity recognition. In _LoujBioIEMNLP_, 2018. * Shin et al. [2020] Hoo-Chang Shin, Yang Zhang, Freylina Rakhtrutin, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raglaw Mani. Bioemorrelation: Large biomedical domain language model. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4700-4706, 2020. * Shoeybi et al. [2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megetorn-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:2008.08053_, 2019. * Singh et al. [2022] Prajwal Singh, Pankaj Pandey, Krishna P. Miyapuram, and Shamungaathan Raman. EEGIMAGE: image reconstruction in EEG brain signals. _CoRR_, abs/2302.10121, 2022. * Singhal et al. [2022] Karan Singhal,* [84] Baris E Surek, Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H Wu. Unified: comprehensive and non-redundant unipot reference clusters. _Bioinformatics_, 23(10):1282-1288, 2007. * [85] Baris E Surek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniter clusters: a comprehensive and scalable alternative for improving sequence similarity searches. _Bioinformatics_, 31(6):926-932, 2015. * [86] Jerry Tang, Amanda Lebel, Shailze Jain, and Alexander G Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. _Nature Neuroscience_, pages 1-9, 2023. * [87] Xiangru Yang, Amin Zu, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingya Zhang, Arman Cohan, and Mark Gersstein. Medagnets: Large language models as collaborators for zero-shot medical reasoning, 2024. * [88] Ross Taylor, Marich Karads, Guillem Cucurull, Thomas Scaliano, Anthony Hartshorn, Elis Sarsia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactic: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022. * [89] Christina V Theodoris, Ling Xiao, Anant Chopra, Mark D Chaffin, Zeina R Al Sayed, Matthew C Hill, Helene Mantineo, Elizabeth M Brydon, Zeeian Zeng, X Shirley Liu, et al. Transfer learning enables predictions in network biology. _Nature_, 618(7056):656-624, 2023. * [90] Ajan J Thirunavukaras, Daniel S J Ting, Karthikeyan Elangovan, Liva Gutierrez, Tien F Tan, and Daniel S W Tung. Large language models in medicine. _Nat Med._, 29(8):1930-1940, Aug 2023. Eppos 2031 Jul 17. * [91] Benyou Wang, Qianqian Xie, Jiahan Pei, Zihong Chen, Prayag Tiwari, Zhao Li, and Jie Fu. Pr-trained language models in biomedical domain: A systematic survey. _ACM Computing Surveys_, 2021. * [92] Christopher Wang, Vignesh Subramaniam, Adam Ur Yairi, Gabriel Kreiman, Boris Katz, Ignacio Cases, and Andrei Barbu. Brainpied: Self-supervised representation learning for interactor recordings. In _The Eleventh International Conference on Learning Representations_, 2022. * [93] Sheng Wang, Zhao Zhao, Viuyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large language models, 2023. * [94] Xuan Wang, Vivian Hu, Minhao Jiang, Yu Zhang, Jinfeng Xiao, Danielle Cherceiving, Heng Ji, Martin Burke, and Jiawei Han. Reactclass: Cross-modal supervision for subword-guided reactant entity classification. In _2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)_, pages 844-847. IEEE, 2022. * [95] Xuan Wang, Vivian Hu, Xiangchen Song, Shweta Garg, Jinfeng Xiao, and Jiawei Han. Chemnet: fine-grained chemistry named entity recognition with ontology-guided distant supervision. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, 2021. * [96] Zhenlihong Wang and Heng Ji. Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 5350-5358, 2022. * [97] Chaoyi Wu, Jaya Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, and Weidi Xie. Can gpt-4v(si) serv medical applications? case studies on gpt-4v for multimodal medical diagnosis, 2023. * [98] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards general formulation model for radiology by leveraging web-scale 2d&3d medical data, 2023. * [99] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maximik Fikkin, Yuan Cau, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_, 2016. * [100] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yuen Ding, Boyang Hong, Ming Zhang, Junxue Wang, Seng, Zinyu Zhou, Rui Zheng, Xiaoman Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangru Xing, Hanjunge Yu, Shihuan Dou, Rongxiang Wang, Wensen Cheng, Qi Zhang, Wenyuan Qin, Yongxang Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey, 2023. * [101] Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. _arXiv preprint arXiv:2201.12040_, 2023. * [102] Keisuke Yamada and Michiaki Hamada. Prediction of rna-protein interactions using a nucleotide language model. _Bioinformatics Advances_, 2(1):vba023, 2022. * [103] Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Liu, and Jianhua Yao. selectr as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data. _Nature Machine Intelligence_, 4(10):852-866, 2022. * [104] Jingfeng Tang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatright and beyond. _arXiv preprint arXiv:2304.13712_, 2023. * [105] Xi Yang, Adokun Chen, Nima Powlejian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Moma G Flores, et al. A large language model for electronic health records. _NIP digital medicine_, 5(1):194, 2022. * [106] Michihiro Yasunaga, Jure Leskovec, and Percy Liang. Linkbert: Pretraining language models with document links. In _Association for Computational Linguistics (ACL)_, 2022. * [107] Ke Yi, Yanen Wang, Kan Ren, and Dongsheng Li. Learning topology-agnostic eeg representations with geometry-aware modeling. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. * [108] Xuan Zhang, Limei Wang, Jacob Heijiro, Youxin Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. _arXiv preprint arXiv:2307.08423_, 2023. * [109] Yikun Zhang, Mei Lang, Juhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Liffin, Ke Chen, Jawawinder Singh, Xians Huang, Gaoli Song, et al. Multiplespace-alignment-based rna language model and its application to structural inferencing. _bioRxiv_, pages 2023-03, 2023. * [110] Wei-Long Zheng, Wei Liu, Yifei Lu, Bao-Liang Liu, and Andrei Chechocki. Emotionometer: A multimodal framework for recognizing human emotions. _IEEE transactions on cybernetics_, 49(3):1110-1122, 2018. * [111] Wei-Long Zheng and Bao-Liang Liu. Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks. _IEEE Transactions on autonomous mental development_, 7(3):162-175, 2015. * [112] Zaxiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structure-informed language models are protein designs. _bioRxiv_, pages 2023-02, 2023. * [113] Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yirhu Jiao, Xuan Wang, and Jiawei Han. React: Enhancing chemical reaction extraction with weak supervision. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 12120-121302, 2023. * [114] Jian Zhou and Olga Croyroyanskaya. Predicting effects of noncoding variants with deep learning-based sequence model. _Nat. Methods_, 12(10):931-934, October 2015. * [115] Henghui Zhu, Ioannis Ch. Paschalidis, and Amir M. Tahmasebi. Clinical concept extraction with contextual word embedding. _ArXiv_, abs/1810.10566, 2018. * [116] Maxim Zvyagin, Alexander Bruce, Kyle Hrippe, Yunntin Deng, Bin Zhang, Crady Orozco Babouque, Austin City, Bakar Kale, Danilo Perez-Rivera, Heng Ma, et al. Genslms: Genome-scale language models reveal sars-cov-2 evolutionary dynamics. _bioRxiv_, pages 2022-10, 2022."
    }
  ]
}