{
  "title": "LaVy: Vietnamese Multimodal Large Language Model",
  "authors": [
    "Chi Tran",
    "Huong Le Thanh"
  ],
  "abstract": "\n Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. All code and model weights are public at  https://github.com/baochi0212/LaVy   \n",
  "references": [
    {
      "id": null,
      "title": "LaVy: Vietnamese Multimodal Large Language Model",
      "authors": [
        "Chi Tran",
        "Huong Le Thanh"
      ],
      "year": "",
      "venue": "",
      "doi": ""
    },
    {
      "id": "b0",
      "title": "Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Adler",
        "Steven",
        "Agarwal",
        "Sandhini",
        "Ahmad",
        "Lama",
        "Akkaya",
        "Ilge",
        "Florencia Aleman",
        "Leoni",
        "Almeida",
        "Diogo",
        "Altenschmidt",
        "Janko",
        "Altman",
        "Sam",
        "Anadkat",
        "Shyamal"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "doi": ""
    },
    {
      "id": "b1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean - Alayrac",
        "Baptiste",
        "Jeff Donahue",
        "Luc",
        "Pauline",
        "Miech",
        "Antoine",
        "Iain Barr",
        "Hasson",
        "Yana",
        "Lenc",
        "Karel",
        "Mensch",
        "Arthur",
        "Millican",
        "Katherine",
        "Reynolds",
        "Malcolm"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b2",
      "title": "Peacock: A family of arabic multimodal large language models and benchmarks",
      "authors": [],
      "year": "2024",
      "venue": "Peacock: A family of arabic multimodal large language models and benchmarks",
      "doi": ""
    },
    {
      "id": "b3",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew M Dai",
        "Anja Hauth",
        "Katie Millican",
        "David Silver",
        "Melvin Johnson",
        "Ioannis Antonoglou",
        "Julian Schrittwieser",
        "Amelia Glaese",
        "Jilin Chen",
        "Emily Pitler"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "doi": ""
    },
    {
      "id": "b4",
      "title": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "authors": [
        "Gregor Geigle",
        "Abhay Jain",
        "Radu Timofte",
        "Goran Glavaš"
      ],
      "year": "2023",
      "venue": "mblip: Efficient bootstrapping of multilingual vision-llms",
      "doi": ""
    },
    {
      "id": "b5",
      "title": "Efficient multimodal learning from data-centric perspective",
      "authors": [
        "Muyang He",
        "Yexin Liu",
        "Boya Wu",
        "Jianhao Yuan",
        "Yueze Wang",
        "Tiejun Huang",
        "Bo Zhao"
      ],
      "year": "2024",
      "venue": "Efficient multimodal learning from data-centric perspective",
      "doi": ""
    },
    {
      "id": "b6",
      "title": "Mistral 7b",
      "authors": [
        "Albert Q Jiang",
        "Alexandre Sablayrolles",
        "Arthur Mensch",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Lample",
        "Lucile Saulnier",
        "Renard Lélio",
        "Marie-Anne Lavaud",
        "Pierre Lachaux",
        "Teven Stock",
        "Thibaut Le Scao",
        "Thomas Lavril",
        "Timothée Wang",
        "William El Lacroix",
        "Sayed"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "doi": ""
    },
    {
      "id": "b7",
      "title": "BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Li",
        "Dongxu",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": ""
    },
    {
      "id": "b8",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Thirtyseventh Conference on Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b9",
      "title": "Open models based on gemini research and technology",
      "authors": [
        "Thomas Mesnard",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Surya Bhupatiraju",
        "Shreya Pathak",
        "Laurent Sifre",
        "Morgane Rivière",
        "Mihir Sanjay Kale",
        "Juliette Love",
        "Pouya Tafti",
        "Léonard Hussenot",
        "Aakanksha Chowdhery",
        "Adam Roberts",
        "Aditya Barua",
        "Alex Botev",
        "Alex Castro-Ros",
        "Ambrose Slone",
        "Amélie Héliou",
        "Andrea Tacchetti",
        "Anna Bulanova",
        "Antonia Paterson",
        "Beth Tsai",
        "Bobak Shahriari",
        "Charline Le Lan",
        "Christopher A Choquette-Choo",
        "Clément Crepy",
        "Daniel Cer",
        "Daphne Ippolito",
        "David Reid",
        "Elena Buchatskaya",
        "Eric Ni",
        "Eric Noland",
        "Geng Yan",
        "George Tucker",
        "George-Christian Muraru",
        "Grigory Rozhdestvenskiy",
        "Henryk Michalewski",
        "Ian Tenney",
        "Ivan Grishchenko",
        "Jacob Austin",
        "James Keeling",
        "Jane Labanowski",
        "Jean-Baptiste Lespiau",
        "Jeff Stanway",
        "Jenny Brennan",
        "Jeremy Chen",
        "Johan Ferret",
        "Justin Chiu",
        "Justin Mao-Jones",
        "Katherine Lee",
        "Kathy Yu",
        "Katie Millican",
        "Lars Lowe Sjoesund",
        "Lisa Lee",
        "Lucas Dixon",
        "Machel Reid",
        "Maciej Mikuła",
        "Mateo Wirth",
        "Michael Sharman",
        "Nikolai Chinaev",
        "Nithum Thain",
        "Olivier Bachem",
        "Oscar Chang",
        "Oscar Wahltinez",
        "Paige Bailey",
        "Paul Michel",
        "Petko Yotov",
        "Giuseppe Pier",
        "Rahma Sessa",
        "Ramona Chaabouni",
        "Reena Comanescu",
        "Rohan Jana",
        "Ross Anil",
        "Ruibo Mcilroy",
        "Ryan Liu",
        "Mullins",
        "L Samuel",
        "Sebastian Smith",
        "Wojciech Borgeaud",
        "Yu Stokowiec",
        "Zafarali Hui Chen",
        "Zhitao Ahmed",
        "Tris Gong",
        "Ludovic Warkentin",
        "Minh Peran",
        "Clément Giang",
        "Oriol Farabet",
        "Jeff Vinyals",
        "Koray Dean",
        "Kavukcuoglu ; Zoubin",
        "Douglas Ghahramani",
        "Eck"
      ],
      "year": "",
      "venue": "Open models based on gemini research and technology",
      "doi": ""
    },
    {
      "id": "b10",
      "title": "2023a. Vistral-7b-chat -towards a state-of-the-art large language model for vietnamese",
      "authors": [
        "Chien Van Nguyen",
        "Thuat Nguyen",
        "Quan Nguyen",
        "Huy Nguyen",
        "Björn Plüster",
        "Nam Pham",
        "Huu Nguyen",
        "Patrick Schramowski",
        "Thien Nguyen"
      ],
      "year": "",
      "venue": "2023a. Vistral-7b-chat -towards a state-of-the-art large language model for vietnamese",
      "doi": ""
    },
    {
      "id": "b11",
      "title": "Dung Ngoc Nguyen, Dinh Phung, and Hung Bui. 2023b. Phogpt: Generative pre-training for vietnamese",
      "authors": [
        "Linh The Dat Quoc Nguyen",
        "Chi Nguyen",
        "Tran"
      ],
      "year": "",
      "venue": "Dung Ngoc Nguyen, Dinh Phung, and Hung Bui. 2023b. Phogpt: Generative pre-training for vietnamese",
      "doi": ""
    },
    {
      "id": "b12",
      "title": "Openvivqa: Task and dataset and and multimodal fusion models for visual question answering in vietnamese",
      "authors": [
        "Hieu Nghia",
        "Nguyen",
        "T D Duong",
        "Kiet Vo",
        "Ngan Van Nguyen",
        "-Thuy Luu",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Openvivqa: Task and dataset and and multimodal fusion models for visual question answering in vietnamese",
      "doi": ""
    },
    {
      "id": "b13",
      "title": "A Vietnamese-English Neural Machine Translation System",
      "authors": [
        "Hai Thien",
        "Nguyen",
        "H Tuan-Duy",
        "Duy Nguyen",
        "Duy Phung",
        "Tran-Cong Nguyen",
        "Minh Hieu",
        "Manh Tran",
        "Tin Duy Luong",
        "Hung Hai Vo",
        "Dinh Bui",
        "Dat Quoc Phung",
        "Nguyen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 23rd Annual Conference of the International Speech Communication Association: Show and Tell",
      "doi": ""
    },
    {
      "id": "b14",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th Annual Meeting of the Associations for Computational Linguistics (ACL)",
      "doi": ""
    },
    {
      "id": "b15",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Learning transferable visual models from natural language supervision",
      "doi": ""
    },
    {
      "id": "b16",
      "title": "2023a. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Lavril",
        "Thibaut",
        "Izacard",
        "Gautier",
        "Martinet",
        "Xavier",
        "Marie-Anne Lachaux",
        "Lacroix",
        "Timothée",
        "Rozière",
        "Baptiste",
        "Goyal",
        "Naman",
        "Hambro",
        "Eric",
        "Azhar",
        "Faisal"
      ],
      "year": "",
      "venue": "2023a. Llama: Open and efficient foundation language models",
      "doi": ""
    },
    {
      "id": "b17",
      "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Martin",
        "Louis",
        "Kevin Stone",
        "Albert",
        "Peter",
        "Almahairi",
        "Amjad",
        "Yasmine Babaei",
        "Bashlykov",
        "Nikolay",
        "Batra",
        "Soumya",
        "Bhargava",
        "Prajjwal",
        "Bhosale",
        "Shruti"
      ],
      "year": "",
      "venue": "2023b. Llama 2: Open foundation and fine-tuned chat models",
      "doi": ""
    },
    {
      "id": "b18",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": ""
    },
    {
      "id": "b19",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Ying Sheng",
        "Siyuan Zhuang",
        "Zhanghao Wu",
        "Yonghao Zhuang",
        "Zi Lin",
        "Zhuohan Li",
        "Dacheng Li",
        "Eric P Xing",
        "Hao Zhang",
        "Joseph E Gonzalez",
        "Ion Stoica"
      ],
      "year": "2023",
      "venue": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "doi": ""
    },
    {
      "id": "b20",
      "title": "MiniGPT-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations",
      "doi": ""
    }
  ],
  "sections": [
    {
      "title": "Lavy: Vietnamese Multimodal Large Language Model",
      "text": "Chi Tran Hanoi University of Science and Technology \\(\\mathrm{chi.tb200083@sis.hust.edu.vn}\\) Huong Le Thanh Hanoi University of Science and Technology \\(\\mathrm{huonglt@soict.hust.edu.vn}\\)"
    },
    {
      "title": "Abstract",
      "text": "Large Language Models (LLMs) and Multimodal Large language models (MLLMs) have taken the world by storm with impressive abilities in complex reasoning and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese Large Language Models, the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench benchmark designated for evaluating MLLMs's understanding on Vietnamese visual language tasks. All code and model weights are public at [https://github.com/baochi0212/LaVy](https://github.com/baochi0212/LaVy)"
    },
    {
      "title": "1 Introduction",
      "text": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, showcasing their proficiency in complex reasoning and linguistic comprehension. The success of LLMs has inspired researchers to explore the potential of Multimodal Large Language Models (MLLMs), which incorporate visual information alongside textual data. MLLMs have shown promising results in tasks that require understanding the interplay between language and vision, such as image captioning, visual question answering, and multimodal machine translation. While there has been significant progress in developing Vietnamese LLMs, the lack of high-quality multimodal resources has hindered the advancement of Vietnamese MLLMs. The availability of diverse and well-annotated datasets is crucial for training and evaluating MLLMs, as they rely on the integration of visual and textual information to perform multimodal tasks effectively. To address this limitation and foster research in Vietnamese multimodal language understanding, we introduce LaVy, Vietnamese first MLLM and achieve state-of-the-art performance in Vietnamese vision language tasks. LaVy is designed to leverage the rich visual and linguistic information present in Vietnamese data, enabling it to tackle a wide range of multimodal tasks with improved performance. Our model outperforms a multilingual baseline mBLIP (Geigle et al., 2023) on different tasks by a large margin. By developing LaVy, we aim to bridge the gap between Vietnamese LLMs and MLLMs, providing researchers and practitioners with a powerful tool for exploring the intersection of language and vision in the Vietnamese context. Furthermore, to facilitate the evaluation and comparison of Vietnamese MLLMs, we propose the LaVy-Bench benchmark. This benchmark consists an open VQA task and an in-the-wild test set, specifically designed to assess the visual language understanding and generation capabilities of MLLMs in the Vietnamese and in-the-wild images. By establishing a standardized evaluation framework, we aim to promote the development and benchmarking of Vietnamese MLLMs, driving innovation and collaboration within the research community. In this paper, we present LaVy and the LaVy-Bench benchmark as significant contributions to the field of Vietnamese multimodal language understanding. We provide a detailed description of LaVy's architecture, data curation and training procedure. Additionally, we introduce the LaVy-Bench benchmark, discussing its design principles, task composition, and evaluation metrics. Through extensive experiments and analysis, we demonstrate the effectiveness of LaVy and the utility of the LaVy-Bench benchmark in advancing Vietnamese MLLM research. Related work"
    },
    {
      "title": "Large Language Model",
      "text": "Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various natural language processing tasks, including dialogue, creative writing, and problem-solving. Models such as LLaMA [21, 17], Mistral [16], and Gemma [22] have leveraged scalable Transformer-based architectures [20] and large-scale data to become foundation models for general reasoning tasks. These models have demonstrated impressive performance and have set new benchmarks in the field. Following the trend of LLMs, several Vietnamese language models emerged such as PhoGPT [23], Vistral [24] perform outstandingly in Vietnamese LLM benchmarks and NLP tasks."
    },
    {
      "title": "Multimodal Large Language Model",
      "text": "Witness the exceptional performance of GPT-4 [1] and Gemini Pro Vision [1] in visual language tasks, recent research has focused on developing Multimodal Large Language Models (MLLMs) to achieve unified understanding and reasoning across different modalities, building upon the success of Large Language Models (LLMs). Various methods have been proposed to integrate information from multiple modalities into pre-trained LLM architectures. For instance, Flamingo [1] and BLIP-2 [11] introduce different techniques for fusing visual tokens with frozen LLMs using gated attention or Q-former. Inspired by the effectiveness of instruction tuning, LLaVA [12] and MiniGPT-4 [13] align visual input with LLMs through visual instruction tuning, demonstrating impressive results. Another active line of work is researching efficient MLLMs, resulting in lightweight model families like Bunny [15]. Meanwhile, recent works are pioneering development in vision-language tasks for low-resource languages, such as Peacock [1]"
    },
    {
      "title": "3 Lavy",
      "text": ""
    },
    {
      "title": "Architecture",
      "text": "Our model are built with LlaVA architecture [12] using three main components: * **Vision Encoder:** The CLIP-Large model [1], is used as the vision encoder. * **MLP Projector:** A two-layer Multi-Layer Perceptron (MLP) projector is employed to align the output representations from the visual and language modalities. This projector ensures that the visual and textual information is transformed into a common space. * **Language Model:** The third component is a Large Language Model, which is a language model responsible for generating textual information, take the aligned representations from the MLP projector."
    },
    {
      "title": "Data Curation",
      "text": "The barrier for Vietnamese MLLMs' development is resource for training, which is handled by the our novel pipeline of data collection. * **Translated then Refined:** We utilize LlaVA training data composing of filtered 558K LAION-CC-SBU captions and 150K GPT-generated multimodal instructions. With the acknowledgement of the insufficient competency of open-source translation projects and translation API like VinAI Translate [23], Google Translate,... in convert English data to Vietnamese high-quality data, we firstly translate a sample with VinAI translation, then we prompt Gemini Pro to rewrite it in more accurate and natural way for Vietnamese language by pairing translated sample and original English sample in Gemini prompt. Because the captions dataset is massive, we only refine a subset of 150K randomly sampled captions, and combine it with 558K non-write captions to finalize a captioning dataset. Finally, GPT-generated instructions is all refined to make 150K high-quality Vietnamese instructions * **Synthetic:** Understanding the difference between Vietnamese images and LlaVA's images, we crawl 8.000 images from web in various topics (for example: Vietnamese event images with keyword _Anh su kien Viet Nam_) and prompt Gemini Pro Vision to generated concise and detailed description of them to enhance LaVy's performance in Vietnamese images. Totally, we have 16.000 Vietnamese descriptions for crawled images and merge them with rewritten instructions In eventuality, we curated Vietnamese datasets with 708K image-caption pairs for pretraining and 166K high-quality instructions for finetuning in training step. Our pipeline is clearly illustrated in Image 1."
    },
    {
      "title": "Training Procedure",
      "text": "The training procedure is divided into 2 steps: * **Pretraining:** Align the vision embeddings from a pre-trained vision encoder with the text embeddings from the LLM by optimizing only the cross-modality projector using a cross-entropy loss for next token prediction. * **Finetuning:** We apply visual instruction tuning to fully utilize the MLLM's capabilities across different multimodal tasks. We use the same cross-entropy loss as in the pre-training stage, but this time, they employ Low-Rank Adaptation (LoRA) to train both the cross-modality projector and the LLM backbone."
    },
    {
      "title": "4 Experiment",
      "text": ""
    },
    {
      "title": "Implementation Details",
      "text": "We use Vistral 7B as LLM backbone and CLIP large visual encoder. The training process for the LaVy consists of two stages. In the first stage, the model is pretrained using a dataset of 708k captions for 1 epoch, with a global batch size of 64 and a learning rate of 1e-3. During this stage, all model parameters are frozen, except the MLP layers. Besides, we don't shuffle data but train the model to learn from unrefined data to refined data. The second stage involves finetuning the model using an instructions dataset. This stage also spans 1 epoch, with a global batch size of 32 and a learning rate of 2e-5. In this phase, only the newly introduced LoRA (Low-Rank Adaptation) parameters are trainable. Besides, in evaluation, we apply greedy decoding to generate all models' responses"
    },
    {
      "title": "Lavy-Bench",
      "text": "We construct LaVy-Bench to benchmark models' Vietnamese Visual Language understanding. We use mBLIP models as multilingual baselines. Additionally, we compare performance with close source projects Gemini Pro Vision and SeaLLLM 1 Footnote 1: [https://huggingface.co/spaces/SeaLLMs/SeaLLM-7B](https://huggingface.co/spaces/SeaLLMs/SeaLLM-7B)"
    },
    {
      "title": "4.2.1 Zero-Shot Visual Question Answering (Vqa)",
      "text": "We evaluate the zero-shot Visual Question Answering (VQA) performance of models on the OpenViVQA Nguyen et al. (2023c) dev set, which consists of 3,505 samples. This dataset challenges the models' understanding of the relationships between Vietnamese images and natural language. Furthermore, we propose a new metric for automatic evaluation to replace older metrics, such as BLEU Papineni et al. (2002), which do not accurately reflect models' competency in the VQA tasks. Our metric is inspired by LLM-as-a-Judge Zheng et al. (2023), which utilizes Gemini Pro to verify the accuracy of generated responses for question-answer pairs. In Table 1, it's apparent that LaVy's zero-shot VQA performance (45.3%) out-shadows mBLIP-Bloomz-7B (27.9%) and mBLIP-mT0-XL-5B (20.0%). However, roughly half of the OpenViVQA dataset is composed of TextQA samples (1,733 samples) that do not appear in our training dataset, making OpenViVQA particularly challenging for our model, not mention to our training instructions just includes descriptions of 8.000 Vietnamese crawled images. Notably, Gemini Pro (Vision) obviously showcase the best performance, but only correctly answers 66.8% questions."
    },
    {
      "title": "4.2.2 In-The-Wild Benchmark",
      "text": "To further assess the models' comprehension, we follow the evaluation methodology LLaVA benchmark (in-the-wild) Liu et al. (2023) and recollect set of 24 diverse images and 60 questions in 3 main types: Complex Reasoning, Detail Description and Conversation. The collected images and manually \\begin{table} \\end{table} Table 1: Data pipeline crafted questions aim to diversify the test set in various aspects: culture, race, image types,... and avoiding opting for images in crawled training images. For each question, we then prompt Gemini Pro Vision to generate detailed description of image and answer the question, and finally use Gemini Pro to rate the models' response on the scale of 1 to 5 while taking Gemini Pro's response as ground-truth. We opted against using detailed descriptions as references (unlike LlaVA) due to their occasional irrelevance. Instead, we found Gemini Pro Vision's in-the-wild benchmark responses to be closely pertinent. Then we ask model to score outputs in 3 criteria: relevancy, accuracy and naturalness, and sum them all for final score at the end In comparison with mBLIP baselines in Table 2, LaVy outperforms sharply in all types of questions: Conversation (+42.8%), Detail Description (+72.4%) and Complex Reasoning (+50.9%). In overall, our model is scored 67.2 by Gemini Pro. Further, Our model is only marginalized by SeaLLLM in Detail aspect, and scored higher by Gemini Pro (67.2% vs 65.9%). Some qualitative test cases are depicted in Table 3."
    },
    {
      "title": "5 Limitations",
      "text": "Our model still have several limitations: * Although LaVy exhibits deep understanding in Vietnamese visual language tasks but still faces many challenges, for example: TextQA, due to lack of high-quality annotated data for these tasks. * Moreover, like other MLLMs, our model still suffers from hallucination where it generates irrelevant information, redundant details or misinformation."
    },
    {
      "title": "6 Conclusion",
      "text": "In this paper, we have introduced LaVy, a pioneering state-of-the-art Vietnamese Multimodal Large Language Model (MLLM) that aims to address the lack of high-quality resources in multimodality for the Vietnamese language. LaVy represents a significant step forward in the development of Vietnamese MLLMs, enabling complex reasoning and linguistic comprehension in tasks that involve both visual and textual information. Furthermore, we have presented LaVy-Bench, a comprehensive benchmark designed specifically for evaluating the performance of MLLMs on Vietnamese visual language tasks. This benchmark provides a standardized platform for assessing the capabilities of Vietnamese MLLMs, facilitating the comparison and advancement of these models. Our model also have proved SOTA perfomance in comparison with mBLIP baselines in test sets of benchmark. As future work, we plan to expand the capabilities of LaVy by incorporate diverse instructions to entirely handle challenging tasks like Vietnamese OCR, Object Counting. We hope our work will contribute to the advancement of Vietnamese MLLMs' development. \\begin{table} \\begin{tabular}{|l|c|} \\hline **Model** & **Accuracy** \\\\ \\hline **mBLIP (mT0-XL-5B)** & 20.0 \\\\ **mBLIP (BLOOMZ-7B)** & 27.9 \\\\ **LaVy** & 45.3 \\\\ **Gemini Pro-20240526** & **66.8** \\\\ \\hline \\end{tabular} \\end{table} Table 1: Zero-shot VQA on OpenViVQA dev set. Models’ output accuracy are evaluated by Gemini Pro \\begin{table} \\begin{tabular}{l|c c c|c} \\hline **Model** & **Conversation** & **Detail description** & **Complex reasoning** & **All** \\\\ \\hline **mBLIP (mT0-XL-5B)** & 48.5 & 30.8 & 28.9 & 36.1 \\\\ **mBLIP (BLOOMZ-7B)** & 54.2 & 30.8 & 47.1 & 44.0 \\\\ **LaVy** & **77.4** & 53.1 & **71.1** & **67.2** \\\\ **SeaLMMM-20240526** & 72.3 & **55.3** & 70.1 & 65.9 \\\\ \\hline \\end{tabular} \\end{table} Table 2: Performance on in-the-wild benchmark [MISSING_PAGE_FAIL:5]"
    },
    {
      "title": "References",
      "text": "* Achiam et al. (2023) Achiam, Josh, Adler, Steven, Agarwal, Sandhini, Ahmad, Lama, Akkaya, Ilge, Aleman, Florencia Leoni, Almeida, Diogo, Altenschmidt, Janko, Altman, Sam, Anadkat, Shyamal, et al. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_. * Alayrac et al. (2022) Alayrac, Jean-Baptiste, Donahue, Jeff, Luc, Pauline, Miech, Antoine, Barr, Iain, Hasson, Yana, Lenc, Karel, Mensch, Arthur, Millican, Katherine, Reynolds, Malcolm, et al. 2022. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736. * Alwajih et al. (2024) Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, and Muhammad Abdul-Mageed. 2024. Peacock: A family of arabic multimodal large language models and benchmarks. _arXiv preprint arXiv:2403.01031_. * Anil et al. (2023) Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_. * Geigle et al. (2023) Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavas. 2023. mblip: Efficient bootstrapping of multilingual vision-llms. _arXiv preprint arXiv:2307.06930_. * He et al. (2024) Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. 2024. Efficient multimodal learning from data-centric perspective. _arXiv preprint arXiv:2402.11530_. * Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_. * Li et al. (2023) Li, Junnan, Li, Dongxu, Savarese, Silvio, Hoi, and Steven. 2023. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 19730-19742. PMLR. * Liu et al. (2023) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In _Thirty-seventh Conference on Neural Information Processing Systems_. * Mesnard et al. (2020) Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryy Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Sianak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gahrahmani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_. * towards a state-of-the-art large language model for vietnamese. * Nguyen et al. (2023b) Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Dinh Phung, and Hung Bui. 2023b. Phogpt: Generative pre-training for vietnamese. _arXiv preprint arXiv:2311.02945_. * Nguyen et al. (2022) Nghia Hieu Nguyen, Duong T.D. Vo, Kiet Van Nguyen, and Ngan Luu-Thuy Nguyen. 2023c. Openvivqa: Task and dataset and and and multimodal fusion models for visual question answering in vietnamese. _arXiv preprint arXiv:2305.04183_. * Nguyen et al. (2022) Thien Hai Nguyen, Tuan-Duy H. Nguyen, Duy Phung, Duy Tran-Cong Nguyen, Hieu Minh Tran, Manh Luong, Tin Duy Vo, Hung Hai Bui, Dinh Phung, and Dat Quoc Nguyen. 2022. A Vietnamese-English Neural Machine Translation System. In _Proceedings of the 23rd Annual Conference of the International Speech Communication Association: Show and Tell (INTERSPEECH)_. * Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward,, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the40th Annual Meeting of the Associations for Computational Linguistics (ACL)_. * Radford et al. (2023) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2023. Learning transferable visual models from natural language supervision. _arXiv preprint arXiv:2103.00020_. * Touvron et al. (2023a) Touvron, Hugo, Lavril, Thibaut, Izacard, Gautier, Martinet, Xavier, Lachaux, Marie-Anne, Lacroix, Timothee, Roziere, Baptiste, Goyal, Naman, Hambro, Eric, Azhar, Faisal, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_. * Touvron et al. (2023b) Touvron, Hugo, Martin, Louis, Stone, Kevin, Albert, Peter, Almahairi, Amjad, Babaei, Yasmine, Bashlykov, Nikolay, Batra, Soumya, Bhargava, Prajwal, Bhosale, Shruti, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_. * Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems 30 (NIPS 2017)_. * Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llvm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_. * Zhu et al. (2024) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In _The Twelfth International Conference on Learning Representations_."
    }
  ]
}