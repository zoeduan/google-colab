{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"whEoXEceRrwq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746552493045,"user_tz":240,"elapsed":24492,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"outputId":"66b3d5b2-1991-4c65-b615-49beb4c754a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"iEIfG42qzYRa"},"source":["\n","# Prompt Engineering for Literature Review\n","  1. What is prompt engineering?   \n","    - Framework   \n","\n","    - Useful prompts for literature study  \n","      - Summarization (Extract key findings)  \n","      - Context-Aware Querying   \n","      - Named Entity Recognition\n","      \n","\n","\n","  # Prompt Engineering for Literature Review\n","\n","1. What is prompt engineering?  \n","   - Definition and core principles  \n","   - Framework  \n","\n","\n","2. Effective prompt design  \n","   - General principles (clarity, specificity, format)  \n","   - Example prompts for literature study  \n","     - Summarization (extract key findings)  \n","     - Context-aware querying  \n","     - Named entity recognition  \n","   - Iterative prompting and refinement\n","\n","3. Limitations and ethical considerations  \n","   - Risks of hallucination  \n","   - Need for human oversight  \n","\n","\n","4. üìö Further Reading ‚Äì Recommended Resources to Explore After the Workshop\n","\n","     - In-context learning (ICL)  \n","     - Zero-shot inference  \n","     - One-shot inference  \n","     - Few-shot inference  \n","    - Leveraging Research-Friendly Prompts for Literature Analysis with LangChain\n"]},{"cell_type":"markdown","metadata":{"id":"h9X01ztpzYRb"},"source":["## 1. What is prompt engineering?\n","Prompt engineering is the process of designing and optimizing prompts to guide AI models‚Äîparticularly large language models (LLMs)‚Äîtowards generating useful, relevant, and accurate responses. Since LLMs rely heavily on natural language instructions without task-specific training, well-crafted prompts help them understand your intent and deliver more meaningful outputs.\n","\n","Think of it as providing a roadmap for the AI: you steer it toward the kind of result you want by carefully framing the question, supplying background context, or giving examples.\n","\n","Effective prompting can greatly improve model performance‚Äîbut it‚Äôs not magic. The quality of the prompt, the capabilities of the model, and the unpredictable nature of language generation all shape the results.\n","\n","Core principles of good prompting:\n","\n","Be clear and specific\n","Provide necessary context or background\n","Specify the desired output format\n","Iterate and refine based on the model‚Äôs responses\n","\n","\n","\n","Reference: https://cloud.google.com/discover/what-is-prompt-engineering\n"]},{"cell_type":"markdown","metadata":{"id":"vXPzNsxczYRb"},"source":["###Introducing the PROPER Framework\n","To make the most of LLMs, it‚Äôs not enough to simply ask a question‚Äîyou need to structure your prompts carefully. The PROPER framework offers a systematic approach to crafting effective prompts that guide the model toward producing useful, accurate, and well-formatted responses.\n","\n","Each element of the framework addresses a specific aspect of communication with the model, helping you clarify what you want, how you want it delivered, and how to improve it through iteration. This is especially valuable when working in research contexts where precision, transparency, and reproducibility matter.\n","\n","Here‚Äôs a breakdown of the PROPER framework components:\n","\n","####  Proper Framework\n","\n","\n","| Component | Description |\n","|-----------|-------------|\n","| P - Persona | Which role should the model take? |\n","| R - Request | What task should it do? |\n","| O - Operation | In which way / method? |\n","| P - Presentation | In which format / style / tone? |\n","| E - Examples | Give an example or template to follow |\n","| R - Refinement | Give feedback, iterate and improve result |\n","\n","*Source: Peter Gruber. \"Using ChatGPT for Efficient Data Analysis with R\" Workshop.*\n"]},{"cell_type":"markdown","metadata":{"id":"MShj8Y_nzYRb"},"source":["Prompt Example"]},{"cell_type":"markdown","metadata":{"id":"cFsaamnXzYRc"},"source":["1. **P - Persona**: Your are an experienced researcher specializing in health and wellness.\n","2. **R - Request**: Please suggest some keywords related to my research topic which is \"sleep and weight control\".\n","3. **O - Operation(optional)**: Analyze the topic \"sleep and weight control\" and use your extensive database to identify the most relevant and frequently associated topics, terms, and phrases.\n","4. **P - Presentation**: List the result in bullet points.\n","5. **E - Examples (optional)**:\n","    - Keywords related to sleep:\n","        - sleep deprivation\n","        - sleep disorder\n","    - Keywords related to weight control:\n","        - weight gain\n","        - diet\n","6. **R - Refinement (optional)**\n","After the first round of keyword suggestions, I will provide feedback on which terms are most helpful and which are less relevant. You will then refine your suggestions based on this feedback to provide a more targeted list.\n","    - Follow-up prompt:\n","        - Give me 5 more keywords on each list\n","        - Can you list them in table format instead\n"]},{"cell_type":"markdown","source":["# üß™ Hands-on Activity\n","\n","Break into pairs and create a prompt using the PROPER framework for a different research topic of your choice. Then swap and evaluate each other's prompts."],"metadata":{"id":"AxogWGxIJ5Yo"}},{"cell_type":"markdown","source":["### Ask ChatGPT"],"metadata":{"id":"_-ItZvQGF2vR"}},{"cell_type":"markdown","source":["Loading api keys from '/content/drive/MyDrive/Colab_Notebooks/AI/api_keys.txt'"],"metadata":{"id":"VRTd4XHK-uGl"}},{"cell_type":"code","source":["import os\n","\n","api_keys_path = '/content/drive/MyDrive/Colab_Notebooks/AI/api_keys.txt'\n","\n","with open(api_keys_path) as f:\n","    for line in f:\n","        key, value = line.strip().split('=',1)\n","        os.environ[key] = value\n","        #print(key, value)"],"metadata":{"id":"_YS0xzd7SL3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import openai\n","\n","openai_api_key = os.environ['OPENAI_API_KEY']\n"],"metadata":{"id":"j97RmAYNSUL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you prefer to use Google Colab Secrets instead of using the api_keys.txt file, please uncomment the following code."],"metadata":{"id":"DyDnBesT-xB_"}},{"cell_type":"code","source":["# from google.colab import userdata\n","# import os\n","# import openai\n","\n","# # Securely access the API key from Colab's Secrets\n","# try:\n","#     openai_api_key = userdata.get('OPENAI_API_KEY')\n","# except Exception as e:\n","#     print(\"Error: Please add your OpenAI API key to Colab Secrets\")\n","#     print(\"Steps: 1. Click the 'key' icon in the left panel\")\n","#     print(\"       2. Add a secret named OPENAI_API_KEY with your API key\")\n","#     raise e\n","\n","# # Optional: Set as environment variable for libraries that expect it\n","# os.environ['OPENAI_API_KEY'] = openai_api_key"],"metadata":{"id":"DB9V8JtsR6W5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper Function: `ask_chatgpt_full_example()`\n","\n","The cell below defines a ready-to-edit wrapper around **`openai.ChatCompletion.create()`**.  \n","It shows every message-level feature you might need when building research workflows in Colab:\n","\n","* **System / user / assistant / tool** roles  \n","* Optional `name` tags for multi-user chats  \n","* Built-in **function/tool calls** (`tool_calls`, `tool_call_id`)  \n","* All common sampling and safety knobs (`temperature`, `top_p`, penalties, etc.)\n","\n","Use it as a template:\n","\n","1. **Replace** the example `messages` list with your own dialogue.  \n","2. **Remove** any parameters you don‚Äôt need (they fall back to the OpenAI defaults).  \n","3. **Run** the function with `ask_chatgpt_full_example(\"Your question here\")` or point it to a different `model=`.\n","\n","> **Why include all parameters explicitly?**  \n","> Seeing them in one place makes it easier to teach (or remember) what can be tuned‚Äîand lets students experiment by simply editing values in the notebook.\n","\n","Feel free to trim the function down once you know which settings matter for your project.\n"],"metadata":{"id":"9NY_EhJg_TTM"}},{"cell_type":"markdown","source":["def ask_chatgpt_full_example(\n","    user_question: str,\n","    model: str = \"gpt-4o-mini\",\n","):\n","    \"\"\"\n","    Demonstrates *all* message-level options supported by the Chat Completions API\n","    (roles, name, tool calls, function calls, etc.).\n","    Replace or remove the parts you don‚Äôt need.\n","    \"\"\"\n","\n","    messages = [\n","        # 1) SYSTEM  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        {\n","            \"role\": \"system\",\n","            \"content\": (\n","                \"You are a meticulous academic assistant who follows the \"\n","                \"PROPER prompting framework and cites sources in APA style.\"\n","            )\n","        },\n","\n","        # 2) USER  (with optional `name`) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        {\n","            \"role\": \"user\",\n","            \"name\": \"professor_smith\",    # `name` is optional; 64-char max; no spaces\n","            \"content\": user_question\n","        },\n","\n","        # 3) ASSISTANT  (previous reply, if any) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        # Including prior assistant turns helps the model maintain context.\n","        {\n","            \"role\": \"assistant\",\n","            \"content\": \"Certainly‚Äîhere is an initial outline. Let me know what to refine.\"\n","        },\n","\n","        # 4) ASSISTANT ‚Üí TOOL CALL  (function/tool invocation) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        # Instead of free-text content, the assistant can declare a tool call.\n","        {\n","            \"role\": \"assistant\",\n","            \"tool_calls\": [\n","                {\n","                    \"id\": \"call_1\",\n","                    \"type\": \"function\",\n","                    \"function\": {\n","                        \"name\": \"search_pubmed\",\n","                        \"arguments\": \"{\\\"query\\\": \\\"sleep AND weight control\\\"}\"\n","                    }\n","                }\n","            ]\n","            # No \"content\" field here‚ÄîLLM is *requesting* the tool call\n","        },\n","\n","        # 5) TOOL  (returning the result of that call) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        {\n","            \"role\": \"tool\",\n","            \"tool_call_id\": \"call_1\",     # must match the id above\n","            \"content\": (\n","                \"PMID: 12345678 | Title: Association between Sleep Duration and \"\n","                \"Weight Regulation: A Meta-analysis (2023)\\n\"\n","                \"PMID: 87654321 | Title: Intermittent Sleep Loss and Obesity Risk (2024)\"\n","            )\n","        },\n","\n","        # 6) USER  (follow-up) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"Please summarise those two studies in a 100-word paragraph.\"\n","        }\n","    ]\n","\n","    chat_completion = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","\n","        # -------- Core sampling / control parameters --------\n","        temperature=0.7,\n","        top_p=1,\n","        max_tokens=800,\n","        stop=None,\n","        n=1,\n","        stream=False,\n","\n","        # -------- Penalties --------\n","        presence_penalty=0.0,\n","        frequency_penalty=0.0,\n","        logit_bias=None,\n","\n","        # -------- Metadata --------\n","        user=\"academic-demo\"\n","    )\n","\n","    return chat_completion.choices[0].message.content\n","    "],"metadata":{"id":"NWW_lZi-ATR-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLvQbESqzYRc"},"outputs":[],"source":["client = openai.OpenAI(\n","    api_key=openai_api_key,\n",")\n","\n","def ask_chatgpt(question, temp = 0.7,topp = 1, system_instruction = 'you are a chatbot', model=\"gpt-4\",):\n","    chat_completion = client.chat.completions.create(\n","\n","    messages = [\n","       # {\"role\": \"system\", \"content\": system_instruction},\n","        {\"role\": \"user\",   \"content\": question}\n","        ],\n","        model = model)\n","        # -------- Core sampling / control parameters --------\n","    temperature=temp, #range 0-2 higher temp = wider distribution of tokens\n","    top_p=topp,          #range 0-1 %of cumulative probability distribution\n","    max_tokens=800,\n","    stop=None,\n","    n=1,\n","    stream=False,\n","    return chat_completion.choices[0].message.content"]},{"cell_type":"markdown","source":["# üîÑ Experiment:\n","Let's compare the results of two different prompt approaches for the same query:\n"],"metadata":{"id":"b2EZKf9xKFY3"}},{"cell_type":"code","source":["question = \"explain what the ferimion sign problem\"\n","\n","print(ask_chatgpt(question,temp = 0.7,topp = 1))\n","print(\"**\")\n","print(ask_chatgpt(question,temp = 1.5,topp = 1))\n","print(\"***\")\n","print(ask_chatgpt(question,temp = 1.5,topp = 0.3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DueqbS0tMgT","executionInfo":{"status":"ok","timestamp":1746552561106,"user_tz":240,"elapsed":36857,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"outputId":"1fc186df-b680-4fdb-c061-09f5cd11ffcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The fermion sign problem is a mathematical issue that arises in numerical simulations related to quantum systems, particularly those involving fermions ‚Äî the fundamental particles that make up matter, such as electrons and quarks. \n","\n","In basic terms, the problem is that quantum mechanical systems often produce complex or negative probabilities, in contrast to the real, positive probabilities used in classical statistics. This makes simulations less accurate and can give rise to paradoxical situations, like predictions of probabilities greater than 1, or less than 0. \n","\n","This issue is especially prevalent in quantum Monte Carlo methods, which perform statistical sampling to estimate the energy and other properties of a quantum system. Because of the fermion sign problem, these methods have so far been unable to accurately simulate many systems involving fermions, such as high-temperature superconductors or nuclear matter.\n","\n","While the exact solution to the fermion sign problem remains elusive, several approximate methods have been developed to mitigate it. However, because these methods introduce their own approximations and assumptions, they can't fully resolve the problem and may have limitations in certain contexts.\n","**\n","The fermion sign problem is a computational challenge encountered in quantum mechanics and quantum computing when attempting to simulate systems of fermions, like electrons, neutrons, and protons. It arises due to the quantum mechanical behavior of these particles that are subject to the Pauli exclusion principle - a principle stating that two or more identical fermions cannot occupy the same quantum state within a quantum system.\n","\n","In simple terms, when simulating fermionic systems, probabilities can be both positive and negative due to quantum interference and this can cause substantial issues in computations. If we tried to apply classical statistical methods to these types of problems, they would fail miserably since classical probabilities are always positive.\n","\n","The \"sign\" problem refers to the difficulty in dealing with these positive and negative signs in quantum computations, especially when you have many fermions involved. The negative signs can interfere destructively with the calculations very easily, making it exponentially hard to carry out numerical simulations as the number of particles increases. Hence, it is referred to as \"the sign problem\".\n","\n","This problem is a major obstacle in quantum Monte Carlo calculations, a technique used to understand the behavior of quantum systems, and it is also identified as a critical challenge in developing useful quantum computers. More accurate solutions to the sign problem are an active area of research in quantum physics and quantum computing.\n","***\n","The fermion sign problem is a fundamental issue in physics, specifically in numerical simulations of quantum many-body systems. \n","\n","In quantum mechanics, particles are classified into two categories: fermions and bosons. Fermions follow Fermi-Dirac statistics and include particles such as electrons and protons. A key property of fermions is that they obey the Pauli Exclusion Principle - two fermions cannot occupy the same quantum state simultaneously.\n","\n","In computations, the fermion sign problem occurs when calculating the path integral (a mathematical object used in quantum mechanics that sums over all possible histories of a system), especially for systems involving fermions at finite density or in real-time evolution.\n","\n","The problem arises when the integral receives contributions from both positive and negative terms, leading to a cancellation or significant reduction in the numerical value. This makes the estimates to become notoriously instable with large statistical uncertainties, thereby causing significant difficulties in numerical simulations. \n","\n","In other words, stochastic approaches based on importance sampling break down due to the fact that quantum mechanical observables are not always positive.\n","\n","This problem is a significant obstacle in many areas of theoretical physics, including quantum chromodynamics, condensed-matter physics, nuclear physics, and quantum chemistry. Despite ongoing research, a general solution to the fermion sign problem has yet to be found.\n"]}]},{"cell_type":"markdown","source":["#### No framework"],"metadata":{"id":"jGIxgpNEhqLG"}},{"cell_type":"code","source":["question = \"Can you give me the keywords for research topic 'sleep and weight control'\"\n","\n","print(ask_chatgpt(question,temp = 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AurTfXDshw-7","executionInfo":{"status":"ok","timestamp":1746509513792,"user_tz":240,"elapsed":4458,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"outputId":"4225704c-0469-45d0-9dbd-b7b6bc56f055"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. Sleep\n","2. Weight Control\n","3. Sleep Deprivation\n","4. Obesity\n","5. Sleep Patterns\n","6. Weight Management\n","7. Healthy Sleep Habits\n","8. Sleep Disorders\n","9. Insomnia\n","10. Metabolism and Sleep\n","11. Body Mass Index (BMI)\n","12. Restfulness\n","13. Physical Activity\n","14. Sleep Cycle\n","15. Caloric Intake\n","16. Sleep Duration\n","17. Sleep Quality\n","18. Body Weight Regulation\n","19. Sleep and Nutrition\n","20. Rapid Eye Movement Sleep\n","21. Non-rapid Eye Movement Sleep\n","22. Appetite Regulation\n","23. Sleep and Diet\n","24. Sleep and Exercise\n","25. Chronic Sleep Deprivation.\n"]}]},{"cell_type":"markdown","source":["#### With Framework"],"metadata":{"id":"i3dY1yfdijjE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmsGKOMtzYRc","executionInfo":{"status":"ok","timestamp":1746509523675,"user_tz":240,"elapsed":9872,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"outputId":"15c13637-0ef3-44de-d025-99573b5cc758"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sure, here are the relevant keywords:\n","\n","Keywords related to sleep diagnosis and anatomy: \n","- Sleep Deprivation\n","- Sleep Disorder\n","- Insomnia\n","- REM Sleep\n","- Deep Sleep\n","- Circadian Rhythm\n","- Sleep Cycle\n","- Sleep Apnea\n","- Sleep Quality\n","\n","Keywords related to diet and weight control:\n","- Weight Gain Diet\n","- Weight Loss\n","- Nutrition \n","- Caloric Intake\n","- Obesity\n","- Metabolism\n","- Body Mass Index \n","- Healthy Diet\n","- Physical Activity\n","- Portion Control\n","- Meal Planning \n","\n","Related Generated Keywords:\n","- Sleep and Metabolism\n","- Sleep and Obesity\n","- Sleep Deprivation and Weight Gain\n","- Sleep Duration and Weight Control\n","- Insomnia and Weight Loss \n","- Sleep Quality and Nutrition \n","- REM Sleep and Caloric Intake\n","- Circadian Rhythm and Diet\n","- Sleep Problems and Overeating\n","- Sleep and Exercise\n","\n","The following are potential key phrases or topics associated with 'Sleep and Weight Control':\n","- The impact of poor sleep on weight control\n","- Exploring the relationship between sleep duration and obesity\n","- The role of sleep quality in managing weight \n","- How sleep can affect your metabolism \n","- Understanding the link between poor sleep and weight gain\n","- The importance of sleep in maintaining a healthy weight\n","- The effects of sleep apnea on weight and diet control.\n"]}],"source":["question = \"Your are an experienced research specializing in health and wellness.Please suggest some keywords related to my research topic which is 'sleep and weight control'.Analyze the topic 'sleep and weight control' and use your extensive database to identify the most relevant and frequently associated topics, terms, and phrases.List the result in bullet points.Keywords related to sleep: sleep deprivation, sleep disorder. Keywords related to weight control: weight gain diet, please give me a table\"\n","\n","print(ask_chatgpt(question))"]},{"cell_type":"markdown","source":["# üß† Knowledge Check:\n","Compare the two outputs above. What differences do you notice? Which response is more useful for a research project and why?\n","\n","# üèÜ Challenge Activity:\n","Create your own prompt using the PROPER framework for a topic of your choice. Experiment with different components and see how they affect the output. Share your most successful prompt with the group!"],"metadata":{"id":"8W3q_t8vKSV1"}},{"cell_type":"markdown","source":["# üîç Interactive Exercise:\n","**Try it yourself!** Write a simple prompt and then think about how you might improve it."],"metadata":{"id":"VOKpSaawJp81"}},{"cell_type":"code","source":["# Your Turn: Write and test your own prompt using the PROPER framework\n","your_prompt = \"\"\"\n","# Replace this with your own PROPER framework prompt\n","P - Persona:\n","R - Request:\n","O - Operation:\n","P - Presentation:\n","E - Examples:\n","R - Refinement:\n","\"\"\"\n","\n","# Uncomment and run when ready\n","# your_response = ask_chatgpt(your_prompt)\n","# print(your_response)"],"metadata":{"id":"DLJUYJXcKXKL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prompting for a literature Review\n","\n","So far you have experimented with the **basics** of in-context learning.  \n","In a real literature-review workflow you will usually cycle through at least three repeatable micro-tasks:\n","\n","1. **Summarization** ‚Äì extract the paper‚Äôs key contributions.  \n","2. **Research-Gap Identification** ‚Äì spot what the study leaves unsolved.  \n","3. **Named-Entity Recognition (NER)** ‚Äì pull structured facts (people, methods, datasets, etc.).\n","\n","Below you will find **ready-made PROPER prompt templates** for each task, followed by an exercise section where we will use the paper All You Need Is Attention to test our skills.\n","\n","Feel free to tweak the *Persona*, *Request*, or *Presentation* rows to match your taste.\n"],"metadata":{"id":"7673ogDOEVNs"}},{"cell_type":"markdown","metadata":{"id":"KCW0cgbszYRd"},"source":["#### Summarization (Extract key findings)\n","\n","| Category                | Instructions                                                                                                      |\n","|-------------------------|-------------------------------------------------------------------------------------------------------------------|\n","| **Persona**             | Act as an expert academic researcher skilled in synthesizing literature. Your goal is to distill papers into concise, structured summaries highlighting core contributions. |\n","| **Request**             | Extract key findings from a paper. Prioritize clarity, brevity, and depth for usability in literature reviews or meta-analyses. |\n","| **Operation**           | **1. Skim strategically:** Abstract, introduction, conclusion, and headings first.<br><br> **2. Core sections:** Focus on results, discussion, methodology.<br><br> **3. Extract:**<br>- Research question/hypothesis<br>- Methodology (design, data sources)<br>- Key results/insights<br>- How findings address gaps<br>- Limitations and future directions.<br><br> **4. Filter:** Remove redundant or overly technical details. |\n","| **Presentation**        | - **Bullet points** with bold headings (e.g., **Research Goal**, **Key Results**).<br>- Plain language, avoid jargon.<br>- 1‚Äì2 sentences of context if needed to clarify significance. |\n","| **Example**             | **Paper Title:** \"The Impact of Urban Green Spaces on Mental Health: A Longitudinal Study\"<br>- **Research Goal:** Assess if urban green spaces reduce anxiety/depression over 5 years.<br>- **Methodology:** Longitudinal cohort study (N=2,000) with GIS mapping in 10 cities.<br>- **Key Results:** 23% depression reduction in green space users; strongest effects in low-income areas.<br>- **Contribution:** Supports equitable green space policies.<br>- **Limitations:** Self-reported data; no control for socioeconomic changes. |\n"]},{"cell_type":"markdown","source":["#### Research Gap Identification (Recognize the Research Gap)\n","\n","\n","| Category                | Instructions                                                                                                      |\n","|-------------------------|-------------------------------------------------------------------------------------------------------------------|\n","| **Persona**             | Act as a seasoned researcher with expertise in critical analysis of academic literature across disciplines. Your goal is to pinpoint gaps, contradictions, or understudied areas in a paper or body of work. |\n","| **Request**             | Identify and articulate research gaps by analyzing the paper‚Äôs limitations, methodological flaws, unanswered questions, or overlooked variables. |\n","| **Operation**           | **1. Review scope:** Compare the paper‚Äôs claims with its methodology and results.<br><br> **2. Analyze literature:** Highlight contradictions, unresolved debates, or missing evidence.<br><br> **3. Identify gaps:**<br>- Theoretical gaps (unaddressed concepts)<br>- Methodological gaps (flaws/limitations in design)<br>- Empirical gaps (untested variables or populations)<br>- Contextual gaps (lack of cross-disciplinary or cultural perspectives). |\n","| **Presentation**        | - Bullet points with bold headings (e.g., **Theoretical Gap**, **Methodological Limitation**).<br>- Use phrases like \"Lacks exploration of...\" or \"Fails to account for...\"<br>- Prioritize gaps with high real-world impact. |\n","| **Example**             | **Paper Title:** \"AI-Driven Renewable Energy Optimization in Urban Areas\"<br><br>- **Theoretical Gap:** Overlooks socioeconomic disparities in energy access (e.g., low-income vs. high-income neighborhoods).<br>- **Methodological Gap:** Relies on simulated data without field validation.<br>- **Empirical Gap:** Excludes rural or semi-urban contexts, limiting generalizability. |\n"],"metadata":{"id":"ubVXkW5tg2u2"}},{"cell_type":"markdown","metadata":{"id":"ftRK7E6qzYRe"},"source":["#### Named Entity Recognition\n","\n","| Category                | Instructions                                                                                                      |\n","|-------------------------|-------------------------------------------------------------------------------------------------------------------|\n","| **Persona**             | Act as a computational linguist or NLP specialist with expertise in entity recognition across domains. Your goal is to accurately identify and classify named entities in unstructured text. |\n","| **Request**             | Extract and categorize named entities (e.g., persons, organizations, locations) from the provided text. Ensure consistency in labeling and resolve ambiguities contextually. |\n","| **Operation**           | **1. Preprocess text:** Tokenize, normalize casing, and handle punctuation.<br><br> **2. Contextual analysis:**<br>- Identify entity boundaries and semantic context.<br>- Disambiguate entities (e.g., \"Apple\" as company vs. fruit).<br><br> **3. Classify entities:** Use predefined labels (e.g., 'PER', 'ORG', 'LOC', 'DATE', 'GPE').<br><br> **4. Validate:** Cross-check for overlapping/multi-label entities. |\n","| **Presentation**        | - **Structured list:** Entity type, text span, position (start/end indices).<br>- Format: '[ENTITY]: \"text_span\" (Type: LABEL, Position: X-Y)'.<br>- Avoid markdown; use plain text for interoperability.<br>- Flag low-confidence annotations with '[UNCERTAIN]'. |\n","| **Example**             | **Input Text:** \"Apple Inc. plans to open a new campus in Toronto by 2026, said CEO Tim Cook.\"<br><br>**Output:**<br>- 'ORG': \"Apple Inc.\" (Position: 0-10)<br>- 'GPE': \"Toronto\" (Position: 38-45)<br>- 'DATE': \"2026\" (Position: 49-53)<br>- 'PER': \"Tim Cook\" (Position: 60-68) |\n","\n","<!-- - Persona:\n","    - Act as an NLP specialist with expertise in extracting structured information from unstructured academic texts. Your goal is to identify and categorize key entities (e.g., methods, chemicals, theories) in research papers with high precision.\n","\n","-  Request:\n","    - Analyze a given text and extract named entities (e.g., genes, institutions, methodologies) relevant to the paper‚Äôs domain. Prioritize accuracy, consistency, and contextual relevance.\n","\n","- Operation:\n","\n","    - Preprocess text: Remove noise (e.g., citations, URLs) and segment sentences.\n","\n","    - Identify entity boundaries:\n","\n","        - Entity types:\n","\n","            - Domain-specific: Methods (e.g., CRISPR-Cas9), chemicals, theories, instruments.\n","\n","            - General: Authors, institutions, datasets, funding sources.\n","\n","        - Flag ambiguous terms (e.g., acronyms like ‚ÄúAI‚Äù vs. ‚ÄúArtificial Intelligence‚Äù).\n","\n","    - Classify entities: Use context to resolve ambiguity (e.g., ‚ÄúPython‚Äù as a programming language vs. snake).\n","\n","    - Cross-reference: Link entities to external databases (e.g., UniProt for proteins, MeSH for medical terms).\n","\n","    - Validate: Ensure entities align with the paper‚Äôs focus (e.g., exclude irrelevant terms in a biology paper).\n","\n","- Presentation:\n","\n","    - Format as a list with bold entity types and italicized examples:\n","\n","        - Method: RNA-seq analysis\n","\n","        - Chemical: Dopamine\n","\n","        - Institution: MIT\n","\n","    - For acronyms, include expansions in parentheses (e.g., CNN (Convolutional Neural Network)).\n","\n","    - Group related entities (e.g., CRISPR-Cas9 and gene editing under Method).\n","\n","- Example:\n","- Text: ‚ÄúThe study used fMRI to analyze dopamine levels in Parkinson‚Äôs patients, funded by NIH Grant R01-12345.‚Äù\n","\n","    - Method: fMRI (functional Magnetic Resonance Imaging)\n","\n","    - Chemical: Dopamine\n","\n","    - Disease: Parkinson‚Äôs\n","\n","    - Institution: NIH (National Institutes of Health)\n","\n","    - Funding ID: R01-12345 -->\n","\n"]},{"cell_type":"code","source":["#We need to connect to the json file Attention is All you Need.\n","import json\n","from google.colab import drive\n","\n","file_path = '/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/paper/Attention_Is_All_You_Need.json'\n","\n","with open(file_path, 'r') as file:\n","    paper = json.load(file)\n","\n"],"metadata":{"id":"2EY1CwEz9aCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_prompt = f\"\"\"\n","You are a seasoned researcher specialising in the critical analysis of academic work.\n","\n","Summarize the following paper in one paragraph.\n","\n","### Paper JSON\n","{paper['sections']}\n","\"\"\"\n","\n","print(ask_chatgpt(summary_prompt))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"n1e2twEDJPEK","executionInfo":{"status":"error","timestamp":1746554023300,"user_tz":240,"elapsed":620,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"outputId":"0e36fd7a-5552-489f-d78f-6d69f0ab32a9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9394 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-90c7e27eadcb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mask_chatgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-19167dce8fe9>\u001b[0m in \u001b[0;36mask_chatgpt\u001b[0;34m(question, temp, topp, system_instruction, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_chatgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_instruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'you are a chatbot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     messages = [\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    924\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    926\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9394 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"]}]},{"cell_type":"code","source":["full_body = \"\".join(sec[\"text\"] for sec in paper[\"sections\"] if sec.get(\"text\"))\n","\n","print(f\"Total characters: {len(full_body):,}\")\n","print(full_body[:1000], \"...\")"],"metadata":{"id":"7QSBe-LeMDsM","executionInfo":{"status":"ok","timestamp":1746553953600,"user_tz":240,"elapsed":6,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9ed2cdb-e67c-47bc-eced-2b66b8c34e98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total characters: 34,999\n","Ashish Vaswani Google Brain avaswani@google.com &Noam Shazeer Google Brain noam@google.com &Niki Parmar Google Research nikip@google.com &Jakob Uszkoreit Google Research usz@google.com &Lilion Jones Google Research llion@google.com &Aidan N. Gomez University of Toronto aidan@cs.toronto.edu &Lukasz Kaiser Google Brain lukaszkaiser@google.com &Illia Polosukhin illia.polosukhin@gmail.com Work performed while at Google Brain.Work performed while at Google Research.The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring sign ...\n"]}]},{"cell_type":"code","source":["summary_prompt = f\"\"\"\n","You are a seasoned researcher specialising in the critical analysis of academic work.\n","\n","Summarize the following paper in one paragraph.\n","\n","### Paper JSON\n","{full_body[:5000]}\n","\"\"\"\n","\n","print(ask_chatgpt(summary_prompt))"],"metadata":{"id":"4VPJcah5MPFe","executionInfo":{"status":"ok","timestamp":1746554128344,"user_tz":240,"elapsed":12054,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"37427616-319b-4a32-a3fd-5a10f1c06985"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The authors introduce a new network architecture known as the Transformer, which is based exclusively on attention mechanisms, thereby eliminating the need for recurrent or convolutional neural networks. The Transformer excelled in machine translation tasks, showing remarkable improvement in quality, parallelization, and training time. The model achieved a 28.4 BLEU score in the WMT 2014 English-to-German translation task and a record-breaking single-model BLEU score of 41.8 for the WMT 2014 English-to-French translation task after 3.5 days of training. Aside from its impressive efficiency, the Transformer was successfully applied to English constituency parsing tasks, showing that it can generalize well to other applications. This novel architecture addresses the fundamental issue of sequential computation and offers potential for advanced language modeling.\n"]}]},{"cell_type":"code","source":["gap_prompt = f\"\"\"\n","You are a seasoned researcher specialising in critical analysis of academic work.\n","\n","Identify **research gaps** in the following study.\n","Classify them as **Theoretical**, **Methodological**, **Empirical**, or **Contextual**.\n","\n","### Paper JSON\n","{paper['abstract']}\n","\"\"\"\n","\n","print(ask_chatgpt(gap_prompt))"],"metadata":{"id":"2REdzr-3HRSK","executionInfo":{"status":"ok","timestamp":1746552647174,"user_tz":240,"elapsed":20933,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd9c9b3a-aa4b-44d3-dec6-f75342ecd703"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. **Theoretical Gap:** The study did not provide an elaborate discussion on the underlying theories that could explain the superior performance of the Transformer model over recurrent or convolutional neural networks. A theoretical framework that elucidates the mechanics of the new architecture may lend credibility and establish a basis for further studies.\n","\n","2. **Methodological Gap:** The paper does not describe in detail about the testing settings, parameters and conditions during the evaluation of the transformer model performance in language translation tasks.\n","\n","3. **Empirical Gap:** The authors have not compared the Transformer model with other attention-based models. A comparative analysis with other models would have added depth to the research.\n","\n","4. **Contextual Gap:** The study was conducted by only assessing English to German and English to French translation tasks. Although it claims to generalize well to other tasks, it does not contain empirical proof to support this claim. It lacks the exploration of the application of the model in a broader context. \n","\n","5. **Methodological Gap:** The authors have not clarified if alternative models or architectures were also evaluated during their initial phase of research, and why the Transformer was chosen over others.\n","\n","6. **Theoretical Gap:** The paper did not thoroughly explain why recurrent and convolutional neural networks were dismissed. It is important to establish why these widely used technologies were considered nonviable in this context. \n","\n","7. **Contextual Gap:** The paper does not elaborate on the influence of different training parameters like the length of the training period or the type of GPU used on model performance. Understanding these could be essential for successful implementation in different computational environments.\n"]}]},{"cell_type":"code","source":["ner_prompt = f\"\"\"\n","You are a computational linguist specialising in named-entity recognition.\n","\n","Extract all PERSON, ORGANISATION, LOCATION, DATE, and GPE entities from the text below.\n","Return them as:  '[ENTITY]: \"span\" (Type: LABEL, Position: X‚ÄìY)'\n","\n","### Paper JSON\n","{full_body[:5000]}\n","\"\"\"\n","\n","\n","\n","\n","print(ask_chatgpt(ner_prompt))"],"metadata":{"id":"ErQn4U6dM9Y_","executionInfo":{"status":"ok","timestamp":1746552687855,"user_tz":240,"elapsed":40682,"user":{"displayName":"Alex","userId":"04489683878034913543"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d44e5f27-2433-46f1-a109-f2ce0e453bdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'[ENTITY]: \"Ashish Vaswani\" (Type: PERSON, Position: 0‚Äì13)'\n","'[ENTITY]: \"Google Brain\" (Type: ORGANISATION, Position: 14‚Äì25)'\n","'[ENTITY]: \"Noam Shazeer\" (Type: PERSON, Position: 39‚Äì51)'\n","'[ENTITY]: \"Google Brain\" (Type: ORGANISATION, Position: 52‚Äì63)'\n","'[ENTITY]: \"Niki Parmar\" (Type: PERSON, Position: 78‚Äì89)'\n","'[ENTITY]: \"Google Research\" (Type: ORGANISATION, Position: 90‚Äì104)'\n","'[ENTITY]: \"Jakob Uszkoreit\" (Type: PERSON, Position: 119‚Äì134)'\n","'[ENTITY]: \"Google Research\" (Type: ORGANISATION, Position: 135‚Äì149)'\n","'[ENTITY]: \"Lilion Jones\" (Type: PERSON, Position: 164‚Äì176)'\n","'[ENTITY]: \"Google Research\" (Type: ORGANISATION, Position: 177‚Äì191)'\n","'[ENTITY]: \"Aidan N. Gomez\" (Type: PERSON, Position: 196‚Äì209)'\n","'[ENTITY]: \"University of Toronto\" (Type: ORGANISATION, Position: 210‚Äì230)'\n","'[ENTITY]: \"Lukasz Kaiser\" (Type: PERSON, Position: 244‚Äì257)'\n","'[ENTITY]: \"Google Brain\" (Type: ORGANISATION, Position: 258‚Äì269)'\n","'[ENTITY]: \"Illia Polosukhin\" (Type: PERSON, Position: 283‚Äì298)'\n","'[ENTITY]: \"Google Brain\" (Type: ORGANISATION, Position: 355‚Äì366)'\n","'[ENTITY]: \"Google Research\" (Type: ORGANISATION, Position: 409‚Äì423)'\n","'[ENTITY]: \"WMT 2014 English-to-German\" (Type: EVENT, Position: 1105‚Äì1132)'\n","'[ENTITY]: \"WMT 2014 English-to-French\" (Type: EVENT, Position: 1359‚Äì1386)'\n","'[ENTITY]: \"3.5 days\" (Type: DATE, Position: 1425‚Äì1433)'\n","'[ENTITY]: \"eight GPUs\" (Type: ORGANISATION, Position: 1438‚Äì1447)'\n","'[ENTITY]: \"Transformer\" (Type: PRODUCT, Position: 1917‚Äì1928)'\n","'[ENTITY]: \"as little as twelve hours\" (Type: DATE, Position: 2058‚Äì2083)'\n","'[ENTITY]: \"eight P100 GPUs\" (Type: ORGANISATION, Position: 2087‚Äì2101)'\n","'[ENTITY]: \"Extended Neural GPU [16], ByteNet [18] and ConvS2S [9]\" (Type: PRODUCT, Position: 2124‚Äì2169)'\n","'[ENTITY]: \"Transformer\" (Type: PRODUCT, Position: 2390‚Äì2401)'\n","'[ENTITY]: \"Multi-Head Attention\" (Type: PRODUCT, Position: 2738‚Äì2757)'\n","'[ENTITY]: \"section 3.2\" (Type: LAW, Position: 2775‚Äì2786)'\n","'[ENTITY]: \"Self-attention\" (Type: PRODUCT, Position: 2800‚Äì2813)'\n"]}]},{"cell_type":"markdown","metadata":{"id":"TG6PmJy7zYRc"},"source":["\n","### In-context learning(ICL)\n","\n","### Introduction to In-Context Learning (ICL)\n","\n","One of the most powerful abilities of large language models (LLMs) is **in-context learning (ICL)** ‚Äî the ability to learn patterns, tasks, or formats directly from the examples you include in the prompt, without needing to retrain the model.\n","\n","In simple terms, you can ‚Äúteach‚Äù the model *what you want* just by showing it **examples** or providing clear instructions inside the prompt. The model picks up on the pattern and tries to continue it when answering your new query.\n","\n","There are three main types of in-context learning:\n","\n","- **Zero-shot** ‚Üí You give **only instructions**, no examples.  \n","  *Example:* ‚ÄúSummarize this research article in one paragraph.‚Äù\n","\n","- **One-shot** ‚Üí You give **one example** + instructions.  \n","  *Example:* Show one example of a summarized paper, then provide a new paper to summarize.\n","\n","- **Few-shot** ‚Üí You give **multiple examples** + instructions.  \n","  *Example:* Provide 3‚Äì5 summarized papers, then ask the model to summarize a new one in the same style.\n","\n","Why it matters for research:\n","- Helps you customize outputs to your specific field or style.\n","- Reduces errors or off-topic answers.\n","- Improves reproducibility, especially when used systematically.\n","\n","In this notebook, you‚Äôll get hands-on practice with zero-shot, one-shot, and few-shot prompting to see how they work and when to use them.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qFca4Q1gzYRd"},"source":["### Zero shot inference"]},{"cell_type":"code","source":["from IPython.display import Image, display\n","display(Image( \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/image/zero_shot_correct.png\", width=600))"],"metadata":{"id":"maVJMog3zy_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Omu5imtHzYRd"},"source":["With GPT2 or other small models, the completion is incorrect with zero shot inference"]},{"cell_type":"code","source":["from IPython.display import Image, display\n","display(Image( \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/image/zero_shot_incorrect.png\", width=600))"],"metadata":{"id":"CsWR9E_Bz5-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsImt4iIzYRd"},"source":["### One shot inference"]},{"cell_type":"code","source":["from IPython.display import Image, display\n","display(Image( \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/image/one_shot_correct.png\", width=600))"],"metadata":{"id":"S8YHAgtPz-wh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jv6nGiqzYRd"},"source":["### Few shot inference"]},{"cell_type":"code","source":["from IPython.display import Image, display\n","display(Image( \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/image/few_shot_correct.png\", width=600))"],"metadata":{"id":"I_v7YPbw0Doh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TNr-IvOzYRd"},"source":["# ‚ö†Ô∏è Important Note on Context Window\n","When adding five or six examples doesn't improve performance, it may be time to consider more advanced methods. Remember that each example consumes token space in your context window.\n","\n","# üèÜ Challenge Activity: Few-shot Prompt Design\n","Working in pairs, design a few-shot prompt (3-5 examples) for a complex classification task. Consider:\n","- How carefully crafted examples can guide the model\n","- The diversity of your examples\n","- The ordering of your examples\n"]},{"cell_type":"markdown","source":["### Chain of Thought Prompts\n","\n","CoT prompting encourages the model to break down complex reasoning into a series of intermediate steps, leading to a more comprehensive and well-structured final output.\n","\n","For instance; \"Solve this problem step-by-step:\n","John has 5 apples, he eats 2. How many apples does he have left?\n","\n","Step 1: John starts with 5 apples.\n","\n","Step 2: He eats 2 apples, so we need to subtract 2 from 5.\n","\n","Step 3: 5 - 2 = 3. Answer: John has 3 apples left.\""],"metadata":{"id":"hI_kTjrxG-Pm"}},{"cell_type":"markdown","source":["# üí≠ Guided Exercise: Creating CoT Prompts\n","Create a Chain of Thought prompt for each of these scenarios:\n","1. A multi-step math word problem\n","2. Analyzing the logical reasoning in an argument\n","3. Planning a project with multiple dependencies\n"],"metadata":{"id":"apD-DHiKi_vW"}},{"cell_type":"markdown","metadata":{"id":"atQ2bpJ3zYRd"},"source":["### Useful Prompts for Literature Study"]},{"cell_type":"markdown","source":["Other format of prompt\n","\n","\n","| Component | Description |\n","|-----------|-------------|\n","| C - Character | Which role should the model take? |\n","| R - Request | What task should it do? |\n","| E - Examples | Give an example or template to follow |\n","| A - Adjustments | In which way / method? |\n","| T - Type of output | In which format / style / tone? |\n","| E - Extras | Give feedback, iterate and improve result |\n","\n","*Source: Dave Birss. \"How to Research and Write Using Generative AI Tools\" Online Course.*"],"metadata":{"id":"ANCz77TDgIVA"}},{"cell_type":"markdown","metadata":{"id":"uW334HB41CdV"},"source":["# üìö Further Reading ‚Äì Recommended Resources to Explore After the Workshop\n","\n","## Leveraging Research-Friendly Prompts for Literature Analysis with LangChain"]},{"cell_type":"markdown","metadata":{"id":"89a648a6"},"source":["LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries‚Äîfor example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining. (https://aws.amazon.com/what-is/langchain/)\n","\n","- Chains - Allow you to combine multiple operations into sequences, such as retrieving documents, passing them to an LLM, and processing the output. For example, you could create a chain that retrieves relevant documents from a database, summarizes them using an LLM, and then answers questions based on those summaries.\n","- Document Loaders - Built-in tools for loading various document types (PDFs, web pages, databases, etc.) into your application. This makes it easy to work with external data sources.\n","- Memory - Mechanisms for maintaining conversation history and context across interactions, which is crucial for building chatbots and other conversational applications.\n","- Agents - Components that can use LLMs to decide what actions to take, such as choosing which tools to use or how to break down complex tasks.\n","- Prompts - A system for managing and optimizing prompts, including templates and example selectors to help generate effective prompts dynamically.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["https://medium.com/towards-data-science/4-ways-of-question-answering-in-langchain-188c6707cc5a\n","\n","\n","\n","\n"],"metadata":{"id":"FYeskARo860_"}},{"cell_type":"markdown","metadata":{"id":"3c8c895e"},"source":["<img src=\"https://d2908q01vomqb2.cloudfront.net/887309d048beef83ad3eabf2a79a64a389ab1c9f/2023/07/13/DBBLOG-3334-image001.png\" alt=\"drawing\" width=\"900\"/>\n","Image source:Amazon"]},{"cell_type":"markdown","metadata":{"id":"k1gQpYlF1CdW"},"source":["### LangChain\n","This section mainly refers to the langchain project at coursera: https://www.coursera.org/projects/langchain-chat-with-your-data-project\n"]},{"cell_type":"code","source":["!pip install -U langchain-community langgraph langchain_openai  chromadb langchain_ollama PyPDF2 pypdf &> /dev/null"],"metadata":{"id":"Nq1v20HD7Pla"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"height":132,"tags":[],"id":"83558969-e5aa-4fbc-933c-9c1534decf0a"},"outputs":[],"source":["llm_name = \"gpt-3.5-turbo\" # We use gpt-3.5-turbo because its context window is larger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1650386"},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from PyPDF2 import PdfReader\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import RetrievalQA\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","from langchain.chains import ConversationalRetrievalChain\n"]},{"cell_type":"markdown","metadata":{"id":"27ba4735"},"source":["#### **Extract text from PDF**"]},{"cell_type":"markdown","source":["# üí° Discussion Question:\n","Why might you want to extract text from PDFs rather than use them directly? What challenges do PDFs present for NLP applications?\n","\n","\n","<details>\n","<summary>Click here for answers</summary>\n","\n","Challenges PDFs present for NLP:\n","\n","- Inconsistent text ordering: Text might be stored by visual position, not reading order (especially in multi-column layouts). NLP expects linear text, but PDFs might mix headings, footnotes, sidebars, etc.\n","\n","- No semantic structure: No tags for titles, paragraphs, sections‚Äîjust raw coordinates and fonts. Hard to distinguish between body text, captions, headers, footnotes, etc.\n","\n","- Embedded non-text elements: Tables, charts, and images contain information that isn't in extractable text form. OCR (optical character recognition) might be needed if text is stored as an image.\n","\n","- Fonts and encodings: Some PDFs use custom fonts or character encodings that make text extraction error-prone (e.g., ligatures, non-Unicode characters).\n","\n","- Noise and artifacts: Extracted text may include unwanted line breaks, spacing issues, or repeated content like headers and footers on each page.\n","</details>"],"metadata":{"id":"eLFbD0GfLtoz"}},{"cell_type":"code","execution_count":null,"metadata":{"height":98,"tags":[],"id":"ab80dc86"},"outputs":[],"source":["pdf_path = \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/paper/research_paper.pdf\"\n","reader = PdfReader(pdf_path)\n","raw_text = \"\"\n","for page in reader.pages:\n","    page_text = page.extract_text()\n","    if page_text:\n","        raw_text += page_text\n","\n","if not raw_text.strip():\n","    raise ValueError(\"The PDF content could not be extracted or is empty.\")\n"]},{"cell_type":"markdown","source":["# ‚ú® Interactive Exercise:\n","Check how much text was extracted from your PDF. How many pages and approximately how many words?"],"metadata":{"id":"CpXju0mEL0t_"}},{"cell_type":"code","source":["num_pages = len(reader.pages)\n","approx_word_count = len(raw_text.split())\n","\n","print(f\"Successfully extracted text from {num_pages} pages\")\n","print(f\"Approximate word count: {approx_word_count}\")\n","print(f\"First 150 characters: {raw_text[:150]}...\")"],"metadata":{"id":"PxGhswfGL1n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Text Chunking\n","\n","Breaking text into manageable chunks is essential for working with large documents."],"metadata":{"id":"aBkFMl5FL-EM"}},{"cell_type":"code","source":["# Split text into smaller chunks\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap=200,\n","    length_function=len\n",")\n","texts = text_splitter.split_text(raw_text)"],"metadata":{"id":"Thofi6yysFdt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîç Examine Your Chunks:\n","Let's check how the text was split:"],"metadata":{"id":"EF6UTCYLMLYD"}},{"cell_type":"code","source":["print(f\"Your document was split into {len(texts)} chunks\")\n","print(\"\\nSample chunk (chunk #3):\")\n","if len(texts) > 2:\n","    print(texts[2][:200] + \"...\")\n","else:\n","    print(\"Not enough chunks to display chunk #3\")\n"],"metadata":{"id":"ZFfm9ZolMMRn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Vector Embedding and Storage"],"metadata":{"id":"kDg4xeLVMQL4"}},{"cell_type":"markdown","metadata":{"id":"0ce8f652"},"source":["- Text Embedding: Converts the PDF text into a format that can be stored and queried efficiently using vector similarity.\n","- Persistent Storage: Allows the embeddings and metadata to be saved and reused across sessions, avoiding the need to reprocess the same text repeatedly.\n","- Facilitating Retrieval: Enables downstream tasks like retrieving relevant information or answering questions based on the stored text data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36a279b4"},"outputs":[],"source":["persist_directory = \"/content/drive/MyDrive/Colab_Notebooks/AI/section4/data/PersistedData\"\n","embedding = OpenAIEmbeddings()\n","vectordb = Chroma.from_texts(texts, embedding, persist_directory=persist_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"height":64,"tags":[],"id":"01c58c2b"},"outputs":[],"source":["llm = ChatOpenAI(model_name=llm_name, temperature=0)\n","llm.predict(\"Hello!\")"]},{"cell_type":"code","source":["template = \"\"\"\n","You are a seasoned researcher with expertise in critically analyzing academic literature.\n","\n","{context}\n","\n","Question: {question}\n","\n","\n","If you don't have enough information, just say: \"I don't have enough information to answer that question.\"\n","\"\"\""],"metadata":{"id":"inzlq76Zsq0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"height":351,"tags":[],"id":"b8aa6c11"},"outputs":[],"source":["# Build prompt\n","QA_CHAIN_PROMPT = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d3443b5"},"outputs":[],"source":["qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(search_kwargs={\"k\": 4}),\n","    return_source_documents=True,\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"ulncJSe1Qwyi"}},{"cell_type":"markdown","metadata":{"id":"84f8d2e2"},"source":["#### Using Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"height":98,"tags":[],"id":"7e99e754"},"outputs":[],"source":["from langchain.memory import ConversationBufferMemory\n","\n","memory = ConversationBufferMemory(\n","    memory_key=\"chat_history\",\n","    return_messages=True\n",")"]},{"cell_type":"code","source":[],"metadata":{"id":"zQ4LnWCpSavZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6afd9f2"},"outputs":[],"source":["from langchain.chains import ConversationalRetrievalChain\n","retriever=vectordb.as_retriever()\n","qa = ConversationalRetrievalChain.from_llm(\n","    llm,\n","    retriever = vectordb.as_retriever(search_kwargs={\"k\": 1}),\n","    memory=memory\n",")"]},{"cell_type":"markdown","source":["## Advanced Use Cases"],"metadata":{"id":"b8oS36oYSYfY"}},{"cell_type":"markdown","metadata":{"id":"70vgFUxg1Cda"},"source":["### Summarization (Extract key findings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3b38f655"},"outputs":[],"source":["question_summarization = \"Act as an expert academic researcher skilled in synthesizing complex literature across disciplines. Your task is to generate concise, structured summaries of this pdf by strategically skimming the abstract, introduction, conclusion, and section headings to grasp the paper‚Äôs scope. Focus on core sections like methodology, results, and discussion to extract the primary research question, experimental design (e.g., data sources, tools), major findings (quantitative/qualitative insights), and how these address gaps or controversies in the field. Highlight limitations and future directions proposed by the authors, filtering out redundant or overly technical details. Present the summary as bullet points with bold headings (e.g., Research Goal, Key Results) using plain language and paraphrasing to avoid plagiarism. For example, for a paper titled ‚ÄúThe Impact of Urban Green Spaces on Mental Health: A Longitudinal Study‚Äù, summarize the goal (assessing green space effects on mental health over 5 years), methodology (longitudinal cohort study with GIS mapping), key results (23% depression reduction in users), and limitations (self-reported data bias). Ensure usability for tasks like literature reviews or research planning.\"\n","result = qa({\"question\": question_summarization})\n","result['answer']"]},{"cell_type":"markdown","metadata":{"id":"FZjtq1bj1Cda"},"source":["### Resarch Gap Identification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b46abdd9"},"outputs":[],"source":["question_GAP = \"Act as a seasoned researcher with expertise in critically analyzing academic literature across disciplines. Your task is to systematically identify research gaps of the pdf by reviewing the limitations, methodological flaws, contradictions with existing studies, or overlooked variables in the PDF. Begin by comparing the paper‚Äôs claims with its methodology and results to assess alignment. Next, analyze the broader literature to highlight unresolved debates, missing evidence, or inconsistencies. Focus on four key gap categories: theoretical gaps (e.g., unaddressed concepts or frameworks), methodological gaps (e.g., flawed study design or data limitations), empirical gaps (e.g., untested variables or underrepresented populations), and contextual gaps (e.g., lack of cross-disciplinary or cultural perspectives). Present findings in concise bullet points using bold headings (e.g., Methodological Limitation) and phrases like ‚ÄúFails to account for‚Ä¶‚Äù or ‚ÄúLacks exploration of‚Ä¶‚Äù, prioritizing gaps with real-world relevance. For example, in a paper titled ‚ÄúAI-Driven Renewable Energy Optimization in Urban Areas‚Äù, you might note: a theoretical gap in addressing socioeconomic disparities in energy access, a methodological gap in relying solely on simulated data without field validation, and an empirical gap in excluding rural contexts. please find the reseach gap or limitation\"\n","result = qa({\"question\": question_GAP})\n","result['answer']"]},{"cell_type":"markdown","metadata":{"id":"J63r9-pH1Cda"},"source":["### Named Entity Recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3e5f6e33"},"outputs":[],"source":["question_NER = \"Act as a computational linguist or NLP specialist with expertise in identifying and classifying named entities across diverse text types. Your task is to analyze this pdf to extract entities such as persons (PER), organizations (ORG), locations (LOC), dates (DATE), and geopolitical entities (GPE). Begin by preprocessing the text through tokenization, case normalization, and punctuation handling. Conduct contextual analysis to resolve ambiguities (e.g., distinguishing 'Apple' as a company versus a fruit) and refine entity boundaries. Classify entities using standardized labels and validate results by cross-checking overlapping or multi-label cases. Present findings as a structured list with each entity‚Äôs type, exact text span, and positional indices (e.g., ORG: 'Apple Inc.' [Position: 0-10]). Flag uncertain annotations (e.g., ambiguous entities) with a [UNCERTAIN] tag for transparency. For example, in the sentence 'Apple Inc. plans to open a new campus in Toronto by 2026, said CEO Tim Cook,' output: ORG: 'Apple Inc.', GPE: 'Toronto', DATE: '2026', and PER: 'Tim Cook', with positional indices reflecting their spans in the text. Prioritize precision and interoperability by avoiding markdown and using plain-text formatting. Please find the named entities\"\n","result = qa({\"question\": question_NER})\n","result['answer']\n"]},{"cell_type":"markdown","source":["# üöÄ Your Turn: Create a Research-Friendly Prompt\n","\n","## üèÜ Challenge Activity:\n","Design your own research-oriented prompt that would help with literature review, methodology analysis, or connecting findings across papers. Consider what aspects of academic papers are most time-consuming to analyze manually and how LLMs could help."],"metadata":{"id":"ehwC6Yt5kVAZ"}},{"cell_type":"code","source":["# Your custom research prompt here\n","your_research_prompt = \"\"\"\n","Act as a [YOUR DESIRED EXPERT ROLE] with expertise in [FIELD/METHODOLOGY].\n","\n","Your task is to [DESCRIBE THE ANALYSIS TASK] for this academic paper by [DESCRIBE APPROACH].\n","\n","Focus on [KEY ASPECTS TO ANALYZE] while considering [IMPORTANT FACTORS/CONTEXT].\n","\n","Present your findings as [DESIRED OUTPUT FORMAT] with [SPECIFIC ORGANIZATION/STRUCTURE].\n","\n","For example, [PROVIDE A BRIEF EXAMPLE OF EXPECTED OUTPUT].\n","\"\"\"\n","\n","# Uncomment to run your prompt:\n","# result = qa({\"question\": your_research_prompt})\n","# print(result['answer'])"],"metadata":{"id":"8NL1vOvuSlRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JqklWdeJSmPH"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}